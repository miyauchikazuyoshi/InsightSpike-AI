"""
Layer 1: Stream Processor - SSM-based Episode Generation
=======================================================

Processes input streams into episodes using lightweight models.
Initially uses existing EmbeddingManager, with future SSM support.
"""

import asyncio
import logging
import re
import time
from collections import deque
from dataclasses import dataclass
from typing import Any, AsyncIterator, Dict, List, Optional, Tuple

import numpy as np

try:
    import networkx as nx
    NX_AVAILABLE = True
except ImportError:
    NX_AVAILABLE = False
    nx = None

from ...core.episode import Episode
from ...processing.embedder import EmbeddingManager

logger = logging.getLogger(__name__)


@dataclass
class L1Episode:
    """Episode generated by Layer1 stream processor"""
    embedding: np.ndarray      # shape: (768,), dtype: float16
    text: str                  # Original text
    token_count: int          # Number of tokens
    timestamp: float          # Generation timestamp
    entropy_score: float      # Predictive entropy (0-1)
    boundary_type: str        # 'user_turn', 'forced', 'silence'
    entity_graph: Optional[Any] = None  # nx.DiGraph when NER enabled
    
    def to_episode(self) -> Episode:
        """Convert to standard Episode format"""
        return Episode(
            text=self.text,
            embedding=self.embedding,
            c_value=0.5,  # Initial value
            metadata={
                'token_count': self.token_count,
                'entropy': self.entropy_score,
                'boundary': self.boundary_type,
                'timestamp': self.timestamp
            }
        )


@dataclass
class Layer1Config:
    """Configuration for Layer1 stream processor"""
    # Buffer settings
    buffer_size: int = 4096      # Max tokens to hold
    chunk_size: int = 256        # Processing chunk size
    flush_threshold: int = 512   # Force flush threshold
    
    # Model settings
    embedding_dim: int = 768
    use_fp16: bool = True
    
    # SSM settings (future)
    ssm_model: str = "sentence-transformer"  # or "mamba-tiny"
    
    # NER settings
    enable_ner: bool = False
    max_entities: int = 10
    
    # Performance
    batch_timeout: float = 0.1   # seconds
    async_processing: bool = True


class BoundaryDetector:
    """Detects episode boundaries in text stream"""
    
    def __init__(self, config: Layer1Config):
        self.config = config
        self.patterns = {
            'user_turn': re.compile(r'\n\n|。{2,}|\?\s*$|！\s*$|．{2,}'),
            'sentence_end': re.compile(r'[。！？\.\!\?]\s*$'),
        }
        self.silence_threshold = 2.0  # seconds
        
    def should_flush(self, 
                    tokens: List[str], 
                    elapsed_time: float,
                    buffer_size: int) -> Tuple[bool, str]:
        """
        Determine if buffer should be flushed
        
        Returns:
            Tuple of (should_flush, boundary_type)
        """
        # Force flush on size
        if buffer_size >= self.config.flush_threshold:
            return True, 'forced'
            
        # Check for user turn patterns
        if len(tokens) > 10:
            recent_text = ''.join(tokens[-20:])
            if self.patterns['user_turn'].search(recent_text):
                return True, 'user_turn'
                
        # Time-based flush (for audio/streaming)
        if elapsed_time > self.silence_threshold:
            return True, 'silence'
            
        return False, ''


class Layer1StreamProcessor:
    """
    Stream processor for converting raw input into episodes.
    
    Features:
    - Chunked processing with sliding windows
    - Boundary detection for natural episode splits
    - Async processing pipeline
    - Future SSM model support
    """
    
    def __init__(self, config: Optional[Layer1Config] = None):
        self.config = config or Layer1Config()
        
        # Initialize embedder (will be replaced by SSM in future)
        self.embedder = EmbeddingManager()
        
        # Buffer management
        self.token_buffer = deque(maxlen=self.config.buffer_size)
        self.processing_lock = asyncio.Lock()
        
        # Boundary detection
        self.boundary_detector = BoundaryDetector(self.config)
        
        # Output queue for episodes
        self.output_queue: asyncio.Queue[L1Episode] = asyncio.Queue()
        
        # State tracking
        self.last_flush_time = time.time()
        self.total_tokens_processed = 0
        
        logger.info(f"Layer1 initialized with buffer_size={self.config.buffer_size}")
        
    async def process_stream(self, token_stream: AsyncIterator[str]) -> None:
        """
        Main processing loop for token stream
        
        Args:
            token_stream: Async iterator of tokens
        """
        chunk_buffer = []
        
        async for token in token_stream:
            async with self.processing_lock:
                # Add to buffers
                self.token_buffer.append(token)
                chunk_buffer.append(token)
                
                # Check for flush conditions
                elapsed = time.time() - self.last_flush_time
                should_flush, boundary_type = self.boundary_detector.should_flush(
                    list(self.token_buffer),
                    elapsed,
                    len(self.token_buffer)
                )
                
                if should_flush:
                    # Create and queue episode
                    episode = await self._create_episode(boundary_type)
                    if episode:
                        await self.output_queue.put(episode)
                        
                    # Reset buffers
                    self.token_buffer.clear()
                    chunk_buffer.clear()
                    self.last_flush_time = time.time()
                    
    async def process_text(self, text: str) -> List[L1Episode]:
        """
        Process a complete text into episodes
        
        Args:
            text: Input text
            
        Returns:
            List of generated episodes
        """
        # Split into sentences/paragraphs
        segments = self._split_text(text)
        episodes = []
        
        for segment in segments:
            if segment.strip():
                episode = await self._create_episode_from_text(segment, 'text_split')
                if episode:
                    episodes.append(episode)
                    
        return episodes
        
    def _split_text(self, text: str) -> List[str]:
        """Split text into natural segments"""
        # First try paragraph splits
        paragraphs = text.split('\n\n')
        
        segments = []
        for para in paragraphs:
            if len(para) > self.config.flush_threshold * 4:  # Rough char estimate
                # Split long paragraphs by sentences
                sentences = re.split(r'[。！？\.\!\?]+', para)
                segments.extend(s.strip() for s in sentences if s.strip())
            else:
                segments.append(para.strip())
                
        return segments
        
    async def _create_episode(self, boundary_type: str) -> Optional[L1Episode]:
        """Create episode from current buffer"""
        if not self.token_buffer:
            return None
            
        # Convert tokens to text
        text = ''.join(self.token_buffer).strip()
        if not text:
            return None
            
        return await self._create_episode_from_text(text, boundary_type)
        
    async def _create_episode_from_text(self, 
                                      text: str, 
                                      boundary_type: str) -> Optional[L1Episode]:
        """Create episode from text"""
        try:
            # Generate embedding
            embedding = self.embedder.embed_text(text)
            if self.config.use_fp16:
                embedding = embedding.astype(np.float16)
                
            # Calculate entropy (simple approximation)
            entropy = self._calculate_entropy(embedding)
            
            # Entity extraction (if enabled)
            entity_graph = None
            if self.config.enable_ner and NX_AVAILABLE:
                entity_graph = self._extract_entities(text)
                
            # Update stats
            self.total_tokens_processed += len(text.split())
            
            return L1Episode(
                embedding=embedding,
                text=text,
                token_count=len(text.split()),  # Simple tokenization
                timestamp=time.time(),
                entropy_score=entropy,
                boundary_type=boundary_type,
                entity_graph=entity_graph
            )
            
        except Exception as e:
            logger.error(f"Failed to create episode: {e}")
            return None
            
    def _calculate_entropy(self, embedding: np.ndarray) -> float:
        """
        Calculate entropy score from embedding
        
        Simple approximation based on embedding sparsity
        """
        # Normalize embedding
        norm_embedding = embedding / (np.linalg.norm(embedding) + 1e-9)
        
        # Use absolute values as pseudo-probabilities
        abs_values = np.abs(norm_embedding)
        abs_values = abs_values / (abs_values.sum() + 1e-9)
        
        # Calculate entropy
        entropy = -np.sum(abs_values * np.log(abs_values + 1e-9))
        
        # Normalize to [0, 1]
        max_entropy = np.log(len(embedding))
        normalized = entropy / max_entropy if max_entropy > 0 else 0.0
        
        return float(np.clip(normalized, 0.0, 1.0))
        
    def _extract_entities(self, text: str) -> Optional[Any]:
        """
        Extract entities and create graph (placeholder)
        
        Future: Use spaCy or similar for NER
        """
        if not NX_AVAILABLE:
            return None
            
        # Simple keyword extraction as placeholder
        g = nx.DiGraph()
        
        # Extract capitalized words as entities
        entities = re.findall(r'\b[A-Z][a-z]+\b', text)
        
        for i, entity in enumerate(entities[:self.config.max_entities]):
            g.add_node(entity, type='entity', index=i)
            
        # Add simple co-occurrence edges
        for i in range(len(entities) - 1):
            if i < len(entities) - 1:
                g.add_edge(entities[i], entities[i+1], weight=1.0)
                
        return g if g.number_of_nodes() > 0 else None
        
    async def get_episode(self, timeout: Optional[float] = None) -> Optional[L1Episode]:
        """
        Get next episode from output queue
        
        Args:
            timeout: Max wait time in seconds
            
        Returns:
            Episode or None if timeout
        """
        try:
            if timeout:
                return await asyncio.wait_for(
                    self.output_queue.get(), 
                    timeout=timeout
                )
            else:
                return await self.output_queue.get()
        except asyncio.TimeoutError:
            return None
            
    def get_stats(self) -> Dict[str, Any]:
        """Get processing statistics"""
        return {
            'total_tokens': self.total_tokens_processed,
            'buffer_size': len(self.token_buffer),
            'queue_size': self.output_queue.qsize(),
            'config': {
                'buffer_size': self.config.buffer_size,
                'chunk_size': self.config.chunk_size,
                'flush_threshold': self.config.flush_threshold,
            }
        }


# Integration with L2MemoryManager
class L1IntegratedMemory:
    """Helper class to integrate L1 with existing L2MemoryManager"""
    
    def __init__(self, l2_memory, layer1_config: Optional[Layer1Config] = None):
        self.l2_memory = l2_memory
        self.layer1 = Layer1StreamProcessor(layer1_config)
        self.consumer_task = None
        
    async def start(self):
        """Start the episode consumer"""
        if self.consumer_task is None:
            self.consumer_task = asyncio.create_task(self._consume_episodes())
            
    async def stop(self):
        """Stop the episode consumer"""
        if self.consumer_task:
            self.consumer_task.cancel()
            try:
                await self.consumer_task
            except asyncio.CancelledError:
                pass
            self.consumer_task = None
            
    async def _consume_episodes(self):
        """Consume episodes from Layer1 and add to L2"""
        while True:
            try:
                l1_episode = await self.layer1.get_episode(timeout=1.0)
                if l1_episode:
                    episode = l1_episode.to_episode()
                    self.l2_memory.add_episode(episode)
                    logger.debug(f"Added episode: {episode.text[:50]}...")
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error consuming episode: {e}")
                
    async def process_text(self, text: str) -> List[Episode]:
        """Process text and add to memory"""
        l1_episodes = await self.layer1.process_text(text)
        episodes = []
        
        for l1_ep in l1_episodes:
            episode = l1_ep.to_episode()
            self.l2_memory.add_episode(episode)
            episodes.append(episode)
            
        return episodes