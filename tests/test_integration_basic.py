#!/usr/bin/env python3
"""
åŸºæœ¬çš„ãªçµ±åˆãƒ†ã‚¹ãƒˆ - InsightSpikeã®ä¸»è¦æ©Ÿèƒ½ã‚’ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§ãƒ†ã‚¹ãƒˆ
"""

import json
import sys
import time
from pathlib import Path
from typing import Any, Dict, List

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from insightspike.core.agents.main_agent import MainAgent
from insightspike.core.config import Config
from insightspike.utils.error_handler import get_logger


class TestScenario:
    """ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªã®åŸºåº•ã‚¯ãƒ©ã‚¹"""

    def __init__(self, name: str):
        self.name = name
        self.logger = get_logger(f"test.{name}")
        self.results = []

    def run(self) -> bool:
        """ã‚·ãƒŠãƒªã‚ªã‚’å®Ÿè¡Œ"""
        raise NotImplementedError

    def assert_condition(self, condition: bool, message: str):
        """æ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦çµæœã‚’è¨˜éŒ²"""
        self.results.append(
            {"condition": condition, "message": message, "passed": condition}
        )
        if not condition:
            self.logger.error(f"Assertion failed: {message}")
        else:
            self.logger.debug(f"Assertion passed: {message}")

    def get_summary(self) -> Dict[str, Any]:
        """çµæœã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        passed = sum(1 for r in self.results if r["passed"])
        total = len(self.results)
        return {
            "scenario": self.name,
            "passed": passed,
            "total": total,
            "success_rate": passed / total if total > 0 else 0,
            "details": self.results,
        }


class BasicSpikDetectionScenario(TestScenario):
    """åŸºæœ¬çš„ãªã‚¹ãƒ‘ã‚¤ã‚¯æ¤œå‡ºã‚·ãƒŠãƒªã‚ª"""

    def __init__(self):
        super().__init__("basic_spike_detection")

    def run(self) -> bool:
        """ã‚¹ãƒ‘ã‚¤ã‚¯æ¤œå‡ºã®åŸºæœ¬å‹•ä½œã‚’ãƒ†ã‚¹ãƒˆ"""
        print(f"\n=== {self.name} ===")

        # è¨­å®š
        config = Config()
        config.llm.safe_mode = True  # å®‰å®šæ€§ã®ãŸã‚ãƒ¢ãƒƒã‚¯ã‚’ä½¿ç”¨
        config.spike.spike_ged = 0.001  # æ•æ„Ÿãªé–¾å€¤
        config.spike.spike_ig = 0.001

        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–
        agent = MainAgent(config=config)

        try:
            agent.initialize()
            self.assert_condition(True, "Agent initialization successful")
        except Exception as e:
            self.assert_condition(False, f"Agent initialization failed: {e}")
            return False

        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¿½åŠ 
        episodes = [
            "ã‚·ã‚¹ãƒ†ãƒ Aã¯ç‹¬ç«‹ã—ã¦å‹•ä½œã™ã‚‹ã€‚",
            "ã‚·ã‚¹ãƒ†ãƒ Bã‚‚ç‹¬ç«‹ã—ã¦å‹•ä½œã™ã‚‹ã€‚",
            "ã‚·ã‚¹ãƒ†ãƒ Aã¨Bã‚’çµ±åˆã™ã‚‹ã¨ã€æ–°ã—ã„æ€§è³ªãŒç”Ÿã¾ã‚Œã‚‹ã€‚",  # ã“ã“ã§ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’æœŸå¾…
            "ã“ã®çµ±åˆã«ã‚ˆã‚Šã€å…¨ä½“ã®åŠ¹ç‡ãŒå‘ä¸Šã™ã‚‹ã€‚",
        ]

        spike_detected = False
        for i, episode in enumerate(episodes):
            try:
                result = agent.add_episode_with_graph_update(text=episode)

                self.assert_condition(
                    "graph_analysis" in result, f"Episode {i+1}: graph_analysis present"
                )

                if result.get("graph_analysis", {}).get("spike_detected", False):
                    spike_detected = True
                    self.logger.info(f"Spike detected at episode {i+1}: {episode}")

            except Exception as e:
                self.assert_condition(False, f"Episode {i+1} processing failed: {e}")

        # æœ€çµ‚ç¢ºèª
        self.assert_condition(
            result.get("graph_nodes", 0) == len(episodes),
            f"Graph size matches episode count",
        )

        return all(r["passed"] for r in self.results)


class MemoryRetrievalScenario(TestScenario):
    """ãƒ¡ãƒ¢ãƒªæ¤œç´¢ã‚·ãƒŠãƒªã‚ª"""

    def __init__(self):
        super().__init__("memory_retrieval")

    def run(self) -> bool:
        """ãƒ¡ãƒ¢ãƒªæ¤œç´¢æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆ"""
        print(f"\n=== {self.name} ===")

        config = Config()
        config.llm.safe_mode = True

        agent = MainAgent(config=config)
        agent.initialize()

        # çŸ¥è­˜ã‚’è¿½åŠ 
        knowledge = [
            "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯é‡ã­åˆã‚ã›ã‚’åˆ©ç”¨ã™ã‚‹ã€‚",
            "å¤å…¸ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ãƒ“ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã€‚",
            "é‡å­ãƒ“ãƒƒãƒˆã¯0ã¨1ã®é‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’å–ã‚Œã‚‹ã€‚",
            "ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã¯é‡å­ã®ç‰¹å¾´çš„ãªç¾è±¡ã§ã‚ã‚‹ã€‚",
        ]

        for text in knowledge:
            agent.add_episode_with_graph_update(text=text)

        # è³ªå•ã—ã¦æ¤œç´¢ã‚’ãƒ†ã‚¹ãƒˆ
        questions = ["é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ", "ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"]

        for question in questions:
            try:
                result = agent.process_question(question)

                self.assert_condition(
                    isinstance(result, dict), f"Question returns dict result"
                )

                self.assert_condition(
                    "response" in result, f"Response contains 'response' field"
                )

                self.assert_condition(
                    len(result.get("response", "")) > 0, f"Response is not empty"
                )

                self.logger.info(f"Q: {question}")
                self.logger.info(f"A: {result.get('response', 'N/A')[:100]}...")

            except Exception as e:
                self.assert_condition(False, f"Question processing failed: {e}")

        return all(r["passed"] for r in self.results)


class ErrorRecoveryScenario(TestScenario):
    """ã‚¨ãƒ©ãƒ¼ãƒªã‚«ãƒãƒªãƒ¼ã‚·ãƒŠãƒªã‚ª"""

    def __init__(self):
        super().__init__("error_recovery")

    def run(self) -> bool:
        """ã‚¨ãƒ©ãƒ¼ã‹ã‚‰ã®å›å¾©ã‚’ãƒ†ã‚¹ãƒˆ"""
        print(f"\n=== {self.name} ===")

        config = Config()
        config.llm.safe_mode = True

        agent = MainAgent(config=config)
        agent.initialize()

        # æ­£å¸¸ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
        result = agent.add_episode_with_graph_update(text="æ­£å¸¸ãªãƒ†ã‚­ã‚¹ãƒˆ")
        self.assert_condition(result.get("success", False), "Normal episode succeeds")

        # ç©ºã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
        try:
            result = agent.add_episode_with_graph_update(text="")
            # ç©ºã§ã‚‚å‡¦ç†ã§ãã‚‹ã¯ãš
            self.assert_condition(True, "Empty episode handled")
        except Exception as e:
            self.assert_condition(True, f"Empty episode rejected appropriately: {e}")

        # éå¸¸ã«é•·ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
        long_text = "ã“ã‚Œã¯éå¸¸ã«é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚" * 1000
        try:
            result = agent.add_episode_with_graph_update(text=long_text)
            self.assert_condition(
                result.get("success", False), "Long episode processed"
            )
        except Exception as e:
            self.assert_condition(False, f"Long episode failed: {e}")

        # ç‰¹æ®Šæ–‡å­—ã‚’å«ã‚€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
        special_text = "ç‰¹æ®Šæ–‡å­—ãƒ†ã‚¹ãƒˆ: ğŸ¯ âœ¨ æ•°å¼: âˆ‘âˆ«âˆ‚ HTML: <div>test</div>"
        try:
            result = agent.add_episode_with_graph_update(text=special_text)
            self.assert_condition(
                result.get("success", False), "Special characters handled"
            )
        except Exception as e:
            self.assert_condition(False, f"Special characters failed: {e}")

        return all(r["passed"] for r in self.results)


class PerformanceScenario(TestScenario):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""

    def __init__(self):
        super().__init__("performance")

    def run(self) -> bool:
        """åŸºæœ¬çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ãƒ†ã‚¹ãƒˆ"""
        print(f"\n=== {self.name} ===")

        config = Config()
        config.llm.safe_mode = True

        agent = MainAgent(config=config)

        # åˆæœŸåŒ–æ™‚é–“
        start = time.time()
        agent.initialize()
        init_time = time.time() - start

        self.assert_condition(
            init_time < 5.0,
            f"Initialization completes in < 5s (actual: {init_time:.2f}s)",
        )

        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¿½åŠ æ™‚é–“
        episode_times = []
        for i in range(10):
            start = time.time()
            agent.add_episode_with_graph_update(text=f"ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {i+1}")
            episode_time = time.time() - start
            episode_times.append(episode_time)

        avg_episode_time = sum(episode_times) / len(episode_times)
        self.assert_condition(
            avg_episode_time < 1.0,
            f"Average episode time < 1s (actual: {avg_episode_time:.3f}s)",
        )

        # è³ªå•å¿œç­”æ™‚é–“
        start = time.time()
        result = agent.process_question("ãƒ†ã‚¹ãƒˆè³ªå•")
        question_time = time.time() - start

        self.assert_condition(
            question_time < 2.0,
            f"Question processing < 2s (actual: {question_time:.2f}s)",
        )

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç¢ºèªï¼ˆç°¡æ˜“ç‰ˆï¼‰
        import os

        import psutil

        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024

        self.assert_condition(
            memory_mb < 1000, f"Memory usage < 1GB (actual: {memory_mb:.0f}MB)"  # 1GBä»¥ä¸‹
        )

        return all(r["passed"] for r in self.results)


def run_integration_tests():
    """çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    print("=== Running Integration Tests ===")

    scenarios = [
        BasicSpikDetectionScenario(),
        MemoryRetrievalScenario(),
        ErrorRecoveryScenario(),
        PerformanceScenario(),
    ]

    results = []
    for scenario in scenarios:
        try:
            scenario.run()
            summary = scenario.get_summary()
            results.append(summary)

            print(f"\n{scenario.name}: {summary['passed']}/{summary['total']} passed")

        except Exception as e:
            print(f"\nâœ— {scenario.name} crashed: {e}")
            import traceback

            traceback.print_exc()
            results.append(
                {
                    "scenario": scenario.name,
                    "passed": 0,
                    "total": 1,
                    "success_rate": 0,
                    "error": str(e),
                }
            )

    # çµæœã®ã‚µãƒãƒªãƒ¼
    print("\n=== Integration Test Summary ===")
    total_passed = sum(r["passed"] for r in results)
    total_tests = sum(r["total"] for r in results)

    for result in results:
        status = "âœ“" if result["success_rate"] == 1.0 else "âœ—"
        print(
            f"{status} {result['scenario']}: {result['passed']}/{result['total']} ({result['success_rate']*100:.0f}%)"
        )

    print(
        f"\nOverall: {total_passed}/{total_tests} passed ({total_passed/total_tests*100:.1f}%)"
    )

    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_file = Path("test_results_integration.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(
            {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "scenarios": results,
                "summary": {
                    "total_passed": total_passed,
                    "total_tests": total_tests,
                    "success_rate": total_passed / total_tests
                    if total_tests > 0
                    else 0,
                },
            },
            f,
            indent=2,
            ensure_ascii=False,
        )

    print(f"\nResults saved to: {output_file}")

    return total_passed == total_tests


if __name__ == "__main__":
    success = run_integration_tests()
    sys.exit(0 if success else 1)
