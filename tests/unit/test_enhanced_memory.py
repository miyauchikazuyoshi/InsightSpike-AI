"""
Tests for L2EnhancedScalableMemory with O(n log n) performance
"""
import pytest
import numpy as np
import tempfile
import os
import json
from unittest.mock import Mock, patch

from insightspike.core.layers.layer2_enhanced_scalable import L2EnhancedScalableMemory
from insightspike.utils.embedder import get_model
from pathlib import Path


class TestL2EnhancedScalableMemory:
    """Test the enhanced scalable memory implementation"""
    
    @pytest.fixture
    def memory(self):
        """Create memory instance for testing"""
        # Create mock config
        mock_config = Mock()
        mock_config.memory.nlist = 100
        mock_config.memory.pq_segments = 8
        mock_config.memory.c_value_gamma = 1.0
        mock_config.memory.c_value_min = 0.1
        mock_config.memory.c_value_max = 1.0
        mock_config.memory.index_file = "data/test_index.faiss"
        mock_config.embedding.dimension = 384
        mock_config.reasoning.similarity_threshold = 0.3
        mock_config.reasoning.graph_top_k = 50
        mock_config.reasoning.conflict_threshold = 0.8
        
        # Create mock embedder
        mock_embedder = Mock()
        mock_embedder.get_sentence_embedding_dimension.return_value = 384
        
        def encode_side_effect(texts, **kwargs):
            # Return normalized embeddings
            if isinstance(texts, str):
                texts = [texts]
            embeddings = []
            for text in texts:
                # Create deterministic embeddings based on text content
                np.random.seed(hash(text) % 2**32)
                emb = np.random.randn(384).astype(np.float32)
                emb = emb / np.linalg.norm(emb)
                embeddings.append(emb)
            return np.array(embeddings)
        
        mock_embedder.encode.side_effect = encode_side_effect
        mock_embedder.embed_documents = encode_side_effect
        
        # Fix the specific issue with division in episode integration
        type(mock_embedder).__truediv__ = lambda self, other: 1.0
        type(mock_embedder).__rtruediv__ = lambda self, other: 1.0
        
        with patch('insightspike.utils.embedder.get_model', return_value=mock_embedder):
            with patch('insightspike.core.layers.layer2_enhanced_scalable.get_model', return_value=mock_embedder):
                with patch('insightspike.core.layers.layer2_memory_manager.get_model', return_value=mock_embedder):
                    with patch('insightspike.core.layers.layer2_enhanced_scalable.get_config', return_value=mock_config):
                        with patch('insightspike.core.layers.layer2_memory_manager.get_config', return_value=mock_config):
                            return L2EnhancedScalableMemory(config=mock_config)
    
    def test_initialization(self, memory):
        """Test memory initialization"""
        assert memory.episodes == []
        assert memory.scalable_graph is not None
        assert memory.index is not None
        
    def test_store_single_episode(self, memory):
        """Test storing a single episode"""
        result = memory.store_episode("Test content", metadata={"source": "test"})
        
        assert result == True
        assert len(memory.episodes) == 1
        assert memory.episodes[0].text == "Test content"
        assert memory.episodes[0].c > 0
        
    def test_store_multiple_episodes(self, memory):
        """Test storing multiple episodes"""
        texts = ["First episode", "Second episode", "Third episode"]
        
        for text in texts:
            memory.store_episode(text, metadata={"source": "test"})
        
        assert len(memory.episodes) == 3
        assert memory.episodes[0].text == "First episode"
        assert memory.episodes[2].text == "Third episode"
        
    def test_search_episodes(self, memory):
        """Test searching episodes with FAISS"""
        # Store episodes
        memory.store_episode("Python programming basics", metadata={"type": "tutorial"})
        memory.store_episode("Advanced Python concepts", metadata={"type": "tutorial"})
        memory.store_episode("Machine learning with Python", metadata={"type": "ml"})
        memory.store_episode("Data structures in Java", metadata={"type": "tutorial"})
        
        # Create a better mock for the search method
        mock_embedder = Mock()
        mock_embedder.encode.return_value = np.random.randn(1, 384).astype(np.float32)
        
        with patch('insightspike.core.layers.layer2_memory_manager.get_model', return_value=mock_embedder):
            # Search for Python-related content
            results = memory.search_episodes("Python programming", k=2)
            
            # Due to random embeddings, we can't guarantee which episodes will be returned
            # So we just check basic properties
            assert len(results) <= 2
            if len(results) > 0:
                assert "text" in results[0]
                assert "weighted_score" in results[0]
                assert "similarity" in results[0]
                assert "c_value" in results[0]
                
                # Check ordering by weighted score
                if len(results) == 2:
                    assert results[0]["weighted_score"] >= results[1]["weighted_score"]
        
    def test_update_importance_graph_based(self, memory):
        """Test that importance is updated based on graph structure"""
        # Store episodes
        memory.store_episode("Core concept", metadata={})
        memory.store_episode("Related concept", metadata={})
        memory.store_episode("Another related concept", metadata={})
        
        # Initial C-values should be set
        assert all(ep.c > 0 for ep in memory.episodes)
        
        # Get current C-values
        initial_c_values = [ep.c for ep in memory.episodes]
        
        # Update episode importance manually (simulating graph-based update)
        memory._update_episode_importance(0, 0.8)
        
        # C-value should have changed
        assert memory.episodes[0].c != initial_c_values[0]
        assert 0 <= memory.episodes[0].c <= 1
            
    def test_conflict_detection_and_splitting(self, memory):
        """Test automatic conflict detection and splitting"""
        # Create conflicting episodes
        memory.store_episode("Python is interpreted", metadata={"type": "fact"})
        memory.store_episode("Python is compiled to bytecode", metadata={"type": "fact"})
        
        # These should be detected as potential conflicts if similar enough
        # The system should handle them appropriately
        assert len(memory.episodes) >= 2
        
    def test_save_and_load_state(self, memory):
        """Test saving and loading memory state"""
        # Store episodes
        memory.store_episode("Test episode 1", metadata={"meta": "data1"})
        memory.store_episode("Test episode 2", metadata={"meta": "data2"})
        
        with tempfile.TemporaryDirectory() as tmpdir:
            # Save state using standard save method
            index_path = os.path.join(tmpdir, "test_index.faiss")
            memory.save(Path(index_path))
            
            # Check files were created
            assert os.path.exists(index_path)
            assert os.path.exists(os.path.join(tmpdir, "scalable_index.faiss"))
            
            # Create new memory with same config
            mock_config = memory.config
            with patch('insightspike.utils.embedder.get_model') as mock_get_model:
                mock_embedder = Mock()
                mock_embedder.get_sentence_embedding_dimension.return_value = 384
                mock_get_model.return_value = mock_embedder
                
                with patch('insightspike.core.layers.layer2_enhanced_scalable.get_model', return_value=mock_embedder):
                    with patch('insightspike.core.layers.layer2_memory_manager.get_model', return_value=mock_embedder):
                        with patch('insightspike.core.layers.layer2_enhanced_scalable.get_config', return_value=mock_config):
                            with patch('insightspike.core.layers.layer2_memory_manager.get_config', return_value=mock_config):
                                new_memory = L2EnhancedScalableMemory(config=mock_config)
                                new_memory.load(Path(index_path))
                                
                                # Verify loaded state
                                assert len(new_memory.episodes) == 2
                                assert new_memory.episodes[0].text == "Test episode 1"
                                assert new_memory.episodes[1].text == "Test episode 2"
            
    def test_performance_with_many_episodes(self, memory):
        """Test performance doesn't degrade with many episodes"""
        import time
        
        # Add 100 episodes and measure time
        start = time.time()
        for i in range(100):
            memory.store_episode(f"Episode {i}", c_value=0.5, metadata={"index": i})
        time_100 = time.time() - start
        
        # Add another 100 and measure
        start = time.time()
        for i in range(100, 200):
            memory.store_episode(f"Episode {i}", c_value=0.5, metadata={"index": i})
        time_200 = time.time() - start
        
        # Should be O(n log n), not O(nÂ²)
        ratio = time_200 / time_100
        assert ratio < 2.5, f"Performance degradation too high: {ratio:.2f}x"
        
        # Test search performance with mock embedder
        mock_embedder = Mock()
        mock_embedder.encode.return_value = np.random.randn(1, 384).astype(np.float32)
        
        with patch('insightspike.core.layers.layer2_memory_manager.get_model', return_value=mock_embedder):
            start = time.time()
            results = memory.search_episodes("Episode 150", k=5)
            search_time = time.time() - start
            
            assert len(results) <= 5
            assert search_time < 0.5  # Should be fast even with 200 episodes
        
    def test_get_graph_state(self, memory):
        """Test getting current graph state"""
        # Store some episodes
        memory.store_episode("Node 1", metadata={})
        memory.store_episode("Node 2", metadata={})
        memory.store_episode("Node 3", metadata={})
        
        # Get graph through scalable_graph
        graph = memory.scalable_graph.graph
        
        assert graph is not None
        assert graph.x.shape[0] == 3  # 3 nodes
        
    def test_integration_with_similar_episodes(self, memory):
        """Test that very similar episodes are integrated"""
        # Store very similar episodes
        memory.store_episode("Machine learning is powerful", metadata={"source": "A"})
        memory.store_episode("Machine learning is very powerful", metadata={"source": "B"})
        
        # With high similarity, these might be integrated
        # The exact behavior depends on threshold settings
        assert len(memory.episodes) >= 1  # At least one should exist
        
    def test_empty_memory_operations(self, memory):
        """Test operations on empty memory"""
        # Search empty memory
        results = memory.search_episodes("test query", k=5)
        assert results == []
        
        # Get graph from empty memory
        graph = memory.scalable_graph.graph
        assert graph.x.shape[0] == 0
        
    def test_metadata_preservation(self, memory):
        """Test that metadata is preserved through operations"""
        # Store with rich metadata
        metadata = {
            "source": "test",
            "timestamp": "2025-01-06",
            "author": "system",
            "tags": ["test", "unit"]
        }
        
        result = memory.store_episode("Test content", metadata=metadata)
        assert result  # Should succeed
        
        # Check metadata is preserved in the stored episode
        if memory.episodes:
            stored_episode = memory.episodes[-1]
            for key, value in metadata.items():
                assert stored_episode.metadata.get(key) == value
            
        # Save and load
        with tempfile.TemporaryDirectory() as tmpdir:
            index_path = os.path.join(tmpdir, "test_index.faiss")
            memory.save(Path(index_path))
            
            # Create new memory with same config
            mock_config = memory.config
            with patch('insightspike.utils.embedder.get_model') as mock_get_model:
                mock_embedder = Mock()
                mock_embedder.get_sentence_embedding_dimension.return_value = 384
                mock_get_model.return_value = mock_embedder
                
                with patch('insightspike.core.layers.layer2_enhanced_scalable.get_model', return_value=mock_embedder):
                    with patch('insightspike.core.layers.layer2_memory_manager.get_model', return_value=mock_embedder):
                        with patch('insightspike.core.layers.layer2_enhanced_scalable.get_config', return_value=mock_config):
                            with patch('insightspike.core.layers.layer2_memory_manager.get_config', return_value=mock_config):
                                new_memory = L2EnhancedScalableMemory(config=mock_config)
                                new_memory.load(Path(index_path))
                                
                                # Check metadata still preserved
                                loaded_ep = new_memory.episodes[0]
                                for key, value in metadata.items():
                                    assert loaded_ep.metadata.get(key) == value