{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e60573",
   "metadata": {},
   "source": [
    "# üîç InsightSpike-AI Large-Scale Dependency Investigation Notebook\n",
    "\n",
    "**Comprehensive Dependency Analysis and Resolution for Large-Scale Colab Experiments**\n",
    "\n",
    "This notebook investigates and resolves dependency issues in Google Colab environment with **2025-optimized setup** for **large-scale experiments and production workloads**.\n",
    "\n",
    "‚ö° **Recommended Runtime**: A100 GPU or V100 for large-scale experiments  \n",
    "üî• **Fallback Runtime**: T4 GPU for medium-scale testing  \n",
    "üíæ **Memory Requirements**: High-RAM runtime for large datasets\n",
    "\n",
    "## üöÄ Large-Scale Dependency Investigation Process\n",
    "\n",
    "**Key areas of investigation:**\n",
    "1. **NumPy 2.x Compatibility** - Modern environment analysis with large array handling\n",
    "2. **FAISS Installation** - GPU optimization for million+ vector operations\n",
    "3. **PyTorch Integration** - CUDA compatibility with batch processing\n",
    "4. **Poetry vs Pip** - Package management for complex dependency chains\n",
    "5. **Memory Management** - Large dataset handling strategies\n",
    "6. **Performance Monitoring** - Resource utilization tracking\n",
    "\n",
    "## üìä Investigation Results for Large-Scale Operations\n",
    "\n",
    "| Component | Small Scale | Large Scale | Optimization |\n",
    "|-----------|-------------|-------------|--------------|\n",
    "| NumPy 2.x | ‚úÖ Supported | ‚úÖ Optimized | Vectorized operations |\n",
    "| FAISS-GPU | ‚ö†Ô∏è Warnings | ‚úÖ Required | Million+ vectors |\n",
    "| PyTorch | ‚úÖ Working | ‚úÖ Batch-ready | Multi-GPU support |\n",
    "| Poetry | ‚úÖ Alternative | ‚úÖ Dependency lock | Reproducible builds |\n",
    "| Memory | ‚úÖ Basic | ‚ö†Ô∏è Monitor | High-RAM runtime |\n",
    "\n",
    "üí° **Key Finding:** Large-scale Colab experiments require careful resource management and optimized dependency configurations.\n",
    "\n",
    "üéØ **Target Workloads:**\n",
    "- Processing 100K+ documents\n",
    "- Vector databases with 1M+ embeddings\n",
    "- Multi-hour training sessions\n",
    "- Batch inference on large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee6df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ Repository Setup\n",
    "import os\n",
    "\n",
    "# Check if already cloned (for re-runs)\n",
    "if not os.path.exists('InsightSpike-AI'):\n",
    "    print(\"üìã Cloning repository...\")\n",
    "    !git clone https://github.com/miyauchikazuyoshi/InsightSpike-AI.git\n",
    "    print(\"‚úÖ Repository cloned\")\n",
    "else:\n",
    "    print(\"‚úÖ Repository already exists\")\n",
    "\n",
    "%cd InsightSpike-AI\n",
    "\n",
    "# Set permissions for simplified setup scripts\n",
    "print(\"üîß Setting up scripts...\")\n",
    "!chmod +x scripts/colab/setup_colab.sh\n",
    "!chmod +x scripts/colab/setup_colab_debug.sh\n",
    "print(\"‚úÖ Scripts ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ba942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Large-Scale Performance Monitoring Setup\n",
    "import psutil\n",
    "import GPUtil\n",
    "from datetime import datetime\n",
    "\n",
    "class ColabResourceMonitor:\n",
    "    def __init__(self):\n",
    "        self.start_time = datetime.now()\n",
    "        self.log_data = []\n",
    "    \n",
    "    def log_resources(self, stage_name):\n",
    "        \"\"\"Log current resource usage\"\"\"\n",
    "        try:\n",
    "            # CPU and Memory\n",
    "            cpu_percent = psutil.cpu_percent(interval=1)\n",
    "            memory = psutil.virtual_memory()\n",
    "            \n",
    "            # GPU (if available)\n",
    "            gpu_info = \"N/A\"\n",
    "            try:\n",
    "                gpus = GPUtil.getGPUs()\n",
    "                if gpus:\n",
    "                    gpu = gpus[0]\n",
    "                    gpu_info = f\"GPU: {gpu.memoryUtil*100:.1f}% memory, {gpu.load*100:.1f}% load\"\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            log_entry = {\n",
    "                'stage': stage_name,\n",
    "                'timestamp': datetime.now(),\n",
    "                'cpu_percent': cpu_percent,\n",
    "                'memory_percent': memory.percent,\n",
    "                'memory_gb': memory.used / (1024**3),\n",
    "                'gpu_info': gpu_info\n",
    "            }\n",
    "            \n",
    "            self.log_data.append(log_entry)\n",
    "            \n",
    "            print(f\"üìä {stage_name}:\")\n",
    "            print(f\"   CPU: {cpu_percent:.1f}% | Memory: {memory.percent:.1f}% ({memory.used/(1024**3):.2f}GB)\")\n",
    "            print(f\"   {gpu_info}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Resource monitoring error: {e}\")\n",
    "    \n",
    "    def get_runtime_summary(self):\n",
    "        \"\"\"Get summary of experiment runtime\"\"\"\n",
    "        runtime = datetime.now() - self.start_time\n",
    "        return f\"üïí Total runtime: {runtime}\"\n",
    "\n",
    "# Initialize resource monitor for large-scale experiments\n",
    "resource_monitor = ColabResourceMonitor()\n",
    "resource_monitor.log_resources(\"Initial Setup\")\n",
    "\n",
    "print(\"üöÄ Large-Scale Resource Monitoring Active\")\n",
    "print(\"üí° Use resource_monitor.log_resources('stage_name') throughout experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734041b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Dependency Investigation: NumPy 2.x Reality Check\n",
    "# Comprehensive analysis of 2025 Colab environment challenges\n",
    "\n",
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"üîç InsightSpike-AI Dependency Investigation\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Purpose: Analyze and resolve dependency conflicts in modern Colab\")\n",
    "print(\"Focus: NumPy 2.x + FAISS compatibility\")\n",
    "print()\n",
    "\n",
    "# Environment Analysis\n",
    "print(\"üî¨ Environment Analysis\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Check NumPy version and compatibility\n",
    "try:\n",
    "    import numpy\n",
    "    numpy_version = numpy.__version__\n",
    "    numpy_major = int(numpy_version.split('.')[0])\n",
    "    print(f\"üìä NumPy Version: {numpy_version} (Major: {numpy_major})\")\n",
    "    \n",
    "    if numpy_major >= 2:\n",
    "        print(\"‚úÖ NumPy 2.x detected - Modern environment\")\n",
    "        print(\"‚ö†Ô∏è FAISS-GPU may show dependency warnings\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è NumPy 1.x detected - Legacy compatibility\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ùå NumPy not available\")\n",
    "    numpy_major = 0\n",
    "\n",
    "# Check PyTorch compatibility\n",
    "try:\n",
    "    import torch\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    device_name = torch.cuda.get_device_name(0) if gpu_available else \"CPU\"\n",
    "    print(f\"‚ö° PyTorch: {torch.__version__} ({device_name})\")\n",
    "    if gpu_available:\n",
    "        cuda_version = torch.version.cuda\n",
    "        print(f\"üî• CUDA Version: {cuda_version}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå PyTorch not available\")\n",
    "    gpu_available = False\n",
    "\n",
    "print()\n",
    "\n",
    "# FAISS Investigation\n",
    "print(\"üß™ FAISS Compatibility Investigation\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "faiss_success = False\n",
    "faiss_method = \"none\"\n",
    "\n",
    "# Test FAISS-GPU installation\n",
    "if numpy_major >= 2:\n",
    "    print(\"üîÑ Testing FAISS-GPU with NumPy 2.x...\")\n",
    "    try:\n",
    "        result = subprocess.run([sys.executable, '-m', 'pip', 'install', 'faiss-gpu-cu12'], \n",
    "                              capture_output=True, text=True, timeout=120)\n",
    "        \n",
    "        # Check if FAISS actually works despite warnings\n",
    "        import faiss\n",
    "        gpu_count = faiss.get_num_gpus() if hasattr(faiss, 'get_num_gpus') else 0\n",
    "        \n",
    "        if gpu_count > 0:\n",
    "            print(f\"‚úÖ FAISS-GPU working: {gpu_count} GPU(s) available\")\n",
    "            faiss_success = True\n",
    "            faiss_method = \"GPU (with warnings)\"\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è FAISS-GPU installed but no GPUs detected\")\n",
    "            faiss_success = True\n",
    "            faiss_method = \"CPU fallback\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå FAISS-GPU failed: {str(e)[:100]}...\")\n",
    "        \n",
    "        # Try CPU fallback\n",
    "        print(\"üîÑ Trying FAISS-CPU fallback...\")\n",
    "        try:\n",
    "            subprocess.run([sys.executable, '-m', 'pip', 'install', 'faiss-cpu'], \n",
    "                          capture_output=True, text=True, timeout=60)\n",
    "            import faiss\n",
    "            print(\"‚úÖ FAISS-CPU installed successfully\")\n",
    "            faiss_success = True\n",
    "            faiss_method = \"CPU only\"\n",
    "        except Exception as cpu_e:\n",
    "            print(f\"‚ùå FAISS-CPU also failed: {str(cpu_e)[:100]}...\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Generate Investigation Report\n",
    "print(\"üìã Dependency Investigation Report\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"üñ•Ô∏è Environment: Google Colab 2025\")\n",
    "print(f\"üìä NumPy: {numpy_version if 'numpy' in locals() else 'N/A'}\")\n",
    "print(f\"‚ö° PyTorch: {torch.__version__ if 'torch' in locals() else 'N/A'}\")\n",
    "print(f\"üß† FAISS: {faiss_method}\")\n",
    "print(f\"üéØ GPU Available: {'Yes' if gpu_available else 'No'}\")\n",
    "\n",
    "if faiss_success:\n",
    "    print(\"\\n‚úÖ Resolution: Dependencies resolved with optimal configuration\")\n",
    "    if \"warnings\" in faiss_method:\n",
    "        print(\"üí° Note: FAISS warnings expected but functionality maintained\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Resolution: Alternative vector search methods required\")\n",
    "\n",
    "print(f\"\\n‚è∞ Investigation completed: {time.strftime('%H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5793d7",
   "metadata": {},
   "source": [
    "## üìä Investigation Findings\n",
    "\n",
    "### üîç Key Dependencies Analysis\n",
    "\n",
    "**NumPy 2.x Compatibility:**\n",
    "- ‚úÖ Modern Colab environments use NumPy 2.x by default\n",
    "- ‚ö†Ô∏è Some packages may show deprecation warnings\n",
    "- üí° Solution: Use packages with NumPy 2.x support\n",
    "\n",
    "**FAISS Installation Strategy:**\n",
    "- üéØ FAISS-GPU: May work despite warnings in NumPy 2.x\n",
    "- üõ°Ô∏è FAISS-CPU: Reliable fallback with full compatibility\n",
    "- üìà Performance: CPU version sufficient for most use cases\n",
    "\n",
    "**PyTorch Integration:**\n",
    "- ‚úÖ Pre-installed with CUDA support in Colab\n",
    "- üî• Compatible with modern GPU runtimes\n",
    "- ‚ö° No installation conflicts observed\n",
    "\n",
    "### üí° Recommended Installation Strategy\n",
    "\n",
    "1. **Smart FAISS Installation**: Try GPU first, fallback to CPU\n",
    "2. **Leverage Pre-installed Packages**: Use Colab's PyTorch/NumPy\n",
    "3. **Error-Tolerant Approach**: Handle warnings gracefully\n",
    "4. **Performance Optimization**: CPU FAISS + GPU PyTorch hybrid\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "Ready to proceed with InsightSpike-AI installation using adaptive dependency resolution!\n",
    "\n",
    "# üîç Large-Scale Dependency Investigation: NumPy 2.x + Performance Analysis\n",
    "# Comprehensive analysis of 2025 Colab environment for large-scale workloads\n",
    "\n",
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "print(\"üîç InsightSpike-AI Large-Scale Dependency Investigation\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Purpose: Analyze dependencies for large-scale Colab experiments\")\n",
    "print(\"Focus: NumPy 2.x + FAISS + Performance optimization\")\n",
    "print(\"Target: 100K+ documents, 1M+ vectors, multi-hour sessions\")\n",
    "print()\n",
    "\n",
    "resource_monitor.log_resources(\"Dependency Investigation Start\")\n",
    "\n",
    "# Environment Analysis with Performance Testing\n",
    "print(\"üî¨ Environment Analysis + Performance Testing\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check NumPy version and large array performance\n",
    "try:\n",
    "    import numpy\n",
    "    numpy_version = numpy.__version__\n",
    "    numpy_major = int(numpy_version.split('.')[0])\n",
    "    print(f\"üìä NumPy Version: {numpy_version} (Major: {numpy_major})\")\n",
    "    \n",
    "    # Test large array performance\n",
    "    print(\"üß™ Testing large array performance...\")\n",
    "    start_time = time.time()\n",
    "    large_array = np.random.random((10000, 1000))  # 10K x 1K array (~80MB)\n",
    "    dot_product = np.dot(large_array, large_array.T)\n",
    "    performance_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   ‚ö° Large array operation: {performance_time:.2f}s\")\n",
    "    print(f\"   üìê Array shape: {large_array.shape} ({large_array.nbytes/1024/1024:.1f}MB)\")\n",
    "    \n",
    "    if numpy_major >= 2:\n",
    "        print(\"‚úÖ NumPy 2.x detected - Optimized for large-scale operations\")\n",
    "        print(\"‚ö†Ô∏è FAISS-GPU may show dependency warnings but will work\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è NumPy 1.x detected - Consider upgrading for large-scale performance\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ùå NumPy not available\")\n",
    "    numpy_major = 0\n",
    "    performance_time = float('inf')\n",
    "\n",
    "resource_monitor.log_resources(\"NumPy Performance Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c48f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Large-Scale Experiment Configuration\n",
    "# Setup for processing 100K+ documents and 1M+ vectors\n",
    "\n",
    "print(\"üöÄ Large-Scale Experiment Configuration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Batch processing configuration\n",
    "class LargeScaleConfig:\n",
    "    def __init__(self):\n",
    "        # Batch sizes optimized for Colab resources\n",
    "        self.small_batch_size = 1000    # For T4 GPU\n",
    "        self.medium_batch_size = 5000   # For V100 GPU\n",
    "        self.large_batch_size = 10000   # For A100 GPU\n",
    "        \n",
    "        # Memory management\n",
    "        self.max_memory_usage = 0.8     # 80% memory limit\n",
    "        self.checkpoint_interval = 10000 # Save every 10K processed\n",
    "        \n",
    "        # Processing limits\n",
    "        self.max_documents = 1000000    # 1M documents\n",
    "        self.max_vectors = 1000000      # 1M vectors\n",
    "        self.embedding_dim = 768        # Standard transformer dimension\n",
    "        \n",
    "        # Performance thresholds\n",
    "        self.max_processing_time = 3600 # 1 hour limit per batch\n",
    "        self.memory_warning_threshold = 0.9 # 90% memory warning\n",
    "    \n",
    "    def get_optimal_batch_size(self):\n",
    "        \"\"\"Determine optimal batch size based on available resources\"\"\"\n",
    "        try:\n",
    "            memory = psutil.virtual_memory()\n",
    "            available_gb = memory.available / (1024**3)\n",
    "            \n",
    "            # GPU detection\n",
    "            gpu_detected = False\n",
    "            gpu_memory = 0\n",
    "            try:\n",
    "                gpus = GPUtil.getGPUs()\n",
    "                if gpus:\n",
    "                    gpu_detected = True\n",
    "                    gpu_memory = gpus[0].memoryTotal\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            if gpu_memory > 40:  # A100 or similar\n",
    "                return self.large_batch_size\n",
    "            elif gpu_memory > 15:  # V100 or similar\n",
    "                return self.medium_batch_size\n",
    "            else:  # T4 or CPU\n",
    "                return self.small_batch_size\n",
    "                \n",
    "        except:\n",
    "            return self.small_batch_size\n",
    "    \n",
    "    def estimate_processing_time(self, total_items):\n",
    "        \"\"\"Estimate total processing time\"\"\"\n",
    "        batch_size = self.get_optimal_batch_size()\n",
    "        num_batches = (total_items + batch_size - 1) // batch_size\n",
    "        \n",
    "        # Assume 10 seconds per batch for large-scale processing\n",
    "        estimated_seconds = num_batches * 10\n",
    "        hours = estimated_seconds // 3600\n",
    "        minutes = (estimated_seconds % 3600) // 60\n",
    "        \n",
    "        return f\"{hours}h {minutes}m (approx)\"\n",
    "\n",
    "# Initialize large-scale configuration\n",
    "large_scale_config = LargeScaleConfig()\n",
    "optimal_batch = large_scale_config.get_optimal_batch_size()\n",
    "\n",
    "print(f\"üìä Optimal batch size: {optimal_batch:,} items\")\n",
    "print(f\"üéØ Target capacity: {large_scale_config.max_documents:,} documents\")\n",
    "print(f\"‚ö° Estimated time for 100K items: {large_scale_config.estimate_processing_time(100000)}\")\n",
    "print(f\"üöÄ Estimated time for 1M items: {large_scale_config.estimate_processing_time(1000000)}\")\n",
    "\n",
    "resource_monitor.log_resources(\"Large-Scale Configuration\")\n",
    "\n",
    "# Memory optimization tips\n",
    "print(\"\\nüí° Large-Scale Optimization Tips:\")\n",
    "print(\"   üîÑ Use batch processing with checkpoints\")\n",
    "print(\"   üíæ Enable High-RAM runtime for 1M+ vectors\")\n",
    "print(\"   ‚ö° Use A100 GPU for fastest processing\")\n",
    "print(\"   üìÅ Save intermediate results frequently\")\n",
    "print(\"   üßπ Clear memory between batches with gc.collect()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c55d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Checkpoint and Recovery System for Long-Running Experiments\n",
    "# Essential for multi-hour large-scale processing\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "class ExperimentCheckpoint:\n",
    "    def __init__(self, experiment_name=\"large_scale_experiment\"):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.checkpoint_dir = Path(f\"/content/checkpoints/{experiment_name}\")\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.checkpoint_file = self.checkpoint_dir / \"checkpoint.json\"\n",
    "        self.data_file = self.checkpoint_dir / \"experiment_data.pkl\"\n",
    "        \n",
    "    def save_checkpoint(self, progress_data, processed_count, total_count):\n",
    "        \"\"\"Save experiment checkpoint\"\"\"\n",
    "        checkpoint_info = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'processed_count': processed_count,\n",
    "            'total_count': total_count,\n",
    "            'progress_percent': (processed_count / total_count) * 100,\n",
    "            'experiment_name': self.experiment_name,\n",
    "            'runtime': str(datetime.now() - resource_monitor.start_time)\n",
    "        }\n",
    "        \n",
    "        # Save checkpoint metadata\n",
    "        with open(self.checkpoint_file, 'w') as f:\n",
    "            json.dump(checkpoint_info, f, indent=2)\n",
    "        \n",
    "        # Save progress data\n",
    "        with open(self.data_file, 'wb') as f:\n",
    "            pickle.dump(progress_data, f)\n",
    "        \n",
    "        print(f\"üíæ Checkpoint saved: {processed_count:,}/{total_count:,} ({checkpoint_info['progress_percent']:.1f}%)\")\n",
    "        \n",
    "        # Memory cleanup\n",
    "        gc.collect()\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Load experiment checkpoint if exists\"\"\"\n",
    "        if self.checkpoint_file.exists() and self.data_file.exists():\n",
    "            try:\n",
    "                # Load checkpoint metadata\n",
    "                with open(self.checkpoint_file, 'r') as f:\n",
    "                    checkpoint_info = json.load(f)\n",
    "                \n",
    "                # Load progress data\n",
    "                with open(self.data_file, 'rb') as f:\n",
    "                    progress_data = pickle.load(f)\n",
    "                \n",
    "                print(f\"üìÇ Checkpoint loaded: {checkpoint_info['processed_count']:,} items processed\")\n",
    "                print(f\"‚è∞ Previous runtime: {checkpoint_info['runtime']}\")\n",
    "                \n",
    "                return checkpoint_info, progress_data\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Checkpoint loading failed: {e}\")\n",
    "                return None, None\n",
    "        else:\n",
    "            print(\"üÜï No checkpoint found - starting fresh experiment\")\n",
    "            return None, None\n",
    "    \n",
    "    def cleanup_checkpoints(self):\n",
    "        \"\"\"Clean up checkpoint files\"\"\"\n",
    "        import shutil\n",
    "        if self.checkpoint_dir.exists():\n",
    "            shutil.rmtree(self.checkpoint_dir)\n",
    "            print(\"üßπ Checkpoint files cleaned up\")\n",
    "\n",
    "# Initialize checkpoint system\n",
    "checkpoint_system = ExperimentCheckpoint(\"dependency_investigation\")\n",
    "\n",
    "# Check for existing checkpoint\n",
    "checkpoint_info, checkpoint_data = checkpoint_system.load_checkpoint()\n",
    "\n",
    "print(\"‚úÖ Checkpoint system ready for large-scale experiments\")\n",
    "print(\"üí° Use checkpoint_system.save_checkpoint() every 10K processed items\")\n",
    "\n",
    "resource_monitor.log_resources(\"Checkpoint System Ready\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
