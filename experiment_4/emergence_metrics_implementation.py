#!/usr/bin/env python3
"""
Implementation of Emergence Metrics for InsightSpike-AI
å‰µç™ºæ€§ã‚’å®šé‡çš„ã«è©•ä¾¡ã™ã‚‹æŒ‡æ¨™ã®å®Ÿè£…
"""

import numpy as np
import networkx as nx
from scipy.stats import entropy
from scipy.spatial.distance import cosine
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict
import torch
from typing import List, Dict, Tuple, Any

class EmergenceMetrics:
    """å‰µç™ºæ€§è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®Ÿè£…"""
    
    def __init__(self):
        self.history = []
        
    def calculate_structural_emergence(self, 
                                     graph_before: nx.Graph, 
                                     graph_after: nx.Graph) -> Dict[str, float]:
        """æ§‹é€ çš„å‰µç™ºæ€§ã®è¨ˆç®—"""
        
        # 1. Edge Surprise Score (æ–°è¦ã‚¨ãƒƒã‚¸ã®æ„å¤–æ€§)
        new_edges = set(graph_after.edges()) - set(graph_before.edges())
        edge_surprise = 0
        
        if len(new_edges) > 0:
            # æ—¢å­˜ã®æ§‹é€ ã‹ã‚‰æ–°è¦ã‚¨ãƒƒã‚¸ã®ç¢ºç‡ã‚’æ¨å®š
            degree_before = dict(graph_before.degree())
            total_possible = len(graph_before.nodes()) * (len(graph_before.nodes()) - 1) / 2
            
            for u, v in new_edges:
                if u in degree_before and v in degree_before:
                    # æ¬¡æ•°ç©ã«åŸºã¥ãæ¥ç¶šç¢ºç‡
                    prob = (degree_before[u] * degree_before[v]) / total_possible
                    edge_surprise += -np.log(max(prob, 1e-10))
            
            edge_surprise /= len(new_edges)
        
        # 2. Clustering Evolution (ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä¿‚æ•°ã®å¤‰åŒ–)
        clustering_before = nx.average_clustering(graph_before)
        clustering_after = nx.average_clustering(graph_after)
        clustering_delta = clustering_after - clustering_before
        
        # 3. Hub Emergence (æ–°ã—ã„ãƒãƒ–ã®å‡ºç¾)
        centrality_before = nx.betweenness_centrality(graph_before)
        centrality_after = nx.betweenness_centrality(graph_after)
        
        # ä¸­å¿ƒæ€§ãŒå¤§å¹…ã«å¢—åŠ ã—ãŸãƒãƒ¼ãƒ‰ã‚’æ¤œå‡º
        hub_emergence = 0
        for node in centrality_after:
            if node in centrality_before:
                delta = centrality_after[node] - centrality_before[node]
                if delta > 0.1:  # é–¾å€¤
                    hub_emergence += delta
        
        # 4. Modularity Change (ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒªãƒ†ã‚£ã®å¤‰åŒ–)
        communities_before = list(nx.community.greedy_modularity_communities(graph_before))
        communities_after = list(nx.community.greedy_modularity_communities(graph_after))
        
        modularity_before = nx.community.modularity(graph_before, communities_before)
        modularity_after = nx.community.modularity(graph_after, communities_after)
        modularity_delta = modularity_after - modularity_before
        
        return {
            'edge_surprise': edge_surprise,
            'clustering_evolution': clustering_delta,
            'hub_emergence': hub_emergence,
            'modularity_change': modularity_delta,
            'structural_score': (edge_surprise + abs(clustering_delta) + hub_emergence + abs(modularity_delta)) / 4
        }
    
    def calculate_semantic_emergence(self, 
                                   embeddings_before: np.ndarray,
                                   embeddings_after: np.ndarray,
                                   texts: List[str]) -> Dict[str, float]:
        """æ„å‘³çš„å‰µç™ºæ€§ã®è¨ˆç®—"""
        
        # 1. Semantic Drift (æ„å‘³ç©ºé–“ã®ç§»å‹•)
        centroid_before = np.mean(embeddings_before, axis=0)
        centroid_after = np.mean(embeddings_after, axis=0)
        semantic_drift = 1 - cosine(centroid_before, centroid_after)
        
        # 2. Concept Diversity (æ¦‚å¿µã®å¤šæ§˜æ€§å¢—åŠ )
        diversity_before = np.mean(np.std(embeddings_before, axis=0))
        diversity_after = np.mean(np.std(embeddings_after, axis=0))
        diversity_increase = (diversity_after - diversity_before) / diversity_before
        
        # 3. Semantic Coherence (æ„å‘³çš„ä¸€è²«æ€§)
        # æ–°ã—ãè¿½åŠ ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆé–“ã®ä¸€è²«æ€§
        if len(embeddings_after) > len(embeddings_before):
            new_embeddings = embeddings_after[len(embeddings_before):]
            coherence = np.mean(cosine_similarity(new_embeddings))
        else:
            coherence = 0
        
        # 4. Contextual Novelty (æ–‡è„ˆçš„æ–°è¦æ€§)
        # æ–°ã—ã„æ¦‚å¿µãŒæ—¢å­˜æ¦‚å¿µã¨ã©ã‚Œã ã‘ç•°ãªã‚‹ã‹
        novelty = 0
        if len(embeddings_after) > len(embeddings_before):
            new_embeddings = embeddings_after[len(embeddings_before):]
            old_embeddings = embeddings_before
            
            for new_emb in new_embeddings:
                max_sim = np.max(cosine_similarity([new_emb], old_embeddings))
                novelty += (1 - max_sim)
            
            novelty /= len(new_embeddings)
        
        return {
            'semantic_drift': semantic_drift,
            'diversity_increase': diversity_increase,
            'coherence': coherence,
            'contextual_novelty': novelty,
            'semantic_score': (semantic_drift + diversity_increase + coherence + novelty) / 4
        }
    
    def calculate_information_emergence(self,
                                      graph_before: nx.Graph,
                                      graph_after: nx.Graph,
                                      embeddings_after: np.ndarray) -> Dict[str, float]:
        """æƒ…å ±ç†è«–çš„å‰µç™ºæ€§ã®è¨ˆç®—"""
        
        # 1. Graph Entropy Change (ã‚°ãƒ©ãƒ•ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®å¤‰åŒ–)
        def graph_entropy(G):
            degrees = [d for n, d in G.degree()]
            if sum(degrees) == 0:
                return 0
            degree_dist = np.array(degrees) / sum(degrees)
            return entropy(degree_dist)
        
        entropy_before = graph_entropy(graph_before)
        entropy_after = graph_entropy(graph_after)
        entropy_change = entropy_after - entropy_before
        
        # 2. Compression Potential (åœ§ç¸®å¯èƒ½æ€§)
        # ã‚ˆã‚Šæ§‹é€ åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¯ã‚ˆã‚Šåœ§ç¸®å¯èƒ½
        if len(embeddings_after) > 0:
            # SVDã«ã‚ˆã‚‹åœ§ç¸®å¯èƒ½æ€§ã®è©•ä¾¡
            U, s, Vt = np.linalg.svd(embeddings_after, full_matrices=False)
            # ç‰¹ç•°å€¤ã®æ¸›è¡°ç‡
            compression_potential = 1 - (s[-1] / s[0]) if len(s) > 0 else 0
        else:
            compression_potential = 0
        
        # 3. Mutual Information Increase (ç›¸äº’æƒ…å ±é‡ã®å¢—åŠ )
        # ç°¡ç•¥åŒ–: ãƒãƒ¼ãƒ‰é–“ã®æ¥ç¶šãƒ‘ã‚¿ãƒ¼ãƒ³ã®æƒ…å ±é‡
        adj_before = nx.adjacency_matrix(graph_before).todense()
        adj_after = nx.adjacency_matrix(graph_after).todense()
        
        # æ¥ç¶šãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å¤‰åŒ–
        mi_change = np.abs(entropy(adj_after.flatten()) - entropy(adj_before.flatten()))
        
        return {
            'entropy_change': entropy_change,
            'compression_potential': compression_potential,
            'mutual_info_change': mi_change,
            'information_score': (abs(entropy_change) + compression_potential + mi_change) / 3
        }
    
    def calculate_emergence_spike(self, 
                                current_metrics: Dict[str, float],
                                threshold: float = 2.0) -> bool:
        """å‰µç™ºã‚¹ãƒ‘ã‚¤ã‚¯ã®æ¤œå‡º"""
        
        if len(self.history) < 3:
            self.history.append(current_metrics)
            return False
        
        # ç§»å‹•å¹³å‡ã¨æ¨™æº–åå·®
        recent_scores = [h['total_score'] for h in self.history[-10:]]
        mean_score = np.mean(recent_scores)
        std_score = np.std(recent_scores)
        
        # ç¾åœ¨ã®ã‚¹ã‚³ã‚¢ãŒå¹³å‡+2Ïƒã‚’è¶…ãˆãŸã‚‰ã‚¹ãƒ‘ã‚¤ã‚¯
        current_total = current_metrics['total_score']
        is_spike = current_total > mean_score + threshold * std_score
        
        self.history.append(current_metrics)
        
        return is_spike
    
    def calculate_total_emergence(self,
                                graph_before: nx.Graph,
                                graph_after: nx.Graph,
                                embeddings_before: np.ndarray,
                                embeddings_after: np.ndarray,
                                texts: List[str]) -> Dict[str, Any]:
        """ç·åˆçš„ãªå‰µç™ºæ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        
        # å„ã‚«ãƒ†ã‚´ãƒªã®å‰µç™ºæ€§ã‚’è¨ˆç®—
        structural = self.calculate_structural_emergence(graph_before, graph_after)
        semantic = self.calculate_semantic_emergence(embeddings_before, embeddings_after, texts)
        information = self.calculate_information_emergence(graph_before, graph_after, embeddings_after)
        
        # ç·åˆã‚¹ã‚³ã‚¢
        total_score = (structural['structural_score'] + 
                      semantic['semantic_score'] + 
                      information['information_score']) / 3
        
        # çµæœã‚’ã¾ã¨ã‚ã‚‹
        results = {
            'structural': structural,
            'semantic': semantic,
            'information': information,
            'total_score': total_score,
            'timestamp': len(self.history)
        }
        
        # ã‚¹ãƒ‘ã‚¤ã‚¯æ¤œå‡º
        is_spike = self.calculate_emergence_spike(results)
        results['is_emergence_spike'] = is_spike
        
        return results


def analyze_insightspike_emergence(agent, new_texts: List[str]):
    """InsightSpike-AIã®å‰µç™ºæ€§ã‚’åˆ†æ"""
    
    metrics = EmergenceMetrics()
    
    # åˆæœŸçŠ¶æ…‹ã‚’è¨˜éŒ²
    initial_state = agent.get_memory_graph_state()
    initial_graph = nx.Graph()  # å®Ÿéš›ã®ã‚°ãƒ©ãƒ•ã«å¤‰æ›
    initial_embeddings = []  # å®Ÿéš›ã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—
    
    # æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ ã—ãªãŒã‚‰å‰µç™ºæ€§ã‚’æ¸¬å®š
    for i, text in enumerate(new_texts):
        # ç¾åœ¨ã®çŠ¶æ…‹ã‚’ä¿å­˜
        before_graph = initial_graph.copy()
        before_embeddings = initial_embeddings.copy()
        
        # æ–°ã—ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¿½åŠ 
        result = agent.add_episode_with_graph_update(text)
        
        # æ›´æ–°å¾Œã®çŠ¶æ…‹ã‚’å–å¾—
        after_state = agent.get_memory_graph_state()
        after_graph = nx.Graph()  # æ›´æ–°ã•ã‚ŒãŸã‚°ãƒ©ãƒ•
        after_embeddings = []  # æ›´æ–°ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿
        
        # å‰µç™ºæ€§ã‚’è¨ˆç®—
        emergence = metrics.calculate_total_emergence(
            before_graph, after_graph,
            np.array(before_embeddings), np.array(after_embeddings),
            new_texts[:i+1]
        )
        
        if emergence['is_emergence_spike']:
            print(f"ğŸ¯ Emergence Spike detected at step {i}!")
            print(f"   Total score: {emergence['total_score']:.3f}")
            print(f"   Structural: {emergence['structural']['structural_score']:.3f}")
            print(f"   Semantic: {emergence['semantic']['semantic_score']:.3f}")
            print(f"   Information: {emergence['information']['information_score']:.3f}")
    
    return metrics.history


if __name__ == "__main__":
    # ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    print("=== Emergence Metrics Demo ===\n")
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚°ãƒ©ãƒ•ã®ä½œæˆ
    G1 = nx.karate_club_graph()
    G2 = G1.copy()
    G2.add_edges_from([(0, 10), (5, 15), (20, 30)])  # æ–°ã—ã„ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
    
    # ã‚µãƒ³ãƒ—ãƒ«åŸ‹ã‚è¾¼ã¿
    embeddings1 = np.random.randn(34, 128)
    embeddings2 = np.vstack([embeddings1, np.random.randn(5, 128)])
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—
    metrics = EmergenceMetrics()
    result = metrics.calculate_total_emergence(
        G1, G2, embeddings1, embeddings2, 
        ["text1", "text2", "text3", "text4", "text5"]
    )
    
    print("Emergence Analysis Results:")
    print(f"Total Emergence Score: {result['total_score']:.3f}")
    print(f"- Structural: {result['structural']['structural_score']:.3f}")
    print(f"- Semantic: {result['semantic']['semantic_score']:.3f}")
    print(f"- Information: {result['information']['information_score']:.3f}")
    print(f"Spike Detected: {result['is_emergence_spike']}")