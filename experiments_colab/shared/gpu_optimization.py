"""
GPUæœ€é©åŒ–ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
å¤§è¦æ¨¡å®Ÿé¨“ã®ãŸã‚ã®é«˜æ€§èƒ½è¨ˆç®—ã‚µãƒãƒ¼ãƒˆ
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from typing import Dict, Any, Optional, List, Tuple
import numpy as np
from contextlib import contextmanager


class GPUOptimizer:
    """GPUæœ€é©åŒ–ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, device: Optional[str] = None):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.mixed_precision = torch.cuda.is_available()
        self.scaler = torch.cuda.amp.GradScaler() if self.mixed_precision else None
    
    def optimize_model(self, model: nn.Module) -> nn.Module:
        """ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–"""
        model = model.to(self.device)
        
        if self.device == 'cuda':
            # CUDAãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            # ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆPyTorch 2.0+ï¼‰
            if hasattr(torch, 'compile'):
                try:
                    model = torch.compile(model, mode='max-autotune')
                    print("ğŸš€ Model compiled with torch.compile")
                except Exception as e:
                    print(f"âš ï¸ Compilation failed: {e}")
        
        return model
    
    def optimize_dataloader(self, dataset, batch_size: int = 32, 
                          num_workers: int = 2) -> DataLoader:
        """DataLoaderã®æœ€é©åŒ–"""
        
        # GPUç’°å¢ƒã§ã¯å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ä½¿ç”¨
        if self.device == 'cuda':
            batch_size = min(batch_size * 2, 128)
            num_workers = min(num_workers * 2, 8)
        
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=self.device == 'cuda',
            persistent_workers=num_workers > 0
        )
    
    @contextmanager
    def mixed_precision_context(self):
        """è‡ªå‹•æ··åˆç²¾åº¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"""
        if self.mixed_precision:
            with torch.cuda.amp.autocast():
                yield
        else:
            yield
    
    def get_memory_stats(self) -> Dict[str, float]:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨çµ±è¨ˆ"""
        if self.device != 'cuda':
            return {}
        
        return {
            'allocated_gb': torch.cuda.memory_allocated() / 1024**3,
            'reserved_gb': torch.cuda.memory_reserved() / 1024**3,
            'max_allocated_gb': torch.cuda.max_memory_allocated() / 1024**3,
            'utilization_%': (torch.cuda.memory_allocated() / 
                            torch.cuda.get_device_properties(0).total_memory) * 100
        }


class BatchProcessor:
    """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒå‡¦ç†"""
    
    def __init__(self, batch_size: int = 1000, device: str = 'cuda'):
        self.batch_size = batch_size
        self.device = device
    
    def process_embeddings(self, texts: List[str], model, 
                         show_progress: bool = True) -> torch.Tensor:
        """å¤§è¦æ¨¡ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿å‡¦ç†"""
        embeddings = []
        
        if show_progress:
            try:
                from tqdm import tqdm
                iterator = tqdm(range(0, len(texts), self.batch_size), 
                              desc="Processing embeddings")
            except ImportError:
                iterator = range(0, len(texts), self.batch_size)
        else:
            iterator = range(0, len(texts), self.batch_size)
        
        with torch.no_grad():
            for i in iterator:
                batch_texts = texts[i:i + self.batch_size]
                batch_embeddings = model.encode(batch_texts, 
                                              convert_to_tensor=True,
                                              device=self.device)
                embeddings.append(batch_embeddings)
        
        return torch.cat(embeddings, dim=0)
    
    def process_similarity_matrix(self, embeddings: torch.Tensor) -> torch.Tensor:
        """å¤§è¦æ¨¡é¡ä¼¼åº¦è¡Œåˆ—ã®è¨ˆç®—"""
        n = embeddings.size(0)
        similarity_matrix = torch.zeros(n, n, device=self.device)
        
        # ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§è¨ˆç®—ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
        chunk_size = min(self.batch_size, 1000)
        
        for i in range(0, n, chunk_size):
            for j in range(0, n, chunk_size):
                chunk_i = embeddings[i:i+chunk_size]
                chunk_j = embeddings[j:j+chunk_size]
                
                similarity_chunk = torch.mm(chunk_i, chunk_j.t())
                similarity_matrix[i:i+chunk_size, j:j+chunk_size] = similarity_chunk
        
        return similarity_matrix


def optimize_pytorch_settings():
    """PyTorchæœ€é©åŒ–è¨­å®š"""
    if torch.cuda.is_available():
        # CUDAãƒ¡ãƒ¢ãƒªç®¡ç†æœ€é©åŒ–
        torch.cuda.empty_cache()
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        
        # ãƒ¡ãƒ¢ãƒªãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¯¾ç­–
        torch.cuda.set_per_process_memory_fraction(0.9)
        
        print("ğŸš€ PyTorch GPU settings optimized")
    
    # CPUã‚¹ãƒ¬ãƒƒãƒ‰æ•°æœ€é©åŒ–
    torch.set_num_threads(min(torch.get_num_threads(), 8))
    print(f"ğŸ”§ CPU threads: {torch.get_num_threads()}")


def benchmark_operations(device: str = 'cuda', size: int = 1000) -> Dict[str, float]:
    """GPU/CPUæ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    import time
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    x = torch.randn(size, size, device=device)
    y = torch.randn(size, size, device=device)
    
    # è¡Œåˆ—ä¹—ç®—ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    torch.cuda.synchronize() if device == 'cuda' else None
    start_time = time.time()
    
    for _ in range(10):
        result = torch.mm(x, y)
    
    torch.cuda.synchronize() if device == 'cuda' else None
    end_time = time.time()
    
    matrix_multiply_time = (end_time - start_time) / 10
    
    # ãƒ¡ãƒ¢ãƒªè»¢é€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    if device == 'cuda':
        cpu_tensor = torch.randn(size, size)
        
        start_time = time.time()
        for _ in range(10):
            gpu_tensor = cpu_tensor.cuda()
        torch.cuda.synchronize()
        end_time = time.time()
        
        memory_transfer_time = (end_time - start_time) / 10
    else:
        memory_transfer_time = 0.0
    
    return {
        'matrix_multiply_ms': matrix_multiply_time * 1000,
        'memory_transfer_ms': memory_transfer_time * 1000,
        'device': device
    }
