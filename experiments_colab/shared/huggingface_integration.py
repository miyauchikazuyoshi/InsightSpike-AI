"""
HuggingFaceÁµ±Âêà„É¢„Ç∏„É•„Éº„É´
‰∫ãÂâçË®ìÁ∑¥Ê∏à„Åø„É¢„Éá„É´„Å®„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂäπÁéáÁöÑÂà©Áî®
"""

import torch
from transformers import (
    AutoModel, AutoTokenizer, AutoConfig,
    TrainingArguments, Trainer,
    pipeline
)
from sentence_transformers import SentenceTransformer
from datasets import Dataset, load_dataset
from typing import Dict, List, Any, Optional, Tuple
import numpy as np


class HuggingFaceModelManager:
    """HuggingFace„É¢„Éá„É´ÁÆ°ÁêÜ„ÇØ„É©„Çπ"""
    
    def __init__(self, device: str = 'cuda'):
        self.device = device
        self.models = {}
        self.tokenizers = {}
    
    def load_sentence_transformer(self, model_name: str = 'all-MiniLM-L6-v2') -> SentenceTransformer:
        """Sentence Transformer„É¢„Éá„É´„ÅÆ„É≠„Éº„Éâ"""
        if model_name not in self.models:
            model = SentenceTransformer(model_name, device=self.device)
            self.models[model_name] = model
            print(f"‚úÖ Loaded SentenceTransformer: {model_name}")
        
        return self.models[model_name]
    
    def load_language_model(self, model_name: str = 'microsoft/DialoGPT-medium') -> Tuple[AutoModel, AutoTokenizer]:
        """Ë®ÄË™û„É¢„Éá„É´„Å®„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÅÆ„É≠„Éº„Éâ"""
        if model_name not in self.models:
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModel.from_pretrained(model_name).to(self.device)
            
            self.models[model_name] = model
            self.tokenizers[model_name] = tokenizer
            print(f"‚úÖ Loaded Language Model: {model_name}")
        
        return self.models[model_name], self.tokenizers[model_name]
    
    def create_pipeline(self, task: str, model_name: str = None) -> pipeline:
        """HuggingFace pipeline„ÅÆ‰ΩúÊàê"""
        return pipeline(
            task,
            model=model_name,
            device=0 if self.device == 'cuda' else -1,
            torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32
        )
    
    def get_embeddings_batch(self, texts: List[str], model_name: str = 'all-MiniLM-L6-v2',
                           batch_size: int = 32) -> np.ndarray:
        """„Éê„ÉÉ„ÉÅÂá¶ÁêÜ„Åß„ÅÆÂüã„ÇÅËæº„ÅøÁîüÊàê"""
        model = self.load_sentence_transformer(model_name)
        
        embeddings = model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True,
            device=self.device
        )
        
        return embeddings


class HuggingFaceDatasetManager:
    """HuggingFace„Éá„Éº„Çø„Çª„ÉÉ„ÉàÁÆ°ÁêÜ„ÇØ„É©„Çπ"""
    
    def __init__(self):
        self.datasets = {}
    
    def load_text_dataset(self, dataset_name: str = 'wikitext', 
                         config: str = 'wikitext-2-raw-v1',
                         split: str = 'train') -> Dataset:
        """„ÉÜ„Ç≠„Çπ„Éà„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„É≠„Éº„Éâ"""
        key = f"{dataset_name}_{config}_{split}"
        
        if key not in self.datasets:
            dataset = load_dataset(dataset_name, config, split=split)
            self.datasets[key] = dataset
            print(f"‚úÖ Loaded dataset: {dataset_name} ({len(dataset)} samples)")
        
        return self.datasets[key]
    
    def create_custom_dataset(self, texts: List[str], labels: Optional[List] = None) -> Dataset:
        """„Ç´„Çπ„Çø„É†„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ‰ΩúÊàê"""
        data_dict = {"text": texts}
        if labels is not None:
            data_dict["labels"] = labels
        
        dataset = Dataset.from_dict(data_dict)
        print(f"‚úÖ Created custom dataset: {len(dataset)} samples")
        
        return dataset
    
    def preprocess_for_embeddings(self, dataset: Dataset, 
                                text_column: str = 'text',
                                max_length: int = 512) -> Dataset:
        """Âüã„ÇÅËæº„ÅøÁî®ÂâçÂá¶ÁêÜ"""
        def preprocess_function(examples):
            # „ÉÜ„Ç≠„Çπ„ÉàÈï∑Âà∂Èôê
            texts = examples[text_column]
            processed_texts = [text[:max_length] if len(text) > max_length else text 
                             for text in texts]
            return {"processed_text": processed_texts}
        
        processed_dataset = dataset.map(preprocess_function, batched=True)
        print(f"‚úÖ Preprocessed dataset for embeddings")
        
        return processed_dataset


class ScalableRAGSystem:
    """„Çπ„Ç±„Éº„É©„Éñ„É´RAG„Ç∑„Çπ„ÉÜ„É†"""
    
    def __init__(self, device: str = 'cuda'):
        self.device = device
        self.model_manager = HuggingFaceModelManager(device)
        self.dataset_manager = HuggingFaceDatasetManager()
        self.embeddings_cache = {}
    
    def setup_retrieval_system(self, documents: List[str],
                             embedding_model: str = 'all-MiniLM-L6-v2') -> Dict[str, Any]:
        """Ê§úÁ¥¢„Ç∑„Çπ„ÉÜ„É†„ÅÆ„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó"""
        # ÊñáÊõ∏Âüã„ÇÅËæº„ÅøÁîüÊàê
        embeddings = self.model_manager.get_embeddings_batch(
            documents, 
            model_name=embedding_model,
            batch_size=64
        )
        
        # FAISS „Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ‰ΩúÊàê
        try:
            import faiss
            
            dimension = embeddings.shape[1]
            index = faiss.IndexFlatIP(dimension)  # ÂÜÖÁ©çÔºà„Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶Ôºâ
            
            # GPUÂà©Áî®ÂèØËÉΩ„Å™Â†¥Âêà„ÅØGPU„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ‰ΩøÁî®
            if self.device == 'cuda' and faiss.get_num_gpus() > 0:
                index = faiss.index_cpu_to_gpu(faiss.StandardGpuResources(), 0, index)
                print("üöÄ Using FAISS GPU index")
            
            # Ê≠£Ë¶èÂåñ„Åó„Å¶ËøΩÂä†
            faiss.normalize_L2(embeddings)
            index.add(embeddings.astype(np.float32))
            
            return {
                'index': index,
                'embeddings': embeddings,
                'documents': documents,
                'embedding_model': embedding_model
            }
            
        except ImportError:
            print("‚ö†Ô∏è FAISS not available, using simple similarity search")
            return {
                'embeddings': embeddings,
                'documents': documents,
                'embedding_model': embedding_model
            }
    
    def retrieve_documents(self, query: str, retrieval_system: Dict[str, Any],
                         top_k: int = 5) -> List[Tuple[str, float]]:
        """ÊñáÊõ∏Ê§úÁ¥¢"""
        # „ÇØ„Ç®„É™Âüã„ÇÅËæº„ÅøÁîüÊàê
        query_embedding = self.model_manager.get_embeddings_batch(
            [query], 
            model_name=retrieval_system['embedding_model']
        )
        
        if 'index' in retrieval_system:
            # FAISSÊ§úÁ¥¢
            faiss.normalize_L2(query_embedding)
            scores, indices = retrieval_system['index'].search(
                query_embedding.astype(np.float32), top_k
            )
            
            results = [
                (retrieval_system['documents'][idx], score)
                for idx, score in zip(indices[0], scores[0])
            ]
        else:
            # ÂçòÁ¥îÈ°û‰ººÂ∫¶Ê§úÁ¥¢
            embeddings = retrieval_system['embeddings']
            similarities = np.dot(embeddings, query_embedding.T).flatten()
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            
            results = [
                (retrieval_system['documents'][idx], similarities[idx])
                for idx in top_indices
            ]
        
        return results


def benchmark_huggingface_models(device: str = 'cuda') -> Dict[str, Dict[str, float]]:
    """HuggingFace„É¢„Éá„É´„ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ"""
    import time
    
    model_manager = HuggingFaceModelManager(device)
    
    # „ÉÜ„Çπ„Éà„Éá„Éº„Çø
    test_texts = [
        "This is a test sentence for benchmarking.",
        "HuggingFace models are powerful for NLP tasks.",
        "GPU acceleration significantly improves performance."
    ] * 100  # 300„ÉÜ„Ç≠„Çπ„Éà„Åß„ÉÜ„Çπ„Éà
    
    results = {}
    
    # Sentence Transformer „Éô„É≥„ÉÅ„Éû„Éº„ÇØ
    models_to_test = [
        'all-MiniLM-L6-v2',
        'all-mpnet-base-v2',
        'paraphrase-multilingual-MiniLM-L12-v2'
    ]
    
    for model_name in models_to_test:
        try:
            start_time = time.time()
            embeddings = model_manager.get_embeddings_batch(
                test_texts[:50],  # 50„ÉÜ„Ç≠„Çπ„Éà„Åß„ÉÜ„Çπ„Éà
                model_name=model_name,
                batch_size=16
            )
            end_time = time.time()
            
            results[model_name] = {
                'processing_time_s': end_time - start_time,
                'texts_per_second': 50 / (end_time - start_time),
                'embedding_dimension': embeddings.shape[1]
            }
            
            print(f"‚úÖ {model_name}: {results[model_name]['texts_per_second']:.1f} texts/sec")
            
        except Exception as e:
            print(f"‚ùå {model_name}: {e}")
            results[model_name] = {'error': str(e)}
    
    return results
