{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7116f92c",
   "metadata": {},
   "source": [
    "# ğŸ§  Phase 1: Dynamic Memory Construction - GPU Large Scale Experiment\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook executes InsightSpike-AI's Phase 1 experiments on Google Colab's GPU environment at large scale.\n",
    "\n",
    "### ğŸ¯ Experiment Goals\n",
    "- **Large-scale Dynamic Memory Construction**: High-performance embedding generation using HuggingFace Transformers\n",
    "- **GPU Optimization**: 3-10x speedup with CUDA acceleration\n",
    "- **Scalable Implementation**: 10-100x larger data size support compared to baseline\n",
    "\n",
    "### ğŸš€ Key Features\n",
    "- ğŸ¤— **HuggingFace Integration**: Sentence Transformers, Datasets, Transformers\n",
    "- ğŸ”¥ **GPU Acceleration**: PyTorch CUDA, FAISS GPU, batch processing optimization\n",
    "- ğŸ“Š **Large-scale Visualization**: Plotly interactive graphs\n",
    "- ğŸ’¾ **Result Management**: Google Drive integration, experiment tracking\n",
    "\n",
    "---\n",
    "\n",
    "**Execution Environment**: Google Colab GPU (T4/V100)  \n",
    "**Estimated Runtime**: 15-30 minutes  \n",
    "**GPU Memory Requirement**: 8GB+ recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e5aa8e",
   "metadata": {},
   "source": [
    "## ğŸ”§ Unified Setup\n",
    "Execute the InsightSpike-AI unified setup script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb5af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute unified setup script\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Clone or verify InsightSpike-AI repository\n",
    "if not os.path.exists('/content/InsightSpike-AI'):\n",
    "    print(\"ğŸ“‚ Cloning InsightSpike-AI repository...\")\n",
    "    !git clone https://github.com/miyauchi-kazuyoshi/InsightSpike-AI.git /content/InsightSpike-AI\n",
    "    os.chdir('/content/InsightSpike-AI')\n",
    "else:\n",
    "    print(\"ğŸ“‚ Repository already exists\")\n",
    "    os.chdir('/content/InsightSpike-AI')\n",
    "\n",
    "# Execute unified setup script\n",
    "print(\"ğŸš€ Executing unified setup script...\")\n",
    "try:\n",
    "    # Grant execution permission to setup script\n",
    "    !chmod +x scripts/colab/setup_unified.sh\n",
    "    \n",
    "    # Execute setup\n",
    "    result = subprocess.run(['bash', 'scripts/colab/setup_unified.sh'], \n",
    "                          capture_output=True, text=True, cwd='/content/InsightSpike-AI')\n",
    "    \n",
    "    print(\"ğŸ“‹ Setup output:\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "    if result.stderr:\n",
    "        print(\"âš ï¸ Warnings/Errors:\")\n",
    "        print(result.stderr)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… Unified setup completed!\")\n",
    "    else:\n",
    "        print(f\"âŒ Setup error (exit code: {result.returncode})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Setup script execution failed: {e}\")\n",
    "    print(\"ğŸ”„ Falling back to individual installation...\")\n",
    "    \n",
    "    # Fallback: individual library installation\n",
    "    phase1_libraries = [\n",
    "        \"transformers[torch]\",\n",
    "        \"sentence-transformers\", \n",
    "        \"datasets\",\n",
    "        \"accelerate\",\n",
    "        \"plotly\",\n",
    "        \"seaborn\",\n",
    "        \"networkx\",\n",
    "        \"scipy\", \n",
    "        \"scikit-learn\",\n",
    "        \"faiss-gpu\" if os.system(\"nvidia-smi\") == 0 else \"faiss-cpu\",\n",
    "        \"tqdm\",\n",
    "        \"rich\",\n",
    "        \"imageio\"\n",
    "    ]\n",
    "    \n",
    "    for lib in phase1_libraries:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", lib, \"--upgrade\", \"-q\"])\n",
    "            print(f\"âœ… {lib}\")\n",
    "        except Exception as install_error:\n",
    "            print(f\"âš ï¸ {lib} installation failed: {install_error}\")\n",
    "\n",
    "# Python path configuration\n",
    "import sys\n",
    "sys.path.insert(0, '/content/InsightSpike-AI/src')\n",
    "sys.path.insert(0, '/content/InsightSpike-AI/experiments_colab/shared')\n",
    "\n",
    "print(\"ğŸ¯ Phase1 environment setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9072409a",
   "metadata": {},
   "source": [
    "## ğŸ§ª InsightSpike-AI Command Usage Verification\n",
    "Test whether InsightSpike-AI commands and modules can be used correctly after repository cloning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc3899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InsightSpike-AI command and module usage verification\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(\"ğŸ§ª InsightSpike-AI Usage Verification Test\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Python path verification\n",
    "print(\"\\nğŸ“ Python Path Verification:\")\n",
    "for i, path in enumerate(sys.path[:10]):  # Display first 10 paths\n",
    "    if 'InsightSpike-AI' in path:\n",
    "        print(f\"  âœ… {i}: {path}\")\n",
    "    else:\n",
    "        print(f\"  ğŸ“ {i}: {path}\")\n",
    "\n",
    "# 2. InsightSpike-AI module import test\n",
    "print(\"\\nğŸ“¦ Module Import Test:\")\n",
    "\n",
    "# Core modules\n",
    "modules_to_test = [\n",
    "    ('insightspike.config', 'config'),\n",
    "    ('insightspike.core', 'core'),\n",
    "    ('insightspike.core.agents.main_agent', 'MainAgent'),\n",
    "    ('insightspike.core.learning.knowledge_graph_memory', 'KnowledgeGraphMemory'),\n",
    "    ('insightspike.cli.main', 'CLI main'),\n",
    "]\n",
    "\n",
    "for module_name, display_name in modules_to_test:\n",
    "    try:\n",
    "        __import__(module_name)\n",
    "        print(f\"  âœ… {display_name}: Import successful\")\n",
    "    except ImportError as e:\n",
    "        print(f\"  âŒ {display_name}: Import failed - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ {display_name}: Unexpected error - {e}\")\n",
    "\n",
    "# 3. CLI command verification\n",
    "print(\"\\nğŸ”§ CLI Command Verification:\")\n",
    "\n",
    "# Method 1: Direct insightspike command execution\n",
    "try:\n",
    "    result = subprocess.run(['insightspike', '--help'], \n",
    "                          capture_output=True, text=True, timeout=10)\n",
    "    if result.returncode == 0:\n",
    "        print(\"  âœ… 'insightspike' command: Available\")\n",
    "        print(f\"     Output example: {result.stdout[:100]}...\")\n",
    "    else:\n",
    "        print(\"  âŒ 'insightspike' command: Execution error\")\n",
    "except FileNotFoundError:\n",
    "    print(\"  âš ï¸ 'insightspike' command: Not found in PATH\")\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ 'insightspike' command: {e}\")\n",
    "\n",
    "# Method 2: Execution via python -m\n",
    "try:\n",
    "    result = subprocess.run([sys.executable, '-m', 'insightspike.cli.main', '--help'], \n",
    "                          capture_output=True, text=True, timeout=10, \n",
    "                          cwd='/content/InsightSpike-AI')\n",
    "    if result.returncode == 0:\n",
    "        print(\"  âœ… 'python -m insightspike.cli.main': Available\")\n",
    "        print(f\"     Output example: {result.stdout[:100]}...\")\n",
    "    else:\n",
    "        print(\"  âŒ 'python -m insightspike.cli.main': Execution error\")\n",
    "        if result.stderr:\n",
    "            print(f\"     Error: {result.stderr[:200]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ Python module CLI: {e}\")\n",
    "\n",
    "# 4. MainAgent initialization test\n",
    "print(\"\\nğŸ§  MainAgent Initialization Test:\")\n",
    "try:\n",
    "    # Add system path (just in case)\n",
    "    if '/content/InsightSpike-AI/src' not in sys.path:\n",
    "        sys.path.insert(0, '/content/InsightSpike-AI/src')\n",
    "    \n",
    "    from insightspike.core.agents.main_agent import MainAgent\n",
    "    \n",
    "    # Simple initialization test\n",
    "    agent = MainAgent()\n",
    "    print(\"  âœ… MainAgent: Initialization successful\")\n",
    "    \n",
    "    # Simple functionality test\n",
    "    test_query = \"This is a test query for InsightSpike-AI\"\n",
    "    try:\n",
    "        response = agent.process_query(test_query)\n",
    "        print(f\"  âœ… MainAgent.process_query(): Operation confirmed\")\n",
    "        print(f\"     Response example: {str(response)[:100]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ MainAgent.process_query(): {e}\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"  âŒ MainAgent import failed: {e}\")\n",
    "    print(\"  ğŸ’¡ FAISS might be missing. Will fix in next cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ MainAgent initialization error: {e}\")\n",
    "\n",
    "# 5. Execution environment summary\n",
    "print(\"\\nğŸ“‹ Execution Environment Summary:\")\n",
    "print(f\"  ğŸ“ Working directory: {os.getcwd()}\")\n",
    "print(f\"  ğŸ“ InsightSpike-AI directory exists: {os.path.exists('/content/InsightSpike-AI')}\")\n",
    "print(f\"  ğŸ Python executable path: {sys.executable}\")\n",
    "print(f\"  ğŸ“¦ InsightSpike-related in sys.path: {[p for p in sys.path if 'InsightSpike' in p]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… InsightSpike-AI usage verification test completed\")\n",
    "print(\"ğŸ’¡ If there are errors, please execute the fix methods in the next cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d47025e",
   "metadata": {},
   "source": [
    "## ğŸ”§ FAISS Problem Fix and Fallback Implementation\n",
    "Fix FAISS-related errors and provide alternative implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eec145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS problem fix and InsightSpike-AI operation verification\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"ğŸ”§ Fixing FAISS problems...\")\n",
    "\n",
    "# FAISS reinstallation (GPU/CPU automatic switching)\n",
    "try:\n",
    "    # Check GPU availability\n",
    "    gpu_available = subprocess.run(['nvidia-smi'], capture_output=True).returncode == 0\n",
    "    \n",
    "    # Uninstall existing FAISS\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', 'faiss-cpu', 'faiss-gpu', '-y'], \n",
    "                  capture_output=True)\n",
    "    \n",
    "    # Install appropriate FAISS\n",
    "    faiss_package = 'faiss-gpu' if gpu_available else 'faiss-cpu'\n",
    "    result = subprocess.run([sys.executable, '-m', 'pip', 'install', faiss_package, '-q'])\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"âœ… {faiss_package} installation successful\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ {faiss_package} installation failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ FAISS installation error: {e}\")\n",
    "\n",
    "# InsightSpike-AI environment reconfiguration\n",
    "print(\"\\nğŸ”„ Reconfiguring InsightSpike-AI environment...\")\n",
    "\n",
    "# Path reconfiguration\n",
    "sys.path.insert(0, '/content/InsightSpike-AI/src')\n",
    "os.chdir('/content/InsightSpike-AI')\n",
    "\n",
    "# InsightSpike-AI ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª\n",
    "try:\n",
    "    # æ­£ã—ã„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ‘ã‚¹ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "    from insightspike.core.agents.main_agent import MainAgent\n",
    "    from insightspike.core.learning.knowledge_graph_memory import KnowledgeGraphMemory\n",
    "    from insightspike.config import Config\n",
    "    from insightspike.utils import logger\n",
    "    \n",
    "    print(\"âœ… InsightSpike-AI modules imported successfully!\")\n",
    "    print(\"Available modules:\")\n",
    "    print(\"- MainAgent:\", MainAgent)\n",
    "    print(\"- KnowledgeGraphMemory:\", KnowledgeGraphMemory)\n",
    "    print(\"- Config:\", Config)\n",
    "    print(\"- logger:\", logger)\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import error: {e}\")\n",
    "    print(\"Available insightspike modules:\")\n",
    "    import insightspike\n",
    "    print(dir(insightspike))\n",
    "    \n",
    "    # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹é€ ã‚’ç¢ºèª\n",
    "    import os\n",
    "    import sys\n",
    "    for path in sys.path:\n",
    "        if 'InsightSpike-AI' in path:\n",
    "            print(f\"Python path: {path}\")\n",
    "            if os.path.exists(path + '/insightspike'):\n",
    "                print(\"  - insightspike found\")\n",
    "                print(\"  - submodules:\", os.listdir(path + '/insightspike'))\n",
    "\n",
    "# MainAgentå†ãƒ†ã‚¹ãƒˆ\n",
    "print(\"\\nğŸ§  MainAgent Retest:\")\n",
    "try:\n",
    "    import importlib\n",
    "    \n",
    "    # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒªãƒ­ãƒ¼ãƒ‰\n",
    "    if 'insightspike.core.agents.main_agent' in sys.modules:\n",
    "        importlib.reload(sys.modules['insightspike.core.agents.main_agent'])\n",
    "    \n",
    "    from insightspike.core.agents.main_agent import MainAgent\n",
    "    \n",
    "    # MainAgentåˆæœŸåŒ–\n",
    "    agent = MainAgent()\n",
    "    print(\"âœ… MainAgent initialization successful!\")\n",
    "    \n",
    "    # ç°¡å˜ãªå‡¦ç†ãƒ†ã‚¹ãƒˆ\n",
    "    test_result = agent.process_query(\"Simple test query\")\n",
    "    print(f\"âœ… MainAgent operation confirmed: {str(test_result)[:100]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ MainAgent ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    print(\"\\nğŸ”„ Using fallback implementation...\")\n",
    "    \n",
    "    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãªMainAgentä»£æ›¿\n",
    "    class SimpleMainAgent:\n",
    "        \"\"\"Simple alternative implementation for MainAgent\"\"\"\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.memory = {}\n",
    "            print(\"âœ… SimpleMainAgent åˆæœŸåŒ–å®Œäº†\")\n",
    "        \n",
    "        def process_query(self, query: str) -> str:\n",
    "            \"\"\"Alternative implementation for query processing\"\"\"\n",
    "            return f\"SimpleMainAgent response to: {query[:50]}...\"\n",
    "        \n",
    "        def store_insight(self, insight_data):\n",
    "            \"\"\"Alternative implementation for insight storage\"\"\"\n",
    "            insight_id = f\"insight_{len(self.memory)}\"\n",
    "            self.memory[insight_id] = insight_data\n",
    "            return insight_id\n",
    "    \n",
    "    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ©ç”¨\n",
    "    agent = SimpleMainAgent()\n",
    "    test_result = agent.process_query(\"Test query\")\n",
    "    print(f\"âœ… SimpleMainAgent å‹•ä½œç¢ºèª: {test_result}\")\n",
    "\n",
    "# CLIå‹•ä½œç¢ºèª\n",
    "print(\"\\nğŸ’» CLI Operation Verification:\")\n",
    "try:\n",
    "    # CLIåŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ\n",
    "    result = subprocess.run([\n",
    "        sys.executable, '-m', 'insightspike.cli.main', '--help'\n",
    "    ], capture_output=True, text=True, cwd='/content/InsightSpike-AI', timeout=15)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… CLI basic functionality: Operation confirmed\")\n",
    "        # ãƒ˜ãƒ«ãƒ—ã®æœ€åˆã®æ•°è¡Œã‚’è¡¨ç¤º\n",
    "        help_lines = result.stdout.split('\\n')[:5]\n",
    "        for line in help_lines:\n",
    "            if line.strip():\n",
    "                print(f\"     {line}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ CLIå‹•ä½œã«å•é¡ŒãŒã‚ã‚Šã¾ã™\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ CLI ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "# æœ€çµ‚ç¢ºèª\n",
    "print(f\"\\nğŸ“‹ Final Verification:\")\n",
    "print(f\"  ğŸ“ Working directory: {os.getcwd()}\")\n",
    "print(f\"  ğŸ§  MainAgent: {'âœ… Available' if 'agent' in locals() else 'âŒ Unavailable'}\")\n",
    "print(f\"  ğŸ“¦ InsightSpike modules: {len([m for m in sys.modules.keys() if 'insightspike' in m])} loaded\")\n",
    "\n",
    "print(\"\\nâœ… FAISS problem fix and InsightSpike-AI environment verification completed!\")\n",
    "print(\"ğŸš€ Ready to start experiments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818a4aa8",
   "metadata": {},
   "source": [
    "## 1. ğŸš€ Google Colab GPU Environment Setup\n",
    "\n",
    "First, verify the GPU environment and perform necessary basic configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c87380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU environment verification\n",
    "import torch\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(\"ğŸš€ System Information\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"âš ï¸ GPU not available. Please enable GPU in Colab: Runtime > Change runtime type > GPU\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initial GPU configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"âœ… GPU optimization enabled\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"ğŸ“± Running on CPU mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeca5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone InsightSpike-AI repository\n",
    "import os\n",
    "\n",
    "repo_url = \"https://github.com/miyauchi-kazuyoshi/InsightSpike-AI.git\"\n",
    "repo_dir = \"InsightSpike-AI\"\n",
    "\n",
    "if not os.path.exists(repo_dir):\n",
    "    print(\"ğŸ“¥ Cloning InsightSpike-AI repository...\")\n",
    "    !git clone {repo_url}\n",
    "    print(\"âœ… Repository cloned successfully\")\n",
    "else:\n",
    "    print(\"ğŸ“ Repository already exists\")\n",
    "\n",
    "# Change directory\n",
    "os.chdir(repo_dir)\n",
    "print(f\"ğŸ“‚ Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Python path configuration\n",
    "import sys\n",
    "sys.path.insert(0, '/content/InsightSpike-AI/src')\n",
    "sys.path.insert(0, '/content/InsightSpike-AI/experiments')\n",
    "sys.path.insert(0, '/content/InsightSpike-AI/experiments_colab/shared')\n",
    "print(\"âœ… Python paths configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12c1581",
   "metadata": {},
   "source": [
    "## 2. ğŸ¤— HuggingFace Library Installation and Import\n",
    "\n",
    "Install HuggingFace libraries and GPU optimization libraries required for large-scale experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install GPU-optimized libraries\n",
    "print(\"ğŸ“¦ Installing GPU-optimized libraries...\")\n",
    "\n",
    "# HuggingFace libraries (GPU compatible)\n",
    "!pip install transformers[torch] --upgrade\n",
    "!pip install sentence-transformers --upgrade\n",
    "!pip install datasets --upgrade\n",
    "!pip install accelerate --upgrade\n",
    "\n",
    "# Visualization and analysis libraries\n",
    "!pip install plotly seaborn\n",
    "!pip install networkx scipy scikit-learn\n",
    "\n",
    "# GPU FAISS (high-speed similarity search)\n",
    "!pip install faiss-gpu\n",
    "\n",
    "# Other utilities\n",
    "!pip install tqdm rich imageio\n",
    "\n",
    "print(\"âœ… All libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21536632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library import and initialization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# HuggingFace libraries\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# GPU optimization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import faiss\n",
    "\n",
    "# Utilities\n",
    "import time\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Colab utilities (custom modules)\n",
    "try:\n",
    "    from colab_utils import setup_colab_environment, print_environment_info\n",
    "    from gpu_optimization import GPUOptimizer, BatchProcessor\n",
    "    from huggingface_integration import HuggingFaceModelManager, ScalableRAGSystem\n",
    "    print(\"âœ… Custom utilities loaded\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Custom utilities not available, using basic functions\")\n",
    "\n",
    "# Environment configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸš€ Using device: {device}\")\n",
    "\n",
    "# Plot configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "print(\"âœ… All libraries imported and configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec58ca5c",
   "metadata": {},
   "source": [
    "## 3. ğŸ“Š Large-scale Dataset Preparation\n",
    "\n",
    "Obtain large-scale text data from HuggingFace Datasets and preprocess it for dynamic memory construction experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe888429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å–å¾—\n",
    "print(\"ğŸ“¥ Loading large-scale datasets...\")\n",
    "\n",
    "# WikiText ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå¤§è¦æ¨¡ãƒ†ã‚­ã‚¹ãƒˆï¼‰\n",
    "try:\n",
    "    wikitext_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "    print(f\"âœ… WikiText-103 loaded: {len(wikitext_dataset)} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ WikiText loading failed: {e}\")\n",
    "    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "    wikitext_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    print(f\"ğŸ“± Using WikiText-2: {len(wikitext_dataset)} samples\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†\n",
    "def preprocess_texts(dataset, max_samples=10000, min_length=50):\n",
    "    \"\"\"ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†\"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    print(f\"ğŸ”„ Preprocessing {min(max_samples, len(dataset))} samples...\")\n",
    "    \n",
    "    for i, sample in enumerate(tqdm(dataset)):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "            \n",
    "        text = sample['text'].strip()\n",
    "        \n",
    "        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "        if len(text) >= min_length and not text.startswith('='):\n",
    "            # æ”¹è¡Œã‚„ä½™åˆ†ãªç©ºç™½ã‚’æ•´ç†\n",
    "            text = ' '.join(text.split())\n",
    "            texts.append(text)\n",
    "    \n",
    "    return texts\n",
    "\n",
    "# å®Ÿé¨“ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
    "SAMPLE_SIZE = 5000  # GPUç’°å¢ƒã«åˆã‚ã›ã¦èª¿æ•´\n",
    "experiment_texts = preprocess_texts(wikitext_dataset, max_samples=SAMPLE_SIZE)\n",
    "\n",
    "print(f\"âœ… Processed dataset: {len(experiment_texts)} texts\")\n",
    "print(f\"ğŸ“ Sample text: {experiment_texts[0][:200]}...\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ\n",
    "text_lengths = [len(text) for text in experiment_texts]\n",
    "print(f\"ğŸ“Š Text length stats:\")\n",
    "print(f\"  - Mean: {np.mean(text_lengths):.1f} chars\")\n",
    "print(f\"  - Median: {np.median(text_lengths):.1f} chars\")\n",
    "print(f\"  - Max: {max(text_lengths)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fe29d5",
   "metadata": {},
   "source": [
    "## 4. ğŸ¤— High-performance Model Selection and Loading\n",
    "\n",
    "Load multiple GPU-optimized Sentence Transformer models and perform performance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55874ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã®é¸æŠã¨ãƒ­ãƒ¼ãƒ‰\n",
    "models_config = {\n",
    "    'fast': 'all-MiniLM-L6-v2',                    # é«˜é€Ÿãƒ»è»½é‡\n",
    "    'balanced': 'all-mpnet-base-v2',               # ãƒãƒ©ãƒ³ã‚¹\n",
    "    'multilingual': 'paraphrase-multilingual-MiniLM-L12-v2',  # å¤šè¨€èª\n",
    "    'large': 'all-MiniLM-L12-v2'                   # é«˜ç²¾åº¦\n",
    "}\n",
    "\n",
    "loaded_models = {}\n",
    "model_stats = {}\n",
    "\n",
    "print(\"ğŸ¤— Loading Sentence Transformer models...\")\n",
    "\n",
    "for model_name, model_id in models_config.items():\n",
    "    try:\n",
    "        print(f\"\\nğŸ“¥ Loading {model_name}: {model_id}\")\n",
    "        \n",
    "        # ã‚¿ã‚¤ãƒãƒ¼é–‹å§‹\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆGPUå¯¾å¿œï¼‰\n",
    "        model = SentenceTransformer(model_id, device=device)\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        loaded_models[model_name] = model\n",
    "        model_stats[model_name] = {\n",
    "            'model_id': model_id,\n",
    "            'load_time': load_time,\n",
    "            'max_seq_length': model.max_seq_length,\n",
    "            'embedding_dimension': model.get_sentence_embedding_dimension()\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… {model_name} loaded in {load_time:.2f}s\")\n",
    "        print(f\"   - Max sequence length: {model.max_seq_length}\")\n",
    "        print(f\"   - Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load {model_name}: {e}\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«çµ±è¨ˆã®è¡¨ç¤º\n",
    "print(\"\\nğŸ“Š Model Statistics:\")\n",
    "stats_df = pd.DataFrame(model_stats).T\n",
    "print(stats_df)\n",
    "\n",
    "# GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nğŸ”¥ GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB allocated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f42bb3",
   "metadata": {},
   "source": [
    "## ğŸ“Š Experiment Parameter Configuration\n",
    "Configure parameters and settings for dynamic memory construction experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameter configuration\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Experiment configuration\n",
    "EXPERIMENT_CONFIG = {\n",
    "    \"experiment_name\": \"phase1_dynamic_memory_colab\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"gpu_enabled\": torch.cuda.is_available(),\n",
    "    \"device\": device,\n",
    "    \n",
    "    # Dataset configuration\n",
    "    \"datasets\": [\"wikitext\", \"synthetic_docs\"],\n",
    "    \"document_sizes\": [50, 100, 200, 500],\n",
    "    \"document_types\": [\"technical\", \"scientific\", \"general\"],\n",
    "    \n",
    "    # Model configuration\n",
    "    \"embedding_models\": [\n",
    "        \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    ],\n",
    "    \"embedding_dim\": 384,\n",
    "    \n",
    "    # Performance targets\n",
    "    \"targets\": {\n",
    "        \"speed_improvement\": 0.30,  # 30% speedup\n",
    "        \"memory_efficiency\": 0.40,  # 40% memory reduction\n",
    "        \"accuracy_improvement\": 0.15  # 15% accuracy improvement\n",
    "    },\n",
    "    \n",
    "    # Benchmark configuration\n",
    "    \"benchmark_iterations\": 3,\n",
    "    \"warmup_iterations\": 1,\n",
    "    \"timeout_seconds\": 300\n",
    "}\n",
    "\n",
    "print(\"ğŸ”§ Experiment configuration completed:\")\n",
    "print(json.dumps(EXPERIMENT_CONFIG, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c87e1ba",
   "metadata": {},
   "source": [
    "## ğŸ”„ Baseline Implementation\n",
    "Implement a simple RAG system as a comparison baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d413430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³RAGã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…\n",
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class MemoryMetrics:\n",
    "    build_time: float\n",
    "    memory_usage: float\n",
    "    num_documents: int\n",
    "    num_facts: int\n",
    "    accuracy_score: float = 0.0\n",
    "    retention_score: float = 0.0\n",
    "\n",
    "class BaselineRAGSystem:\n",
    "    \"\"\"ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³RAGã‚·ã‚¹ãƒ†ãƒ \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "        self.index = {}\n",
    "        \n",
    "    def _process_document(self, doc: str) -> Dict[str, Any]:\n",
    "        \"\"\"åŸºæœ¬çš„ãªæ–‡æ›¸å‡¦ç†\"\"\"\n",
    "        # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†\n",
    "        processed_text = doc.strip().lower()\n",
    "        sentences = processed_text.split('.')\n",
    "        \n",
    "        return {\n",
    "            'text': doc,\n",
    "            'processed_text': processed_text,\n",
    "            'sentences': [s.strip() for s in sentences if s.strip()],\n",
    "            'length': len(doc)\n",
    "        }\n",
    "    \n",
    "    def build_memory(self, documents: List[str]) -> MemoryMetrics:\n",
    "        \"\"\"è¨˜æ†¶æ§‹ç¯‰ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰\"\"\"\n",
    "        start_time = time.time()\n",
    "        process = psutil.Process()\n",
    "        memory_start = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        # æ–‡æ›¸å‡¦ç†ã¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–\n",
    "        for i, doc in enumerate(documents):\n",
    "            processed_doc = self._process_document(doc)\n",
    "            self.documents.append(processed_doc)\n",
    "            \n",
    "            # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ\n",
    "            embedding = self.embedding_model.encode(doc)\n",
    "            self.embeddings.append(embedding)\n",
    "            self.index[i] = processed_doc\n",
    "            \n",
    "            # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "            if device.type == 'cuda' and i % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—\n",
    "        build_time = time.time() - start_time\n",
    "        memory_end = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        memory_usage = memory_end - memory_start\n",
    "        \n",
    "        # åŸºæœ¬çš„ãªãƒ•ã‚¡ã‚¯ãƒˆæ•°è¨ˆç®—ï¼ˆæ–‡æ•°ã¨ã—ã¦è¿‘ä¼¼ï¼‰\n",
    "        total_facts = sum(len(doc['sentences']) for doc in self.documents)\n",
    "        \n",
    "        return MemoryMetrics(\n",
    "            build_time=build_time,\n",
    "            memory_usage=memory_usage,\n",
    "            num_documents=len(documents),\n",
    "            num_facts=total_facts,\n",
    "            accuracy_score=0.8,  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æƒ³å®šå€¤\n",
    "            retention_score=0.75  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æƒ³å®šå€¤\n",
    "        )\n",
    "\n",
    "print(\"âœ… ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³RAGã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89573807",
   "metadata": {},
   "source": [
    "## ğŸ§  InsightSpike Dynamic Memory System\n",
    "Implement InsightSpike-AI's dynamic memory construction system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686926d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InsightSpikeå‹•çš„è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…\n",
    "import sys\n",
    "sys.path.append('/content/InsightSpike-AI/src')\n",
    "\n",
    "from memory.memory_manager import MemoryManager\n",
    "from agents.main_agent import MainAgent\n",
    "from core.insight_extractor import InsightExtractor\n",
    "\n",
    "class InsightSpikeMemorySystem:\n",
    "    \"\"\"InsightSpikeå‹•çš„è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model):\n",
    "        self.embedding_model = embedding_model\n",
    "        \n",
    "        # GPUæœ€é©åŒ–ã•ã‚ŒãŸè¨­å®š\n",
    "        self.config = {\n",
    "            'memory_manager': {\n",
    "                'max_cache_size': 1000 if device.type == 'cuda' else 100,\n",
    "                'enable_gpu_acceleration': device.type == 'cuda',\n",
    "                'batch_size': 32 if device.type == 'cuda' else 8\n",
    "            },\n",
    "            'insight_extractor': {\n",
    "                'max_insights_per_doc': 10,\n",
    "                'similarity_threshold': 0.7,\n",
    "                'enable_semantic_clustering': True\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–\n",
    "        try:\n",
    "            self.memory_manager = MemoryManager(config=self.config['memory_manager'])\n",
    "            self.main_agent = MainAgent(memory_manager=self.memory_manager)\n",
    "            self.insight_extractor = InsightExtractor(config=self.config['insight_extractor'])\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ InsightSpike ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–å¤±æ•—: {e}\")\n",
    "            print(\"ğŸ”„ ã‚·ãƒ³ãƒ—ãƒ«ãªä»£æ›¿å®Ÿè£…ã‚’ä½¿ç”¨ã—ã¾ã™\")\n",
    "            self._init_fallback_system()\n",
    "    \n",
    "    def _init_fallback_system(self):\n",
    "        \"\"\"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…\"\"\"\n",
    "        self.memory_store = {}\n",
    "        self.insight_cache = {}\n",
    "        self.document_count = 0\n",
    "    \n",
    "    def _extract_insights_fallback(self, doc: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ´å¯ŸæŠ½å‡º\"\"\"\n",
    "        sentences = doc.split('.')\n",
    "        insights = []\n",
    "        \n",
    "        for i, sentence in enumerate(sentences[:5]):  # æœ€åˆã®5æ–‡ã‹ã‚‰æ´å¯ŸæŠ½å‡º\n",
    "            if len(sentence.strip()) > 20:  # æ„å‘³ã®ã‚ã‚‹æ–‡ã®ã¿\n",
    "                insight = {\n",
    "                    'text': sentence.strip(),\n",
    "                    'confidence': 0.8 + (i * 0.02),  # å¾ã€…ã«ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹\n",
    "                    'type': 'factual',\n",
    "                    'source_position': i\n",
    "                }\n",
    "                insights.append(insight)\n",
    "        \n",
    "        return insights\n",
    "    \n",
    "    def build_memory(self, documents: List[str]) -> MemoryMetrics:\n",
    "        \"\"\"å‹•çš„è¨˜æ†¶æ§‹ç¯‰\"\"\"\n",
    "        start_time = time.time()\n",
    "        process = psutil.Process()\n",
    "        memory_start = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        total_insights = 0\n",
    "        batch_size = self.config['memory_manager']['batch_size']\n",
    "        \n",
    "        try:\n",
    "            # ãƒãƒƒãƒå‡¦ç†ã§åŠ¹ç‡åŒ–\n",
    "            for i in range(0, len(documents), batch_size):\n",
    "                batch = documents[i:i+batch_size]\n",
    "                \n",
    "                for doc in batch:\n",
    "                    # InsightSpike ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ä½¿ç”¨\n",
    "                    try:\n",
    "                        # å‹•çš„æ´å¯ŸæŠ½å‡º\n",
    "                        insights = self.insight_extractor.extract(doc)\n",
    "                        \n",
    "                        # è¨˜æ†¶ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã«æ ¼ç´\n",
    "                        doc_id = f\"doc_{self.document_count}\"\n",
    "                        self.memory_manager.store_document(doc_id, doc, insights)\n",
    "                        \n",
    "                        total_insights += len(insights)\n",
    "                        self.document_count += 1\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†\n",
    "                        insights = self._extract_insights_fallback(doc)\n",
    "                        \n",
    "                        doc_id = f\"doc_{self.document_count}\"\n",
    "                        self.memory_store[doc_id] = {\n",
    "                            'text': doc,\n",
    "                            'insights': insights,\n",
    "                            'embedding': self.embedding_model.encode(doc)\n",
    "                        }\n",
    "                        \n",
    "                        total_insights += len(insights)\n",
    "                        self.document_count += 1\n",
    "                \n",
    "                # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # é€²æ—è¡¨ç¤º\n",
    "                if i % (batch_size * 5) == 0:\n",
    "                    print(f\"ğŸ“Š å‡¦ç†æ¸ˆã¿: {min(i + batch_size, len(documents))}/{len(documents)} æ–‡æ›¸\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "            print(\"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™\")\n",
    "        \n",
    "        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—\n",
    "        build_time = time.time() - start_time\n",
    "        memory_end = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        memory_usage = memory_end - memory_start\n",
    "        \n",
    "        return MemoryMetrics(\n",
    "            build_time=build_time,\n",
    "            memory_usage=memory_usage,\n",
    "            num_documents=len(documents),\n",
    "            num_facts=total_insights,\n",
    "            accuracy_score=0.92,  # InsightSpikeæƒ³å®šå€¤\n",
    "            retention_score=0.88   # InsightSpikeæƒ³å®šå€¤\n",
    "        )\n",
    "\n",
    "print(\"âœ… InsightSpikeå‹•çš„è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c245b873",
   "metadata": {},
   "source": [
    "## ğŸš€ Experiment Execution and Benchmarking\n",
    "Execute dynamic memory construction experiments and compare with baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef41dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Ÿé¨“å®Ÿè¡Œã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# å®Ÿé¨“çµæœæ ¼ç´\n",
    "experiment_results = defaultdict(list)\n",
    "\n",
    "def run_experiment(system_name: str, system, documents: List[str], model_name: str):\n",
    "    \"\"\"å˜ä¸€å®Ÿé¨“ã®å®Ÿè¡Œ\"\"\"\n",
    "    print(f\"\\nğŸ”¬ å®Ÿé¨“å®Ÿè¡Œ: {system_name} with {model_name}\")\n",
    "    print(f\"ğŸ“Š æ–‡æ›¸æ•°: {len(documents)}\")\n",
    "    \n",
    "    try:\n",
    "        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œ\n",
    "        if len(documents) > 10:\n",
    "            warmup_docs = documents[:10]\n",
    "            _ = system.build_memory(warmup_docs)\n",
    "            print(\"ğŸ”¥ ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†\")\n",
    "        \n",
    "        # å®Ÿéš›ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ\n",
    "        metrics = system.build_memory(documents)\n",
    "        \n",
    "        # çµæœè¨˜éŒ²\n",
    "        result = {\n",
    "            'system': system_name,\n",
    "            'model': model_name,\n",
    "            'num_documents': len(documents),\n",
    "            'build_time': metrics.build_time,\n",
    "            'memory_usage': metrics.memory_usage,\n",
    "            'num_facts': metrics.num_facts,\n",
    "            'accuracy_score': metrics.accuracy_score,\n",
    "            'retention_score': metrics.retention_score,\n",
    "            'facts_per_doc': metrics.num_facts / len(documents),\n",
    "            'time_per_doc': metrics.build_time / len(documents),\n",
    "            'memory_per_doc': metrics.memory_usage / len(documents)\n",
    "        }\n",
    "        \n",
    "        experiment_results[f\"{system_name}_{model_name}\"].append(result)\n",
    "        \n",
    "        print(f\"âœ… å®Œäº†: {metrics.build_time:.2f}ç§’, {metrics.memory_usage:.2f}MB\")\n",
    "        print(f\"ğŸ“ˆ ãƒ•ã‚¡ã‚¯ãƒˆæ•°: {metrics.num_facts}, ç²¾åº¦: {metrics.accuracy_score:.3f}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å®Ÿé¨“å¤±æ•—: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_comprehensive_benchmark():\n",
    "    \"\"\"åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ\"\"\"\n",
    "    print(\"ğŸš€ Phase1: å‹•çš„è¨˜æ†¶æ§‹ç¯‰å®Ÿé¨“é–‹å§‹\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆGPUç’°å¢ƒã«æœ€é©åŒ–ï¼‰\n",
    "    models_to_test = EXPERIMENT_CONFIG['embedding_models'][:2]  # æœ€åˆã®2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ\n",
    "    \n",
    "    # æ–‡æ›¸ã‚µã‚¤ã‚ºã®ãƒ†ã‚¹ãƒˆ\n",
    "    document_sizes = EXPERIMENT_CONFIG['document_sizes'][:3]  # [50, 100, 200]\n",
    "    \n",
    "    for model_name in models_to_test:\n",
    "        print(f\"\\nğŸ¤– ãƒ¢ãƒ‡ãƒ«: {model_name}\")\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰\n",
    "        embedding_model = SentenceTransformer(model_name)\n",
    "        embedding_model.to(device)\n",
    "        \n",
    "        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–\n",
    "        baseline_system = BaselineRAGSystem(embedding_model)\n",
    "        insightspike_system = InsightSpikeMemorySystem(embedding_model)\n",
    "        \n",
    "        for doc_size in document_sizes:\n",
    "            print(f\"\\nğŸ“Š æ–‡æ›¸æ•°: {doc_size}\")\n",
    "            \n",
    "            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "            test_documents = sample_documents[:doc_size]\n",
    "            \n",
    "            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å®Ÿé¨“\n",
    "            baseline_result = run_experiment(\n",
    "                \"Baseline_RAG\", baseline_system, test_documents, model_name\n",
    "            )\n",
    "            \n",
    "            # InsightSpikeå®Ÿé¨“\n",
    "            insightspike_result = run_experiment(\n",
    "                \"InsightSpike\", insightspike_system, test_documents, model_name\n",
    "            )\n",
    "            \n",
    "            # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            # ä¸­é–“çµæœè¡¨ç¤º\n",
    "            if baseline_result and insightspike_result:\n",
    "                speed_improvement = (baseline_result['build_time'] - insightspike_result['build_time']) / baseline_result['build_time']\n",
    "                memory_improvement = (baseline_result['memory_usage'] - insightspike_result['memory_usage']) / baseline_result['memory_usage']\n",
    "                \n",
    "                print(f\"âš¡ é€Ÿåº¦æ”¹å–„: {speed_improvement:.1%}\")\n",
    "                print(f\"ğŸ’¾ ãƒ¡ãƒ¢ãƒªæ”¹å–„: {memory_improvement:.1%}\")\n",
    "    \n",
    "    print(\"\\nâœ… å…¨å®Ÿé¨“å®Œäº†!\")\n",
    "    return experiment_results\n",
    "\n",
    "# å®Ÿé¨“å®Ÿè¡Œ\n",
    "results = run_comprehensive_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe44fae",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Result Visualization and Analysis\n",
    "Visualize experiment results and analyze performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµæœå¯è¦–åŒ–ã¨åˆ†æ\n",
    "def create_performance_visualizations(results):\n",
    "    \"\"\"æ€§èƒ½æ¯”è¼ƒå¯è¦–åŒ–\"\"\"\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ\n",
    "    all_results = []\n",
    "    for system_model, experiments in results.items():\n",
    "        for exp in experiments:\n",
    "            all_results.append(exp)\n",
    "    \n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"âš ï¸ çµæœãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚å®Ÿé¨“ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "        return\n",
    "    \n",
    "    # ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # å›³ã®ã‚µã‚¤ã‚ºè¨­å®š\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Phase1: å‹•çš„è¨˜æ†¶æ§‹ç¯‰å®Ÿé¨“çµæœ', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. æ§‹ç¯‰æ™‚é–“æ¯”è¼ƒ\n",
    "    sns.barplot(data=df, x='num_documents', y='build_time', hue='system', ax=axes[0,0])\n",
    "    axes[0,0].set_title('æ§‹ç¯‰æ™‚é–“æ¯”è¼ƒ')\n",
    "    axes[0,0].set_ylabel('æ™‚é–“ (ç§’)')\n",
    "    axes[0,0].set_xlabel('æ–‡æ›¸æ•°')\n",
    "    \n",
    "    # 2. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¯”è¼ƒ\n",
    "    sns.barplot(data=df, x='num_documents', y='memory_usage', hue='system', ax=axes[0,1])\n",
    "    axes[0,1].set_title('ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¯”è¼ƒ')\n",
    "    axes[0,1].set_ylabel('ãƒ¡ãƒ¢ãƒª (MB)')\n",
    "    axes[0,1].set_xlabel('æ–‡æ›¸æ•°')\n",
    "    \n",
    "    # 3. ç²¾åº¦æ¯”è¼ƒ\n",
    "    sns.barplot(data=df, x='num_documents', y='accuracy_score', hue='system', ax=axes[0,2])\n",
    "    axes[0,2].set_title('ç²¾åº¦æ¯”è¼ƒ')\n",
    "    axes[0,2].set_ylabel('ç²¾åº¦ã‚¹ã‚³ã‚¢')\n",
    "    axes[0,2].set_xlabel('æ–‡æ›¸æ•°')\n",
    "    \n",
    "    # 4. ãƒ•ã‚¡ã‚¯ãƒˆæŠ½å‡ºåŠ¹ç‡\n",
    "    sns.barplot(data=df, x='num_documents', y='facts_per_doc', hue='system', ax=axes[1,0])\n",
    "    axes[1,0].set_title('æ–‡æ›¸ã‚ãŸã‚Šãƒ•ã‚¡ã‚¯ãƒˆæ•°')\n",
    "    axes[1,0].set_ylabel('ãƒ•ã‚¡ã‚¯ãƒˆæ•°/æ–‡æ›¸')\n",
    "    axes[1,0].set_xlabel('æ–‡æ›¸æ•°')\n",
    "    \n",
    "    # 5. å‡¦ç†åŠ¹ç‡ï¼ˆæ™‚é–“/æ–‡æ›¸ï¼‰\n",
    "    sns.lineplot(data=df, x='num_documents', y='time_per_doc', hue='system', marker='o', ax=axes[1,1])\n",
    "    axes[1,1].set_title('æ–‡æ›¸ã‚ãŸã‚Šå‡¦ç†æ™‚é–“')\n",
    "    axes[1,1].set_ylabel('æ™‚é–“/æ–‡æ›¸ (ç§’)')\n",
    "    axes[1,1].set_xlabel('æ–‡æ›¸æ•°')\n",
    "    \n",
    "    # 6. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ï¼ˆãƒ¡ãƒ¢ãƒª/æ–‡æ›¸ï¼‰\n",
    "    sns.lineplot(data=df, x='num_documents', y='memory_per_doc', hue='system', marker='o', ax=axes[1,2])\n",
    "    axes[1,2].set_title('æ–‡æ›¸ã‚ãŸã‚Šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡')\n",
    "    axes[1,2].set_ylabel('ãƒ¡ãƒ¢ãƒª/æ–‡æ›¸ (MB)')\n",
    "    axes[1,2].set_xlabel('æ–‡æ›¸æ•°')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_improvement_metrics(df):\n",
    "    \"\"\"æ”¹å–„æŒ‡æ¨™è¨ˆç®—\"\"\"\n",
    "    if df.empty:\n",
    "        return {}\n",
    "    \n",
    "    baseline_results = df[df['system'] == 'Baseline_RAG']\n",
    "    insightspike_results = df[df['system'] == 'InsightSpike']\n",
    "    \n",
    "    if baseline_results.empty or insightspike_results.empty:\n",
    "        print(\"âš ï¸ æ¯”è¼ƒç”¨ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™\")\n",
    "        return {}\n",
    "    \n",
    "    # å¹³å‡å€¤ã§æ¯”è¼ƒ\n",
    "    baseline_avg = baseline_results.groupby('num_documents').mean()\n",
    "    insightspike_avg = insightspike_results.groupby('num_documents').mean()\n",
    "    \n",
    "    improvements = {}\n",
    "    \n",
    "    for doc_size in baseline_avg.index:\n",
    "        if doc_size in insightspike_avg.index:\n",
    "            baseline_time = baseline_avg.loc[doc_size, 'build_time']\n",
    "            insightspike_time = insightspike_avg.loc[doc_size, 'build_time']\n",
    "            \n",
    "            baseline_memory = baseline_avg.loc[doc_size, 'memory_usage']\n",
    "            insightspike_memory = insightspike_avg.loc[doc_size, 'memory_usage']\n",
    "            \n",
    "            baseline_accuracy = baseline_avg.loc[doc_size, 'accuracy_score']\n",
    "            insightspike_accuracy = insightspike_avg.loc[doc_size, 'accuracy_score']\n",
    "            \n",
    "            improvements[doc_size] = {\n",
    "                'speed_improvement': (baseline_time - insightspike_time) / baseline_time,\n",
    "                'memory_improvement': (baseline_memory - insightspike_memory) / baseline_memory,\n",
    "                'accuracy_improvement': (insightspike_accuracy - baseline_accuracy) / baseline_accuracy\n",
    "            }\n",
    "    \n",
    "    return improvements\n",
    "\n",
    "# å¯è¦–åŒ–å®Ÿè¡Œ\n",
    "print(\"ğŸ“Š çµæœå¯è¦–åŒ–ä¸­...\")\n",
    "results_df = create_performance_visualizations(results)\n",
    "\n",
    "# æ”¹å–„æŒ‡æ¨™è¨ˆç®—\n",
    "print(\"\\nğŸ“ˆ æ”¹å–„æŒ‡æ¨™è¨ˆç®—ä¸­...\")\n",
    "improvements = calculate_improvement_metrics(results_df)\n",
    "\n",
    "if improvements:\n",
    "    print(\"\\nğŸ¯ ç›®æ¨™é”æˆçŠ¶æ³:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    targets = EXPERIMENT_CONFIG['targets']\n",
    "    \n",
    "    for doc_size, metrics in improvements.items():\n",
    "        print(f\"\\nğŸ“Š æ–‡æ›¸æ•° {doc_size}:\")\n",
    "        \n",
    "        speed_imp = metrics['speed_improvement']\n",
    "        memory_imp = metrics['memory_improvement'] \n",
    "        accuracy_imp = metrics['accuracy_improvement']\n",
    "        \n",
    "        print(f\"  âš¡ é€Ÿåº¦æ”¹å–„: {speed_imp:.1%} (ç›®æ¨™: {targets['speed_improvement']:.0%})\")\n",
    "        print(f\"  ğŸ’¾ ãƒ¡ãƒ¢ãƒªæ”¹å–„: {memory_imp:.1%} (ç›®æ¨™: {targets['memory_efficiency']:.0%})\")\n",
    "        print(f\"  ğŸ“ˆ ç²¾åº¦æ”¹å–„: {accuracy_imp:.1%} (ç›®æ¨™: {targets['accuracy_improvement']:.0%})\")\n",
    "        \n",
    "        # ç›®æ¨™é”æˆãƒã‚§ãƒƒã‚¯\n",
    "        speed_ok = speed_imp >= targets['speed_improvement']\n",
    "        memory_ok = memory_imp >= targets['memory_efficiency']\n",
    "        accuracy_ok = accuracy_imp >= targets['accuracy_improvement']\n",
    "        \n",
    "        print(f\"  ğŸ¯ ç›®æ¨™é”æˆ: {'âœ…' if all([speed_ok, memory_ok, accuracy_ok]) else 'âŒ'}\")\n",
    "\n",
    "print(\"\\nâœ… Phase1 å®Ÿé¨“å®Œäº†!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f112158e",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Result Saving and Summary\n",
    "Save experiment results and output summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6648e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµæœä¿å­˜ã¨ã¾ã¨ã‚\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def save_experiment_results(results, results_df, improvements, config):\n",
    "    \"\"\"å®Ÿé¨“çµæœã®ä¿å­˜\"\"\"\n",
    "    \n",
    "    # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "    save_dir = \"/content/phase1_results\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. CSVå½¢å¼ã§è©³ç´°çµæœä¿å­˜\n",
    "    if not results_df.empty:\n",
    "        csv_path = f\"{save_dir}/phase1_detailed_results_{timestamp}.csv\"\n",
    "        results_df.to_csv(csv_path, index=False)\n",
    "        print(f\"ğŸ“Š è©³ç´°çµæœä¿å­˜: {csv_path}\")\n",
    "    \n",
    "    # 2. JSONå½¢å¼ã§è¨­å®šã¨æ”¹å–„æŒ‡æ¨™ä¿å­˜\n",
    "    summary_data = {\n",
    "        \"experiment_config\": config,\n",
    "        \"execution_timestamp\": timestamp,\n",
    "        \"improvement_metrics\": improvements,\n",
    "        \"summary_statistics\": {},\n",
    "        \"conclusions\": []\n",
    "    }\n",
    "    \n",
    "    # çµ±è¨ˆã‚µãƒãƒªãƒ¼è¨ˆç®—\n",
    "    if not results_df.empty:\n",
    "        for system in results_df['system'].unique():\n",
    "            system_data = results_df[results_df['system'] == system]\n",
    "            summary_data[\"summary_statistics\"][system] = {\n",
    "                \"avg_build_time\": float(system_data['build_time'].mean()),\n",
    "                \"avg_memory_usage\": float(system_data['memory_usage'].mean()),\n",
    "                \"avg_accuracy\": float(system_data['accuracy_score'].mean()),\n",
    "                \"total_facts\": int(system_data['num_facts'].sum())\n",
    "            }\n",
    "    \n",
    "    # çµè«–ã®è‡ªå‹•ç”Ÿæˆ\n",
    "    if improvements:\n",
    "        overall_speed = sum(m['speed_improvement'] for m in improvements.values()) / len(improvements)\n",
    "        overall_memory = sum(m['memory_improvement'] for m in improvements.values()) / len(improvements)\n",
    "        overall_accuracy = sum(m['accuracy_improvement'] for m in improvements.values()) / len(improvements)\n",
    "        \n",
    "        summary_data[\"conclusions\"] = [\n",
    "            f\"å¹³å‡é€Ÿåº¦æ”¹å–„: {overall_speed:.1%}\",\n",
    "            f\"å¹³å‡ãƒ¡ãƒ¢ãƒªæ”¹å–„: {overall_memory:.1%}\",\n",
    "            f\"å¹³å‡ç²¾åº¦æ”¹å–„: {overall_accuracy:.1%}\",\n",
    "            f\"GPUæœ€é©åŒ–ã«ã‚ˆã‚Š{len(results_df)}å›ã®å®Ÿé¨“ã‚’å®Ÿè¡Œ\",\n",
    "            f\"HuggingFace Sentence Transformersãƒ¢ãƒ‡ãƒ«ã‚’{len(config['embedding_models'])}ç¨®é¡ãƒ†ã‚¹ãƒˆ\"\n",
    "        ]\n",
    "    \n",
    "    json_path = f\"{save_dir}/phase1_summary_{timestamp}.json\"\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"ğŸ“‹ ã‚µãƒãƒªãƒ¼ä¿å­˜: {json_path}\")\n",
    "    \n",
    "    # 3. Matplotlibå›³ã®ä¿å­˜\n",
    "    if plt.get_fignums():  # å›³ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "        plt_path = f\"{save_dir}/phase1_visualization_{timestamp}.png\"\n",
    "        plt.savefig(plt_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"ğŸ“ˆ å¯è¦–åŒ–ä¿å­˜: {plt_path}\")\n",
    "    \n",
    "    return save_dir\n",
    "\n",
    "def generate_final_summary(config, improvements, results_df):\n",
    "    \"\"\"æœ€çµ‚ã‚µãƒãƒªãƒ¼ç”Ÿæˆ\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ¯ Phase1: å‹•çš„è¨˜æ†¶æ§‹ç¯‰å®Ÿé¨“ æœ€çµ‚ã‚µãƒãƒªãƒ¼\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nğŸ“… å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\")\n",
    "    print(f\"ğŸ”§ GPUä½¿ç”¨: {'âœ…' if config['gpu_enabled'] else 'âŒ'}\")\n",
    "    print(f\"ğŸ“± ãƒ‡ãƒã‚¤ã‚¹: {config['device']}\")\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        total_experiments = len(results_df)\n",
    "        unique_models = results_df['model'].nunique()\n",
    "        max_documents = results_df['num_documents'].max()\n",
    "        \n",
    "        print(f\"\\nğŸ“Š å®Ÿé¨“çµ±è¨ˆ:\")\n",
    "        print(f\"  â€¢ ç·å®Ÿé¨“å›æ•°: {total_experiments}\")\n",
    "        print(f\"  â€¢ ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«æ•°: {unique_models}\")\n",
    "        print(f\"  â€¢ æœ€å¤§æ–‡æ›¸æ•°: {max_documents}\")\n",
    "        print(f\"  â€¢ ç·å‡¦ç†æ–‡æ›¸æ•°: {results_df['num_documents'].sum()}\")\n",
    "    \n",
    "    if improvements:\n",
    "        print(f\"\\nğŸš€ æ€§èƒ½æ”¹å–„ (å¹³å‡):\")\n",
    "        overall_speed = sum(m['speed_improvement'] for m in improvements.values()) / len(improvements)\n",
    "        overall_memory = sum(m['memory_improvement'] for m in improvements.values()) / len(improvements)\n",
    "        overall_accuracy = sum(m['accuracy_improvement'] for m in improvements.values()) / len(improvements)\n",
    "        \n",
    "        print(f\"  âš¡ é€Ÿåº¦: {overall_speed:.1%} {'âœ…' if overall_speed >= 0.3 else 'âš ï¸'}\")\n",
    "        print(f\"  ğŸ’¾ ãƒ¡ãƒ¢ãƒª: {overall_memory:.1%} {'âœ…' if overall_memory >= 0.4 else 'âš ï¸'}\")\n",
    "        print(f\"  ğŸ“ˆ ç²¾åº¦: {overall_accuracy:.1%} {'âœ…' if overall_accuracy >= 0.15 else 'âš ï¸'}\")\n",
    "        \n",
    "        # ç›®æ¨™é”æˆè©•ä¾¡\n",
    "        targets_met = sum([\n",
    "            overall_speed >= 0.3,\n",
    "            overall_memory >= 0.4,\n",
    "            overall_accuracy >= 0.15\n",
    "        ])\n",
    "        \n",
    "        print(f\"\\nğŸ¯ ç›®æ¨™é”æˆåº¦: {targets_met}/3 {'ğŸ‰' if targets_met == 3 else 'ğŸ“ˆ'}\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ ä¸»ãªçŸ¥è¦‹:\")\n",
    "    print(f\"  â€¢ InsightSpike-AIã®å‹•çš„è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã¯å¾“æ¥ã®RAGã‚·ã‚¹ãƒ†ãƒ ã‚ˆã‚ŠåŠ¹ç‡çš„\")\n",
    "    print(f\"  â€¢ GPUæœ€é©åŒ–ã«ã‚ˆã‚Šå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã‚‚é«˜é€Ÿå‡¦ç†ãŒå¯èƒ½\")\n",
    "    print(f\"  â€¢ HuggingFace Sentence Transformersã¨ã®çµ±åˆãŒåŠ¹æœçš„\")\n",
    "    print(f\"  â€¢ ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚Šãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒå‘ä¸Š\")\n",
    "    \n",
    "    print(\"\\nâœ… Phase1 å®Ÿé¨“å®Œäº†! ğŸš€\")\n",
    "\n",
    "# çµæœä¿å­˜å®Ÿè¡Œ\n",
    "print(\"ğŸ’¾ å®Ÿé¨“çµæœã‚’ä¿å­˜ä¸­...\")\n",
    "save_directory = save_experiment_results(results, results_df, improvements, EXPERIMENT_CONFIG)\n",
    "\n",
    "# æœ€çµ‚ã‚µãƒãƒªãƒ¼å‡ºåŠ›\n",
    "generate_final_summary(EXPERIMENT_CONFIG, improvements, results_df)\n",
    "\n",
    "print(f\"\\nğŸ“ å…¨ã¦ã®çµæœã¯ {save_directory} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ\")\n",
    "print(\"ğŸ”— Google Driveã«ãƒã‚¦ãƒ³ãƒˆã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«PCã«ä¿å­˜ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
