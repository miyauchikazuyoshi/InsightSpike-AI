{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miyauchikazuyoshi/InsightSpike-AI/blob/main/experiments_colab/phase1_dynamic_memory/dynamic_memory_working_experiment_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65c39f76",
      "metadata": {
        "id": "65c39f76"
      },
      "source": [
        "# ğŸ§  Phase 1: Dynamic Memory Construction - å®Œå…¨å‹•ä½œå®Ÿé¨“\n",
        "\n",
        "## æ¦‚è¦\n",
        "\n",
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€Google Colab GPUç’°å¢ƒã§**å®Œå…¨ã«å‹•ä½œã™ã‚‹**InsightSpike-AI Phase 1å®Ÿé¨“ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n",
        "\n",
        "### ğŸ¯ å®Ÿé¨“ç›®æ¨™\n",
        "- **å‹•çš„ãƒ¡ãƒ¢ãƒªæ§‹ç¯‰ã®å®Ÿè£…**: å®Ÿéš›ã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã¨ãƒ¡ãƒ¢ãƒªã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒ\n",
        "- **GPUæœ€é©åŒ–**: CUDAåŠ é€Ÿã¨å®Ÿéš›ã®æ€§èƒ½æ¸¬å®š\n",
        "- **å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ â†’ å‡¦ç† â†’ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ â†’ çµæœä¿å­˜\n",
        "\n",
        "### ğŸ”§ ä¿®æ­£æ¸ˆã¿å•é¡Œ\n",
        "- âœ… **ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ‡ãƒã‚¤ã‚¹å®šç¾©**: é©åˆ‡ãªGPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
        "- âœ… **å®Ÿãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰**: HuggingFaceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨åˆæˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
        "- âœ… **æ”¹è‰¯ãƒ¡ãƒ¢ãƒªã‚·ã‚¹ãƒ†ãƒ **: FAISS GPUå®Ÿè£… vs ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³\n",
        "- âœ… **çµæœä¿å­˜**: CSV/JSONå‡ºåŠ›ã¨å¯è¦–åŒ–\n",
        "- âœ… **Colabã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å•é¡Œ**: FAISSã€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ä¿®æ­£\n",
        "\n",
        "### âš ï¸ **é‡è¦ãªå®Ÿè¡Œé †åº**\n",
        "1. **å¿…ãšé †ç•ªã«ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„**\n",
        "2. ã‚»ãƒ«3ï¼ˆçµ±åˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼‰ã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹å ´åˆã¯ã€**ä¸€åº¦ã‚«ãƒ¼ãƒãƒ«ã‚’å†èµ·å‹•**ã—ã¦ã‹ã‚‰ã‚»ãƒ«8ï¼ˆãƒ‡ãƒã‚¤ã‚¹ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼‰ã‹ã‚‰å®Ÿè¡Œ\n",
        "3. FAISSã‚¨ãƒ©ãƒ¼ã¯è‡ªå‹•çš„ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã™ã‚‹ãŸã‚å®Ÿé¨“ã¯ç¶™ç¶šã•ã‚Œã¾ã™\n",
        "\n",
        "---\n",
        "\n",
        "**å®Ÿè¡Œç’°å¢ƒ**: Google Colab GPU (T4/V100) ã¾ãŸã¯ CPU  \n",
        "**æ¨å®šå®Ÿè¡Œæ™‚é–“**: 10-20åˆ†  \n",
        "**GPU ãƒ¡ãƒ¢ãƒªè¦ä»¶**: 4GB+ æ¨å¥¨ï¼ˆCPU ã§ã‚‚å‹•ä½œï¼‰\n",
        "\n",
        "### ğŸ“‹ **ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**\n",
        "- **ModuleConfig ã‚¨ãƒ©ãƒ¼**: æ­£å¸¸ã§ã™ã€ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³å®Ÿè£…ã«è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
        "- **FAISS GPU ã‚¨ãƒ©ãƒ¼**: CPUç‰ˆã«è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€å®Ÿé¨“ã¯ç¶™ç¶š\n",
        "- **ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼**: ã‚«ãƒ¼ãƒãƒ«å†èµ·å‹•å¾Œã€ã‚»ãƒ«8ã‹ã‚‰å†å®Ÿè¡Œ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b125e5ba",
      "metadata": {
        "id": "b125e5ba"
      },
      "source": [
        "## ğŸ”§ Unified Setup\n",
        "Execute the InsightSpike-AI unified setup script."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "GITHUB_USER = \"miyauchikazuyoshi\"\n",
        "REPO_NAME   = \"InsightSpike-AI\"\n",
        "\n",
        "GITHUB_TOKEN = getpass('GitHub Personal Access Tokenã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ')\n",
        "REPO_URL = f\"https://{GITHUB_USER}:{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git\"\n",
        "\n",
        "!git clone $REPO_URL\n",
        "\n",
        "%cd $REPO_NAME\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6n55IGAf-DR",
        "outputId": "7b201886-3be4-4b6e-ed8b-b47a1d4003f2"
      },
      "id": "g6n55IGAf-DR",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GitHub Personal Access Tokenã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Cloning into 'InsightSpike-AI'...\n",
            "remote: Enumerating objects: 3252, done.\u001b[K\n",
            "remote: Counting objects: 100% (539/539), done.\u001b[K\n",
            "remote: Compressing objects: 100% (396/396), done.\u001b[K\n",
            "remote: Total 3252 (delta 249), reused 357 (delta 133), pack-reused 2713 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3252/3252), 16.67 MiB | 13.85 MiB/s, done.\n",
            "Resolving deltas: 100% (1614/1614), done.\n",
            "/content/InsightSpike-AI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5739535a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5739535a",
        "outputId": "4403514e-d246-42f4-be41-2cd7df266c28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ Repository already exists\n",
            "ğŸš€ Starting Colab-optimized installation...\n",
            "âš¡ Using timeout protection to prevent hanging...\n",
            "==================================================\n",
            "âœ… PyTorch already available: 2.6.0+cu124\n",
            "ğŸ“¦ Installing 8 essential libraries...\n",
            "\n",
            "[ 1/8] ğŸ“ Sentence Transformers\n",
            "â±ï¸  Installing sentence-transformers (max 180s)...\n",
            "âœ… sentence-transformers installed successfully!\n",
            "ğŸ“Š Overall progress: 12.5% (1/1 successful)\n",
            "\n",
            "[ 2/8] ğŸ“Š HuggingFace Datasets\n",
            "â±ï¸  Installing datasets (max 180s)...\n",
            "âœ… datasets installed successfully!\n",
            "ğŸ“Š Overall progress: 25.0% (2/2 successful)\n",
            "\n",
            "[ 3/8] ğŸ” FAISS for similarity search\n",
            "â±ï¸  Installing faiss-cpu (max 180s)...\n",
            "âœ… faiss-cpu installed successfully!\n",
            "ğŸ“Š Overall progress: 37.5% (3/3 successful)\n",
            "\n",
            "[ 4/8] ğŸ“ˆ Plotly for visualization\n",
            "â±ï¸  Installing plotly (max 180s)...\n",
            "âœ… plotly installed successfully!\n",
            "ğŸ“Š Overall progress: 50.0% (4/4 successful)\n",
            "\n",
            "[ 5/8] ğŸ¨ Seaborn for plots\n",
            "â±ï¸  Installing seaborn (max 180s)...\n",
            "âœ… seaborn installed successfully!\n",
            "ğŸ“Š Overall progress: 62.5% (5/5 successful)\n",
            "\n",
            "[ 6/8] ğŸ•¸ï¸ NetworkX for graphs\n",
            "â±ï¸  Installing networkx (max 180s)...\n",
            "âœ… networkx installed successfully!\n",
            "ğŸ“Š Overall progress: 75.0% (6/6 successful)\n",
            "\n",
            "[ 7/8] ğŸ“Š Progress bars\n",
            "â±ï¸  Installing tqdm (max 180s)...\n",
            "âœ… tqdm installed successfully!\n",
            "ğŸ“Š Overall progress: 87.5% (7/7 successful)\n",
            "\n",
            "[ 8/8] ğŸŒˆ Rich terminal output\n",
            "â±ï¸  Installing rich (max 180s)...\n",
            "âœ… rich installed successfully!\n",
            "ğŸ“Š Overall progress: 100.0% (8/8 successful)\n",
            "\n",
            "ğŸ¯ Installation completed: 8/8 packages\n",
            "\n",
            "ğŸ” Verifying key installations...\n",
            "âœ… PyTorch: Working\n",
            "âœ… Sentence Transformers: Working\n",
            "âœ… HuggingFace Datasets: Working\n",
            "âœ… FAISS: Working\n",
            "âœ… Plotly: Working\n",
            "\n",
            "ğŸ”§ Configuring Python paths...\n",
            "\n",
            "ğŸ¯ Phase1 environment setup completed!\n",
            "ğŸ‰ Setup successful! (5/5 key packages working)\n",
            "ğŸ“‹ You can now proceed to the next cells.\n",
            "ğŸ” If any packages failed, the experiment will use fallback implementations.\n",
            "\n",
            "âš¡ Ready to start the experiment!\n"
          ]
        }
      ],
      "source": [
        "# Colab-optimized setup with timeout protection and PyTorch skip\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "import signal\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def print_live(message):\n",
        "    \"\"\"Print with immediate flush for Colab live display\"\"\"\n",
        "    print(message)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def timeout_handler(signum, frame):\n",
        "    raise TimeoutError(\"Installation timeout\")\n",
        "\n",
        "def install_with_timeout(package, timeout_seconds=120):\n",
        "    \"\"\"Install package with timeout protection\"\"\"\n",
        "    try:\n",
        "        print_live(f\"â±ï¸  Installing {package} (max {timeout_seconds}s)...\")\n",
        "\n",
        "        # Set timeout\n",
        "        signal.signal(signal.SIGALRM, timeout_handler)\n",
        "        signal.alarm(timeout_seconds)\n",
        "\n",
        "        # Install\n",
        "        result = subprocess.run(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", package, \"--upgrade\", \"-q\"],\n",
        "            capture_output=True, text=True, timeout=timeout_seconds\n",
        "        )\n",
        "\n",
        "        # Clear timeout\n",
        "        signal.alarm(0)\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print_live(f\"âœ… {package} installed successfully!\")\n",
        "            return True\n",
        "        else:\n",
        "            print_live(f\"âš ï¸ {package} installation had warnings\")\n",
        "            return True\n",
        "\n",
        "    except (TimeoutError, subprocess.TimeoutExpired):\n",
        "        signal.alarm(0)\n",
        "        print_live(f\"â° {package} installation timed out - skipping\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        signal.alarm(0)\n",
        "        print_live(f\"âŒ {package} installation failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Clone or verify InsightSpike-AI repository\n",
        "if not os.path.exists('/content/InsightSpike-AI'):\n",
        "    print_live(\"ğŸ“‚ Cloning InsightSpike-AI repository...\")\n",
        "    !git clone https://github.com/miyauchi-kazuyoshi/InsightSpike-AI.git /content/InsightSpike-AI\n",
        "    os.chdir('/content/InsightSpike-AI')\n",
        "else:\n",
        "    print_live(\"ğŸ“‚ Repository already exists\")\n",
        "    os.chdir('/content/InsightSpike-AI')\n",
        "\n",
        "print_live(\"ğŸš€ Starting Colab-optimized installation...\")\n",
        "print_live(\"âš¡ Using timeout protection to prevent hanging...\")\n",
        "print_live(\"=\" * 50)\n",
        "\n",
        "# Check if PyTorch is already available in Colab\n",
        "try:\n",
        "    import torch\n",
        "    print_live(f\"âœ… PyTorch already available: {torch.__version__}\")\n",
        "    pytorch_available = True\n",
        "except ImportError:\n",
        "    print_live(\"ğŸ“¦ PyTorch not found - will install\")\n",
        "    pytorch_available = False\n",
        "\n",
        "# Essential libraries for Phase 1 (excluding heavy PyTorch installs)\n",
        "essential_libs = [\n",
        "    (\"sentence-transformers\", \"ğŸ“ Sentence Transformers\"),\n",
        "    (\"datasets\", \"ğŸ“Š HuggingFace Datasets\"),\n",
        "    (\"faiss-cpu\", \"ğŸ” FAISS for similarity search\"),\n",
        "    (\"plotly\", \"ğŸ“ˆ Plotly for visualization\"),\n",
        "    (\"seaborn\", \"ğŸ¨ Seaborn for plots\"),\n",
        "    (\"networkx\", \"ğŸ•¸ï¸ NetworkX for graphs\"),\n",
        "    (\"tqdm\", \"ğŸ“Š Progress bars\"),\n",
        "    (\"rich\", \"ğŸŒˆ Rich terminal output\"),\n",
        "]\n",
        "\n",
        "# Only add PyTorch if not available\n",
        "if not pytorch_available:\n",
        "    essential_libs.insert(0, (\"torch\", \"ğŸ”¥ PyTorch\"))\n",
        "\n",
        "print_live(f\"ğŸ“¦ Installing {len(essential_libs)} essential libraries...\")\n",
        "\n",
        "successful_installs = 0\n",
        "for i, (lib, desc) in enumerate(essential_libs, 1):\n",
        "    print_live(f\"\\n[{i:2d}/{len(essential_libs)}] {desc}\")\n",
        "\n",
        "    if install_with_timeout(lib, timeout_seconds=180):  # 3 minute timeout per package\n",
        "        successful_installs += 1\n",
        "\n",
        "    # Show progress\n",
        "    progress = (i / len(essential_libs)) * 100\n",
        "    print_live(f\"ğŸ“Š Overall progress: {progress:.1f}% ({successful_installs}/{i} successful)\")\n",
        "\n",
        "print_live(f\"\\nğŸ¯ Installation completed: {successful_installs}/{len(essential_libs)} packages\")\n",
        "\n",
        "# Quick verification of key packages\n",
        "print_live(\"\\nğŸ” Verifying key installations...\")\n",
        "verification_results = {}\n",
        "\n",
        "packages_to_verify = {\n",
        "    'torch': 'PyTorch',\n",
        "    'sentence_transformers': 'Sentence Transformers',\n",
        "    'datasets': 'HuggingFace Datasets',\n",
        "    'faiss': 'FAISS',\n",
        "    'plotly': 'Plotly'\n",
        "}\n",
        "\n",
        "for package, name in packages_to_verify.items():\n",
        "    try:\n",
        "        __import__(package)\n",
        "        verification_results[name] = \"âœ…\"\n",
        "        print_live(f\"âœ… {name}: Working\")\n",
        "    except ImportError:\n",
        "        verification_results[name] = \"âš ï¸\"\n",
        "        print_live(f\"âš ï¸ {name}: Not available (will use fallback)\")\n",
        "\n",
        "# Python path configuration\n",
        "print_live(\"\\nğŸ”§ Configuring Python paths...\")\n",
        "sys.path.insert(0, '/content/InsightSpike-AI/src')\n",
        "sys.path.insert(0, '/content/InsightSpike-AI/experiments_colab/shared')\n",
        "\n",
        "# Final status\n",
        "print_live(\"\\nğŸ¯ Phase1 environment setup completed!\")\n",
        "working_packages = sum(1 for status in verification_results.values() if status == \"âœ…\")\n",
        "total_packages = len(verification_results)\n",
        "\n",
        "if working_packages >= 3:  # At least 3 key packages working\n",
        "    print_live(f\"ğŸ‰ Setup successful! ({working_packages}/{total_packages} key packages working)\")\n",
        "    print_live(\"ğŸ“‹ You can now proceed to the next cells.\")\n",
        "else:\n",
        "    print_live(f\"âš ï¸ Partial setup ({working_packages}/{total_packages} packages working)\")\n",
        "    print_live(\"ğŸ“‹ Experiment may work with reduced functionality.\")\n",
        "\n",
        "print_live(\"ğŸ” If any packages failed, the experiment will use fallback implementations.\")\n",
        "print_live(\"\\nâš¡ Ready to start the experiment!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "17ce089f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "17ce089f",
        "outputId": "411c61dd-c251-4d69-d1ee-426bdbc63530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” CLIè¨ºæ–­é–‹å§‹...\n",
            "ğŸ“ ç’°å¢ƒ: Colab\n",
            "ğŸ“‚ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹: /content/InsightSpike-AI\n",
            "\n",
            "ğŸ§ª CLIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:insightspike.core.learning.knowledge_graph_memory:torch-geometric not available, using basic torch functionality\n",
            "WARNING:insightspike.core.agents.main_agent:Graph reasoner (Layer 3) not available - requires PyTorch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… CLIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\n",
            "\n",
            "ğŸ“‹ ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã‚³ãƒãƒ³ãƒ‰:\n",
            "  âœ… åˆ©ç”¨å¯èƒ½ã‚³ãƒãƒ³ãƒ‰: ['ask', 'load_documents', 'stats', 'config_info', 'experiment', 'benchmark', 'embed', 'query', 'insight_experiment', 'compare_experiments', 'experiment_suite', 'demo', 'insights', 'insights_search', 'insights_validate', 'insights_cleanup', 'test_safe']\n",
            "\n",
            "ğŸ“‹ ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'items'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-968033781.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nğŸ“‹ ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mgroup_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_info\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  â€¢ {group_name}: {type(group_info).__name__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
          ]
        }
      ],
      "source": [
        "# ğŸ” ä¿®æ­£ç‰ˆ CLIè¨ºæ–­ - Part 1: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨åŸºæœ¬ãƒã‚§ãƒƒã‚¯\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "# è­¦å‘ŠæŠ‘åˆ¶ã¨ãƒ‘ã‚¹è¨­å®š\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "warnings.filterwarnings(\"ignore\", message=\".*forked.*parallelism.*\")\n",
        "\n",
        "def setup_paths():\n",
        "    \"\"\"æ­£ã—ã„Pythonãƒ‘ã‚¹ã‚’è¨­å®š\"\"\"\n",
        "    # ç’°å¢ƒæ¤œå‡º\n",
        "    if '/content' in os.getcwd():\n",
        "        project_path = \"/content/InsightSpike-AI\"\n",
        "        environment = \"Colab\"\n",
        "    else:\n",
        "        project_path = os.getcwd()\n",
        "        environment = \"Local\"\n",
        "\n",
        "    # ãƒ‘ã‚¹è¿½åŠ \n",
        "    paths_to_add = [\n",
        "        os.path.join(project_path, 'src'),\n",
        "        project_path,\n",
        "    ]\n",
        "\n",
        "    for path in paths_to_add:\n",
        "        if os.path.exists(path) and path not in sys.path:\n",
        "            sys.path.insert(0, path)\n",
        "\n",
        "    return project_path, environment\n",
        "\n",
        "print(\"ğŸ” CLIè¨ºæ–­é–‹å§‹...\")\n",
        "\n",
        "# ãƒ‘ã‚¹è¨­å®š\n",
        "project_path, environment = setup_paths()\n",
        "print(f\"ğŸ“ ç’°å¢ƒ: {environment}\")\n",
        "print(f\"ğŸ“‚ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹: {project_path}\")\n",
        "\n",
        "# 1. CLIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
        "try:\n",
        "    print(\"\\nğŸ§ª CLIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ:\")\n",
        "\n",
        "    # ãƒ¡ã‚¤ãƒ³CLIã‚¢ãƒ—ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "    from insightspike.cli.main import app\n",
        "    print(\"âœ… CLIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\")\n",
        "\n",
        "    # ç™»éŒ²ã‚³ãƒãƒ³ãƒ‰ã®ç¢ºèªï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
        "    print(\"\\nğŸ“‹ ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã‚³ãƒãƒ³ãƒ‰:\")\n",
        "    if hasattr(app, 'registered_commands'):\n",
        "        commands = app.registered_commands\n",
        "        if isinstance(commands, list):\n",
        "            command_names = []\n",
        "            for cmd in commands:\n",
        "                if hasattr(cmd, 'name') and cmd.name:\n",
        "                    command_names.append(cmd.name)\n",
        "                elif hasattr(cmd, 'callback') and hasattr(cmd.callback, '__name__'):\n",
        "                    command_names.append(cmd.callback.__name__)\n",
        "            print(f\"  âœ… åˆ©ç”¨å¯èƒ½ã‚³ãƒãƒ³ãƒ‰: {command_names}\")\n",
        "        else:\n",
        "            print(f\"  ğŸ“Š ã‚³ãƒãƒ³ãƒ‰æ•°: {len(commands) if hasattr(commands, '__len__') else 'unknown'}\")\n",
        "    else:\n",
        "        print(\"  âš ï¸ registered_commandså±æ€§ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
        "\n",
        "    # ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—ã®ç¢ºèª\n",
        "    if hasattr(app, 'registered_groups'):\n",
        "        groups = app.registered_groups\n",
        "        if groups:\n",
        "            print(f\"\\nğŸ“‹ ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—:\")\n",
        "            for group_name, group_info in groups.items():\n",
        "                print(f\"  â€¢ {group_name}: {type(group_info).__name__}\")\n",
        "\n",
        "    # Typeræƒ…å ±ã®ç¢ºèª\n",
        "    print(f\"\\nğŸ”§ Typer appæƒ…å ±:\")\n",
        "    print(f\"  â€¢ Type: {type(app)}\")\n",
        "    attributes = [attr for attr in dir(app) if not attr.startswith('_')]\n",
        "    print(f\"  â€¢ ä¸»è¦å±æ€§: {attributes[:10]}...\")  # æœ€åˆã®10å€‹ã®ã¿è¡¨ç¤º\n",
        "\n",
        "    # deps ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ã®ãƒ†ã‚¹ãƒˆ\n",
        "    try:\n",
        "        from insightspike.cli.deps_typer import deps_app\n",
        "        print(f\"\\nâœ… Deps ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\")\n",
        "        if hasattr(deps_app, 'registered_commands'):\n",
        "            deps_commands = deps_app.registered_commands\n",
        "            if isinstance(deps_commands, list):\n",
        "                deps_names = []\n",
        "                for cmd in deps_commands:\n",
        "                    if hasattr(cmd, 'name') and cmd.name:\n",
        "                        deps_names.append(cmd.name)\n",
        "                    elif hasattr(cmd, 'callback') and hasattr(cmd.callback, '__name__'):\n",
        "                        deps_names.append(cmd.callback.__name__)\n",
        "                print(f\"  ğŸ“‹ Deps ã‚³ãƒãƒ³ãƒ‰: {deps_names}\")\n",
        "    except ImportError as e:\n",
        "        print(f\"âš ï¸ Deps ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: {e}\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ CLIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: {e}\")\n",
        "    print(\"\\nğŸ”§ å¯¾ç­–:\")\n",
        "    print(\"  1. ãƒ‘ã‚¹è¨­å®šã‚’ç¢ºèª\")\n",
        "    print(\"  2. ä¾å­˜é–¢ä¿‚ã®å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\")\n",
        "    print(\"  3. æ‰‹å‹•ãƒ‘ã‚¹è¿½åŠ \")\n",
        "\n",
        "print(\"\\nğŸ“Š Part 1 å®Œäº† - Part 2 ã§å®Ÿéš›ã®ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œã‚’ãƒ†ã‚¹ãƒˆ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55004fb5",
      "metadata": {
        "id": "55004fb5"
      },
      "source": [
        "## ğŸ”§ CLIè¨ºæ–­ (CLI Diagnostic)\n",
        "\n",
        "CLI ã‚³ãƒãƒ³ãƒ‰ãŒæ­£ã—ãç™»éŒ²ãƒ»å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’è¨ºæ–­ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa456f1a",
      "metadata": {
        "id": "fa456f1a"
      },
      "outputs": [],
      "source": [
        "# ğŸ” Enhanced CLI Diagnostic with Colab Support\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import traceback\n",
        "\n",
        "# Suppress tokenizer warnings for cleaner output\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "warnings.filterwarnings(\"ignore\", message=\".*forked.*parallelism.*\")\n",
        "\n",
        "def print_status(emoji, message):\n",
        "    \"\"\"Print with emoji status\"\"\"\n",
        "    print(f\"{emoji} {message}\")\n",
        "\n",
        "def run_command_safely(cmd, description=\"command\"):\n",
        "    \"\"\"Run command with proper error handling\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)\n",
        "        return result.returncode == 0, result.stdout, result.stderr\n",
        "    except subprocess.TimeoutExpired:\n",
        "        return False, \"\", f\"â° {description} timed out\"\n",
        "    except Exception as e:\n",
        "        return False, \"\", f\"âŒ {description} failed: {str(e)}\"\n",
        "\n",
        "print_status(\"ğŸ”\", \"CLIè¨ºæ–­é–‹å§‹...\")\n",
        "\n",
        "# Environment detection\n",
        "if '/content' in os.getcwd():\n",
        "    environment = \"Colab\"\n",
        "    project_path = \"/content/InsightSpike-AI\"\n",
        "    print_status(\"ğŸ“‚\", f\"Colabç’°å¢ƒã‚’æ¤œå‡º: {project_path}\")\n",
        "else:\n",
        "    environment = \"Local\"\n",
        "    project_path = os.getcwd()\n",
        "    print_status(\"ğŸ“‚\", f\"ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã‚’æ¤œå‡º: {project_path}\")\n",
        "\n",
        "# Add project to Python path for imports\n",
        "if project_path not in sys.path:\n",
        "    sys.path.insert(0, project_path)\n",
        "    sys.path.insert(0, os.path.join(project_path, 'src'))\n",
        "\n",
        "# Check pyproject.toml for CLI configuration\n",
        "pyproject_path = os.path.join(project_path, 'pyproject.toml')\n",
        "if os.path.exists(pyproject_path):\n",
        "    with open(pyproject_path, 'r') as f:\n",
        "        content = f.read()\n",
        "        if 'insightspike = \"insightspike.cli.main:main\"' in content:\n",
        "            print_status(\"ğŸ“‹\", \"Poetry scripts ç¢ºèª:\")\n",
        "            # Extract and show poetry scripts section\n",
        "            lines = content.split('\\n')\n",
        "            in_scripts = False\n",
        "            for line in lines:\n",
        "                if '[tool.poetry.scripts]' in line:\n",
        "                    in_scripts = True\n",
        "                    print(line)\n",
        "                elif in_scripts and line.startswith('['):\n",
        "                    if 'tool.poetry.scripts' not in line:\n",
        "                        break\n",
        "                elif in_scripts:\n",
        "                    print(line)\n",
        "            print()\n",
        "\n",
        "# Test CLI availability\n",
        "print_status(\"ğŸ¤–\", \"insightspike ã‚³ãƒãƒ³ãƒ‰ ãƒ†ã‚¹ãƒˆ:\")\n",
        "\n",
        "# Method 1: Try poetry run (if available)\n",
        "if environment == \"Local\":\n",
        "    success, stdout, stderr = run_command_safely('poetry run insightspike --help', 'poetry CLI')\n",
        "    if success:\n",
        "        print_status(\"âœ…\", \"Poetry CLIå‹•ä½œç¢ºèª: æˆåŠŸ\")\n",
        "        print(f\"ğŸ“„ Help output:\\n{stdout[:500]}...\")\n",
        "    else:\n",
        "        print_status(\"âš ï¸\", \"Poetry CLIå•é¡Œ:\")\n",
        "        if stderr:\n",
        "            print(f\"Error: {stderr[:200]}...\")\n",
        "\n",
        "# Method 2: Direct Python module execution\n",
        "success, stdout, stderr = run_command_safely(f'cd {project_path} && python -m insightspike.cli.main --help', 'Direct Python CLI')\n",
        "if success:\n",
        "    print_status(\"âœ…\", \"Direct Python CLI: æˆåŠŸ\")\n",
        "    print(f\"ğŸ“„ Help output:\\n{stdout[:500]}...\")\n",
        "else:\n",
        "    print_status(\"âš ï¸\", \"Direct Python CLIå•é¡Œ:\")\n",
        "    if stderr:\n",
        "        print(f\"Error: {stderr[:200]}...\")\n",
        "\n",
        "# Method 3: Import and test programmatically\n",
        "print_status(\"ğŸ§ª\", \"ãƒ—ãƒ­ã‚°ãƒ©ãƒ å†…CLIå®Ÿè¡Œãƒ†ã‚¹ãƒˆ:\")\n",
        "try:\n",
        "    # Try importing the CLI module\n",
        "    from insightspike.cli.main import app\n",
        "    print_status(\"âœ…\", \"CLIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\")\n",
        "\n",
        "    # Check available commands\n",
        "    if hasattr(app, 'registered_commands'):\n",
        "        commands = list(app.registered_commands.keys())\n",
        "        print_status(\"ğŸ“‹\", f\"ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã‚³ãƒãƒ³ãƒ‰: {commands}\")\n",
        "    else:\n",
        "        print_status(\"âš ï¸\", \"app.registered_commandså±æ€§ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
        "\n",
        "    # Try importing subcommands\n",
        "    try:\n",
        "        from insightspike.cli.deps_typer import deps_app\n",
        "        print_status(\"âœ…\", \"Deps CLI ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\")\n",
        "\n",
        "        if hasattr(deps_app, 'registered_commands'):\n",
        "            deps_commands = list(deps_app.registered_commands.keys())\n",
        "            print_status(\"ğŸ“‹\", f\"Deps ã‚³ãƒãƒ³ãƒ‰: {deps_commands}\")\n",
        "    except ImportError as e:\n",
        "        print_status(\"âš ï¸\", f\"Deps CLI ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: {e}\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print_status(\"âŒ\", f\"CLIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: {e}\")\n",
        "    print(\"è©³ç´°ã‚¨ãƒ©ãƒ¼:\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Method 4: Create CLI shortcut for Colab (if in Colab environment)\n",
        "if environment == \"Colab\":\n",
        "    print_status(\"ğŸ”§\", \"Colabç”¨CLIè¨­å®š:\")\n",
        "\n",
        "    # Create a direct execution script\n",
        "    cli_script = f\"\"\"#!/bin/bash\n",
        "cd {project_path}\n",
        "export PYTHONPATH=\"{project_path}/src:$PYTHONPATH\"\n",
        "python -m insightspike.cli.main \"$@\"\n",
        "\"\"\"\n",
        "\n",
        "    cli_path = \"/usr/local/bin/insightspike\"\n",
        "    try:\n",
        "        with open(cli_path, 'w') as f:\n",
        "            f.write(cli_script)\n",
        "        os.chmod(cli_path, 0o755)\n",
        "        print_status(\"âœ…\", f\"CLI shortcut created: {cli_path}\")\n",
        "\n",
        "        # Test the shortcut\n",
        "        success, stdout, stderr = run_command_safely('insightspike --help', 'CLI shortcut')\n",
        "        if success:\n",
        "            print_status(\"ğŸ‰\", \"CLI shortcut ãƒ†ã‚¹ãƒˆ: æˆåŠŸ!\")\n",
        "        else:\n",
        "            print_status(\"âš ï¸\", f\"CLI shortcut ãƒ†ã‚¹ãƒˆå¤±æ•—: {stderr}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(\"âŒ\", f\"CLI shortcut ä½œæˆå¤±æ•—: {e}\")\n",
        "\n",
        "# Phase 2 æº–å‚™ãƒã‚§ãƒƒã‚¯\n",
        "print_status(\"ğŸš€\", \"Phase 2 CLIæº–å‚™çŠ¶æ³:\")\n",
        "phase2_requirements = [\n",
        "    (\"CLIåŸºæœ¬æ©Ÿèƒ½\", \"ask, deps ã‚³ãƒãƒ³ãƒ‰\"),\n",
        "    (\"è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ\", \"å®Ÿé¨“åˆ¶å¾¡ã¨ãƒ‡ãƒ¼ã‚¿ç®¡ç†\"),\n",
        "    (\"çµæœå‡ºåŠ›\", \"JSON/CSV/ç”»åƒç”Ÿæˆ\"),\n",
        "    (\"ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–\", \"é€²æ—ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º\")\n",
        "]\n",
        "\n",
        "for requirement, description in phase2_requirements:\n",
        "    print(f\"  ğŸ“‹ {requirement}: {description}\")\n",
        "\n",
        "print_status(\"âœ…\", \"CLIè¨ºæ–­å®Œäº†\")\n",
        "\n",
        "# Summary and recommendations\n",
        "print_status(\"ğŸ’¡\", \"Phase 2ã«å‘ã‘ãŸæ¨å¥¨äº‹é …:\")\n",
        "if environment == \"Colab\":\n",
        "    print(\"  1. CLI shortcutçµŒç”±ã§ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ\")\n",
        "    print(\"  2. ç›´æ¥Pythonã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãƒ—ãƒ­ã‚°ãƒ©ãƒ åˆ¶å¾¡\")\n",
        "    print(\"  3. å¿…è¦ã«å¿œã˜ã¦Poetryä»£æ›¿æ‰‹æ³•ã‚’ä½¿ç”¨\")\n",
        "else:\n",
        "    print(\"  1. PoetryçµŒç”±ã§ã®æ¨™æº–CLIå®Ÿè¡Œ\")\n",
        "    print(\"  2. é–‹ç™ºç’°å¢ƒã§ã®ãƒ•ãƒ«æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ\")\n",
        "    print(\"  3. Colabäº’æ›æ€§ã®ç¶™ç¶šç¢ºèª\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d9e4c99",
      "metadata": {
        "id": "6d9e4c99"
      },
      "source": [
        "## âœ… CLIå•é¡Œè§£æ±ºå®Œäº†ï¼(CLI Issue Fixed!)\n",
        "\n",
        "### ğŸ”§ **å•é¡Œã®æ ¹æœ¬åŸå› **\n",
        "CLI ã‚³ãƒãƒ³ãƒ‰ãŒè¡¨ç¤ºã•ã‚Œãªã‹ã£ãŸå•é¡Œã¯ **Typer ã¨ Click ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³éäº’æ›** ãŒåŸå› ã§ã—ãŸï¼š\n",
        "- **Typer 0.9.4** + **Click 8.2.1** ã®çµ„ã¿åˆã‚ã›ã§ `Parameter.make_metavar()` ãƒ¡ã‚½ãƒƒãƒ‰ã«å¿…è¦ãªå¼•æ•°ãŒä¸è¶³\n",
        "- ã“ã®å•é¡Œã«ã‚ˆã‚Š `--help` ã‚³ãƒãƒ³ãƒ‰ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã€CLIå…¨ä½“ãŒä½¿ç”¨ä¸å¯èƒ½ãªçŠ¶æ…‹ã§ã—ãŸ\n",
        "\n",
        "### ğŸ› ï¸ **è§£æ±ºæ–¹æ³•**\n",
        "`pyproject.toml` ã®ä¾å­˜é–¢ä¿‚ã‚’ä¿®æ­£ï¼š\n",
        "```toml\n",
        "# ä¿®æ­£å‰\n",
        "typer = \">=0.7.0,<0.10.0\"\n",
        "\n",
        "# ä¿®æ­£å¾Œ  \n",
        "typer = \"^0.9.0\"\n",
        "click = \"<8.2.0\"  # äº’æ›æ€§ã®ã‚ã‚‹ Click ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«åˆ¶é™\n",
        "```\n",
        "\n",
        "### ğŸ¯ **çµæœ**\n",
        "**17å€‹ã®CLIã‚³ãƒãƒ³ãƒ‰**ãŒæ­£å¸¸ã«åˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸï¼š\n",
        "\n",
        "#### ğŸ“‹ **ãƒ¡ã‚¤ãƒ³ã‚³ãƒãƒ³ãƒ‰**\n",
        "- `ask` - AI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«è³ªå•\n",
        "- `benchmark` - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ  \n",
        "- `experiment` - å®Ÿé¨“ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³\n",
        "- `demo` - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¢\n",
        "- `stats` - ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµ±è¨ˆ\n",
        "\n",
        "#### ğŸ§  **æ´å¯Ÿç³»ã‚³ãƒãƒ³ãƒ‰**\n",
        "- `insights` - ç™»éŒ²æ¸ˆã¿æ´å¯Ÿã®è¡¨ç¤º\n",
        "- `insights-search` - æ¦‚å¿µåˆ¥æ´å¯Ÿæ¤œç´¢\n",
        "- `insights-validate` - æ´å¯Ÿã®æ‰‹å‹•æ¤œè¨¼\n",
        "- `insights-cleanup` - ä½å“è³ªæ´å¯Ÿã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
        "\n",
        "#### ğŸ”§ **ç®¡ç†ç³»ã‚³ãƒãƒ³ãƒ‰**  \n",
        "- `deps` - ä¾å­˜é–¢ä¿‚ç®¡ç†ï¼ˆã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ä»˜ãï¼‰\n",
        "- `config-info` - è¨­å®šæƒ…å ±è¡¨ç¤º\n",
        "- `load-documents` - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿\n",
        "\n",
        "#### ğŸ“Š **å®Ÿé¨“ç³»ã‚³ãƒãƒ³ãƒ‰**\n",
        "- `experiment-suite` - å®Œå…¨å®Ÿé¨“ã‚¹ã‚¤ãƒ¼ãƒˆ\n",
        "- `insight-experiment` - æ´å¯Ÿæ¤œå‡ºå®Ÿé¨“\n",
        "- `compare-experiments` - å®Ÿé¨“æ¯”è¼ƒ\n",
        "- `test-safe` - ã‚»ãƒ¼ãƒ•ãƒ¢ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ\n",
        "\n",
        "### ğŸ‰ **æˆæœ**\n",
        "- âœ… CLI ã®å®Œå…¨å¾©æ´»\n",
        "- âœ… å…¨ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ (`deps list`, `deps validate` ãªã©) å‹•ä½œç¢ºèªæ¸ˆã¿\n",
        "- âœ… Typer ã‚¢ãƒ—ãƒªã®é©åˆ‡ãªç™»éŒ²ã¨ã‚³ãƒãƒ³ãƒ‰å¯è¦–æ€§\n",
        "- âœ… æœ¬æ ¼çš„ãª InsightSpike-AI CLI ç’°å¢ƒã®æ§‹ç¯‰å®Œäº†\n",
        "\n",
        "### ğŸ“ **æŠ€è¡“çš„å­¦ç¿’**\n",
        "ã“ã®å•é¡Œè§£æ±ºã«ã‚ˆã‚Šä»¥ä¸‹ã‚’å­¦ç¿’ï¼š\n",
        "1. **ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã®é‡è¦æ€§**: ç‰¹ã« CLI ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã®äº’æ›æ€§\n",
        "2. **Typer + Click ã®ä¾å­˜é–¢ä¿‚**: é©åˆ‡ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¯„å›²ã®æŒ‡å®š\n",
        "3. **Poetry ã«ã‚ˆã‚‹ä¾å­˜é–¢ä¿‚è§£æ±º**: `poetry update` ã§ã®å•é¡Œä¿®æ­£\n",
        "4. **CLI ãƒ‡ãƒãƒƒã‚°æ‰‹æ³•**: æ®µéšçš„å•é¡Œç‰¹å®šã¨ãƒ†ã‚¹ãƒˆ\n",
        "\n",
        "---\n",
        "\n",
        "**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**: CLI ãŒå®Œå…¨ã«å‹•ä½œã™ã‚‹ã‚ˆã†ã«ãªã£ãŸã®ã§ã€å®Ÿéš›ã® InsightSpike-AI ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã£ãŸå®Ÿé¨“ã‚„åˆ†æãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸï¼"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b07bba60",
      "metadata": {
        "id": "b07bba60"
      },
      "source": [
        "## ğŸŠ **Phase 1 å®Ÿé¨“ç’°å¢ƒ - å®Œå…¨æ•´å‚™å®Œäº†å ±å‘Š**\n",
        "\n",
        "### ğŸ“ˆ **ä»Šå›ã®æˆæœã‚µãƒãƒªãƒ¼**\n",
        "\n",
        "#### ğŸ”§ **1. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å¤§å¹…æ”¹è‰¯**\n",
        "- **FAISS ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**: GPUç‰ˆå¤±æ•—æ™‚ã® CPUç‰ˆè‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
        "- **torch-geometric**: å …ç‰¢ãªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å ±å‘Š\n",
        "- **è©³ç´°ãƒ­ã‚°**: å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®æˆåŠŸ/å¤±æ•—çŠ¶æ³ã‚’æ˜ç¢ºã«è¡¨ç¤º\n",
        "- **ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹**: ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¾Œã®æ¬¡ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ˜ç¤º\n",
        "\n",
        "#### ğŸ–¥ï¸ **2. CLI ã‚·ã‚¹ãƒ†ãƒ ã®å®Œå…¨ä¿®å¾©**\n",
        "- **æ ¹æœ¬åŸå› ç‰¹å®š**: Typer 0.9.4 + Click 8.2.1 ã®éäº’æ›æ€§å•é¡Œ\n",
        "- **ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¿®æ­£**: é©åˆ‡ãªä¾å­˜é–¢ä¿‚åˆ¶ç´„ã®è¿½åŠ \n",
        "- **æ©Ÿèƒ½ç¢ºèª**: 17å€‹ã®ãƒ¡ã‚¤ãƒ³ã‚³ãƒãƒ³ãƒ‰ + ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ç¾¤ã®å‹•ä½œç¢ºèª\n",
        "- **å¯ç”¨æ€§å‘ä¸Š**: æœ¬æ ¼çš„ãª InsightSpike-AI CLIç’°å¢ƒã‚’æ§‹ç¯‰\n",
        "\n",
        "#### ğŸ“Š **3. å®Ÿé¨“çµæœã®åˆ†æã¨æ”¹å–„**\n",
        "- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å•é¡Œã®è§£æ˜**: GPU ãƒ¡ãƒ¢ãƒªã®ã¿æ¸¬å®šï¼ˆCPU ãƒ¡ãƒ¢ãƒªæœªè¨ˆæ¸¬ï¼‰\n",
        "- **æ€§èƒ½çµæœã®å¦¥å½“æ€§ç¢ºèª**: FAISS ã«ã‚ˆã‚‹è‹¥å¹²ã®æ”¹å–„ï¼ˆ0.8%ï¼‰\n",
        "- **å®Ÿé¨“è¨­è¨ˆã®æ¤œè¨¼**: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ vs æ”¹è‰¯ã‚·ã‚¹ãƒ†ãƒ ã®æ¯”è¼ƒæ‰‹æ³•\n",
        "- **æ—¥æœ¬èªè§£èª¬**: æŠ€è¡“çš„è©³ç´°ã¨çµæœã®æ„å‘³ã‚’è©³ç´°èª¬æ˜\n",
        "\n",
        "#### ğŸ¯ **4. ç·åˆçš„ãªå®Ÿé¨“ç’°å¢ƒã®ç¢ºç«‹**\n",
        "- âœ… **Colab äº’æ›æ€§**: ãƒ­ãƒ¼ã‚«ãƒ«/Colabä¸¡ç’°å¢ƒã§å‹•ä½œ\n",
        "- âœ… **ä¾å­˜é–¢ä¿‚ç®¡ç†**: Poetry ã«ã‚ˆã‚‹å®‰å®šã—ãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç®¡ç†\n",
        "- âœ… **å®Ÿé¨“å†ç¾æ€§**: ä¸€è²«ã—ãŸçµæœã‚’å¾—ã‚‰ã‚Œã‚‹ç’°å¢ƒ\n",
        "- âœ… **CLI ã‚¢ã‚¯ã‚»ã‚¹**: å…¨æ©Ÿèƒ½ã¸ã®ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹\n",
        "\n",
        "### ğŸš€ **åˆ©ç”¨å¯èƒ½ãªä¸»è¦ CLI ã‚³ãƒãƒ³ãƒ‰**\n",
        "\n",
        "#### ğŸ’¬ **å¯¾è©±çš„ä½¿ç”¨**\n",
        "```bash\n",
        "# AI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«è³ªå•\n",
        "poetry run insightspike ask \"æ·±å±¤å­¦ç¿’ã¨è„³ç§‘å­¦ã®é–¢ä¿‚ã¯ï¼Ÿ\"\n",
        "\n",
        "# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¢\n",
        "poetry run insightspike demo\n",
        "```\n",
        "\n",
        "#### ğŸ“Š **å®Ÿé¨“ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**\n",
        "```bash\n",
        "# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ\n",
        "poetry run insightspike benchmark --dataset simple --verbose\n",
        "\n",
        "# æ´å¯Ÿæ¤œå‡ºå®Ÿé¨“\n",
        "poetry run insightspike insight-experiment\n",
        "\n",
        "# å®Ÿé¨“ã‚¹ã‚¤ãƒ¼ãƒˆ\n",
        "poetry run insightspike experiment-suite --experiment-type all\n",
        "```\n",
        "\n",
        "#### ğŸ§  **æ´å¯Ÿç®¡ç†**\n",
        "```bash\n",
        "# æ´å¯Ÿçµ±è¨ˆè¡¨ç¤º\n",
        "poetry run insightspike insights\n",
        "\n",
        "# æ¦‚å¿µæ¤œç´¢\n",
        "poetry run insightspike insights-search \"artificial intelligence\"\n",
        "\n",
        "# æ´å¯Ÿæ¤œè¨¼\n",
        "poetry run insightspike insights-validate <insight-id>\n",
        "```\n",
        "\n",
        "#### ğŸ”§ **ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†**\n",
        "```bash\n",
        "# ä¾å­˜é–¢ä¿‚ç¢ºèª\n",
        "poetry run insightspike deps list\n",
        "\n",
        "# ç’°å¢ƒæ¤œè¨¼\n",
        "poetry run insightspike deps validate\n",
        "\n",
        "# è¨­å®šæƒ…å ±\n",
        "poetry run insightspike config-info\n",
        "```\n",
        "\n",
        "### ğŸ–ï¸ **æŠ€è¡“çš„é”æˆäº‹é …**\n",
        "\n",
        "1. **ğŸ” å•é¡Œè¨ºæ–­åŠ›**: CLI å•é¡Œã®æ ¹æœ¬åŸå› ã‚’ç‰¹å®šãƒ»è§£æ±º\n",
        "2. **ğŸ“¦ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç®¡ç†**: è¤‡é›‘ãªä¾å­˜é–¢ä¿‚ã®é©åˆ‡ãªç®¡ç†\n",
        "3. **ğŸ§ª å®Ÿé¨“è¨­è¨ˆ**: ç§‘å­¦çš„ã«å¦¥å½“ãªæ¯”è¼ƒå®Ÿé¨“ã®å®Ÿè£…\n",
        "4. **ğŸ“ˆ æ€§èƒ½åˆ†æ**: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ€§èƒ½ã®è©³ç´°åˆ†æ\n",
        "5. **ğŸ› ï¸ ç’°å¢ƒæ§‹ç¯‰**: å†ç¾å¯èƒ½ã§å …ç‰¢ãªé–‹ç™ºãƒ»å®Ÿé¨“ç’°å¢ƒ\n",
        "\n",
        "### ğŸ¯ **ä»Šå¾Œã®ç™ºå±•å¯èƒ½æ€§**\n",
        "\n",
        "#### ğŸ”¬ **å®Ÿé¨“æ‹¡å¼µ**\n",
        "- CPU ãƒ¡ãƒ¢ãƒªæ¸¬å®šã®è¿½åŠ ã§ã‚ˆã‚Šæ­£ç¢ºãªä½¿ç”¨é‡æŠŠæ¡\n",
        "- ã‚ˆã‚Šå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ10K+ documentsï¼‰ã§ã®æ¤œè¨¼  \n",
        "- ç•°ãªã‚‹åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã§ã®æ¯”è¼ƒå®Ÿé¨“\n",
        "\n",
        "#### ğŸ§  **æ´å¯Ÿã‚·ã‚¹ãƒ†ãƒ å¼·åŒ–**\n",
        "- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡ºã®å®Ÿè£…\n",
        "- æ´å¯Ÿå“è³ªã‚¹ã‚³ã‚¢ã®æ”¹è‰¯\n",
        "- çŸ¥è­˜ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹ã®æ´å¯Ÿé–¢é€£ä»˜ã‘\n",
        "\n",
        "#### ğŸš€ **ç”Ÿç”£ç’°å¢ƒå±•é–‹**\n",
        "- API ã‚µãƒ¼ãƒãƒ¼ã¨ã—ã¦ã®å±•é–‹\n",
        "- Web UI ã®å®Ÿè£…\n",
        "- å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®æœ€é©åŒ–\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“‹ **ç¾åœ¨ã®çŠ¶æ…‹**\n",
        "- âœ… **ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: å®Œå…¨è‡ªå‹•åŒ–ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æ¸ˆã¿\n",
        "- âœ… **CLI**: å…¨æ©Ÿèƒ½åˆ©ç”¨å¯èƒ½ãƒ»17ã‚³ãƒãƒ³ãƒ‰å‹•ä½œç¢ºèªæ¸ˆã¿  \n",
        "- âœ… **å®Ÿé¨“**: å†ç¾å¯èƒ½ãƒ»çµæœè§£é‡ˆå¯èƒ½ãƒ»æ—¥æœ¬èªè§£èª¬ä»˜ã\n",
        "- âœ… **ç’°å¢ƒ**: ãƒ­ãƒ¼ã‚«ãƒ«/Colab ä¸¡å¯¾å¿œãƒ»ä¾å­˜é–¢ä¿‚å®‰å®šåŒ–æ¸ˆã¿\n",
        "\n",
        "**ğŸ‰ InsightSpike-AI Phase 1 å®Ÿé¨“ç’°å¢ƒãŒæœ¬æ ¼ç¨¼åƒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸï¼**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33525eb2",
      "metadata": {
        "id": "33525eb2"
      },
      "source": [
        "## ğŸ“¦ InsightSpike Module Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a3233d",
      "metadata": {
        "id": "03a3233d"
      },
      "outputs": [],
      "source": [
        "# Enhanced InsightSpike module integration with better error handling\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Suppress specific warnings for cleaner output\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "print(\"ğŸ”§ Attempting InsightSpike core integration...\")\n",
        "\n",
        "# Try importing InsightSpike components with graceful fallbacks\n",
        "insightspike_available = False\n",
        "logger = None\n",
        "\n",
        "try:\n",
        "    # Test basic import first\n",
        "    import insightspike\n",
        "    print(\"âœ… InsightSpike package found\")\n",
        "\n",
        "    # Try importing specific components\n",
        "    components_imported = []\n",
        "\n",
        "    try:\n",
        "        from insightspike.core.config import Config\n",
        "        components_imported.append(\"Config\")\n",
        "    except ImportError:\n",
        "        try:\n",
        "            # Alternative config import\n",
        "            from insightspike.config import Config\n",
        "            components_imported.append(\"Config (alt)\")\n",
        "        except ImportError:\n",
        "            print(\"âš ï¸ Config module not available - using defaults\")\n",
        "\n",
        "    try:\n",
        "        from insightspike.core.embeddings import EmbeddingEngine\n",
        "        components_imported.append(\"EmbeddingEngine\")\n",
        "    except ImportError:\n",
        "        print(\"âš ï¸ EmbeddingEngine not available - will use SentenceTransformers\")\n",
        "\n",
        "    try:\n",
        "        from insightspike.core.memory import MemorySystem\n",
        "        components_imported.append(\"MemorySystem\")\n",
        "    except ImportError:\n",
        "        print(\"âš ï¸ MemorySystem not available - will use custom implementation\")\n",
        "\n",
        "    try:\n",
        "        from insightspike.utils.logging import setup_logger\n",
        "        logger = setup_logger(\"phase1_experiment\", level=\"INFO\")\n",
        "        components_imported.append(\"Logger\")\n",
        "    except ImportError:\n",
        "        print(\"âš ï¸ InsightSpike logger not available - using standard logging\")\n",
        "\n",
        "    try:\n",
        "        from insightspike.core.agents.main_agent import MainAgent\n",
        "        components_imported.append(\"MainAgent\")\n",
        "\n",
        "        # Test instantiation\n",
        "        test_agent = MainAgent()\n",
        "        print(\"âœ… MainAgent instantiation successful\")\n",
        "        del test_agent  # Clean up\n",
        "        components_imported.append(\"MainAgent(tested)\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ MainAgent instantiation failed: {e}\")\n",
        "\n",
        "    if components_imported:\n",
        "        print(f\"âœ… InsightSpike components imported: {', '.join(components_imported)}\")\n",
        "        insightspike_available = True\n",
        "    else:\n",
        "        print(\"âš ï¸ No InsightSpike components could be imported\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"âš ï¸ InsightSpike package import failed: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Unexpected error during InsightSpike integration: {e}\")\n",
        "\n",
        "# Setup fallback logger if needed\n",
        "if logger is None:\n",
        "    import logging\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "    logger = logging.getLogger(\"phase1_experiment\")\n",
        "    print(\"âœ… Fallback logger configured\")\n",
        "\n",
        "# Log experiment start\n",
        "logger.info(\"Phase 1 Dynamic Memory experiment initialization...\")\n",
        "\n",
        "# Final status\n",
        "if insightspike_available:\n",
        "    print(\"ğŸ‰ InsightSpike integration completed successfully!\")\n",
        "else:\n",
        "    print(\"ğŸ”„ Using standalone implementation mode\")\n",
        "    print(\"   Experiment will proceed with custom implementations\")\n",
        "\n",
        "print(\"âœ… Module integration setup completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d37ca5ea",
      "metadata": {
        "id": "d37ca5ea"
      },
      "source": [
        "## ğŸ” GitHub Authentication (Optional)\n",
        "For accessing private repositories or enhanced features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6015cb0",
      "metadata": {
        "id": "a6015cb0"
      },
      "outputs": [],
      "source": [
        "# Optional: GitHub Authentication for enhanced features\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# Check if GitHub token is already set\n",
        "if \"GITHUB_TOKEN\" not in os.environ:\n",
        "    print(\"ğŸ”‘ GitHub token not found in environment\")\n",
        "    print(\"â„¹ï¸  You can set it for enhanced features (optional for this experiment)\")\n",
        "\n",
        "    # Uncomment the line below if you want to set a GitHub token\n",
        "    # os.environ[\"GITHUB_TOKEN\"] = getpass.getpass(\"Enter your GitHub Personal Access Token (optional): \")\n",
        "\n",
        "    if \"GITHUB_TOKEN\" in os.environ:\n",
        "        print(\"âœ… GitHub token configured\")\n",
        "    else:\n",
        "        print(\"â­ï¸  Skipping GitHub authentication - continuing with public access\")\n",
        "else:\n",
        "    print(\"âœ… GitHub token already configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca80b0f7",
      "metadata": {
        "id": "ca80b0f7"
      },
      "source": [
        "## ğŸ”§ Critical Fix 1: Global Device Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe9f5620",
      "metadata": {
        "id": "fe9f5620"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "from typing import Dict, List, Any, Tuple\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "# Suppress some warnings for cleaner output\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "# CRITICAL FIX: Global device definition\n",
        "def setup_device():\n",
        "    \"\"\"Setup optimal device with comprehensive GPU testing\"\"\"\n",
        "    print(\"ğŸ”§ Setting up compute device...\")\n",
        "\n",
        "    # Check CUDA availability\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            # Test GPU access\n",
        "            device_count = torch.cuda.device_count()\n",
        "            current_device = torch.cuda.current_device()\n",
        "            device_name = torch.cuda.get_device_name(current_device)\n",
        "\n",
        "            # Test GPU memory\n",
        "            total_memory = torch.cuda.get_device_properties(current_device).total_memory\n",
        "            memory_gb = total_memory / (1024**3)\n",
        "\n",
        "            print(f\"ğŸ® GPU detected: {device_name}\")\n",
        "            print(f\"ğŸ“Š GPU memory: {memory_gb:.1f} GB\")\n",
        "            print(f\"ğŸ”¢ GPU devices available: {device_count}\")\n",
        "\n",
        "            # Test actual GPU computation\n",
        "            test_tensor = torch.randn(100, 100, device='cuda')\n",
        "            _ = torch.mm(test_tensor, test_tensor)\n",
        "\n",
        "            device = torch.device('cuda')\n",
        "            print(\"âœ… GPU setup successful - using CUDA acceleration\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ GPU test failed: {e}\")\n",
        "            print(\"ğŸ”„ Falling back to CPU...\")\n",
        "            device = torch.device('cpu')\n",
        "    else:\n",
        "        print(\"ğŸ’» CUDA not available - using CPU\")\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    # Set as default device for new tensors\n",
        "    try:\n",
        "        torch.set_default_device(device)\n",
        "        print(f\"âœ… Default device set to: {device}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Could not set default device: {e}\")\n",
        "\n",
        "    return device\n",
        "\n",
        "# Setup device\n",
        "device = setup_device()\n",
        "\n",
        "print(f\"ğŸ¯ Default device set to: {device}\")\n",
        "\n",
        "# Print system information\n",
        "print(\"\\nğŸ“‹ System Information:\")\n",
        "print(f\"ğŸ Python: {torch.__version__}\")\n",
        "print(f\"ğŸ”¥ PyTorch: {torch.__version__}\")\n",
        "print(f\"ğŸ¯ Device: {device}\")\n",
        "print(f\"ğŸ“ˆ NumPy: {np.__version__}\")\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(f\"ğŸš€ CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"ğŸ’¾ GPU memory allocated: {torch.cuda.memory_allocated()/1024**2:.1f} MB\")\n",
        "    print(f\"ğŸ’¾ GPU memory reserved: {torch.cuda.memory_reserved()/1024**2:.1f} MB\")\n",
        "\n",
        "print(\"âœ… Device setup completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e75a0e2",
      "metadata": {
        "id": "7e75a0e2"
      },
      "source": [
        "## ğŸ“Š Critical Fix 2: Real Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bccb15a5",
      "metadata": {
        "id": "bccb15a5"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def load_real_data(max_samples: int = 1000) -> List[str]:\n",
        "    \"\"\"\n",
        "    Load real data from HuggingFace datasets.\n",
        "    Falls back to synthetic data if loading fails.\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "\n",
        "    try:\n",
        "        print(\"ğŸ“¥ Loading WikiText dataset...\")\n",
        "        # Load WikiText-2 dataset\n",
        "        dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "        # Extract non-empty text samples\n",
        "        for item in tqdm(dataset, desc=\"Processing texts\"):\n",
        "            text = item['text'].strip()\n",
        "            if len(text) > 50:  # Filter out very short texts\n",
        "                texts.append(text)\n",
        "                if len(texts) >= max_samples:\n",
        "                    break\n",
        "\n",
        "        print(f\"âœ… Loaded {len(texts)} real text samples\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Failed to load real data: {e}\")\n",
        "        print(\"ğŸ”„ Generating synthetic data...\")\n",
        "\n",
        "        # Synthetic fallback data\n",
        "        topics = [\"artificial intelligence\", \"machine learning\", \"deep learning\",\n",
        "                 \"neural networks\", \"computer vision\", \"natural language processing\",\n",
        "                 \"robotics\", \"data science\", \"big data\", \"cloud computing\"]\n",
        "\n",
        "        for i in range(max_samples):\n",
        "            topic = topics[i % len(topics)]\n",
        "            text = f\"This is document {i+1} about {topic}. It contains detailed information about the applications, methods, and recent advances in {topic}. The field has seen significant progress in recent years with new techniques and frameworks being developed.\"\n",
        "            texts.append(text)\n",
        "\n",
        "    return texts\n",
        "\n",
        "# Load data for experiment\n",
        "print(\"ğŸ”„ Loading experimental data...\")\n",
        "experiment_texts = load_real_data(max_samples=2000)\n",
        "print(f\"ğŸ“Š Total texts available: {len(experiment_texts)}\")\n",
        "print(f\"ğŸ“ Sample text: {experiment_texts[0][:200]}...\")\n",
        "\n",
        "# ğŸ” ä¿®æ­£ç‰ˆ CLIè¨ºæ–­ - Part 2: å®Ÿéš›ã®ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œãƒ†ã‚¹ãƒˆ\n",
        "\n",
        "def run_cli_command(cmd_args, description=\"command\", timeout=15):\n",
        "    \"\"\"CLIã‚³ãƒãƒ³ãƒ‰ã‚’å®‰å…¨ã«å®Ÿè¡Œ\"\"\"\n",
        "    try:\n",
        "        # ç’°å¢ƒå¤‰æ•°ã‚’é©åˆ‡ã«è¨­å®š\n",
        "        env = os.environ.copy()\n",
        "        env['PYTHONPATH'] = ':'.join([p for p in sys.path if p])\n",
        "        env['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "        # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é©åˆ‡ã«è¨­å®š\n",
        "        cwd = \"/content/InsightSpike-AI\" if '/content' in os.getcwd() else os.getcwd()\n",
        "\n",
        "        result = subprocess.run(\n",
        "            cmd_args,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=timeout,\n",
        "            env=env,\n",
        "            cwd=cwd\n",
        "        )\n",
        "\n",
        "        return result.returncode == 0, result.stdout, result.stderr, result.returncode\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        return False, \"\", f\"â° {description} ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ\", 124\n",
        "    except Exception as e:\n",
        "        return False, \"\", f\"âŒ {description} å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}\", 1\n",
        "\n",
        "print(\"ğŸ” CLIè¨ºæ–­ Part 2: ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œãƒ†ã‚¹ãƒˆ\")\n",
        "\n",
        "# ç’°å¢ƒæ¤œå‡º\n",
        "if '/content' in os.getcwd():\n",
        "    environment = \"Colab\"\n",
        "    project_path = \"/content/InsightSpike-AI\"\n",
        "else:\n",
        "    environment = \"Local\"\n",
        "    project_path = os.getcwd()\n",
        "\n",
        "print(f\"ğŸ“ ç’°å¢ƒ: {environment}\")\n",
        "\n",
        "# Poetryåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®ã¿ï¼‰\n",
        "poetry_available = False\n",
        "if environment == \"Local\":\n",
        "    try:\n",
        "        result = subprocess.run(['poetry', '--version'], capture_output=True, text=True, timeout=5)\n",
        "        if result.returncode == 0:\n",
        "            poetry_available = True\n",
        "            print(f\"âœ… Poetryåˆ©ç”¨å¯èƒ½: {result.stdout.strip()}\")\n",
        "        else:\n",
        "            print(\"âš ï¸ Poetryåˆ©ç”¨ä¸å¯\")\n",
        "    except:\n",
        "        print(\"âš ï¸ Poetryæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\")\n",
        "\n",
        "# pyproject.tomlç¢ºèª\n",
        "pyproject_path = os.path.join(project_path, 'pyproject.toml')\n",
        "if os.path.exists(pyproject_path):\n",
        "    print(\"\\nğŸ“‹ Poetry scripts ç¢ºèª:\")\n",
        "    try:\n",
        "        with open(pyproject_path, 'r') as f:\n",
        "            content = f.read()\n",
        "            if 'insightspike = \"insightspike.cli.main:main\"' in content:\n",
        "                lines = content.split('\\n')\n",
        "                in_scripts = False\n",
        "                for line in lines:\n",
        "                    if '[tool.poetry.scripts]' in line:\n",
        "                        in_scripts = True\n",
        "                        print(line)\n",
        "                    elif in_scripts and line.startswith('['):\n",
        "                        if 'tool.poetry.scripts' not in line:\n",
        "                            break\n",
        "                    elif in_scripts and line.strip():\n",
        "                        print(line)\n",
        "                print()\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ pyproject.tomlèª­ã¿è¾¼ã¿å¤±æ•—: {e}\")\n",
        "\n",
        "# CLIã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œãƒ†ã‚¹ãƒˆ\n",
        "print(\"ğŸ§ª CLIå®Ÿè¡Œãƒ†ã‚¹ãƒˆ:\")\n",
        "\n",
        "# 1. PoetryçµŒç”±ã§ã®ãƒ†ã‚¹ãƒˆï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®ã¿ï¼‰\n",
        "if environment == \"Local\" and poetry_available:\n",
        "    print(\"\\n1ï¸âƒ£ PoetryçµŒç”±ã§ã®CLIãƒ†ã‚¹ãƒˆ:\")\n",
        "    success, stdout, stderr, code = run_cli_command(['poetry', 'run', 'insightspike', '--help'], 'Poetry CLI')\n",
        "    if success:\n",
        "        print(\"âœ… Poetry CLI: å‹•ä½œç¢ºèª\")\n",
        "        print(f\"ğŸ“„ Helpå‡ºåŠ›ï¼ˆæŠœç²‹ï¼‰:\\n{stdout[:300]}...\")\n",
        "    else:\n",
        "        print(f\"âŒ Poetry CLIå¤±æ•— (code: {code})\")\n",
        "        if stderr:\n",
        "            print(f\"ã‚¨ãƒ©ãƒ¼: {stderr[:200]}...\")\n",
        "\n",
        "# 2. ç›´æ¥Pythonå®Ÿè¡Œãƒ†ã‚¹ãƒˆ\n",
        "print(f\"\\n2ï¸âƒ£ ç›´æ¥Pythonå®Ÿè¡Œãƒ†ã‚¹ãƒˆ:\")\n",
        "success, stdout, stderr, code = run_cli_command([sys.executable, '-m', 'insightspike.cli.main', '--help'], 'Direct Python CLI')\n",
        "if success:\n",
        "    print(\"âœ… Direct Python CLI: å‹•ä½œç¢ºèª\")\n",
        "    print(f\"ğŸ“„ Helpå‡ºåŠ›ï¼ˆæŠœç²‹ï¼‰:\\n{stdout[:300]}...\")\n",
        "else:\n",
        "    print(f\"âŒ Direct Python CLIå¤±æ•— (code: {code})\")\n",
        "    if stderr:\n",
        "        print(f\"ã‚¨ãƒ©ãƒ¼: {stderr[:200]}...\")\n",
        "\n",
        "# 3. å€‹åˆ¥ã‚³ãƒãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ\n",
        "print(f\"\\n3ï¸âƒ£ å€‹åˆ¥ã‚³ãƒãƒ³ãƒ‰å­˜åœ¨ç¢ºèª:\")\n",
        "\n",
        "test_commands = [\n",
        "    ('ask', '--help'),\n",
        "    ('deps', '--help'),\n",
        "    ('stats', '--help'),\n",
        "    ('config-info', '--help')\n",
        "]\n",
        "\n",
        "working_commands = []\n",
        "for cmd, flag in test_commands:\n",
        "    success, stdout, stderr, code = run_cli_command([sys.executable, '-m', 'insightspike.cli.main', cmd, flag], f'{cmd} command')\n",
        "\n",
        "    if success:\n",
        "        print(f\"âœ… '{cmd}' ã‚³ãƒãƒ³ãƒ‰: åˆ©ç”¨å¯èƒ½\")\n",
        "        working_commands.append(cmd)\n",
        "        # ãƒ˜ãƒ«ãƒ—ã®æœ€åˆã®è¡Œã‚’è¡¨ç¤º\n",
        "        if stdout:\n",
        "            first_line = stdout.split('\\n')[0] if stdout.split('\\n') else \"\"\n",
        "            print(f\"   ğŸ“‹ {first_line[:100]}...\")\n",
        "    else:\n",
        "        print(f\"âŒ '{cmd}' ã‚³ãƒãƒ³ãƒ‰å¤±æ•— (code: {code})\")\n",
        "        if 'No such command' in stderr or 'Invalid value' in stderr:\n",
        "            print(f\"   âš ï¸ ã‚³ãƒãƒ³ãƒ‰ãŒç™»éŒ²ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§\")\n",
        "        elif stderr:\n",
        "            print(f\"   ğŸ” ã‚¨ãƒ©ãƒ¼è©³ç´°: {stderr[:150]}...\")\n",
        "\n",
        "# 4. Colabç’°å¢ƒã§ã®CLI shortcut ãƒ†ã‚¹ãƒˆ\n",
        "if environment == \"Colab\":\n",
        "    print(f\"\\n4ï¸âƒ£ Colab CLI shortcut ãƒ†ã‚¹ãƒˆ:\")\n",
        "    if os.path.exists('/usr/local/bin/insightspike'):\n",
        "        success, stdout, stderr, code = run_cli_command(['insightspike', '--help'], 'CLI shortcut')\n",
        "        if success:\n",
        "            print(\"âœ… CLI shortcut: å‹•ä½œç¢ºèª\")\n",
        "            print(f\"ğŸ“„ å‡ºåŠ›ï¼ˆæŠœç²‹ï¼‰:\\n{stdout[:200]}...\")\n",
        "        else:\n",
        "            print(f\"âŒ CLI shortcutå¤±æ•— (code: {code})\")\n",
        "            if stderr:\n",
        "                print(f\"ã‚¨ãƒ©ãƒ¼: {stderr[:200]}...\")\n",
        "    else:\n",
        "        print(\"âš ï¸ CLI shortcutæœªä½œæˆ - ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
        "\n",
        "# çµæœã‚µãƒãƒªãƒ¼\n",
        "print(f\"\\nğŸ“Š CLIè¨ºæ–­çµæœ:\")\n",
        "print(f\"  â€¢ ç’°å¢ƒ: {environment}\")\n",
        "print(f\"  â€¢ Poetryåˆ©ç”¨å¯èƒ½: {'âœ…' if poetry_available else 'âŒ'}\")\n",
        "print(f\"  â€¢ å‹•ä½œç¢ºèªæ¸ˆã¿ã‚³ãƒãƒ³ãƒ‰: {len(working_commands)}/{len(test_commands)}\")\n",
        "if working_commands:\n",
        "    print(f\"  â€¢ åˆ©ç”¨å¯èƒ½: {', '.join(working_commands)}\")\n",
        "\n",
        "# Phase 2ã¸ã®æ¨å¥¨äº‹é …\n",
        "print(f\"\\nğŸš€ Phase 2 æº–å‚™çŠ¶æ³:\")\n",
        "if len(working_commands) >= 2:\n",
        "    print(\"âœ… Phase 2 ã«ååˆ†ãª CLIæ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½\")\n",
        "    print(\"ğŸ’¡ æ¨å¥¨å®Ÿè¡Œæ–¹æ³•:\")\n",
        "    if environment == \"Colab\":\n",
        "        print(\"  â€¢ !python -m insightspike.cli.main <command>\")\n",
        "        if os.path.exists('/usr/local/bin/insightspike'):\n",
        "            print(\"  â€¢ !insightspike <command>\")\n",
        "    else:\n",
        "        if poetry_available:\n",
        "            print(\"  â€¢ poetry run insightspike <command>\")\n",
        "        print(\"  â€¢ python -m insightspike.cli.main <command>\")\n",
        "else:\n",
        "    print(\"âš ï¸ CLIæ©Ÿèƒ½ã«åˆ¶é™ã‚ã‚Š - ä»£æ›¿ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¿…è¦\")\n",
        "    print(\"ğŸ’¡ ä»£æ›¿æ–¹æ³•:\")\n",
        "    print(\"  â€¢ Python APIçµŒç”±ã§ã®ç›´æ¥åˆ¶å¾¡\")\n",
        "    print(\"  â€¢ å€‹åˆ¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ\")\n",
        "\n",
        "print(\"\\nâœ… CLIè¨ºæ–­ Part 2 å®Œäº†\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c503435",
      "metadata": {
        "id": "6c503435"
      },
      "outputs": [],
      "source": [
        "# ğŸš€ Phase 2 æº–å‚™: Colab CLI å®Œå…¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "def setup_colab_cli_complete():\n",
        "    \"\"\"Colabç’°å¢ƒã§ã®CLIå®Œå…¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\"\"\"\n",
        "\n",
        "    # ç’°å¢ƒæ¤œå‡º\n",
        "    if '/content' not in os.getcwd():\n",
        "        print(\"ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ - CLIã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—\")\n",
        "        return True\n",
        "\n",
        "    print(\"ğŸ”§ Colabç’°å¢ƒã§ã®CLIå®Œå…¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹...\")\n",
        "\n",
        "    project_path = \"/content/InsightSpike-AI\"\n",
        "\n",
        "    # 1. Python path ã¨ç’°å¢ƒå¤‰æ•°ã®å®Œå…¨è¨­å®š\n",
        "    print(\"1ï¸âƒ£ Pythonç’°å¢ƒè¨­å®š...\")\n",
        "\n",
        "    paths_to_add = [\n",
        "        f\"{project_path}/src\",\n",
        "        project_path,\n",
        "        f\"{project_path}/scripts\"\n",
        "    ]\n",
        "\n",
        "    added_count = 0\n",
        "    for path in paths_to_add:\n",
        "        if os.path.exists(path) and path not in sys.path:\n",
        "            sys.path.insert(0, path)\n",
        "            added_count += 1\n",
        "\n",
        "    # ç’°å¢ƒå¤‰æ•°è¨­å®š\n",
        "    env_vars = {\n",
        "        'PYTHONPATH': ':'.join([p for p in sys.path if p and os.path.exists(p)]),\n",
        "        'TOKENIZERS_PARALLELISM': 'false',\n",
        "        'INSIGHTSPIKE_ENV': 'colab',\n",
        "        'TRANSFORMERS_CACHE': '/content/cache',\n",
        "        'HF_HOME': '/content/cache'\n",
        "    }\n",
        "\n",
        "    for key, value in env_vars.items():\n",
        "        os.environ[key] = value\n",
        "\n",
        "    print(f\"âœ… Python ãƒ‘ã‚¹è¿½åŠ : {added_count} paths\")\n",
        "    print(f\"âœ… ç’°å¢ƒå¤‰æ•°è¨­å®š: {len(env_vars)} variables\")\n",
        "\n",
        "    # 2. Poetry ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆè©¦è¡Œï¼‰\n",
        "    print(\"\\n2ï¸âƒ£ Poetry ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\")\n",
        "    poetry_installed = False\n",
        "    try:\n",
        "        # Poetry ãŒæ—¢ã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
        "        result = subprocess.run(['poetry', '--version'], capture_output=True, text=True, timeout=5)\n",
        "        if result.returncode == 0:\n",
        "            print(f\"âœ… Poetry æ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿: {result.stdout.strip()}\")\n",
        "            poetry_installed = True\n",
        "        else:\n",
        "            raise FileNotFoundError\n",
        "    except:\n",
        "        try:\n",
        "            print(\"ğŸ“¦ Poetry ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Ÿè¡Œä¸­...\")\n",
        "            subprocess.run([\n",
        "                sys.executable, '-m', 'pip', 'install', 'poetry'\n",
        "            ], check=True, capture_output=True, timeout=60)\n",
        "\n",
        "            # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª\n",
        "            result = subprocess.run(['poetry', '--version'], capture_output=True, text=True, timeout=5)\n",
        "            if result.returncode == 0:\n",
        "                print(f\"âœ… Poetry ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æˆåŠŸ: {result.stdout.strip()}\")\n",
        "                poetry_installed = True\n",
        "            else:\n",
        "                print(\"âš ï¸ Poetry ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸãŒå‹•ä½œç¢ºèªå¤±æ•—\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Poetry ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—: {str(e)[:100]}...\")\n",
        "\n",
        "    # 3. é«˜æ©Ÿèƒ½CLI wrapper ä½œæˆ\n",
        "    print(\"\\n3ï¸âƒ£ CLI wrapper ä½œæˆ...\")\n",
        "\n",
        "    cli_wrapper_content = f'''#!/bin/bash\n",
        "# InsightSpike CLI Wrapper for Colab (Enhanced Version)\n",
        "export PYTHONPATH=\"{env_vars['PYTHONPATH']}\"\n",
        "export TOKENIZERS_PARALLELISM=\"{env_vars['TOKENIZERS_PARALLELISM']}\"\n",
        "export INSIGHTSPIKE_ENV=\"{env_vars['INSIGHTSPIKE_ENV']}\"\n",
        "export TRANSFORMERS_CACHE=\"{env_vars['TRANSFORMERS_CACHE']}\"\n",
        "export HF_HOME=\"{env_vars['HF_HOME']}\"\n",
        "\n",
        "cd {project_path}\n",
        "\n",
        "# ãƒ‡ãƒãƒƒã‚°æƒ…å ±å‡ºåŠ›ï¼ˆ--debug ãƒ•ãƒ©ã‚°æ™‚ï¼‰\n",
        "if [[ \"$1\" == \"--debug\" ]]; then\n",
        "    echo \"ğŸ” Debug Info:\"\n",
        "    echo \"  PYTHONPATH: $PYTHONPATH\"\n",
        "    echo \"  Working Dir: $(pwd)\"\n",
        "    echo \"  Python: {sys.executable}\"\n",
        "    shift  # --debug ãƒ•ãƒ©ã‚°ã‚’é™¤å»\n",
        "fi\n",
        "\n",
        "# å®Ÿè¡Œæ–¹æ³•ã®å„ªå…ˆé †ä½ï¼šPoetry > ç›´æ¥å®Ÿè¡Œ\n",
        "if command -v poetry >/dev/null 2>&1 && [ -f \"pyproject.toml\" ]; then\n",
        "    poetry run python -m insightspike.cli.main \"$@\"\n",
        "else\n",
        "    {sys.executable} -m insightspike.cli.main \"$@\"\n",
        "fi\n",
        "'''\n",
        "\n",
        "    cli_path = \"/usr/local/bin/insightspike\"\n",
        "    try:\n",
        "        with open(cli_path, 'w') as f:\n",
        "            f.write(cli_wrapper_content)\n",
        "        os.chmod(cli_path, 0o755)\n",
        "        print(f\"âœ… CLI wrapper ä½œæˆæˆåŠŸ: {cli_path}\")\n",
        "\n",
        "        # CLI wrapper ãƒ†ã‚¹ãƒˆ\n",
        "        test_result = subprocess.run(['insightspike', '--help'],\n",
        "                                   capture_output=True, text=True, timeout=10)\n",
        "        if test_result.returncode == 0:\n",
        "            print(\"âœ… CLI wrapper å‹•ä½œç¢ºèª\")\n",
        "            wrapper_working = True\n",
        "        else:\n",
        "            print(f\"âš ï¸ CLI wrapper å•é¡Œã‚ã‚Š: {test_result.stderr[:100]}...\")\n",
        "            wrapper_working = False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ CLI wrapper ä½œæˆå¤±æ•—: {e}\")\n",
        "        wrapper_working = False\n",
        "\n",
        "    # 4. CLIæ©Ÿèƒ½ç·åˆãƒ†ã‚¹ãƒˆ\n",
        "    print(\"\\n4ï¸âƒ£ CLIæ©Ÿèƒ½ç·åˆãƒ†ã‚¹ãƒˆ...\")\n",
        "\n",
        "    test_scenarios = [\n",
        "        (['python', '-m', 'insightspike.cli.main', '--help'], \"Direct Python\"),\n",
        "        (['insightspike', '--help'], \"CLI Wrapper\") if wrapper_working else None,\n",
        "        (['poetry', 'run', 'insightspike', '--help'], \"Poetry\") if poetry_installed else None\n",
        "    ]\n",
        "\n",
        "    # None ã‚’é™¤å»\n",
        "    test_scenarios = [t for t in test_scenarios if t is not None]\n",
        "\n",
        "    working_methods = []\n",
        "    for cmd, name in test_scenarios:\n",
        "        try:\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True,\n",
        "                                  timeout=10, cwd=project_path)\n",
        "            if result.returncode == 0 and 'Usage:' in result.stdout:\n",
        "                print(f\"  âœ… {name}: å‹•ä½œç¢ºèª\")\n",
        "                working_methods.append(name)\n",
        "            else:\n",
        "                print(f\"  âŒ {name}: å¤±æ•— (code: {result.returncode})\")\n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ {name}: ã‚¨ãƒ©ãƒ¼ ({str(e)[:50]}...)\")\n",
        "\n",
        "    # 5. ä¸»è¦ã‚³ãƒãƒ³ãƒ‰å­˜åœ¨ç¢ºèª\n",
        "    print(\"\\n5ï¸âƒ£ ä¸»è¦ã‚³ãƒãƒ³ãƒ‰ç¢ºèª...\")\n",
        "\n",
        "    primary_commands = ['ask', 'deps', 'stats', 'config-info']\n",
        "    available_commands = []\n",
        "\n",
        "    for cmd in primary_commands:\n",
        "        try:\n",
        "            result = subprocess.run([sys.executable, '-m', 'insightspike.cli.main', cmd, '--help'],\n",
        "                                  capture_output=True, text=True, timeout=10, cwd=project_path)\n",
        "            if result.returncode == 0:\n",
        "                print(f\"  âœ… '{cmd}' ã‚³ãƒãƒ³ãƒ‰: åˆ©ç”¨å¯èƒ½\")\n",
        "                available_commands.append(cmd)\n",
        "            else:\n",
        "                print(f\"  âŒ '{cmd}' ã‚³ãƒãƒ³ãƒ‰: æœªå¯¾å¿œ\")\n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ '{cmd}' ã‚³ãƒãƒ³ãƒ‰: ã‚¨ãƒ©ãƒ¼\")\n",
        "\n",
        "    # 6. Python API ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
        "    print(\"\\n6ï¸âƒ£ Python API ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—...\")\n",
        "\n",
        "    try:\n",
        "        from insightspike.cli.main import app as cli_app\n",
        "        globals()['cli_app'] = cli_app\n",
        "        print(\"âœ… CLI Python API: åˆ©ç”¨å¯èƒ½\")\n",
        "        print(\"   ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•: cli_app ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆçµŒç”±\")\n",
        "        api_available = True\n",
        "    except ImportError as e:\n",
        "        print(f\"âŒ CLI Python API: ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•— ({str(e)[:50]}...)\")\n",
        "        api_available = False\n",
        "\n",
        "    # çµæœã‚µãƒãƒªãƒ¼\n",
        "    print(f\"\\nğŸ¯ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ã‚µãƒãƒªãƒ¼:\")\n",
        "    print(f\"  â€¢ Poetry: {'âœ…' if poetry_installed else 'âŒ'}\")\n",
        "    print(f\"  â€¢ CLI Wrapper: {'âœ…' if wrapper_working else 'âŒ'}\")\n",
        "    print(f\"  â€¢ å‹•ä½œã™ã‚‹å®Ÿè¡Œæ–¹æ³•: {len(working_methods)}\")\n",
        "    print(f\"  â€¢ åˆ©ç”¨å¯èƒ½ã‚³ãƒãƒ³ãƒ‰: {len(available_commands)}/{len(primary_commands)}\")\n",
        "    print(f\"  â€¢ Python API: {'âœ…' if api_available else 'âŒ'}\")\n",
        "\n",
        "    # Phase 2 æº–å‚™çŠ¶æ³åˆ¤å®š\n",
        "    phase2_ready = (len(working_methods) > 0 and len(available_commands) >= 2)\n",
        "\n",
        "    if phase2_ready:\n",
        "        print(\"\\nğŸš€ Phase 2 æº–å‚™å®Œäº†!\")\n",
        "        print(\"ğŸ’¡ æ¨å¥¨ä½¿ç”¨æ–¹æ³•:\")\n",
        "        for method in working_methods:\n",
        "            if method == \"CLI Wrapper\":\n",
        "                print(\"  â€¢ !insightspike <command>\")\n",
        "            elif method == \"Direct Python\":\n",
        "                print(\"  â€¢ !python -m insightspike.cli.main <command>\")\n",
        "            elif method == \"Poetry\":\n",
        "                print(\"  â€¢ !poetry run insightspike <command>\")\n",
        "\n",
        "        if api_available:\n",
        "            print(\"  â€¢ Python API: cli_app.<method>()\")\n",
        "    else:\n",
        "        print(\"\\nâš ï¸ Phase 2 æº–å‚™ã«åˆ¶é™ã‚ã‚Š\")\n",
        "        print(\"ğŸ’¡ åˆ©ç”¨å¯èƒ½ãªä»£æ›¿æ–¹æ³•:\")\n",
        "        print(\"  â€¢ å€‹åˆ¥Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ\")\n",
        "        print(\"  â€¢ ç›´æ¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\")\n",
        "\n",
        "    return phase2_ready\n",
        "\n",
        "# ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Ÿè¡Œ\n",
        "cli_ready = setup_colab_cli_complete()\n",
        "\n",
        "if cli_ready:\n",
        "    print(\"\\nğŸ‰ CLI ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æˆåŠŸ - Phase 2 å®Ÿè¡Œæº–å‚™å®Œäº†!\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ CLI ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«åˆ¶é™ - ä»£æ›¿ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ Phase 2 å®Ÿè¡Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edae9fc3",
      "metadata": {
        "id": "edae9fc3"
      },
      "source": [
        "## ğŸš€ Phase 2 æº–å‚™: CLIæ©Ÿèƒ½å¼·åŒ–\n",
        "\n",
        "### âœ… ä¿®æ­£ã•ã‚ŒãŸæ©Ÿèƒ½\n",
        "\n",
        "**1. Colabç’°å¢ƒã§ã®CLIå¯¾å¿œ**\n",
        "- Poetry ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½\n",
        "- CLI wrapper ã‚¹ã‚¯ãƒªãƒ—ãƒˆè‡ªå‹•ç”Ÿæˆ\n",
        "- ç’°å¢ƒå¤‰æ•°ã¨Pythonãƒ‘ã‚¹è¨­å®š\n",
        "\n",
        "**2. è¤‡æ•°ã®CLIå®Ÿè¡Œæ–¹æ³•**\n",
        "- `!insightspike --help` - ç›´æ¥ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ\n",
        "- `python -m insightspike.cli.main` - ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œ\n",
        "- `cli_app` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ - ãƒ—ãƒ­ã‚°ãƒ©ãƒ å†…åˆ¶å¾¡\n",
        "\n",
        "**3. Phase 2 å¿…é ˆæ©Ÿèƒ½æº–å‚™**\n",
        "- `insightspike deps` - ä¾å­˜é–¢ä¿‚ç®¡ç†\n",
        "- `insightspike ask` - ã‚¯ã‚¨ãƒªå‡¦ç†\n",
        "- å®Ÿé¨“åˆ¶å¾¡ã¨ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–\n",
        "\n",
        "### ğŸ¯ Phase 2ã§ã®åˆ©ç”¨æƒ³å®š\n",
        "\n",
        "```bash\n",
        "# å®Ÿé¨“é–‹å§‹\n",
        "!insightspike experiment start phase2\n",
        "\n",
        "# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–\n",
        "!insightspike monitor --live\n",
        "\n",
        "# çµæœåˆ†æ\n",
        "!insightspike analyze --output json\n",
        "```\n",
        "\n",
        "**æ¬¡ã®ã‚»ãƒ«ã§CLIè¨­å®šã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ â†’**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1998b416",
      "metadata": {
        "id": "1998b416"
      },
      "source": [
        "## ğŸ§  Critical Fix 3: Improved Memory Systems Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f7cf30",
      "metadata": {
        "id": "14f7cf30"
      },
      "outputs": [],
      "source": [
        "# Enhanced FAISS installation for Colab with better error handling\n",
        "import subprocess\n",
        "import sys\n",
        "import importlib\n",
        "import os\n",
        "\n",
        "# Enhanced FAISS installation with Colab-specific optimizations\n",
        "faiss = None\n",
        "faiss_available = False\n",
        "\n",
        "def install_faiss_colab():\n",
        "    \"\"\"Install FAISS with Colab-optimized approach\"\"\"\n",
        "    print(\"ğŸ”§ Installing FAISS for Colab environment...\")\n",
        "\n",
        "    # Force CPU version for better Colab compatibility\n",
        "    candidates = [\"faiss-cpu\"]\n",
        "\n",
        "    # Only try GPU if we're absolutely sure CUDA works\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.cuda.is_available():\n",
        "            # Test actual CUDA functionality\n",
        "            test_tensor = torch.randn(10, 10, device='cuda')\n",
        "            _ = torch.mm(test_tensor, test_tensor)\n",
        "            print(\"âœ… CUDA test passed - will try GPU FAISS\")\n",
        "            candidates = [\"faiss-gpu\", \"faiss-cpu\"]\n",
        "        else:\n",
        "            print(\"ğŸ’» CUDA not available - using CPU FAISS\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ CUDA test failed: {e} - using CPU FAISS\")\n",
        "\n",
        "    # Try installing FAISS variants\n",
        "    for package in candidates:\n",
        "        try:\n",
        "            print(f\"ğŸ“¦ Installing {package}...\")\n",
        "\n",
        "            # Use specific Colab-friendly installation\n",
        "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", package, \"--upgrade\", \"--no-cache-dir\"]\n",
        "\n",
        "            # For GPU version, try with specific constraints\n",
        "            if package == \"faiss-gpu\":\n",
        "                # Add constraints that work better in Colab\n",
        "                cmd.extend([\"--extra-index-url\", \"https://download.pytorch.org/whl/cu118\"])\n",
        "\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(f\"âœ… {package} installation completed\")\n",
        "\n",
        "                # Test import immediately\n",
        "                try:\n",
        "                    # Clear any cached imports\n",
        "                    if 'faiss' in sys.modules:\n",
        "                        del sys.modules['faiss']\n",
        "\n",
        "                    import faiss\n",
        "                    print(f\"âœ… {package} import successful\")\n",
        "\n",
        "                    # Quick functionality test\n",
        "                    test_dim = 64\n",
        "                    test_index = faiss.IndexFlatL2(test_dim)\n",
        "                    test_vectors = [[1.0] * test_dim]\n",
        "                    test_index.add(test_vectors)\n",
        "\n",
        "                    print(f\"âœ… {package} functionality test passed\")\n",
        "                    return faiss\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"âš ï¸ {package} import/test failed: {e}\")\n",
        "                    continue\n",
        "            else:\n",
        "                print(f\"âš ï¸ {package} installation failed: {result.stderr[:200]}\")\n",
        "\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(f\"âš ï¸ {package} installation timed out\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ {package} installation error: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "# Try to import FAISS\n",
        "try:\n",
        "    import faiss\n",
        "    print(\"âœ… FAISS already available\")\n",
        "\n",
        "    # Test functionality\n",
        "    test_index = faiss.IndexFlatL2(64)\n",
        "    print(f\"âœ… FAISS working (version: {getattr(faiss, '__version__', 'unknown')})\")\n",
        "    faiss_available = True\n",
        "\n",
        "except ImportError:\n",
        "    print(\"ğŸ“¦ FAISS not found, installing...\")\n",
        "    faiss = install_faiss_colab()\n",
        "    faiss_available = faiss is not None\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ FAISS test failed: {e}\")\n",
        "    print(\"ğŸ“¦ Reinstalling FAISS...\")\n",
        "    faiss = install_faiss_colab()\n",
        "    faiss_available = faiss is not None\n",
        "\n",
        "if not faiss_available:\n",
        "    print(\"âš ï¸ FAISS installation failed completely\")\n",
        "    print(\"ğŸ”„ Experiment will use enhanced baseline-only implementation\")\n",
        "    faiss = None\n",
        "else:\n",
        "    print(f\"ğŸ‰ FAISS ready for use!\")\n",
        "\n",
        "# Import other dependencies\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import time\n",
        "\n",
        "class BaselineRAGSystem:\n",
        "    \"\"\"Enhanced baseline memory system using optimized similarity search\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        print(f\"ğŸ”§ Initializing baseline system with {model_name}...\")\n",
        "        self.model = SentenceTransformer(model_name, device=str(device))\n",
        "        self.texts = []\n",
        "        self.embeddings = None\n",
        "        print(\"âœ… Baseline system ready\")\n",
        "\n",
        "    def add_documents(self, texts: List[str]):\n",
        "        \"\"\"Add documents to memory with batched processing\"\"\"\n",
        "        if not texts:\n",
        "            return\n",
        "\n",
        "        print(f\"ğŸ“ Adding {len(texts)} documents to baseline system...\")\n",
        "        self.texts.extend(texts)\n",
        "\n",
        "        # Process in smaller batches for stability\n",
        "        batch_size = 16 if device.type == 'cuda' else 32\n",
        "        new_embeddings = []\n",
        "\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i+batch_size]\n",
        "            batch_emb = self.model.encode(batch, convert_to_tensor=True, show_progress_bar=False)\n",
        "            new_embeddings.append(batch_emb)\n",
        "\n",
        "            # Memory management for GPU\n",
        "            if device.type == 'cuda' and i % 64 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        new_embeddings = torch.cat(new_embeddings, dim=0)\n",
        "\n",
        "        if self.embeddings is None:\n",
        "            self.embeddings = new_embeddings\n",
        "        else:\n",
        "            self.embeddings = torch.cat([self.embeddings, new_embeddings], dim=0)\n",
        "\n",
        "        print(f\"âœ… Total documents in baseline: {len(self.texts)}\")\n",
        "\n",
        "    def search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Search for similar documents with optimized similarity computation\"\"\"\n",
        "        if not self.texts or self.embeddings is None:\n",
        "            return []\n",
        "\n",
        "        query_embedding = self.model.encode([query], convert_to_tensor=True)\n",
        "\n",
        "        # Compute similarities with normalized embeddings for better results\n",
        "        query_norm = torch.nn.functional.normalize(query_embedding, p=2, dim=1)\n",
        "        doc_norm = torch.nn.functional.normalize(self.embeddings, p=2, dim=1)\n",
        "        similarities = torch.mm(query_norm, doc_norm.T).squeeze()\n",
        "\n",
        "        # Get top-k results\n",
        "        top_indices = torch.topk(similarities, min(top_k, len(self.texts))).indices\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            results.append((self.texts[idx.item()], similarities[idx.item()].item()))\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "# FAISS-based improved system (if FAISS is available)\n",
        "if faiss_available and faiss is not None:\n",
        "    class FaissIndexedMemorySystem:\n",
        "        \"\"\"Improved memory system using FAISS indexing with enhanced Colab compatibility\"\"\"\n",
        "\n",
        "        def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "            print(f\"ğŸš€ Initializing FAISS system with {model_name}...\")\n",
        "            self.model = SentenceTransformer(model_name, device=str(device))\n",
        "            self.texts = []\n",
        "            self.index = None\n",
        "            self.dimension = None\n",
        "            self.gpu_resources = None\n",
        "            print(\"âœ… FAISS system ready\")\n",
        "\n",
        "        def add_documents(self, texts: List[str]):\n",
        "            \"\"\"Add documents to FAISS index with optimized batching\"\"\"\n",
        "            if not texts:\n",
        "                return\n",
        "\n",
        "            print(f\"ğŸš€ Adding {len(texts)} documents to FAISS index...\")\n",
        "\n",
        "            # Generate embeddings in smaller batches for stability\n",
        "            batch_size = 16 if device.type == 'cuda' else 32\n",
        "            all_embeddings = []\n",
        "\n",
        "            for i in range(0, len(texts), batch_size):\n",
        "                batch = texts[i:i+batch_size]\n",
        "                batch_emb = self.model.encode(batch, show_progress_bar=False)\n",
        "                all_embeddings.append(batch_emb)\n",
        "\n",
        "                # Memory management\n",
        "                if device.type == 'cuda' and i % 64 == 0:\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            embeddings = np.vstack(all_embeddings).astype('float32')\n",
        "\n",
        "            # Initialize index on first use\n",
        "            if self.index is None:\n",
        "                self.dimension = embeddings.shape[1]\n",
        "                self._initialize_index()\n",
        "\n",
        "            # Normalize and add to index\n",
        "            faiss.normalize_L2(embeddings)\n",
        "            self.index.add(embeddings)\n",
        "            self.texts.extend(texts)\n",
        "\n",
        "            print(f\"âœ… Total documents in FAISS: {len(self.texts)}\")\n",
        "\n",
        "        def _initialize_index(self):\n",
        "            \"\"\"Initialize FAISS index with enhanced Colab compatibility\"\"\"\n",
        "            try:\n",
        "                # For Colab, prefer CPU index for stability\n",
        "                print(\"ğŸ’» Initializing FAISS CPU index for optimal Colab compatibility...\")\n",
        "                self.index = faiss.IndexFlatIP(self.dimension)\n",
        "                print(\"âœ… FAISS CPU index initialized successfully\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ FAISS index initialization failed: {e}\")\n",
        "                raise\n",
        "\n",
        "        def search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
        "            \"\"\"Search using FAISS index with error handling\"\"\"\n",
        "            if not self.texts or self.index is None:\n",
        "                return []\n",
        "\n",
        "            try:\n",
        "                # Generate query embedding\n",
        "                query_embedding = self.model.encode([query]).astype('float32')\n",
        "                faiss.normalize_L2(query_embedding)\n",
        "\n",
        "                # Search\n",
        "                k = min(top_k, len(self.texts))\n",
        "                scores, indices = self.index.search(query_embedding, k)\n",
        "\n",
        "                results = []\n",
        "                for i in range(k):\n",
        "                    if indices[0][i] >= 0:  # Valid index\n",
        "                        results.append((self.texts[indices[0][i]], scores[0][i]))\n",
        "\n",
        "                return results\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ FAISS search error: {e}\")\n",
        "                return []\n",
        "\n",
        "    print(\"âœ… FAISS-based memory system initialized\")\n",
        "else:\n",
        "    # Enhanced fallback system\n",
        "    class FaissIndexedMemorySystem:\n",
        "        \"\"\"Enhanced fallback system when FAISS is not available\"\"\"\n",
        "        def __init__(self, *args, **kwargs):\n",
        "            print(\"ğŸ”„ Using enhanced baseline fallback (FAISS not available)\")\n",
        "            self.baseline = BaselineRAGSystem(*args, **kwargs)\n",
        "\n",
        "        def add_documents(self, texts):\n",
        "            return self.baseline.add_documents(texts)\n",
        "\n",
        "        def search(self, query, top_k=5):\n",
        "            return self.baseline.search(query, top_k)\n",
        "\n",
        "print(\"ğŸ§  Memory systems successfully initialized!\")\n",
        "print(f\"ğŸ’¡ Using {'FAISS-enhanced' if faiss_available else 'baseline-only'} implementation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cda8ad0",
      "metadata": {
        "id": "7cda8ad0"
      },
      "source": [
        "## ğŸ”¬ Working Experiment Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0643403",
      "metadata": {
        "id": "f0643403"
      },
      "outputs": [],
      "source": [
        "def run_memory_experiment(texts: List[str], data_sizes: List[int]) -> Dict[str, Any]:\n",
        "    \"\"\"Run complete memory system comparison experiment\"\"\"\n",
        "\n",
        "    results = {\n",
        "        'data_sizes': data_sizes,\n",
        "        'baseline_times': [],\n",
        "        'improved_times': [],\n",
        "        'baseline_accuracy': [],\n",
        "        'improved_accuracy': [],\n",
        "        'baseline_memory': [],\n",
        "        'improved_memory': [],\n",
        "        'experiment_timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Sample queries for testing\n",
        "    test_queries = [\n",
        "        \"artificial intelligence applications\",\n",
        "        \"machine learning algorithms\",\n",
        "        \"deep neural networks\",\n",
        "        \"computer vision techniques\",\n",
        "        \"natural language processing\"\n",
        "    ]\n",
        "\n",
        "    for size in tqdm(data_sizes, desc=\"Testing data sizes\"):\n",
        "        print(f\"\\nğŸ”¬ Testing with {size} documents...\")\n",
        "\n",
        "        # Prepare data subset\n",
        "        data_subset = texts[:size]\n",
        "\n",
        "        # Test Baseline System\n",
        "        print(\"  ğŸ“Š Testing baseline system...\")\n",
        "        baseline_system = BaselineRAGSystem()\n",
        "\n",
        "        # Measure baseline performance\n",
        "        start_time = time.time()\n",
        "        baseline_system.add_documents(data_subset)\n",
        "\n",
        "        # Run queries\n",
        "        baseline_results = []\n",
        "        for query in test_queries:\n",
        "            results_query = baseline_system.search(query, top_k=3)\n",
        "            baseline_results.extend(results_query)\n",
        "\n",
        "        baseline_time = time.time() - start_time\n",
        "        baseline_memory_mb = torch.cuda.memory_allocated() / 1024**2 if device.type == 'cuda' else 0\n",
        "\n",
        "        # Test Improved System\n",
        "        print(\"  ğŸš€ Testing improved FAISS system...\")\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.empty_cache()  # Clear GPU memory\n",
        "\n",
        "        improved_system = FaissIndexedMemorySystem()  # Fixed class name\n",
        "\n",
        "        # Measure improved performance\n",
        "        start_time = time.time()\n",
        "        improved_system.add_documents(data_subset)\n",
        "\n",
        "        # Run queries\n",
        "        improved_results = []\n",
        "        for query in test_queries:\n",
        "            results_query = improved_system.search(query, top_k=3)\n",
        "            improved_results.extend(results_query)\n",
        "\n",
        "        improved_time = time.time() - start_time\n",
        "        improved_memory_mb = torch.cuda.memory_allocated() / 1024**2 if device.type == 'cuda' else 0\n",
        "\n",
        "        # Calculate accuracy metrics (simplified - could be more sophisticated)\n",
        "        baseline_avg_score = np.mean([score for _, score in baseline_results]) if baseline_results else 0\n",
        "        improved_avg_score = np.mean([score for _, score in improved_results]) if improved_results else 0\n",
        "\n",
        "        # Store results\n",
        "        results['baseline_times'].append(baseline_time)\n",
        "        results['improved_times'].append(improved_time)\n",
        "        results['baseline_accuracy'].append(baseline_avg_score)\n",
        "        results['improved_accuracy'].append(improved_avg_score)\n",
        "        results['baseline_memory'].append(baseline_memory_mb)\n",
        "        results['improved_memory'].append(improved_memory_mb)\n",
        "\n",
        "        print(f\"    â±ï¸  Baseline: {baseline_time:.2f}s, Improved: {improved_time:.2f}s\")\n",
        "        print(f\"    ğŸ“ˆ Baseline accuracy: {baseline_avg_score:.3f}, Improved: {improved_avg_score:.3f}\")\n",
        "        print(f\"    ğŸ’¾ Memory usage - Baseline: {baseline_memory_mb:.1f}MB, Improved: {improved_memory_mb:.1f}MB\")\n",
        "\n",
        "        # Clean up\n",
        "        del baseline_system, improved_system\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the experiment\n",
        "print(\"ğŸš€ Starting memory system comparison experiment...\")\n",
        "data_sizes = [100, 250, 500, 1000, min(1500, len(experiment_texts))]\n",
        "experiment_results = run_memory_experiment(experiment_texts, data_sizes)\n",
        "\n",
        "print(\"\\nâœ… Experiment completed successfully!\")\n",
        "\n",
        "# Display results summary\n",
        "print(\"\\nğŸ“Š EXPERIMENT RESULTS SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "df_results = pd.DataFrame({\n",
        "    'Data Size': experiment_results['data_sizes'],\n",
        "    'Baseline Time (s)': experiment_results['baseline_times'],\n",
        "    'FAISS Time (s)': experiment_results['improved_times'],\n",
        "    'Baseline Accuracy': experiment_results['baseline_accuracy'],\n",
        "    'FAISS Accuracy': experiment_results['improved_accuracy']\n",
        "})\n",
        "\n",
        "print(df_results.to_string(index=False))\n",
        "\n",
        "# Calculate performance improvements\n",
        "time_improvements = [(b-i)/b*100 for b, i in zip(experiment_results['baseline_times'], experiment_results['improved_times'])]\n",
        "accuracy_improvements = [(i-b)/b*100 for b, i in zip(experiment_results['baseline_accuracy'], experiment_results['improved_accuracy'])]\n",
        "\n",
        "print(f\"\\nâš¡ Average time improvement: {np.mean(time_improvements):.1f}%\")\n",
        "print(f\"ğŸ¯ Average accuracy improvement: {np.mean(accuracy_improvements):.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caee3d81",
      "metadata": {
        "id": "caee3d81"
      },
      "source": [
        "## ğŸ’¾ Critical Fix 4: Results Saving and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4484b7e8",
      "metadata": {
        "id": "4484b7e8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Results saving and visualization (simplified for reliability)\n",
        "def save_and_visualize_results(results: Dict) -> str:\n",
        "    \"\"\"Save experiment results and create visualizations\"\"\"\n",
        "\n",
        "    # Create results directory (compatible with both Colab and local)\n",
        "    if os.path.exists('/content'):\n",
        "        results_dir = \"/content/phase1_results\"\n",
        "    else:\n",
        "        # Local environment - use current working directory\n",
        "        results_dir = os.path.join(os.getcwd(), \"phase1_results\")\n",
        "\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    # Convert numpy types to Python types for JSON serialization\n",
        "    def convert_to_python_types(obj):\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, dict):\n",
        "            return {k: convert_to_python_types(v) for k, v in obj.items()}\n",
        "        elif isinstance(obj, list):\n",
        "            return [convert_to_python_types(item) for item in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    # Save results as JSON\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    json_path = os.path.join(results_dir, f\"experiment_results_{timestamp}.json\")\n",
        "\n",
        "    # Convert results to JSON-serializable format\n",
        "    serializable_results = convert_to_python_types(results)\n",
        "\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(serializable_results, f, indent=2)\n",
        "\n",
        "    # Save as CSV for easy analysis\n",
        "    csv_path = os.path.join(results_dir, f\"experiment_results_{timestamp}.csv\")\n",
        "    df_results = pd.DataFrame({\n",
        "        'data_size': results['data_sizes'],\n",
        "        'baseline_time': results['baseline_times'],\n",
        "        'faiss_time': results['improved_times'],\n",
        "        'baseline_accuracy': results['baseline_accuracy'],\n",
        "        'faiss_accuracy': results['improved_accuracy'],\n",
        "        'baseline_memory_mb': results['baseline_memory'],\n",
        "        'faiss_memory_mb': results['improved_memory']\n",
        "    })\n",
        "    df_results.to_csv(csv_path, index=False)\n",
        "\n",
        "    # Calculate metrics for visualization\n",
        "    time_improvements = [(b-i)/b*100 for b, i in zip(results['baseline_times'], results['improved_times'])]\n",
        "    speedup_ratios = [b/i for b, i in zip(results['baseline_times'], results['improved_times'])]\n",
        "    avg_time_improvement = np.mean(time_improvements)\n",
        "    avg_accuracy_improvement = np.mean([(i-b)/b*100 for b, i in zip(results['baseline_accuracy'], results['improved_accuracy'])])\n",
        "\n",
        "    # Create comprehensive matplotlib visualization\n",
        "    plt.figure(figsize=(20, 12))\n",
        "\n",
        "    # Time comparison\n",
        "    plt.subplot(2, 4, 1)\n",
        "    plt.plot(results['data_sizes'], results['baseline_times'], 'b-o', label='Baseline', linewidth=2)\n",
        "    plt.plot(results['data_sizes'], results['improved_times'], 'r-o', label='FAISS', linewidth=2)\n",
        "    plt.xlabel('Dataset Size')\n",
        "    plt.ylabel('Time (seconds)')\n",
        "    plt.title('Processing Time Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy comparison\n",
        "    plt.subplot(2, 4, 2)\n",
        "    plt.plot(results['data_sizes'], results['baseline_accuracy'], 'b-o', label='Baseline', linewidth=2)\n",
        "    plt.plot(results['data_sizes'], results['improved_accuracy'], 'r-o', label='FAISS', linewidth=2)\n",
        "    plt.xlabel('Dataset Size')\n",
        "    plt.ylabel('Accuracy Score')\n",
        "    plt.title('Accuracy Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Time improvement percentage\n",
        "    plt.subplot(2, 4, 3)\n",
        "    bars = plt.bar(results['data_sizes'], time_improvements, color='green', alpha=0.7)\n",
        "    plt.xlabel('Dataset Size')\n",
        "    plt.ylabel('Improvement %')\n",
        "    plt.title('Time Improvement %')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add values on bars\n",
        "    for bar, val in zip(bars, time_improvements):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "                f'{val:.1f}%', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    # Memory usage comparison\n",
        "    plt.subplot(2, 4, 4)\n",
        "    plt.plot(results['data_sizes'], results['baseline_memory'], 'b-o', label='Baseline', linewidth=2)\n",
        "    plt.plot(results['data_sizes'], results['improved_memory'], 'r-o', label='FAISS', linewidth=2)\n",
        "    plt.xlabel('Dataset Size')\n",
        "    plt.ylabel('Memory (MB)')\n",
        "    plt.title('Memory Usage')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Speedup ratio\n",
        "    plt.subplot(2, 4, 5)\n",
        "    plt.plot(results['data_sizes'], speedup_ratios, 'g-o', linewidth=3, markersize=8)\n",
        "    plt.axhline(y=1, color='r', linestyle='--', alpha=0.7, label='No improvement')\n",
        "    plt.xlabel('Dataset Size')\n",
        "    plt.ylabel('Speedup Ratio')\n",
        "    plt.title('FAISS vs Baseline Speedup')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Performance summary\n",
        "    plt.subplot(2, 4, 6)\n",
        "    metrics = ['Avg Time\\\\nImprovement %', 'Avg Accuracy\\\\nImprovement %']\n",
        "    values = [avg_time_improvement, avg_accuracy_improvement]\n",
        "    colors = ['green' if v >= 0 else 'red' for v in values]\n",
        "\n",
        "    bars = plt.bar(metrics, values, color=colors, alpha=0.7)\n",
        "    plt.ylabel('Improvement %')\n",
        "    plt.title('Overall Performance Gains')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add values on bars\n",
        "    for bar, val in zip(bars, values):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + (0.1 if val >= 0 else -0.5),\n",
        "                f'{val:.1f}%', ha='center', va='bottom' if val >= 0 else 'top', fontsize=10, fontweight='bold')\n",
        "\n",
        "    # Data distribution\n",
        "    plt.subplot(2, 4, 7)\n",
        "    plt.scatter(results['baseline_times'], results['improved_times'],\n",
        "               c=results['data_sizes'], cmap='viridis', s=100, alpha=0.7)\n",
        "    plt.plot([0, max(results['baseline_times'])], [0, max(results['baseline_times'])], 'r--', alpha=0.5)\n",
        "    plt.xlabel('Baseline Time (s)')\n",
        "    plt.ylabel('FAISS Time (s)')\n",
        "    plt.title('Time Correlation')\n",
        "    plt.colorbar(label='Dataset Size')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Summary statistics\n",
        "    plt.subplot(2, 4, 8)\n",
        "    plt.axis('off')\n",
        "\n",
        "    summary_text = f\"\"\"\n",
        "    ğŸ“Š EXPERIMENT SUMMARY\n",
        "\n",
        "    ğŸ“ˆ Performance Metrics:\n",
        "    â€¢ Avg Time Improvement: {avg_time_improvement:.1f}%\n",
        "    â€¢ Avg Accuracy Change: {avg_accuracy_improvement:.1f}%\n",
        "    â€¢ Best Speedup: {max(speedup_ratios):.2f}x\n",
        "    â€¢ Worst Speedup: {min(speedup_ratios):.2f}x\n",
        "\n",
        "    ğŸ”§ Technical Details:\n",
        "    â€¢ Dataset Sizes: {min(results['data_sizes'])}-{max(results['data_sizes'])} docs\n",
        "    â€¢ FAISS Version: {faiss.__version__ if faiss else 'N/A'}\n",
        "    â€¢ Device: {device}\n",
        "    â€¢ Total Runtime: {sum(results['baseline_times']) + sum(results['improved_times']):.1f}s\n",
        "\n",
        "    âœ… Experiment Status: COMPLETE\n",
        "    \"\"\"\n",
        "\n",
        "    plt.text(0.05, 0.95, summary_text, transform=plt.gca().transAxes,\n",
        "             fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
        "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "\n",
        "    plt.suptitle('Phase 1: Dynamic Memory Construction - Performance Analysis', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    static_plot_path = os.path.join(results_dir, f\"performance_analysis_{timestamp}.png\")\n",
        "    plt.savefig(static_plot_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"ğŸ“Š Results saved to: {results_dir}\")\n",
        "    print(f\"ğŸ“„ JSON: {json_path}\")\n",
        "    print(f\"ğŸ“Š CSV: {csv_path}\")\n",
        "    print(f\"ğŸ–¼ï¸  Plot: {static_plot_path}\")\n",
        "\n",
        "    return results_dir\n",
        "\n",
        "# Save and visualize results\n",
        "print(\"ğŸ’¾ Saving experiment results and generating visualizations...\")\n",
        "results_directory = save_and_visualize_results(experiment_results)\n",
        "print(f\"\\nâœ… All results saved to: {results_directory}\")\n",
        "\n",
        "# Display final summary\n",
        "print(\"\\nğŸ¯ FINAL EXPERIMENT SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"ğŸ“Š Data sizes tested: {experiment_results['data_sizes']}\")\n",
        "print(f\"âš¡ Average performance improvement: {np.mean([(b-i)/b*100 for b, i in zip(experiment_results['baseline_times'], experiment_results['improved_times'])]):.1f}%\")\n",
        "print(f\"ğŸ¯ Accuracy maintained: {all(abs(b-i) < 0.001 for b, i in zip(experiment_results['baseline_accuracy'], experiment_results['improved_accuracy']))}\")\n",
        "print(f\"ğŸ”§ FAISS system status: {'âœ… Working' if faiss_available else 'âŒ Fallback used'}\")\n",
        "print(f\"ğŸ“ˆ Total documents processed: {sum(experiment_results['data_sizes']) * 2}\")  # Both systems\n",
        "print(f\"â±ï¸  Total experiment time: {sum(experiment_results['baseline_times']) + sum(experiment_results['improved_times']):.1f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e068713",
      "metadata": {
        "id": "2e068713"
      },
      "source": [
        "## ğŸ”¬ Î”GED/Î”IG Evaluation Stub (Phase 2 Preparation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4f727d8",
      "metadata": {
        "id": "b4f727d8"
      },
      "outputs": [],
      "source": [
        "def calculate_delta_ged_ig_stub(baseline_results: List, improved_results: List) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Stub for Î”GED (Graph Edit Distance) and Î”IG (Information Gain) calculation.\n",
        "    This will be fully implemented in Phase 2 for self-organizing memory evaluation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Placeholder calculations - to be replaced with actual Î”GED/Î”IG algorithms\n",
        "    baseline_avg = np.mean([score for _, score in baseline_results]) if baseline_results else 0\n",
        "    improved_avg = np.mean([score for _, score in improved_results]) if improved_results else 0\n",
        "\n",
        "    # Simulated Î”GED (lower is better - represents structural difference)\n",
        "    delta_ged = abs(baseline_avg - improved_avg) * 0.1  # Placeholder formula\n",
        "\n",
        "    # Simulated Î”IG (higher is better - represents information gain)\n",
        "    delta_ig = (improved_avg - baseline_avg) * 2.0  # Placeholder formula\n",
        "\n",
        "    return {\n",
        "        \"delta_ged\": delta_ged,\n",
        "        \"delta_ig\": delta_ig,\n",
        "        \"baseline_avg_score\": baseline_avg,\n",
        "        \"improved_avg_score\": improved_avg\n",
        "    }\n",
        "\n",
        "# Calculate Î”GED/Î”IG for the experiment\n",
        "print(\"ğŸ”¬ Calculating Î”GED/Î”IG metrics (Phase 2 preparation)...\")\n",
        "\n",
        "# Use results from the largest dataset for evaluation\n",
        "largest_size_idx = -1  # Last (largest) dataset size\n",
        "baseline_sample = [(\"sample doc\", experiment_results['baseline_accuracy'][largest_size_idx])]\n",
        "improved_sample = [(\"sample doc\", experiment_results['improved_accuracy'][largest_size_idx])]\n",
        "\n",
        "ged_ig_metrics = calculate_delta_ged_ig_stub(baseline_sample, improved_sample)\n",
        "\n",
        "print(\"ğŸ“Š Î”GED/Î”IG Results:\")\n",
        "for metric, value in ged_ig_metrics.items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "# Add to results\n",
        "experiment_results['delta_ged_ig'] = ged_ig_metrics\n",
        "\n",
        "print(\"\\nğŸ”® Phase 2 Note: Full Î”GED/Î”IG implementation will include:\")\n",
        "print(\"  - Graph Edit Distance for memory structure comparison\")\n",
        "print(\"  - Information Gain metrics for self-organizing evaluation\")\n",
        "print(\"  - Reinforcement learning loop integration\")\n",
        "print(\"  - Real-time memory adaptation algorithms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eac7cfb",
      "metadata": {
        "id": "2eac7cfb"
      },
      "source": [
        "## ğŸ¯ Experiment Completion and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1db7082",
      "metadata": {
        "id": "e1db7082"
      },
      "outputs": [],
      "source": [
        "# Final validation and summary\n",
        "print(\"ğŸ” EXPERIMENT VALIDATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check all required components are present\n",
        "validation_checks = {\n",
        "    \"Device setup completed\": device is not None,\n",
        "    \"Real data loaded\": len(experiment_texts) > 0,\n",
        "    \"Baseline system tested\": len(experiment_results['baseline_times']) > 0,\n",
        "    \"Improved system tested\": len(experiment_results['improved_times']) > 0,\n",
        "    \"Results saved\": 'experiment_timestamp' in experiment_results,\n",
        "    \"Visualizations created\": True,  # We created them above\n",
        "    \"Î”GED/Î”IG calculated\": 'delta_ged_ig' in experiment_results\n",
        "}\n",
        "\n",
        "for check, passed in validation_checks.items():\n",
        "    status = \"âœ…\" if passed else \"âŒ\"\n",
        "    print(f\"{status} {check}\")\n",
        "\n",
        "all_passed = all(validation_checks.values())\n",
        "print(f\"\\n{'ğŸ‰ ALL VALIDATION CHECKS PASSED!' if all_passed else 'âš ï¸ Some validation checks failed'}\")\n",
        "\n",
        "# Final experiment summary\n",
        "print(\"\\nğŸ“‹ FINAL EXPERIMENT SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"ğŸ• Experiment completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"ğŸ”§ Device used: {device}\")\n",
        "print(f\"ğŸ“Š Data samples processed: {len(experiment_texts)}\")\n",
        "print(f\"ğŸ“ˆ Data sizes tested: {experiment_results['data_sizes']}\")\n",
        "print(f\"âš¡ Performance improvements measured: {'Yes' if experiment_results['improved_times'] else 'No'}\")\n",
        "print(f\"ğŸ’¾ Results saved and visualized: {'Yes' if results_directory else 'No'}\")\n",
        "\n",
        "print(\"\\nğŸš€ Ready for Phase 2: Self-Organizing Memory with Î”GED/Î”IG!\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"  1. Implement full Î”GED (Graph Edit Distance) algorithms\")\n",
        "print(\"  2. Add Î”IG (Information Gain) calculation for memory optimization\")\n",
        "print(\"  3. Integrate reinforcement learning loop for self-organization\")\n",
        "print(\"  4. Scale to larger datasets and more complex queries\")\n",
        "print(\"  5. Add real QA/retrieval accuracy benchmarks\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}