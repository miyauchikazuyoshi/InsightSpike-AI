{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d03dbf",
   "metadata": {},
   "source": [
    "# ğŸ” Phase 2: RAG Systems Benchmark - Clean Edition\n",
    "\n",
    "## ğŸ“‹ å®Ÿé¨“è¨­è¨ˆ\n",
    "- **4ã¤ã®RAGã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒ**: InsightSpike vs LangChain vs LlamaIndex vs Haystack\n",
    "- **GPUä¸¦åˆ—å‡¦ç†**: é«˜é€Ÿé¡ä¼¼åº¦æ¤œç´¢ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯\n",
    "- **å®Ÿãƒ‡ãƒ¼ã‚¿è©•ä¾¡**: SQuADã€MS MARCOç­‰ã®æ¨™æº–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "\n",
    "## ğŸš€ å®Ÿé¨“ãƒ•ãƒ­ãƒ¼\n",
    "1. **ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: ç’°å¢ƒæ§‹ç¯‰ãƒ»ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "2. **InsightSpikeå‹•ä½œç¢ºèª**: åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ\n",
    "3. **RAGã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰**: 4ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ãƒ»å‹•ä½œç¢ºèª\n",
    "4. **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿**: HuggingFaceçµŒç”±ã§SQuADç­‰å–å¾—\n",
    "5. **æ¯”è¼ƒå®Ÿé¨“å®Ÿè¡Œ**: çµ±è¨ˆçš„ã«æœ‰æ„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯\n",
    "6. **çµæœå¯è¦–åŒ–**: è«–æ–‡å“è³ªã®åˆ†æãƒ»ã‚°ãƒ©ãƒ•ä½œæˆ\n",
    "\n",
    "---\n",
    "**å®Ÿè¡Œç’°å¢ƒ**: Google Colab GPU (T4/V100) | **æ¨å®šæ™‚é–“**: 20-30åˆ†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba45f762",
   "metadata": {},
   "source": [
    "## ğŸš€ ç‹¬ç«‹ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼šãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³ & ç’°å¢ƒæ§‹ç¯‰\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ **ç‹¬ç«‹ã—ã¦å®Ÿè¡Œå¯èƒ½** ã§ã™ã€‚InsightSpike-AIãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ã‹ã‚‰å…¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’è‡ªå‹•åŒ–ã•ã‚Œã¦ã„ã¾ã™\n",
    "\n",
    "### âš ï¸ é‡è¦ï¼šæ­£ã—ã„å®Ÿè¡Œé †åº\n",
    "\n",
    "**ğŸ”´ ã‚¨ãƒ©ãƒ¼ã‚’é¿ã‘ã‚‹ãŸã‚ã«ä»¥ä¸‹ã®é †åºã§å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š**\n",
    "\n",
    "1. **Cell 3**: ğŸ›ï¸ Execution Control Settingsï¼ˆå¿…é ˆï¼šæœ€åˆã«å®Ÿè¡Œï¼‰\n",
    "2. **Cell 4**: ğŸ”§ Colabç’°å¢ƒç·Šæ€¥ä¿®æ­£ï¼ˆFAISSå•é¡ŒãŒã‚ã‚‹å ´åˆï¼‰\n",
    "3. **Cell 5**: ğŸ”§ Step 1: ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³\n",
    "4. **Cell 7**: ğŸ§  InsightSpikeå‹•ä½œç¢ºèª\n",
    "5. **æ®‹ã‚Šã®ã‚»ãƒ«**: é †ç•ªã«å®Ÿè¡Œ\n",
    "\n",
    "**ğŸ’¡ `eval_config` ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆã¯ Cell 3 ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„**\n",
    "\n",
    "### ğŸ”‘ GitHubèªè¨¼è¨­å®šï¼ˆãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒªãƒã‚¸ãƒˆãƒªç”¨ï¼‰\n",
    "\n",
    "**é‡è¦:** InsightSpike-AIã¯ç¾åœ¨ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒªãƒã‚¸ãƒˆãƒªã§ã™ã€‚ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®æ–¹æ³•ã§GitHubãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "#### æ–¹æ³•1: Colab Secretsï¼ˆæ¨å¥¨ï¼‰\n",
    "1. å·¦å´ãƒ‘ãƒãƒ«ã®ğŸ”‘ã‚¢ã‚¤ã‚³ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯\n",
    "2. ã€Œæ–°ã—ã„ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã€ã‚’ã‚¯ãƒªãƒƒã‚¯  \n",
    "3. åå‰: `GITHUB_TOKEN`\n",
    "4. å€¤: ã‚ãªãŸã®GitHubãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³\n",
    "\n",
    "#### æ–¹æ³•2: ç’°å¢ƒå¤‰æ•°\n",
    "```python\n",
    "import os\n",
    "os.environ['GITHUB_TOKEN'] = 'your_github_token_here'\n",
    "```\n",
    "\n",
    "#### æ–¹æ³•3: æ‰‹å‹•å…¥åŠ›\n",
    "ã‚»ãƒ«å®Ÿè¡Œæ™‚ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å…¥åŠ›ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¸Šéæ¨å¥¨ï¼‰\n",
    "\n",
    "### ğŸ“‹ GitHubãƒˆãƒ¼ã‚¯ãƒ³ã®ä½œæˆæ–¹æ³•\n",
    "1. GitHub â†’ Settings â†’ Developer settings â†’ Personal access tokens â†’ Tokens (classic)\n",
    "2. \"Generate new token (classic)\" ã‚’ã‚¯ãƒªãƒƒã‚¯\n",
    "3. Scopes: `repo` (Full control of private repositories)\n",
    "4. ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ä¸Šè¨˜æ–¹æ³•ã§è¨­å®š\n",
    "\n",
    "### ğŸ“‹ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…å®¹\n",
    "1. **ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³**: InsightSpike-AI ã®æœ€æ–°ç‰ˆã‚’å–å¾—ï¼ˆèªè¨¼ä»˜ãï¼‰\n",
    "2. **ç’°å¢ƒæ§‹ç¯‰**: Pythonä¾å­˜é–¢ä¿‚ã®å®Œå…¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« \n",
    "3. **å†ç¾æ€§ç¢ºä¿**: ã‚·ãƒ¼ãƒ‰å›ºå®šã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†\n",
    "4. **æ¤œè¨¼**: å…¨ã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œç¢ºèª\n",
    "\n",
    "**âš¡ å®Ÿè¡Œæ–¹æ³•**: GitHubãƒˆãƒ¼ã‚¯ãƒ³è¨­å®šå¾Œã€ä¸Šã‹ã‚‰é †ç•ªã«ã‚»ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã ã‘ã§å®Œäº†ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f1e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ›ï¸ Execution Control Settings - RAG Benchmark Configuration\n",
    "import datetime\n",
    "import pickle\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "print(\"ğŸ›ï¸ RAG BENCHMARK EXECUTION CONTROL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ğŸ”§ EvalConfig ã‚¯ãƒ©ã‚¹å®šç¾©\n",
    "class EvalConfig:\n",
    "    \"\"\"å®Ÿé¨“è¨­å®šã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹\"\"\"\n",
    "    def __init__(self, profile=\"demo\"):\n",
    "        self.profile = profile\n",
    "        self.timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.experiment_id = f\"{profile}_{self.timestamp}\"\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å®šç¾©\n",
    "        self.profiles = {\n",
    "            \"demo\": {\n",
    "                \"description\": \"è»½é‡ãƒ‡ãƒ¢å®Ÿè¡Œ - åŸºæœ¬æ©Ÿèƒ½ç¢ºèªç”¨\",\n",
    "                \"sample_sizes\": [1000],\n",
    "                \"max_queries\": 50,\n",
    "                \"datasets\": [\"squad_fallback\", \"test_fallback\"],\n",
    "                \"systems\": [\"llm_only\", \"bm25_llm\", \"insightspike\"],\n",
    "                \"enable_visualization\": True,\n",
    "                \"save_results\": True,\n",
    "                \"memory_cleanup\": True,\n",
    "                \"strict_error_handling\": False\n",
    "            },\n",
    "            \"research\": {\n",
    "                \"description\": \"ç ”ç©¶ç”¨å®Œå…¨å®Ÿè¡Œ - å…¨æ©Ÿèƒ½ãƒ»å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿\",\n",
    "                \"sample_sizes\": [1000, 5000, 10000, 50000],\n",
    "                \"max_queries\": 1000,\n",
    "                \"datasets\": [\"squad\", \"ms_marco\", \"synthetic\"],\n",
    "                \"systems\": [\"llm_only\", \"bm25_llm\", \"insightspike\", \"langchain\", \"llama_index\", \"haystack\"],\n",
    "                \"enable_visualization\": True,\n",
    "                \"save_results\": True,\n",
    "                \"memory_cleanup\": True,\n",
    "                \"strict_error_handling\": False\n",
    "            },\n",
    "            \"presentation\": {\n",
    "                \"description\": \"ç™ºè¡¨ç”¨å®Ÿè¡Œ - ãƒãƒ©ãƒ³ã‚¹é‡è¦–\",\n",
    "                \"sample_sizes\": [1000, 5000, 10000],\n",
    "                \"max_queries\": 200,\n",
    "                \"datasets\": [\"squad\", \"squad_fallback\", \"synthetic\"],\n",
    "                \"systems\": [\"llm_only\", \"bm25_llm\", \"insightspike\", \"langchain\"],\n",
    "                \"enable_visualization\": True,\n",
    "                \"save_results\": True,\n",
    "                \"memory_cleanup\": True,\n",
    "                \"strict_error_handling\": False\n",
    "            },\n",
    "            \"insightspike_only\": {\n",
    "                \"description\": \"InsightSpikeç‰¹åŒ–å®Ÿé¨“ - è©³ç´°åˆ†æç”¨\",\n",
    "                \"sample_sizes\": [1000, 5000, 10000, 20000],\n",
    "                \"max_queries\": 500,\n",
    "                \"datasets\": [\"squad\", \"ms_marco\", \"synthetic\"],\n",
    "                \"systems\": [\"insightspike\"],  # InsightSpikeã®ã¿\n",
    "                \"enable_visualization\": True,\n",
    "                \"save_results\": True,\n",
    "                \"memory_cleanup\": True,\n",
    "                \"strict_error_handling\": False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šã‚’é©ç”¨\n",
    "        if profile not in self.profiles:\n",
    "            print(f\"âŒ Invalid profile: {profile}\")\n",
    "            print(f\"Available profiles: {list(self.profiles.keys())}\")\n",
    "            profile = \"demo\"\n",
    "            \n",
    "        profile_config = self.profiles[profile]\n",
    "        \n",
    "        # è¨­å®šã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã¨ã—ã¦è¨­å®š\n",
    "        self.description = profile_config[\"description\"]\n",
    "        self.sample_sizes = profile_config[\"sample_sizes\"]\n",
    "        self.max_queries = profile_config[\"max_queries\"]\n",
    "        self.datasets = profile_config[\"datasets\"]\n",
    "        self.systems = profile_config[\"systems\"]\n",
    "        self.enable_visualization = profile_config[\"enable_visualization\"]\n",
    "        self.save_results = profile_config[\"save_results\"]\n",
    "        self.memory_cleanup = profile_config[\"memory_cleanup\"]\n",
    "        self.strict_error_handling = profile_config[\"strict_error_handling\"]\n",
    "        \n",
    "        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œåˆ¶å¾¡\n",
    "        self.sections_to_run = {\n",
    "            \"setup\": True,\n",
    "            \"rag_systems\": True,\n",
    "            \"datasets\": True,\n",
    "            \"benchmark\": True,\n",
    "            \"insightspike_specialized\": \"insightspike\" in self.systems,\n",
    "            \"visualization\": self.enable_visualization\n",
    "        }\n",
    "        \n",
    "        # InsightSpikeå¯ç”¨æ€§ãƒ•ãƒ©ã‚°ï¼ˆå¾Œã§æ›´æ–°ã•ã‚Œã‚‹ï¼‰\n",
    "        self.insightspike_available = False\n",
    "        \n",
    "        # çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "        self.results_dir = f\"./results/{self.experiment_id}\"\n",
    "        os.makedirs(self.results_dir, exist_ok=True)\n",
    "\n",
    "    def print_config(self):\n",
    "        \"\"\"è¨­å®šã‚’è¡¨ç¤º\"\"\"\n",
    "        print(f\"ğŸ“‹ Selected Profile: {self.profile}\")\n",
    "        print(f\"ğŸ“ Description: {self.description}\")\n",
    "        print(f\"ğŸ“Š Data sizes: {self.sample_sizes}\")\n",
    "        print(f\"ğŸ” Max queries per dataset: {self.max_queries}\")\n",
    "        print(f\"ğŸ“š Datasets: {self.datasets}\")\n",
    "        print(f\"ğŸ¤– RAG systems: {self.systems}\")\n",
    "        print(f\"\\nğŸ¯ Active Sections:\")\n",
    "        for section, enabled in self.sections_to_run.items():\n",
    "            status = \"âœ…\" if enabled else \"â­ï¸ \"\n",
    "            print(f\"  {status} {section}\")\n",
    "        print(f\"\\nğŸ†” Experiment ID: {self.experiment_id}\")\n",
    "        print(f\"ğŸ’¾ Results directory: {self.results_dir}\")\n",
    "\n",
    "# å®Ÿè¡Œãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ\n",
    "EXECUTION_PROFILE = \"demo\"  # \"demo\", \"research\", \"presentation\", \"insightspike_only\"\n",
    "\n",
    "# EvalConfig ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ\n",
    "eval_config = EvalConfig(EXECUTION_PROFILE)\n",
    "\n",
    "# è¨­å®šè¡¨ç¤º\n",
    "eval_config.print_config()\n",
    "\n",
    "# ä¸‹ä½äº’æ›æ€§ã®ãŸã‚ã®ãƒ¬ã‚¬ã‚·ãƒ¼å¤‰æ•°\n",
    "profile = eval_config.profiles[eval_config.profile]\n",
    "SECTIONS_TO_RUN = eval_config.sections_to_run\n",
    "MEMORY_MANAGEMENT = {\n",
    "    \"cleanup_after_sections\": eval_config.memory_cleanup,\n",
    "    \"checkpoint_save\": eval_config.save_results,\n",
    "    \"memory_monitoring\": True,\n",
    "    \"cuda_cache_clear\": True\n",
    "}\n",
    "EXPERIMENT_ID = eval_config.experiment_id\n",
    "RESULTS_DIR = eval_config.results_dir\n",
    "\n",
    "print(\"\\nâœ… Execution control configured successfully!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6b87c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ COLABç’°å¢ƒç·Šæ€¥ä¿®æ­£: NumPy 2.xå¯¾å¿œ & FAISSæœ€æ–°ç‰ˆ\n",
      "======================================================================\n",
      "\n",
      "ğŸ” NumPy 2.x & FAISS æœ€æ–°äº’æ›æ€§ç¢ºèª...\n",
      "  ğŸ“Š ç¾åœ¨ã®NumPy: 1.26.4\n",
      "  ğŸ“‹ NumPy 1.x - å¾“æ¥ç‰ˆFAISSã‚‚ä½¿ç”¨å¯èƒ½\n",
      "\n",
      "ğŸ“‹ Colabç’°å¢ƒæ¤œå‡º...\n",
      "  ğŸ–¥ï¸ Colabç’°å¢ƒ: ã„ã„ãˆ\n",
      "\n",
      "ğŸš€ NumPy 2.xå¯¾å¿œFAISSæœ€æ–°ç‰ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "\n",
      "ğŸ”„ FAISSå…¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… FAISSå…¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "\n",
      "  ğŸ”§ FAISS CPU 1.9.0 (NumPy 2.xå¯¾å¿œ) ã‚’è©¦è¡Œä¸­...\n",
      "\n",
      "ğŸ”„ FAISS CPU 1.9.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… FAISSå…¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "\n",
      "  ğŸ”§ FAISS CPU 1.9.0 (NumPy 2.xå¯¾å¿œ) ã‚’è©¦è¡Œä¸­...\n",
      "\n",
      "ğŸ”„ FAISS CPU 1.9.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… FAISS CPU 1.9.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "\n",
      "ğŸ”„ FAISSä¾å­˜é–¢ä¿‚ numpy>=1.21.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… FAISS CPU 1.9.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "\n",
      "ğŸ”„ FAISSä¾å­˜é–¢ä¿‚ numpy>=1.21.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… FAISSä¾å­˜é–¢ä¿‚ numpy>=1.21.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "\n",
      "ğŸ”„ FAISSä¾å­˜é–¢ä¿‚ packaging ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… FAISSä¾å­˜é–¢ä¿‚ numpy>=1.21.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "\n",
      "ğŸ”„ FAISSä¾å­˜é–¢ä¿‚ packaging ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… FAISSä¾å­˜é–¢ä¿‚ packaging ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  âœ… FAISS CPU 1.9.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æˆåŠŸ\n",
      "\n",
      "ğŸ“¦ NumPy 2.xå¯¾å¿œã®æœ€æ–°ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ (10å€‹)...\n",
      "  (1/10) numpy>=2.0.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ numpy>=2.0.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… FAISSä¾å­˜é–¢ä¿‚ packaging ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  âœ… FAISS CPU 1.9.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æˆåŠŸ\n",
      "\n",
      "ğŸ“¦ NumPy 2.xå¯¾å¿œã®æœ€æ–°ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ (10å€‹)...\n",
      "  (1/10) numpy>=2.0.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ numpy>=2.0.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ numpy>=2.0.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  (2/10) pandas>=2.0.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ pandas>=2.0.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ numpy>=2.0.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  (2/10) pandas>=2.0.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ pandas>=2.0.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ pandas>=2.0.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  (3/10) scikit-learn>=1.5.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ scikit-learn>=1.5.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ pandas>=2.0.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  (3/10) scikit-learn>=1.5.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ scikit-learn>=1.5.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ scikit-learn>=1.5.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  (4/10) matplotlib>=3.8.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ matplotlib>=3.8.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ scikit-learn>=1.5.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  (4/10) matplotlib>=3.8.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ matplotlib>=3.8.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ matplotlib>=3.8.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  (5/10) seaborn>=0.13.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ seaborn>=0.13.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ matplotlib>=3.8.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  (5/10) seaborn>=0.13.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ seaborn>=0.13.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ seaborn>=0.13.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  (6/10) datasets>=2.18.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ datasets>=2.18.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ seaborn>=0.13.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  (6/10) datasets>=2.18.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ datasets>=2.18.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ datasets>=2.18.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  (7/10) transformers>=4.40.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ transformers>=4.40.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ datasets>=2.18.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  (7/10) transformers>=4.40.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ transformers>=4.40.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ transformers>=4.40.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  (8/10) sentence-transformers>=3.0.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ sentence-transformers>=3.0.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ transformers>=4.40.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  (8/10) sentence-transformers>=3.0.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ sentence-transformers>=3.0.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ sentence-transformers>=3.0.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  (9/10) torch>=2.0.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ torch>=2.0.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ sentence-transformers>=3.0.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  (9/10) torch>=2.0.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ torch>=2.0.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ torch>=2.0.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  (10/10) tqdm>=4.66.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ tqdm>=4.66.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ torch>=2.0.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "  (10/10) tqdm>=4.66.0\n",
      "\n",
      "ğŸ”„ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ tqdm>=4.66.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ tqdm>=4.66.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "\n",
      "ğŸ§ª FAISS & NumPy 2.x çµ±åˆå‹•ä½œç¢ºèª...\n",
      "  âŒ FAISSå‹•ä½œç¢ºèªå¤±æ•—: Error importing numpy: you should not try to import numpy from\n",
      "        its source directory; please exit the numpy source tree, and relaunch\n",
      "        y...\n",
      "  ğŸ”§ NumPyäº’æ›æ€§å•é¡Œæ¤œå‡º\n",
      "  ğŸ’¡ è§£æ±ºã‚ªãƒ—ã‚·ãƒ§ãƒ³:\n",
      "     Option A: Runtime > Restart Runtime ã§ã‚«ãƒ¼ãƒãƒ«å†èµ·å‹•å¾Œã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œ\n",
      "     Option B: æ‰‹å‹•ã§ä»¥ä¸‹ã‚’å®Ÿè¡Œ:\n",
      "               !pip install faiss-cpu==1.9.0 --force-reinstall\n",
      "\n",
      "ğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—...\n",
      "\n",
      "ğŸ“Š æœ€æ–°æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯è¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆ\n",
      "==================================================\n",
      "  ğŸ“Š NumPy: ğŸ“‹ 1.26.4 (1.xç³»)\n",
      "  ğŸ§  FAISS: âŒ è¦ä¿®æ­£\n",
      "  ğŸ–¥ï¸ ç’°å¢ƒ: ãƒ­ãƒ¼ã‚«ãƒ«\n",
      "\n",
      "âš ï¸ FAISSçµ±åˆã«å•é¡ŒãŒã‚ã‚Šã¾ã™\n",
      "ğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:\n",
      "   1. ğŸ“± Runtime > Restart Runtime ã§ã‚«ãƒ¼ãƒãƒ«å†èµ·å‹•\n",
      "   2. ğŸ”„ ã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œ\n",
      "   3. âœ… NumPy 2.x + FAISSçµ±åˆå‹•ä½œç¢ºèª\n",
      "======================================================================\n",
      "ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\n",
      "  1. å®Ÿè¡Œåˆ¶å¾¡ã‚»ãƒ« (eval_config) ã®ç¢ºèª\n",
      "  2. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã®å®Ÿè¡Œ (NumPy 2.xæœ€é©åŒ–)\n",
      "  3. RAGã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ã®å®Ÿè¡Œ (æœ€æ–°æŠ€è¡“æ´»ç”¨)\n",
      "======================================================================\n",
      "  âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ tqdm>=4.66.0 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æˆåŠŸ\n",
      "\n",
      "ğŸ§ª FAISS & NumPy 2.x çµ±åˆå‹•ä½œç¢ºèª...\n",
      "  âŒ FAISSå‹•ä½œç¢ºèªå¤±æ•—: Error importing numpy: you should not try to import numpy from\n",
      "        its source directory; please exit the numpy source tree, and relaunch\n",
      "        y...\n",
      "  ğŸ”§ NumPyäº’æ›æ€§å•é¡Œæ¤œå‡º\n",
      "  ğŸ’¡ è§£æ±ºã‚ªãƒ—ã‚·ãƒ§ãƒ³:\n",
      "     Option A: Runtime > Restart Runtime ã§ã‚«ãƒ¼ãƒãƒ«å†èµ·å‹•å¾Œã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œ\n",
      "     Option B: æ‰‹å‹•ã§ä»¥ä¸‹ã‚’å®Ÿè¡Œ:\n",
      "               !pip install faiss-cpu==1.9.0 --force-reinstall\n",
      "\n",
      "ğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—...\n",
      "\n",
      "ğŸ“Š æœ€æ–°æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯è¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆ\n",
      "==================================================\n",
      "  ğŸ“Š NumPy: ğŸ“‹ 1.26.4 (1.xç³»)\n",
      "  ğŸ§  FAISS: âŒ è¦ä¿®æ­£\n",
      "  ğŸ–¥ï¸ ç’°å¢ƒ: ãƒ­ãƒ¼ã‚«ãƒ«\n",
      "\n",
      "âš ï¸ FAISSçµ±åˆã«å•é¡ŒãŒã‚ã‚Šã¾ã™\n",
      "ğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:\n",
      "   1. ğŸ“± Runtime > Restart Runtime ã§ã‚«ãƒ¼ãƒãƒ«å†èµ·å‹•\n",
      "   2. ğŸ”„ ã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œ\n",
      "   3. âœ… NumPy 2.x + FAISSçµ±åˆå‹•ä½œç¢ºèª\n",
      "======================================================================\n",
      "ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\n",
      "  1. å®Ÿè¡Œåˆ¶å¾¡ã‚»ãƒ« (eval_config) ã®ç¢ºèª\n",
      "  2. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã®å®Ÿè¡Œ (NumPy 2.xæœ€é©åŒ–)\n",
      "  3. RAGã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ã®å®Ÿè¡Œ (æœ€æ–°æŠ€è¡“æ´»ç”¨)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ Colabç’°å¢ƒç·Šæ€¥ä¿®æ­£: NumPy 2.xå¯¾å¿œ & FAISSæœ€æ–°ç‰ˆ\n",
    "\n",
    "print(\"ğŸ”§ COLABç’°å¢ƒç·Šæ€¥ä¿®æ­£: NumPy 2.xå¯¾å¿œ & FAISSæœ€æ–°ç‰ˆ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def run_command(cmd, description, timeout=300, ignore_error=False):\n",
    "    \"\"\"ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œãƒ˜ãƒ«ãƒ‘ãƒ¼\"\"\"\n",
    "    print(f\"\\nğŸ”„ {description}...\")\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout, shell=isinstance(cmd, str))\n",
    "        if result.returncode == 0 or ignore_error:\n",
    "            print(f\"  âœ… {description} æˆåŠŸ\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"  âŒ {description} å¤±æ•—: {result.stderr[:200]}...\")\n",
    "            return False\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"  â° {description} ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({timeout}ç§’)\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"  ğŸ’¥ {description} ä¾‹å¤–: {str(e)[:100]}...\")\n",
    "        return False\n",
    "\n",
    "# 0. NumPy ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª\n",
    "print(\"\\nğŸ” NumPy 2.x & FAISS æœ€æ–°äº’æ›æ€§ç¢ºèª...\")\n",
    "try:\n",
    "    import numpy as np\n",
    "    numpy_version = np.__version__\n",
    "    print(f\"  ğŸ“Š ç¾åœ¨ã®NumPy: {numpy_version}\")\n",
    "    \n",
    "    if numpy_version.startswith('2.'):\n",
    "        print(f\"  âœ… NumPy 2.xæ¤œå‡º - æœ€æ–°FAISS 1.9.0ã§ã‚µãƒãƒ¼ãƒˆï¼\")\n",
    "        use_modern_faiss = True\n",
    "    else:\n",
    "        print(f\"  ğŸ“‹ NumPy 1.x - å¾“æ¥ç‰ˆFAISSã‚‚ä½¿ç”¨å¯èƒ½\")\n",
    "        use_modern_faiss = False\n",
    "        \n",
    "except ImportError:\n",
    "    print(f\"  âŒ NumPyæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\")\n",
    "    use_modern_faiss = True  # æ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ™‚ã¯æœ€æ–°ç‰ˆ\n",
    "\n",
    "# 1. Colabç’°å¢ƒæ¤œå‡ºã¨pyproject_colab.tomlé©ç”¨\n",
    "print(\"\\nğŸ“‹ Colabç’°å¢ƒæ¤œå‡º...\")\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"  ğŸ–¥ï¸ Colabç’°å¢ƒ: {'ã¯ã„' if IN_COLAB else 'ã„ã„ãˆ'}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\\nğŸ”„ pyproject_colab.tomlé©ç”¨ä¸­...\")\n",
    "    \n",
    "    # ãƒªãƒã‚¸ãƒˆãƒªãƒ«ãƒ¼ãƒˆæ¤œå‡º\n",
    "    possible_roots = [\n",
    "        \"/content/InsightSpike-AI\",\n",
    "        \"/content/sample_data/InsightSpike-AI\", \n",
    "        \"./InsightSpike-AI\",\n",
    "        \".\"\n",
    "    ]\n",
    "    \n",
    "    repo_root = None\n",
    "    for root in possible_roots:\n",
    "        if os.path.exists(os.path.join(root, \"pyproject_colab.toml\")):\n",
    "            repo_root = root\n",
    "            break\n",
    "    \n",
    "    if repo_root:\n",
    "        print(f\"  ğŸ“‚ ãƒªãƒã‚¸ãƒˆãƒªãƒ«ãƒ¼ãƒˆæ¤œå‡º: {repo_root}\")\n",
    "        \n",
    "        # pyproject_colab.toml ã‹ã‚‰ pyproject.toml ã¸ã®ã‚³ãƒ”ãƒ¼\n",
    "        colab_toml = os.path.join(repo_root, \"pyproject_colab.toml\")\n",
    "        main_toml = os.path.join(repo_root, \"pyproject.toml\")\n",
    "        \n",
    "        try:\n",
    "            shutil.copy2(colab_toml, main_toml)\n",
    "            print(f\"  âœ… pyproject_colab.toml â†’ pyproject.toml ã‚³ãƒ”ãƒ¼å®Œäº†\")\n",
    "            \n",
    "            # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å¤‰æ›´\n",
    "            os.chdir(repo_root)\n",
    "            print(f\"  ğŸ“ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå¤‰æ›´: {os.getcwd()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ pyproject.toml ã‚³ãƒ”ãƒ¼å¤±æ•—: {e}\")\n",
    "    else:\n",
    "        print(\"  âš ï¸ pyproject_colab.toml ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "\n",
    "# 2. NumPy 2.xå¯¾å¿œFAISSã®æœ€æ–°ç‰ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "print(f\"\\nğŸš€ NumPy 2.xå¯¾å¿œFAISSæœ€æ–°ç‰ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\")\n",
    "\n",
    "# æ—¢å­˜ã®FAISSã‚’å®Œå…¨ã«ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "run_command([sys.executable, \"-m\", \"pip\", \"uninstall\", \"faiss\", \"faiss-gpu\", \"faiss-cpu\", \"-y\"], \n",
    "            \"FAISSå…¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\", ignore_error=True)\n",
    "\n",
    "# NumPy 2.xå¯¾å¿œã®FAISSæœ€æ–°ç‰ˆã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "faiss_versions_modern = [\n",
    "    \"1.9.0\",    # NumPy 2.xå…¬å¼ã‚µãƒãƒ¼ãƒˆ\n",
    "    \"1.8.0.post1\",  # NumPy 2.xå®Ÿé¨“çš„ã‚µãƒãƒ¼ãƒˆ\n",
    "    \"1.8.0\"     # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "]\n",
    "\n",
    "faiss_installed = False\n",
    "for version in faiss_versions_modern:\n",
    "    print(f\"\\n  ğŸ”§ FAISS CPU {version} (NumPy 2.xå¯¾å¿œ) ã‚’è©¦è¡Œä¸­...\")\n",
    "    if run_command([sys.executable, \"-m\", \"pip\", \"install\", f\"faiss-cpu=={version}\", \n",
    "                   \"--force-reinstall\", \"--no-cache-dir\", \"--no-deps\"], \n",
    "                   f\"FAISS CPU {version} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\"):\n",
    "        \n",
    "        # ä¾å­˜é–¢ä¿‚ã‚’å€‹åˆ¥ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "        dependencies = [\"numpy>=1.21.0\", \"packaging\"]\n",
    "        for dep in dependencies:\n",
    "            run_command([sys.executable, \"-m\", \"pip\", \"install\", dep], \n",
    "                       f\"FAISSä¾å­˜é–¢ä¿‚ {dep} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\", ignore_error=True)\n",
    "        \n",
    "        faiss_installed = True\n",
    "        print(f\"  âœ… FAISS CPU {version} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æˆåŠŸ\")\n",
    "        break\n",
    "\n",
    "if not faiss_installed:\n",
    "    print(\"  ğŸ”„ æœ€æ–°ã®FAISS CPUï¼ˆå®Ÿé¨“ç‰ˆï¼‰ã‚’è©¦è¡Œ...\")\n",
    "    run_command([sys.executable, \"-m\", \"pip\", \"install\", \"faiss-cpu\", \"--pre\", \n",
    "                \"--force-reinstall\", \"--no-cache-dir\"], \"FAISS CPU å®Ÿé¨“ç‰ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\")\n",
    "\n",
    "# 3. NumPy 2.xå¯¾å¿œã®æœ€æ–°ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "modern_packages = [\n",
    "    \"numpy>=2.0.0\",  # NumPy 2.xæ˜ç¤ºçš„ã«ç¶­æŒ\n",
    "    \"pandas>=2.0.0\",  # NumPy 2.xå¯¾å¿œç‰ˆ\n",
    "    \"scikit-learn>=1.5.0\",  # NumPy 2.xå¯¾å¿œç‰ˆ\n",
    "    \"matplotlib>=3.8.0\",  # NumPy 2.xå¯¾å¿œç‰ˆ  \n",
    "    \"seaborn>=0.13.0\",  # NumPy 2.xå¯¾å¿œç‰ˆ\n",
    "    \"datasets>=2.18.0\",  # NumPy 2.xå¯¾å¿œç‰ˆ\n",
    "    \"transformers>=4.40.0\",  # NumPy 2.xå¯¾å¿œç‰ˆ\n",
    "    \"sentence-transformers>=3.0.0\",  # NumPy 2.xå¯¾å¿œç‰ˆ\n",
    "    \"torch>=2.0.0\",  # NumPy 2.xå¯¾å¿œç‰ˆ\n",
    "    \"tqdm>=4.66.0\"\n",
    "]\n",
    "\n",
    "print(f\"\\nğŸ“¦ NumPy 2.xå¯¾å¿œã®æœ€æ–°ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ ({len(modern_packages)}å€‹)...\")\n",
    "for i, package in enumerate(modern_packages, 1):\n",
    "    print(f\"  ({i}/{len(modern_packages)}) {package}\")\n",
    "    run_command([sys.executable, \"-m\", \"pip\", \"install\", package, \"--upgrade\"], \n",
    "                f\"ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ {package} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\", ignore_error=True)\n",
    "\n",
    "# 4. FAISS & NumPy 2.x å‹•ä½œç¢ºèª\n",
    "print(\"\\nğŸ§ª FAISS & NumPy 2.x çµ±åˆå‹•ä½œç¢ºèª...\")\n",
    "try:\n",
    "    # NumPyã‚’å†ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "    import importlib\n",
    "    if 'numpy' in sys.modules:\n",
    "        importlib.reload(sys.modules['numpy'])\n",
    "    \n",
    "    import numpy as np\n",
    "    print(f\"  ğŸ“Š NumPy ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {np.__version__}\")\n",
    "    \n",
    "    if np.__version__.startswith('2.'):\n",
    "        print(f\"  âœ… NumPy 2.xç¢ºèª - æœ€æ–°æ©Ÿèƒ½åˆ©ç”¨å¯èƒ½\")\n",
    "    else:\n",
    "        print(f\"  ğŸ“‹ NumPy 1.x - å®‰å®šç‰ˆã¨ã—ã¦å‹•ä½œ\")\n",
    "    \n",
    "    # FAISS ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ\n",
    "    import faiss\n",
    "    faiss_version = getattr(faiss, '__version__', 'ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸æ˜')\n",
    "    print(f\"  âœ… FAISS ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ (ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {faiss_version})\")\n",
    "    \n",
    "    # NumPy 2.x & FAISS çµ±åˆå‹•ä½œãƒ†ã‚¹ãƒˆ\n",
    "    print(f\"  ğŸ”§ NumPy 2.x & FAISS çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...\")\n",
    "    \n",
    "    d = 64\n",
    "    nb = 1000\n",
    "    nq = 10\n",
    "    \n",
    "    # NumPy 2.x ã®æ–°ã—ã„ä¹±æ•°ç”Ÿæˆå™¨ä½¿ç”¨\n",
    "    rng = np.random.default_rng(1234)\n",
    "    xb = rng.random((nb, d), dtype=np.float32)\n",
    "    xq = rng.random((nq, d), dtype=np.float32)\n",
    "    \n",
    "    # FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã¨ãƒ†ã‚¹ãƒˆ\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(xb)\n",
    "    D, I = index.search(xq, 5)\n",
    "    \n",
    "    print(f\"  âœ… NumPy 2.x & FAISS çµ±åˆãƒ†ã‚¹ãƒˆæˆåŠŸ!\")\n",
    "    print(f\"    ğŸ“Š æ¤œç´¢çµæœå½¢çŠ¶: {D.shape}\")\n",
    "    print(f\"    ğŸ”¢ NumPyé…åˆ—å‹: {type(xb)}\")\n",
    "    print(f\"    ğŸ’« FAISSçµæœå‹: {type(D)}\")\n",
    "    faiss_working = True\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    print(f\"  âŒ FAISSå‹•ä½œç¢ºèªå¤±æ•—: {error_msg[:150]}...\")\n",
    "    faiss_working = False\n",
    "    \n",
    "    if \"numpy\" in error_msg.lower() or \"_array_api\" in error_msg:\n",
    "        print(f\"  ğŸ”§ NumPyäº’æ›æ€§å•é¡Œæ¤œå‡º\")\n",
    "        print(f\"  ğŸ’¡ è§£æ±ºã‚ªãƒ—ã‚·ãƒ§ãƒ³:\")\n",
    "        print(f\"     Option A: Runtime > Restart Runtime ã§ã‚«ãƒ¼ãƒãƒ«å†èµ·å‹•å¾Œã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œ\")\n",
    "        print(f\"     Option B: æ‰‹å‹•ã§ä»¥ä¸‹ã‚’å®Ÿè¡Œ:\")\n",
    "        print(f\"               !pip install faiss-cpu==1.9.0 --force-reinstall\")\n",
    "    else:\n",
    "        print(f\"  ğŸ’¡ ä»–ã®è§£æ±ºç­–ã‚’è©¦è¡Œä¸­...\")\n",
    "\n",
    "# 5. ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "print(\"\\nğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—...\")\n",
    "import gc\n",
    "gc.collect()\n",
    "if 'torch' in sys.modules:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"  âœ… CUDA ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å®Œäº†\")\n",
    "\n",
    "# 6. æœ€çµ‚è¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆï¼ˆNumPy 2.x & æœ€æ–°æŠ€è¡“å¯¾å¿œï¼‰\n",
    "print(\"\\nğŸ“Š æœ€æ–°æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯è¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    numpy_status = f\"âœ… {np.__version__}\" if np.__version__.startswith('2.') else f\"ğŸ“‹ {np.__version__} (1.xç³»)\"\n",
    "    print(f\"  ğŸ“Š NumPy: {numpy_status}\")\n",
    "except ImportError:\n",
    "    print(f\"  ğŸ“Š NumPy: âŒ æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\")\n",
    "\n",
    "print(f\"  ğŸ§  FAISS: {'âœ… NumPy 2.xå¯¾å¿œç‰ˆå‹•ä½œä¸­' if faiss_working else 'âŒ è¦ä¿®æ­£'}\")\n",
    "print(f\"  ğŸ–¥ï¸ ç’°å¢ƒ: {'Colab Pro/Pro+æ¨å¥¨' if IN_COLAB else 'ãƒ­ãƒ¼ã‚«ãƒ«'}\")\n",
    "\n",
    "# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±\n",
    "try:\n",
    "    import numpy as np\n",
    "    if np.__version__.startswith('2.'):\n",
    "        print(f\"  ğŸš€ NumPy 2.x æ–°æ©Ÿèƒ½: é«˜é€ŸåŒ–ãƒ»å‹å®‰å…¨æ€§å‘ä¸Šãƒ»æ–°APIåˆ©ç”¨å¯èƒ½\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if faiss_working:\n",
    "    print(f\"\\nâœ… NumPy 2.x + FAISSæœ€æ–°ç‰ˆ çµ±åˆå®Œäº†!\")\n",
    "    print(f\"ğŸš€ æ¬¡ä¸–ä»£InsightSpike-AIæº–å‚™å®Œäº†\")\n",
    "    print(f\"ğŸ’« NumPy 2.x ã®é«˜é€ŸåŒ–ã¨æ–°æ©Ÿèƒ½ã‚’æ´»ç”¨å¯èƒ½\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ FAISSçµ±åˆã«å•é¡ŒãŒã‚ã‚Šã¾ã™\")\n",
    "    print(f\"ğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:\")\n",
    "    print(f\"   1. ğŸ“± Runtime > Restart Runtime ã§ã‚«ãƒ¼ãƒãƒ«å†èµ·å‹•\")\n",
    "    print(f\"   2. ğŸ”„ ã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œ\")\n",
    "    print(f\"   3. âœ… NumPy 2.x + FAISSçµ±åˆå‹•ä½œç¢ºèª\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\")\n",
    "print(\"  1. å®Ÿè¡Œåˆ¶å¾¡ã‚»ãƒ« (eval_config) ã®ç¢ºèª\")\n",
    "print(\"  2. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã®å®Ÿè¡Œ (NumPy 2.xæœ€é©åŒ–)\")\n",
    "print(\"  3. RAGã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ã®å®Ÿè¡Œ (æœ€æ–°æŠ€è¡“æ´»ç”¨)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa69804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ MEMORY MANAGEMENT & CHECKPOINT UTILITIES\n",
      "==================================================\n",
      "âš ï¸ Using fallback configuration: demo\n",
      "ğŸ“Š Initial memory usage: 140.5MB\n",
      "âœ… Memory management utilities loaded\n",
      "ğŸ†” Experiment ID: demo_20250629_233831\n",
      "ğŸ“ Results Directory: ./experiments/results/demo_20250629_233831\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ› ï¸ Memory Management & Checkpoint Utilities\n",
    "\n",
    "print(\"ğŸ› ï¸ MEMORY MANAGEMENT & CHECKPOINT UTILITIES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# å¿…è¦ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import psutil\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "from typing import Dict, Any, Optional\n",
    "import os\n",
    "\n",
    "# å®Ÿè¡Œåˆ¶å¾¡ã‚»ãƒ«ã§å®šç¾©ã•ã‚ŒãŸã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’ç¢ºèªãƒ»å–å¾—\n",
    "if 'eval_config' in globals():\n",
    "    # å®Ÿè¡Œåˆ¶å¾¡ã‚»ãƒ«ãŒæ­£å¸¸ã«å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "    PROFILE_CONFIG = eval_config\n",
    "    EXECUTION_PROFILE = eval_config.profile\n",
    "    EXPERIMENT_ID = eval_config.experiment_id\n",
    "    RESULTS_DIR = eval_config.results_dir\n",
    "    MEMORY_MANAGEMENT = {\n",
    "        \"cleanup_after_sections\": eval_config.memory_cleanup,\n",
    "        \"checkpoint_save\": eval_config.save_results,\n",
    "        \"memory_monitoring\": True,\n",
    "        \"cuda_cache_clear\": True\n",
    "    }\n",
    "    SECTIONS_TO_RUN = eval_config.sections_to_run\n",
    "    print(f\"âœ… Using eval_config from execution control cell\")\n",
    "    print(f\"ğŸ“‹ Profile: {EXECUTION_PROFILE}\")\n",
    "else:\n",
    "    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š\n",
    "    EXECUTION_PROFILE = \"demo\"\n",
    "    EXPERIMENT_ID = f\"demo_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    RESULTS_DIR = f\"./experiments/results/{EXPERIMENT_ID}\"\n",
    "    MEMORY_MANAGEMENT = {\n",
    "        \"cleanup_after_sections\": True,\n",
    "        \"checkpoint_save\": True,\n",
    "        \"memory_monitoring\": True,\n",
    "        \"cuda_cache_clear\": True\n",
    "    }\n",
    "    SECTIONS_TO_RUN = {\n",
    "        \"setup\": True,\n",
    "        \"rag_systems\": True,\n",
    "        \"datasets\": True,\n",
    "        \"experiments\": True,\n",
    "        \"analysis\": True,\n",
    "        \"visualization\": True\n",
    "    }\n",
    "    # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š\n",
    "    class SimpleConfig:\n",
    "        def __init__(self):\n",
    "            self.profile = \"demo\"\n",
    "            self.sample_sizes = [100, 500, 1000]\n",
    "            self.max_queries = 50\n",
    "            self.systems = [\"llm_only\", \"insightspike\"]\n",
    "            self.datasets = [\"squad_fallback\", \"synthetic\"]\n",
    "            self.enable_visualization = True\n",
    "            self.save_results = True\n",
    "            self.memory_cleanup = True\n",
    "    \n",
    "    PROFILE_CONFIG = SimpleConfig()\n",
    "    print(f\"âš ï¸ Using fallback configuration: {EXECUTION_PROFILE}\")\n",
    "\n",
    "def get_memory_usage() -> Dict[str, float]:\n",
    "    \"\"\"ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—\"\"\"\n",
    "    process = psutil.Process()\n",
    "    memory_info = process.memory_info()\n",
    "    \n",
    "    usage = {\n",
    "        'rss_mb': memory_info.rss / 1024 / 1024,  # Resident Set Size\n",
    "        'vms_mb': memory_info.vms / 1024 / 1024,  # Virtual Memory Size\n",
    "        'percent': process.memory_percent()\n",
    "    }\n",
    "    \n",
    "    return usage\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ\"\"\"\n",
    "    try:\n",
    "        # Python garbage collection\n",
    "        collected = gc.collect()\n",
    "        \n",
    "        # GPU memory cleanup (if available)\n",
    "        try:\n",
    "            import torch\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "        except ImportError:\n",
    "            pass\n",
    "        \n",
    "        return collected\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Memory cleanup warning: {e}\")\n",
    "        return 0\n",
    "\n",
    "def save_checkpoint(data: Dict[str, Any], checkpoint_name: str, results_dir: str) -> bool:\n",
    "    \"\"\"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜\"\"\"\n",
    "    try:\n",
    "        # çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«åã®æ­£è¦åŒ–ï¼ˆé•·ã™ãã‚‹å ´åˆã®å¯¾ç­–ï¼‰\n",
    "        if isinstance(checkpoint_name, str) and len(checkpoint_name) > 100:\n",
    "            # ãƒ•ã‚¡ã‚¤ãƒ«åãŒé•·ã™ãã‚‹å ´åˆã¯çŸ­ç¸®\n",
    "            checkpoint_name = checkpoint_name[:90] + \"_truncated\"\n",
    "        \n",
    "        # JSONä¿å­˜\n",
    "        json_path = os.path.join(results_dir, f\"{checkpoint_name}.json\")\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "        # Pickleä¿å­˜ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰\n",
    "        pkl_path = os.path.join(results_dir, f\"{checkpoint_name}.pkl\")\n",
    "        with open(pkl_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        \n",
    "        print(f\"ğŸ’¾ Checkpoint saved: {checkpoint_name}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Checkpoint save failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_checkpoint(checkpoint_name: str, results_dir: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿\"\"\"\n",
    "    try:\n",
    "        # JSONå„ªå…ˆã§èª­ã¿è¾¼ã¿\n",
    "        json_path = os.path.join(results_dir, f\"{checkpoint_name}.json\")\n",
    "        if os.path.exists(json_path):\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        \n",
    "        # Pickleãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "        pkl_path = os.path.join(results_dir, f\"{checkpoint_name}.pkl\")\n",
    "        if os.path.exists(pkl_path):\n",
    "            with open(pkl_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Checkpoint load failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def cleanup_section(section_name: str):\n",
    "    \"\"\"ã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Œäº†å¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\"\"\"\n",
    "    try:\n",
    "        if MEMORY_MANAGEMENT.get('cleanup_after_sections', True):\n",
    "            collected = cleanup_memory()\n",
    "            memory = get_memory_usage()\n",
    "            print(f\"ğŸ§¹ Cleanup after {section_name}: {collected} objects collected, {memory['rss_mb']:.1f}MB used\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Cleanup warning for {section_name}: {e}\")\n",
    "\n",
    "def monitor_memory(func_name: str):\n",
    "    \"\"\"ãƒ¡ãƒ¢ãƒªç›£è¦–ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ï¼ˆå®Ÿè¡Œåˆ¶å¾¡ã‚»ãƒ«ã¨ã®äº’æ›æ€§ï¼‰\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            if not MEMORY_MANAGEMENT['memory_monitoring']:\n",
    "                return func(*args, **kwargs)\n",
    "            \n",
    "            # å®Ÿè¡Œå‰ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡\n",
    "            start_memory = get_memory_usage()\n",
    "            print(f\"ğŸ” Starting {func_name} (Memory: {start_memory['rss_mb']:.1f}MB)\")\n",
    "            \n",
    "            try:\n",
    "                # é–¢æ•°å®Ÿè¡Œ\n",
    "                result = func(*args, **kwargs)\n",
    "                \n",
    "                # å®Ÿè¡Œå¾Œã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡\n",
    "                end_memory = get_memory_usage()\n",
    "                memory_diff = end_memory['rss_mb'] - start_memory['rss_mb']\n",
    "                print(f\"âœ… Completed {func_name} (Memory: {end_memory['rss_mb']:.1f}MB, Î”{memory_diff:+.1f}MB)\")\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                end_memory = get_memory_usage()\n",
    "                print(f\"âŒ Failed {func_name} (Memory: {end_memory['rss_mb']:.1f}MB)\")\n",
    "                raise\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def memory_monitor(section_name: str):\n",
    "    \"\"\"ãƒ¡ãƒ¢ãƒªç›£è¦–ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            try:\n",
    "                # å®Ÿè¡Œå‰ãƒ¡ãƒ¢ãƒªè¨˜éŒ²\n",
    "                start_memory = get_memory_usage()\n",
    "                print(f\"ğŸ” Starting {section_name} (Memory: {start_memory['rss_mb']:.1f}MB)\")\n",
    "                \n",
    "                # é–¢æ•°å®Ÿè¡Œ\n",
    "                result = func(*args, **kwargs)\n",
    "                \n",
    "                # å®Ÿè¡Œå¾Œãƒ¡ãƒ¢ãƒªè¨˜éŒ²\n",
    "                end_memory = get_memory_usage()\n",
    "                memory_diff = end_memory['rss_mb'] - start_memory['rss_mb']\n",
    "                print(f\"âœ… Completed {section_name} (Memory: {end_memory['rss_mb']:.1f}MB)\")\n",
    "                \n",
    "                # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Œäº†å¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "                cleanup_section(section_name)\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Section {section_name} failed: {e}\")\n",
    "                cleanup_section(f\"{section_name} (failed)\")\n",
    "                raise\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def section_control(section_name: str):\n",
    "    \"\"\"ã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œåˆ¶å¾¡ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œãƒã‚§ãƒƒã‚¯\n",
    "            if not SECTIONS_TO_RUN.get(section_name, True):\n",
    "                print(f\"â­ï¸ Skipping section: {section_name}\")\n",
    "                return None\n",
    "            \n",
    "            # ãƒ¡ãƒ¢ãƒªç›£è¦–ä»˜ãã§å®Ÿè¡Œ\n",
    "            monitored_func = memory_monitor(section_name)(func)\n",
    "            return monitored_func(*args, **kwargs)\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# åˆæœŸãƒ¡ãƒ¢ãƒªçŠ¶æ…‹è¨˜éŒ²\n",
    "initial_memory = get_memory_usage()\n",
    "print(f\"ğŸ“Š Initial memory usage: {initial_memory['rss_mb']:.1f}MB\")\n",
    "\n",
    "print(f\"âœ… Memory management utilities loaded\")\n",
    "print(f\"ğŸ†” Experiment ID: {EXPERIMENT_ID}\")\n",
    "print(f\"ğŸ“ Results Directory: {RESULTS_DIR}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4356a7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ INSIGHTSPIKE-AI COMPLETE SETUP\n",
      "==================================================\n",
      "ğŸ“ Current directory: /Users/miyauchikazuyoshi/Documents/GitHub/InsightSpike-AI/experiments_colab/phase2_rag_benchmark\n",
      "ğŸ”‘ GitHub token not found in environment\n",
      "ğŸ’¡ For private repository access, you can:\n",
      "   1. Set GITHUB_TOKEN environment variable\n",
      "   2. Add GITHUB_TOKEN to Colab Secrets\n",
      "   3. Provide token manually (not recommended)\n",
      "ğŸ“¥ Trying public clone from https://github.com/miyauchikazuyoshi/InsightSpike-AI.git...\n",
      "ğŸ“¥ Trying public clone from https://github.com/miyauchikazuyoshi/InsightSpike-AI.git...\n",
      "âœ… Repository cloned successfully (public access)\n",
      "ğŸ“š Added to Python path: /Users/miyauchikazuyoshi/Documents/GitHub/InsightSpike-AI/experiments_colab/phase2_rag_benchmark/InsightSpike-AI/src\n",
      "ğŸ¯ SEED fixed: 42 (random, numpy)\n",
      "\n",
      "ğŸ“Š Repository structure:\n",
      "  ğŸ“ .git/\n",
      "  ğŸ“ .github/\n",
      "  ğŸ“„ .gitignore\n",
      "  ğŸ“ .vscode/\n",
      "  ğŸ“„ CITATION.cff\n",
      "  ğŸ“„ CONTRIBUTING.md\n",
      "  ğŸ“„ LICENSE\n",
      "  ğŸ“„ README.md\n",
      "  ğŸ“ benchmarks/\n",
      "  ğŸ“ data/\n",
      "\n",
      "ğŸ§  InsightSpike modules found:\n",
      "  ğŸ”§ algorithms\n",
      "  ğŸ”§ cli\n",
      "\n",
      "âœ… Repository setup completed!\n",
      "ğŸ“ InsightSpike root: /Users/miyauchikazuyoshi/Documents/GitHub/InsightSpike-AI/experiments_colab/phase2_rag_benchmark/InsightSpike-AI\n",
      "ğŸ Python path configured: False\n",
      "==================================================\n",
      "âœ… Repository cloned successfully (public access)\n",
      "ğŸ“š Added to Python path: /Users/miyauchikazuyoshi/Documents/GitHub/InsightSpike-AI/experiments_colab/phase2_rag_benchmark/InsightSpike-AI/src\n",
      "ğŸ¯ SEED fixed: 42 (random, numpy)\n",
      "\n",
      "ğŸ“Š Repository structure:\n",
      "  ğŸ“ .git/\n",
      "  ğŸ“ .github/\n",
      "  ğŸ“„ .gitignore\n",
      "  ğŸ“ .vscode/\n",
      "  ğŸ“„ CITATION.cff\n",
      "  ğŸ“„ CONTRIBUTING.md\n",
      "  ğŸ“„ LICENSE\n",
      "  ğŸ“„ README.md\n",
      "  ğŸ“ benchmarks/\n",
      "  ğŸ“ data/\n",
      "\n",
      "ğŸ§  InsightSpike modules found:\n",
      "  ğŸ”§ algorithms\n",
      "  ğŸ”§ cli\n",
      "\n",
      "âœ… Repository setup completed!\n",
      "ğŸ“ InsightSpike root: /Users/miyauchikazuyoshi/Documents/GitHub/InsightSpike-AI/experiments_colab/phase2_rag_benchmark/InsightSpike-AI\n",
      "ğŸ Python path configured: False\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ Step 1: ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³ & ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ğŸš€ INSIGHTSPIKE-AI COMPLETE SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª\n",
    "current_dir = Path.cwd()\n",
    "print(f\"ğŸ“ Current directory: {current_dir}\")\n",
    "\n",
    "# InsightSpike-AIãƒªãƒã‚¸ãƒˆãƒªã®ç¢ºèªãƒ»ã‚¯ãƒ­ãƒ¼ãƒ³ï¼ˆãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒªãƒã‚¸ãƒˆãƒªå¯¾å¿œï¼‰\n",
    "repo_name = \"InsightSpike-AI\"\n",
    "repo_public_url = \"https://github.com/miyauchikazuyoshi/InsightSpike-AI.git\"\n",
    "repo_path = current_dir / repo_name\n",
    "\n",
    "# GitHubèªè¨¼ã®ç¢ºèª\n",
    "def get_github_auth():\n",
    "    \"\"\"GitHubèªè¨¼æƒ…å ±ã‚’å–å¾—ï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦è¡Œï¼‰\"\"\"\n",
    "    auth_methods = []\n",
    "    \n",
    "    # æ–¹æ³•1: ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—\n",
    "    github_token = os.environ.get('GITHUB_TOKEN')\n",
    "    if github_token:\n",
    "        auth_methods.append(('token', github_token))\n",
    "        print(\"ğŸ”‘ GitHub token found in environment variables\")\n",
    "    \n",
    "    # æ–¹æ³•2: Google Colab Secrets\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        github_token = userdata.get('GITHUB_TOKEN')\n",
    "        if github_token:\n",
    "            auth_methods.append(('token', github_token))\n",
    "            print(\"ğŸ”‘ GitHub token found in Colab secrets\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # æ–¹æ³•3: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ï¼ˆæ‰‹å‹•ï¼‰\n",
    "    if not auth_methods:\n",
    "        print(\"ğŸ”‘ GitHub token not found in environment\")\n",
    "        print(\"ğŸ’¡ For private repository access, you can:\")\n",
    "        print(\"   1. Set GITHUB_TOKEN environment variable\")\n",
    "        print(\"   2. Add GITHUB_TOKEN to Colab Secrets\")\n",
    "        print(\"   3. Provide token manually (not recommended)\")\n",
    "        \n",
    "        # æ‰‹å‹•å…¥åŠ›ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ³¨æ„ï¼‰\n",
    "        try:\n",
    "            import getpass\n",
    "            manual_token = getpass.getpass(\"Enter GitHub token (optional, press Enter to skip): \")\n",
    "            if manual_token.strip():\n",
    "                auth_methods.append(('token', manual_token.strip()))\n",
    "                print(\"ğŸ”‘ Manual token provided\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    return auth_methods\n",
    "\n",
    "def create_authenticated_url(base_url: str, token: str) -> str:\n",
    "    \"\"\"ãƒˆãƒ¼ã‚¯ãƒ³ä»˜ãGitHub URLã‚’ä½œæˆ\"\"\"\n",
    "    if token and base_url.startswith(\"https://github.com/\"):\n",
    "        # https://token@github.com/user/repo.git å½¢å¼\n",
    "        auth_url = base_url.replace(\"https://github.com/\", f\"https://{token}@github.com/\")\n",
    "        return auth_url\n",
    "    return base_url\n",
    "\n",
    "# GitHubèªè¨¼æƒ…å ±å–å¾—\n",
    "auth_methods = get_github_auth()\n",
    "\n",
    "if repo_path.exists():\n",
    "    print(f\"ğŸ“‚ Repository already exists at {repo_path}\")\n",
    "    \n",
    "    # æ—¢å­˜ãƒªãƒã‚¸ãƒˆãƒªã®æ›´æ–°\n",
    "    try:\n",
    "        print(\"ğŸ”„ Updating existing repository...\")\n",
    "        \n",
    "        # èªè¨¼ä»˜ãã§pullï¼ˆå¿…è¦ãªå ´åˆï¼‰\n",
    "        if auth_methods:\n",
    "            # ãƒªãƒ¢ãƒ¼ãƒˆURLã‚’èªè¨¼ä»˜ãã«æ›´æ–°\n",
    "            auth_type, token = auth_methods[0]\n",
    "            auth_url = create_authenticated_url(repo_public_url, token)\n",
    "            \n",
    "            # ãƒªãƒ¢ãƒ¼ãƒˆURLæ›´æ–°\n",
    "            subprocess.run(\n",
    "                [\"git\", \"remote\", \"set-url\", \"origin\", auth_url],\n",
    "                cwd=repo_path,\n",
    "                capture_output=True,\n",
    "                timeout=30\n",
    "            )\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"pull\", \"origin\", \"main\"], \n",
    "            cwd=repo_path, \n",
    "            capture_output=True, \n",
    "            text=True,\n",
    "            timeout=60\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… Repository updated successfully\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Git pull warning: {result.stderr[:200]}...\")\n",
    "            print(\"ğŸ“¦ Continuing with existing repository...\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Update failed: {str(e)[:100]}...\")\n",
    "        print(\"ğŸ“¦ Using existing repository...\")\n",
    "\n",
    "else:\n",
    "    # æ–°è¦ã‚¯ãƒ­ãƒ¼ãƒ³\n",
    "    clone_success = False\n",
    "    \n",
    "    # èªè¨¼ä»˜ãã‚¯ãƒ­ãƒ¼ãƒ³ã‚’è©¦è¡Œ\n",
    "    for auth_type, token in auth_methods:\n",
    "        try:\n",
    "            auth_url = create_authenticated_url(repo_public_url, token)\n",
    "            print(f\"ğŸ“¥ Cloning repository with {auth_type} authentication...\")\n",
    "            \n",
    "            result = subprocess.run(\n",
    "                [\"git\", \"clone\", auth_url, str(repo_path)], \n",
    "                capture_output=True, \n",
    "                text=True,\n",
    "                timeout=300\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(\"âœ… Repository cloned successfully with authentication\")\n",
    "                clone_success = True\n",
    "                break\n",
    "            else:\n",
    "                print(f\"âš ï¸ Authentication failed: {result.stderr[:100]}...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Clone attempt failed: {str(e)[:100]}...\")\n",
    "            continue\n",
    "    \n",
    "    # èªè¨¼ãªã—ã‚¯ãƒ­ãƒ¼ãƒ³ã‚’è©¦è¡Œï¼ˆãƒ‘ãƒ–ãƒªãƒƒã‚¯ã®å ´åˆï¼‰\n",
    "    if not clone_success:\n",
    "        try:\n",
    "            print(f\"ğŸ“¥ Trying public clone from {repo_public_url}...\")\n",
    "            result = subprocess.run(\n",
    "                [\"git\", \"clone\", repo_public_url, str(repo_path)], \n",
    "                capture_output=True, \n",
    "                text=True,\n",
    "                timeout=300\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(\"âœ… Repository cloned successfully (public access)\")\n",
    "                clone_success = True\n",
    "            else:\n",
    "                print(f\"âŒ Public clone failed: {result.stderr[:100]}...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Public clone exception: {str(e)[:100]}...\")\n",
    "    \n",
    "    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šåŸºæœ¬ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ä½œæˆ\n",
    "    if not clone_success:\n",
    "        print(\"ğŸ”„ All clone attempts failed - creating fallback directory structure...\")\n",
    "        print(\"ğŸ’¡ Manual setup required:\")\n",
    "        print(\"   1. Clone repository manually with your GitHub credentials\")\n",
    "        print(\"   2. Or set GITHUB_TOKEN environment variable\")\n",
    "        print(\"   3. Or add GITHUB_TOKEN to Colab Secrets\")\n",
    "        \n",
    "        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šåŸºæœ¬ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ä½œæˆ\n",
    "        repo_path.mkdir(exist_ok=True)\n",
    "        (repo_path / \"src\").mkdir(exist_ok=True)\n",
    "        (repo_path / \"src\" / \"insightspike\").mkdir(exist_ok=True)\n",
    "        \n",
    "        print(\"âš ï¸ Fallback mode: Some InsightSpike features may not be available\")\n",
    "        print(\"âš ï¸ Please manually clone the repository for full functionality\")\n",
    "\n",
    "# Pythonãƒ‘ã‚¹ã«è¿½åŠ \n",
    "insightspike_src = repo_path / \"src\"\n",
    "if str(insightspike_src) not in sys.path:\n",
    "    sys.path.insert(0, str(insightspike_src))\n",
    "    print(f\"ğŸ“š Added to Python path: {insightspike_src}\")\n",
    "\n",
    "# ç’°å¢ƒå¤‰æ•°è¨­å®š\n",
    "os.environ['INSIGHTSPIKE_ROOT'] = str(repo_path)\n",
    "os.environ['PYTHONPATH'] = str(insightspike_src) + \":\" + os.environ.get('PYTHONPATH', '')\n",
    "\n",
    "# ğŸ¯ å†ç¾æ€§ç¢ºä¿ï¼šã‚·ãƒ¼ãƒ‰å›ºå®šï¼ˆæœ€å„ªå…ˆï¼‰\n",
    "SEED = 42\n",
    "\n",
    "# ğŸ¯ å†ç¾æ€§ç¢ºä¿ï¼šã‚·ãƒ¼ãƒ‰å›ºå®šï¼ˆæœ€å„ªå…ˆï¼‰\n",
    "SEED = 42\n",
    "\n",
    "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆå‰ã®ã‚·ãƒ¼ãƒ‰å›ºå®š\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# PyTorché–¢é€£ã¯å¾Œã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¾Œã«è¨­å®š\n",
    "print(f\"ğŸ¯ SEED fixed: {SEED} (random, numpy)\")\n",
    "\n",
    "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ç¢ºèª\n",
    "print(f\"\\nğŸ“Š Repository structure:\")\n",
    "if repo_path.exists():\n",
    "    for item in sorted(repo_path.iterdir())[:10]:  # æœ€åˆã®10é …ç›®ã®ã¿è¡¨ç¤º\n",
    "        if item.is_dir():\n",
    "            print(f\"  ğŸ“ {item.name}/\")\n",
    "        else:\n",
    "            print(f\"  ğŸ“„ {item.name}\")\n",
    "    \n",
    "    # src/insightspike ã®ç¢ºèª\n",
    "    insightspike_dir = repo_path / \"src\" / \"insightspike\"\n",
    "    if insightspike_dir.exists():\n",
    "        print(f\"\\nğŸ§  InsightSpike modules found:\")\n",
    "        for module in sorted(insightspike_dir.iterdir())[:5]:\n",
    "            if module.is_dir() and not module.name.startswith('__'):\n",
    "                print(f\"  ğŸ”§ {module.name}\")\n",
    "\n",
    "print(f\"\\nâœ… Repository setup completed!\")\n",
    "print(f\"ğŸ“ InsightSpike root: {repo_path}\")\n",
    "print(f\"ğŸ Python path configured: {insightspike_src in sys.path}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18712be",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 1: ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "\n",
    "ç’°å¢ƒæ§‹ç¯‰ã¨å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "110504dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Starting setup (Memory: 75.9MB)\n",
      "ğŸ” Starting environment_setup (Memory: 76.1MB)\n",
      "ğŸŒ± Reproducibility seed fixed: 42\n",
      "  - random.seed(42)\n",
      "  - np.random.seed(42)\n",
      "  - torch.manual_seed(42) [if available]\n",
      "ğŸš€ COMPREHENSIVE ENVIRONMENT SETUP\n",
      "==================================================\n",
      "ğŸ¯ Profile: demo\n",
      "ğŸ”¢ Reproducibility Seed: 42\n",
      "ğŸ“… Experiment Date: 2025-06-29 23:52:18\n",
      "ğŸ“Š Data sizes: [100, 500, 1000]\n",
      "ğŸ” Max queries: 50\n",
      "ğŸ¤– RAG systems: ['llm_only', 'insightspike']\n",
      "ğŸ“š Datasets: ['squad_fallback', 'synthetic']\n",
      "âœ… Completed environment_setup (Memory: 77.4MB, Î”+1.3MB)\n",
      "âœ… Completed setup (Memory: 77.4MB)\n",
      "ğŸ§¹ Cleanup after setup: 2698 objects collected, 254.6MB used\n",
      "\n",
      "ğŸ“‹ Active Configuration:\n",
      "  ğŸ¯ Profile: demo\n",
      "  ğŸ†” Experiment ID: demo_20250629_233831\n",
      "  ğŸ“ Results Directory: ./experiments/results/demo_20250629_233831\n",
      "\n",
      "ğŸ“¦ Loading core libraries...\n",
      "âœ… Core data science libraries loaded\n",
      "\n",
      "ğŸ” Loading RAG libraries...\n",
      "âŒ RAG library import failed: cannot import name 'GenerationMixin' from 'transformers.generation' (/Users/miyauchikazuyoshi/.pyenv/versions/3.11.12/lib/python3.11/site-packages/transformers/generation/__init__.py)\n",
      "ğŸ’¡ Run the Colab emergency fix cell if needed\n",
      "\n",
      "ğŸ§  Testing InsightSpike integration...\n",
      "ğŸ§¹ Cleanup after setup: 2698 objects collected, 254.6MB used\n",
      "\n",
      "ğŸ“‹ Active Configuration:\n",
      "  ğŸ¯ Profile: demo\n",
      "  ğŸ†” Experiment ID: demo_20250629_233831\n",
      "  ğŸ“ Results Directory: ./experiments/results/demo_20250629_233831\n",
      "\n",
      "ğŸ“¦ Loading core libraries...\n",
      "âœ… Core data science libraries loaded\n",
      "\n",
      "ğŸ” Loading RAG libraries...\n",
      "âŒ RAG library import failed: cannot import name 'GenerationMixin' from 'transformers.generation' (/Users/miyauchikazuyoshi/.pyenv/versions/3.11.12/lib/python3.11/site-packages/transformers/generation/__init__.py)\n",
      "ğŸ’¡ Run the Colab emergency fix cell if needed\n",
      "\n",
      "ğŸ§  Testing InsightSpike integration...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'e'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 121\u001b[39m\n\u001b[32m    118\u001b[39m     sys.path.insert(\u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m./InsightSpike-AI/src\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# åŸºæœ¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minsightspike\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayer2_memory_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m L2MemoryManager\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minsightspike\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmain_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MainAgent\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# ç°¡å˜ãªåˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/InsightSpike-AI/experiments_colab/phase2_rag_benchmark/./InsightSpike-AI/src/insightspike/__init__.py:15\u001b[39m\n\u001b[32m     12\u001b[39m LITE_MODE = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mINSIGHTSPIKE_LITE_MODE\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Legacy compatibility exports - import the config.py file specifically\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Export new main agent for easy access\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m LITE_MODE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/InsightSpike-AI/experiments_colab/phase2_rag_benchmark/./InsightSpike-AI/src/insightspike/core/__init__.py:8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Data structures and framework\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperiment_framework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m         BaseExperiment,\n\u001b[32m     10\u001b[39m         ExperimentConfig, \n\u001b[32m     11\u001b[39m         ExperimentResult,\n\u001b[32m     12\u001b[39m         PerformanceMetrics,\n\u001b[32m     13\u001b[39m         ExperimentSuite,\n\u001b[32m     14\u001b[39m         create_simple_experiment_config,\n\u001b[32m     15\u001b[39m         create_performance_metrics\n\u001b[32m     16\u001b[39m     )\n\u001b[32m     17\u001b[39m     EXPERIMENT_FRAMEWORK_AVAILABLE = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/InsightSpike-AI/experiments_colab/phase2_rag_benchmark/./InsightSpike-AI/src/insightspike/core/experiment_framework.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mabc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ABC, abstractmethod\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass, asdict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.12/lib/python3.11/site-packages/seaborn/__init__.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpalettes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrelational\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcategorical\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.12/lib/python3.11/site-packages/seaborn/relational.py:21\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     adjust_legend_subtitles,\n\u001b[32m     15\u001b[39m     _default_color,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     _scatter_legend_artist,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m groupby_apply_include_groups\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_statistics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EstimateAggregator, WeightedAggregator\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01maxisgrid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FacetGrid, _facet_docs\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_docstrings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringComponents, _core_docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.12/lib/python3.11/site-packages/seaborn/_statistics.py:32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gaussian_kde\n\u001b[32m     33\u001b[39m     _no_scipy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.12/lib/python3.11/site-packages/scipy/stats/__init__.py:624\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    619\u001b[39m \n\u001b[32m    620\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_warnings_errors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[32m    623\u001b[39m                                DegenerateDataWarning, FitError)\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_stats_py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_variation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.12/lib/python3.11/site-packages/scipy/stats/_stats_py.py:38\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array, asarray, ma\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspatial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distance_matrix\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m milp, LinearConstraint\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.12/lib/python3.11/site-packages/scipy/sparse/__init__.py:300\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# Original code by Travis Oliphant.\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# Modified and extended by Ed Schofield, Robert Cimrman,\u001b[39;00m\n\u001b[32m    296\u001b[39m \u001b[38;5;66;03m# Nathan Bell, and Jake Vanderplas.\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_warnings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.12/lib/python3.11/site-packages/scipy/sparse/_base.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"Base class for sparse matrices\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[32m      6\u001b[39m                        get_sum_dtype, isdense, isscalarlike,\n\u001b[32m      7\u001b[39m                        matrix, validateaxis, getdtype)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n\u001b[32m     11\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33misspmatrix\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33missparse\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msparray\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     12\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mSparseWarning\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSparseEfficiencyWarning\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.12/lib/python3.11/site-packages/scipy/sparse/_sputils.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prod\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m np_long, np_ulong\n\u001b[32m     13\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33mupcast\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgetdtype\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgetdata\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33misscalarlike\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33misintlike\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     14\u001b[39m            \u001b[33m'\u001b[39m\u001b[33misshape\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33missequence\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33misdense\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mismatrix\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mget_sum_dtype\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     15\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mbroadcast_shapes\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     17\u001b[39m supported_dtypes = [np.bool_, np.byte, np.ubyte, np.short, np.ushort, np.intc,\n\u001b[32m     18\u001b[39m                     np.uintc, np_long, np_ulong, np.longlong, np.ulonglong,\n\u001b[32m     19\u001b[39m                     np.float32, np.float64, np.longdouble,\n\u001b[32m     20\u001b[39m                     np.complex64, np.complex128, np.clongdouble]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.12/lib/python3.11/site-packages/scipy/_lib/_util.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TypeAlias, TypeVar\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_namespace, is_numpy, xp_size\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_docscrape\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionDoc, Parameter\n\u001b[32m     17\u001b[39m AxisError: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mException\u001b[39;00m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.12/lib/python3.11/site-packages/scipy/_lib/_array_api.py:18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnpt\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_compat\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m     is_array_api_obj,\n\u001b[32m     20\u001b[39m     size \u001b[38;5;28;01mas\u001b[39;00m xp_size,\n\u001b[32m     21\u001b[39m     numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat,\n\u001b[32m     22\u001b[39m     device \u001b[38;5;28;01mas\u001b[39;00m xp_device,\n\u001b[32m     23\u001b[39m     is_numpy_namespace \u001b[38;5;28;01mas\u001b[39;00m is_numpy,\n\u001b[32m     24\u001b[39m     is_cupy_namespace \u001b[38;5;28;01mas\u001b[39;00m is_cupy,\n\u001b[32m     25\u001b[39m     is_torch_namespace \u001b[38;5;28;01mas\u001b[39;00m is_torch,\n\u001b[32m     26\u001b[39m     is_jax_namespace \u001b[38;5;28;01mas\u001b[39;00m is_jax,\n\u001b[32m     27\u001b[39m     is_array_api_strict_namespace \u001b[38;5;28;01mas\u001b[39;00m is_array_api_strict\n\u001b[32m     28\u001b[39m )\n\u001b[32m     30\u001b[39m __all__ = [\n\u001b[32m     31\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m_asarray\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33marray_namespace\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33massert_almost_equal\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33massert_array_almost_equal\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     32\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mget_xp_devices\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mxp_take_along_axis\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mxp_unsupported_param_msg\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mxp_vector_norm\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     39\u001b[39m ]\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# To enable array API and strict array-like input validation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.12/lib/python3.11/site-packages/scipy/_lib/array_api_compat/numpy/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m * \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# from numpy import * doesn't overwrite these builtin names\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mabs\u001b[39m, \u001b[38;5;28mmax\u001b[39m, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mround\u001b[39m \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.12/lib/python3.11/site-packages/numpy/__init__.py:333\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _core\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    121\u001b[39m     False_,\n\u001b[32m    122\u001b[39m     ScalarType,\n\u001b[32m    123\u001b[39m     True_,\n\u001b[32m    124\u001b[39m     \u001b[38;5;28mabs\u001b[39m,\n\u001b[32m    125\u001b[39m     absolute,\n\u001b[32m    126\u001b[39m     acos,\n\u001b[32m    127\u001b[39m     acosh,\n\u001b[32m    128\u001b[39m     add,\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mall\u001b[39m,\n\u001b[32m    130\u001b[39m     allclose,\n\u001b[32m    131\u001b[39m     amax,\n\u001b[32m    132\u001b[39m     amin,\n\u001b[32m    133\u001b[39m     \u001b[38;5;28many\u001b[39m,\n\u001b[32m    134\u001b[39m     arange,\n\u001b[32m    135\u001b[39m     arccos,\n\u001b[32m    136\u001b[39m     arccosh,\n\u001b[32m    137\u001b[39m     arcsin,\n\u001b[32m    138\u001b[39m     arcsinh,\n\u001b[32m    139\u001b[39m     arctan,\n\u001b[32m    140\u001b[39m     arctan2,\n\u001b[32m    141\u001b[39m     arctanh,\n\u001b[32m    142\u001b[39m     argmax,\n\u001b[32m    143\u001b[39m     argmin,\n\u001b[32m    144\u001b[39m     argpartition,\n\u001b[32m    145\u001b[39m     argsort,\n\u001b[32m    146\u001b[39m     argwhere,\n\u001b[32m    147\u001b[39m     around,\n\u001b[32m    148\u001b[39m     array,\n\u001b[32m    149\u001b[39m     array2string,\n\u001b[32m    150\u001b[39m     array_equal,\n\u001b[32m    151\u001b[39m     array_equiv,\n\u001b[32m    152\u001b[39m     array_repr,\n\u001b[32m    153\u001b[39m     array_str,\n\u001b[32m    154\u001b[39m     asanyarray,\n\u001b[32m    155\u001b[39m     asarray,\n\u001b[32m    156\u001b[39m     ascontiguousarray,\n\u001b[32m    157\u001b[39m     asfortranarray,\n\u001b[32m    158\u001b[39m     asin,\n\u001b[32m    159\u001b[39m     asinh,\n\u001b[32m    160\u001b[39m     astype,\n\u001b[32m    161\u001b[39m     atan,\n\u001b[32m    162\u001b[39m     atan2,\n\u001b[32m    163\u001b[39m     atanh,\n\u001b[32m    164\u001b[39m     atleast_1d,\n\u001b[32m    165\u001b[39m     atleast_2d,\n\u001b[32m    166\u001b[39m     atleast_3d,\n\u001b[32m    167\u001b[39m     base_repr,\n\u001b[32m    168\u001b[39m     binary_repr,\n\u001b[32m    169\u001b[39m     bitwise_and,\n\u001b[32m    170\u001b[39m     bitwise_count,\n\u001b[32m    171\u001b[39m     bitwise_invert,\n\u001b[32m    172\u001b[39m     bitwise_left_shift,\n\u001b[32m    173\u001b[39m     bitwise_not,\n\u001b[32m    174\u001b[39m     bitwise_or,\n\u001b[32m    175\u001b[39m     bitwise_right_shift,\n\u001b[32m    176\u001b[39m     bitwise_xor,\n\u001b[32m    177\u001b[39m     block,\n\u001b[32m    178\u001b[39m     \u001b[38;5;28mbool\u001b[39m,\n\u001b[32m    179\u001b[39m     bool_,\n\u001b[32m    180\u001b[39m     broadcast,\n\u001b[32m    181\u001b[39m     busday_count,\n\u001b[32m    182\u001b[39m     busday_offset,\n\u001b[32m    183\u001b[39m     busdaycalendar,\n\u001b[32m    184\u001b[39m     byte,\n\u001b[32m    185\u001b[39m     bytes_,\n\u001b[32m    186\u001b[39m     can_cast,\n\u001b[32m    187\u001b[39m     cbrt,\n\u001b[32m    188\u001b[39m     cdouble,\n\u001b[32m    189\u001b[39m     ceil,\n\u001b[32m    190\u001b[39m     character,\n\u001b[32m    191\u001b[39m     choose,\n\u001b[32m    192\u001b[39m     clip,\n\u001b[32m    193\u001b[39m     clongdouble,\n\u001b[32m    194\u001b[39m     complex64,\n\u001b[32m    195\u001b[39m     complex128,\n\u001b[32m    196\u001b[39m     complexfloating,\n\u001b[32m    197\u001b[39m     compress,\n\u001b[32m    198\u001b[39m     concat,\n\u001b[32m    199\u001b[39m     concatenate,\n\u001b[32m    200\u001b[39m     conj,\n\u001b[32m    201\u001b[39m     conjugate,\n\u001b[32m    202\u001b[39m     convolve,\n\u001b[32m    203\u001b[39m     copysign,\n\u001b[32m    204\u001b[39m     copyto,\n\u001b[32m    205\u001b[39m     correlate,\n\u001b[32m    206\u001b[39m     cos,\n\u001b[32m    207\u001b[39m     cosh,\n\u001b[32m    208\u001b[39m     count_nonzero,\n\u001b[32m    209\u001b[39m     cross,\n\u001b[32m    210\u001b[39m     csingle,\n\u001b[32m    211\u001b[39m     cumprod,\n\u001b[32m    212\u001b[39m     cumsum,\n\u001b[32m    213\u001b[39m     cumulative_prod,\n\u001b[32m    214\u001b[39m     cumulative_sum,\n\u001b[32m    215\u001b[39m     datetime64,\n\u001b[32m    216\u001b[39m     datetime_as_string,\n\u001b[32m    217\u001b[39m     datetime_data,\n\u001b[32m    218\u001b[39m     deg2rad,\n\u001b[32m    219\u001b[39m     degrees,\n\u001b[32m    220\u001b[39m     diagonal,\n\u001b[32m    221\u001b[39m     divide,\n\u001b[32m    222\u001b[39m     \u001b[38;5;28mdivmod\u001b[39m,\n\u001b[32m    223\u001b[39m     dot,\n\u001b[32m    224\u001b[39m     double,\n\u001b[32m    225\u001b[39m     dtype,\n\u001b[32m    226\u001b[39m     e,\n\u001b[32m    227\u001b[39m     einsum,\n\u001b[32m    228\u001b[39m     einsum_path,\n\u001b[32m    229\u001b[39m     empty,\n\u001b[32m    230\u001b[39m     empty_like,\n\u001b[32m    231\u001b[39m     equal,\n\u001b[32m    232\u001b[39m     errstate,\n\u001b[32m    233\u001b[39m     euler_gamma,\n\u001b[32m    234\u001b[39m     exp,\n\u001b[32m    235\u001b[39m     exp2,\n\u001b[32m    236\u001b[39m     expm1,\n\u001b[32m    237\u001b[39m     fabs,\n\u001b[32m    238\u001b[39m     finfo,\n\u001b[32m    239\u001b[39m     flatiter,\n\u001b[32m    240\u001b[39m     flatnonzero,\n\u001b[32m    241\u001b[39m     flexible,\n\u001b[32m    242\u001b[39m     float16,\n\u001b[32m    243\u001b[39m     float32,\n\u001b[32m    244\u001b[39m     float64,\n\u001b[32m    245\u001b[39m     float_power,\n\u001b[32m    246\u001b[39m     floating,\n\u001b[32m    247\u001b[39m     floor,\n\u001b[32m    248\u001b[39m     floor_divide,\n\u001b[32m    249\u001b[39m     fmax,\n\u001b[32m    250\u001b[39m     fmin,\n\u001b[32m    251\u001b[39m     fmod,\n\u001b[32m    252\u001b[39m     format_float_positional,\n\u001b[32m    253\u001b[39m     format_float_scientific,\n\u001b[32m    254\u001b[39m     frexp,\n\u001b[32m    255\u001b[39m     from_dlpack,\n\u001b[32m    256\u001b[39m     frombuffer,\n\u001b[32m    257\u001b[39m     fromfile,\n\u001b[32m    258\u001b[39m     fromfunction,\n\u001b[32m    259\u001b[39m     fromiter,\n\u001b[32m    260\u001b[39m     frompyfunc,\n\u001b[32m    261\u001b[39m     fromstring,\n\u001b[32m    262\u001b[39m     full,\n\u001b[32m    263\u001b[39m     full_like,\n\u001b[32m    264\u001b[39m     gcd,\n\u001b[32m    265\u001b[39m     generic,\n\u001b[32m    266\u001b[39m     geomspace,\n\u001b[32m    267\u001b[39m     get_printoptions,\n\u001b[32m    268\u001b[39m     getbufsize,\n\u001b[32m    269\u001b[39m     geterr,\n\u001b[32m    270\u001b[39m     geterrcall,\n\u001b[32m    271\u001b[39m     greater,\n\u001b[32m    272\u001b[39m     greater_equal,\n\u001b[32m    273\u001b[39m     half,\n\u001b[32m    274\u001b[39m     heaviside,\n\u001b[32m    275\u001b[39m     hstack,\n\u001b[32m    276\u001b[39m     hypot,\n\u001b[32m    277\u001b[39m     identity,\n\u001b[32m    278\u001b[39m     iinfo,\n\u001b[32m    279\u001b[39m     indices,\n\u001b[32m    280\u001b[39m     inexact,\n\u001b[32m    281\u001b[39m     inf,\n\u001b[32m    282\u001b[39m     inner,\n\u001b[32m    283\u001b[39m     int8,\n\u001b[32m    284\u001b[39m     int16,\n\u001b[32m    285\u001b[39m     int32,\n\u001b[32m    286\u001b[39m     int64,\n\u001b[32m    287\u001b[39m     int_,\n\u001b[32m    288\u001b[39m     intc,\n\u001b[32m    289\u001b[39m     integer,\n\u001b[32m    290\u001b[39m     intp,\n\u001b[32m    291\u001b[39m     invert,\n\u001b[32m    292\u001b[39m     is_busday,\n\u001b[32m    293\u001b[39m     isclose,\n\u001b[32m    294\u001b[39m     isdtype,\n\u001b[32m    295\u001b[39m     isfinite,\n\u001b[32m    296\u001b[39m     isfortran,\n\u001b[32m    297\u001b[39m     isinf,\n\u001b[32m    298\u001b[39m     isnan,\n\u001b[32m    299\u001b[39m     isnat,\n\u001b[32m    300\u001b[39m     isscalar,\n\u001b[32m    301\u001b[39m     issubdtype,\n\u001b[32m    302\u001b[39m     lcm,\n\u001b[32m    303\u001b[39m     ldexp,\n\u001b[32m    304\u001b[39m     left_shift,\n\u001b[32m    305\u001b[39m     less,\n\u001b[32m    306\u001b[39m     less_equal,\n\u001b[32m    307\u001b[39m     lexsort,\n\u001b[32m    308\u001b[39m     linspace,\n\u001b[32m    309\u001b[39m     little_endian,\n\u001b[32m    310\u001b[39m     log,\n\u001b[32m    311\u001b[39m     log1p,\n\u001b[32m    312\u001b[39m     log2,\n\u001b[32m    313\u001b[39m     log10,\n\u001b[32m    314\u001b[39m     logaddexp,\n\u001b[32m    315\u001b[39m     logaddexp2,\n\u001b[32m    316\u001b[39m     logical_and,\n\u001b[32m    317\u001b[39m     logical_not,\n\u001b[32m    318\u001b[39m     logical_or,\n\u001b[32m    319\u001b[39m     logical_xor,\n\u001b[32m    320\u001b[39m     logspace,\n\u001b[32m    321\u001b[39m     long,\n\u001b[32m    322\u001b[39m     longdouble,\n\u001b[32m    323\u001b[39m     longlong,\n\u001b[32m    324\u001b[39m     matmul,\n\u001b[32m    325\u001b[39m     matrix_transpose,\n\u001b[32m    326\u001b[39m     matvec,\n\u001b[32m    327\u001b[39m     \u001b[38;5;28mmax\u001b[39m,\n\u001b[32m    328\u001b[39m     maximum,\n\u001b[32m    329\u001b[39m     may_share_memory,\n\u001b[32m    330\u001b[39m     mean,\n\u001b[32m    331\u001b[39m     memmap,\n\u001b[32m    332\u001b[39m     \u001b[38;5;28mmin\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     min_scalar_type,\n\u001b[32m    334\u001b[39m     minimum,\n\u001b[32m    335\u001b[39m     mod,\n\u001b[32m    336\u001b[39m     modf,\n\u001b[32m    337\u001b[39m     moveaxis,\n\u001b[32m    338\u001b[39m     multiply,\n\u001b[32m    339\u001b[39m     nan,\n\u001b[32m    340\u001b[39m     ndarray,\n\u001b[32m    341\u001b[39m     ndim,\n\u001b[32m    342\u001b[39m     nditer,\n\u001b[32m    343\u001b[39m     negative,\n\u001b[32m    344\u001b[39m     nested_iters,\n\u001b[32m    345\u001b[39m     newaxis,\n\u001b[32m    346\u001b[39m     nextafter,\n\u001b[32m    347\u001b[39m     nonzero,\n\u001b[32m    348\u001b[39m     not_equal,\n\u001b[32m    349\u001b[39m     number,\n\u001b[32m    350\u001b[39m     object_,\n\u001b[32m    351\u001b[39m     ones,\n\u001b[32m    352\u001b[39m     ones_like,\n\u001b[32m    353\u001b[39m     outer,\n\u001b[32m    354\u001b[39m     partition,\n\u001b[32m    355\u001b[39m     permute_dims,\n\u001b[32m    356\u001b[39m     pi,\n\u001b[32m    357\u001b[39m     positive,\n\u001b[32m    358\u001b[39m     \u001b[38;5;28mpow\u001b[39m,\n\u001b[32m    359\u001b[39m     power,\n\u001b[32m    360\u001b[39m     printoptions,\n\u001b[32m    361\u001b[39m     prod,\n\u001b[32m    362\u001b[39m     promote_types,\n\u001b[32m    363\u001b[39m     ptp,\n\u001b[32m    364\u001b[39m     put,\n\u001b[32m    365\u001b[39m     putmask,\n\u001b[32m    366\u001b[39m     rad2deg,\n\u001b[32m    367\u001b[39m     radians,\n\u001b[32m    368\u001b[39m     ravel,\n\u001b[32m    369\u001b[39m     recarray,\n\u001b[32m    370\u001b[39m     reciprocal,\n\u001b[32m    371\u001b[39m     record,\n\u001b[32m    372\u001b[39m     remainder,\n\u001b[32m    373\u001b[39m     repeat,\n\u001b[32m    374\u001b[39m     require,\n\u001b[32m    375\u001b[39m     reshape,\n\u001b[32m    376\u001b[39m     resize,\n\u001b[32m    377\u001b[39m     result_type,\n\u001b[32m    378\u001b[39m     right_shift,\n\u001b[32m    379\u001b[39m     rint,\n\u001b[32m    380\u001b[39m     roll,\n\u001b[32m    381\u001b[39m     rollaxis,\n\u001b[32m    382\u001b[39m     \u001b[38;5;28mround\u001b[39m,\n\u001b[32m    383\u001b[39m     sctypeDict,\n\u001b[32m    384\u001b[39m     searchsorted,\n\u001b[32m    385\u001b[39m     set_printoptions,\n\u001b[32m    386\u001b[39m     setbufsize,\n\u001b[32m    387\u001b[39m     seterr,\n\u001b[32m    388\u001b[39m     seterrcall,\n\u001b[32m    389\u001b[39m     shape,\n\u001b[32m    390\u001b[39m     shares_memory,\n\u001b[32m    391\u001b[39m     short,\n\u001b[32m    392\u001b[39m     sign,\n\u001b[32m    393\u001b[39m     signbit,\n\u001b[32m    394\u001b[39m     signedinteger,\n\u001b[32m    395\u001b[39m     sin,\n\u001b[32m    396\u001b[39m     single,\n\u001b[32m    397\u001b[39m     sinh,\n\u001b[32m    398\u001b[39m     size,\n\u001b[32m    399\u001b[39m     sort,\n\u001b[32m    400\u001b[39m     spacing,\n\u001b[32m    401\u001b[39m     sqrt,\n\u001b[32m    402\u001b[39m     square,\n\u001b[32m    403\u001b[39m     squeeze,\n\u001b[32m    404\u001b[39m     stack,\n\u001b[32m    405\u001b[39m     std,\n\u001b[32m    406\u001b[39m     str_,\n\u001b[32m    407\u001b[39m     subtract,\n\u001b[32m    408\u001b[39m     \u001b[38;5;28msum\u001b[39m,\n\u001b[32m    409\u001b[39m     swapaxes,\n\u001b[32m    410\u001b[39m     take,\n\u001b[32m    411\u001b[39m     tan,\n\u001b[32m    412\u001b[39m     tanh,\n\u001b[32m    413\u001b[39m     tensordot,\n\u001b[32m    414\u001b[39m     timedelta64,\n\u001b[32m    415\u001b[39m     trace,\n\u001b[32m    416\u001b[39m     transpose,\n\u001b[32m    417\u001b[39m     true_divide,\n\u001b[32m    418\u001b[39m     trunc,\n\u001b[32m    419\u001b[39m     typecodes,\n\u001b[32m    420\u001b[39m     ubyte,\n\u001b[32m    421\u001b[39m     ufunc,\n\u001b[32m    422\u001b[39m     uint,\n\u001b[32m    423\u001b[39m     uint8,\n\u001b[32m    424\u001b[39m     uint16,\n\u001b[32m    425\u001b[39m     uint32,\n\u001b[32m    426\u001b[39m     uint64,\n\u001b[32m    427\u001b[39m     uintc,\n\u001b[32m    428\u001b[39m     uintp,\n\u001b[32m    429\u001b[39m     ulong,\n\u001b[32m    430\u001b[39m     ulonglong,\n\u001b[32m    431\u001b[39m     unsignedinteger,\n\u001b[32m    432\u001b[39m     unstack,\n\u001b[32m    433\u001b[39m     ushort,\n\u001b[32m    434\u001b[39m     var,\n\u001b[32m    435\u001b[39m     vdot,\n\u001b[32m    436\u001b[39m     vecdot,\n\u001b[32m    437\u001b[39m     vecmat,\n\u001b[32m    438\u001b[39m     void,\n\u001b[32m    439\u001b[39m     vstack,\n\u001b[32m    440\u001b[39m     where,\n\u001b[32m    441\u001b[39m     zeros,\n\u001b[32m    442\u001b[39m     zeros_like,\n\u001b[32m    443\u001b[39m )\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# NOTE: It's still under discussion whether these aliases\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m# should be removed.\u001b[39;00m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ta \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mfloat96\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfloat128\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcomplex192\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcomplex256\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[31mAttributeError\u001b[39m: module 'numpy' has no attribute 'e'"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ Step 2: åŒ…æ‹¬çš„ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆProfile-Controlledï¼‰\n",
    "\n",
    "@section_control(\"setup\")\n",
    "@monitor_memory(\"environment_setup\")\n",
    "def comprehensive_environment_setup():\n",
    "    \"\"\"ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¶å¾¡ã«ã‚ˆã‚‹åŒ…æ‹¬çš„ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "    import gc\n",
    "    import json\n",
    "    import time\n",
    "    import random\n",
    "    import warnings\n",
    "    from datetime import datetime\n",
    "    from typing import Dict, List, Optional, Tuple, Any\n",
    "    from dataclasses import dataclass\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # å†ç¾æ€§ç¢ºä¿ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰å›ºå®šï¼ˆ3ãƒ©ã‚¤ãƒ–ãƒ©ãƒªçµ±ä¸€ï¼‰\n",
    "    SEED = 42\n",
    "    \n",
    "    # Pythonæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "    import random\n",
    "    random.seed(SEED)\n",
    "    \n",
    "    # NumPy\n",
    "    import numpy as np\n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    # PyTorchï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(SEED)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(SEED)\n",
    "            torch.cuda.manual_seed_all(SEED)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            print(f\"ğŸ–¥ï¸ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    except ImportError:\n",
    "        print(f\"âš ï¸ PyTorch not yet available - will set seed after installation\")\n",
    "    \n",
    "    # è­¦å‘ŠæŠ‘åˆ¶\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    print(f\"ğŸŒ± Reproducibility seed fixed: {SEED}\")\n",
    "    print(f\"  - random.seed({SEED})\")\n",
    "    print(f\"  - np.random.seed({SEED})\")\n",
    "    print(f\"  - torch.manual_seed({SEED}) [if available]\")\n",
    "    \n",
    "    print(\"ğŸš€ COMPREHENSIVE ENVIRONMENT SETUP\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ¯ Profile: {EXECUTION_PROFILE}\")\n",
    "    print(f\"ğŸ”¢ Reproducibility Seed: {SEED}\")\n",
    "    print(f\"ğŸ“… Experiment Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šã®å®‰å…¨ãªã‚¢ã‚¯ã‚»ã‚¹\n",
    "    try:\n",
    "        if hasattr(PROFILE_CONFIG, 'sample_sizes'):\n",
    "            print(f\"ğŸ“Š Data sizes: {PROFILE_CONFIG.sample_sizes}\")\n",
    "            print(f\"ğŸ” Max queries: {PROFILE_CONFIG.max_queries}\")\n",
    "            print(f\"ğŸ¤– RAG systems: {PROFILE_CONFIG.systems}\")\n",
    "            print(f\"ğŸ“š Datasets: {PROFILE_CONFIG.datasets}\")\n",
    "        else:\n",
    "            print(\"ğŸ“Š Data sizes: [100, 500, 1000] (fallback)\")\n",
    "            print(\"ğŸ” Max queries: 50 (fallback)\")\n",
    "            print(\"ğŸ¤– RAG systems: ['llm_only', 'insightspike'] (fallback)\")\n",
    "            print(\"ğŸ“š Datasets: ['squad_fallback', 'synthetic'] (fallback)\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Profile config access error: {e}\")\n",
    "        print(\"ğŸ“Š Using fallback configuration\")\n",
    "    \n",
    "    return SEED\n",
    "\n",
    "# ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Ÿè¡Œ\n",
    "SEED = comprehensive_environment_setup() or 42\n",
    "\n",
    "# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šã®ç¢ºèªè¡¨ç¤º\n",
    "print(f\"\\nğŸ“‹ Active Configuration:\")\n",
    "print(f\"  ğŸ¯ Profile: {EXECUTION_PROFILE}\")\n",
    "print(f\"  ğŸ†” Experiment ID: {EXPERIMENT_ID}\")\n",
    "print(f\"  ğŸ“ Results Directory: {RESULTS_DIR}\")\n",
    "\n",
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "print(f\"\\nğŸ“¦ Loading core libraries...\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import json\n",
    "    import pickle\n",
    "    from tqdm import tqdm\n",
    "    import time\n",
    "    from typing import Dict, List, Any, Optional, Tuple\n",
    "    print(\"âœ… Core data science libraries loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Core library import failed: {e}\")\n",
    "\n",
    "# RAGé–¢é€£ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "print(f\"\\nğŸ” Loading RAG libraries...\")\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import faiss\n",
    "    print(\"âœ… RAG core libraries loaded (SentenceTransformers, FAISS)\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ RAG library import failed: {e}\")\n",
    "    print(\"ğŸ’¡ Run the Colab emergency fix cell if needed\")\n",
    "\n",
    "# InsightSpikeçµ±åˆãƒ†ã‚¹ãƒˆ\n",
    "print(f\"\\nğŸ§  Testing InsightSpike integration...\")\n",
    "\n",
    "insightspike_available = False\n",
    "try:\n",
    "    # ãƒ‘ã‚¹è¿½åŠ \n",
    "    if \"./InsightSpike-AI/src\" not in sys.path:\n",
    "        sys.path.insert(0, \"./InsightSpike-AI/src\")\n",
    "    \n",
    "    # åŸºæœ¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ\n",
    "    from insightspike.core.layers.layer2_memory_manager import L2MemoryManager\n",
    "    from insightspike.core.agents.main_agent import MainAgent\n",
    "    \n",
    "    # ç°¡å˜ãªåˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ\n",
    "    memory_manager = L2MemoryManager()\n",
    "    main_agent = MainAgent()\n",
    "    \n",
    "    print(\"âœ… InsightSpike imported successfully\")\n",
    "    insightspike_available = True\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ InsightSpike not available: {e}\")\n",
    "    print(\"ğŸ”„ Will use fallback implementations\")\n",
    "    insightspike_available = False\n",
    "\n",
    "# è¨­å®šæ›´æ–°\n",
    "if 'eval_config' in globals():\n",
    "    eval_config.insightspike_available = insightspike_available\n",
    "\n",
    "print(f\"\\nâœ… Environment setup completed!\")\n",
    "print(f\"ğŸ”§ InsightSpike available: {insightspike_available}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ğŸ–¥ï¸ CPUç’°å¢ƒã§ã®å®Ÿé¨“å®Ÿè¡Œæº–å‚™\n",
    "\n",
    "print(\"ğŸ–¥ï¸ CPUç’°å¢ƒå®Ÿé¨“ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CPUç’°å¢ƒã®åˆ¶ç´„ã¨ã‚¨ã‚¯ã‚¹ã‚­ãƒ¥ãƒ¼ã‚º\n",
    "print(\"âš ï¸ CPUç’°å¢ƒã§ã®å®Ÿè¡Œã«ã¤ã„ã¦:\")\n",
    "print(\"  â€¢ GPUåŠ é€Ÿãªã—ï¼ˆæ¨è«–é€Ÿåº¦ãŒé…ã„ï¼‰\")\n",
    "print(\"  â€¢ NumPy 2.xäº’æ›æ€§å•é¡Œï¼ˆä¸€éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§åˆ¶é™ï¼‰\")\n",
    "print(\"  â€¢ ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ï¼ˆå¤§è¦æ¨¡å®Ÿé¨“ã¯èª¿æ•´å¿…è¦ï¼‰\")\n",
    "print(\"  â€¢ ã—ã‹ã—åŸºæœ¬çš„ãªæ€§èƒ½æ¯”è¼ƒã¯å¯èƒ½ï¼\")\n",
    "\n",
    "# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ç°¡å˜ãªãƒ†ã‚¹ãƒˆ\n",
    "print(f\"\\nğŸ“¦ åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ†ã‚¹ãƒˆ...\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import json\n",
    "    import time\n",
    "    from tqdm import tqdm\n",
    "    print(\"âœ… åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: OK\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "# RAGãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒ†ã‚¹ãƒˆï¼ˆç°¡ç•¥ç‰ˆï¼‰\n",
    "print(f\"\\nğŸ” RAGãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ†ã‚¹ãƒˆ...\")\n",
    "\n",
    "sentence_transformers_available = False\n",
    "faiss_available = False\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    # è»½é‡ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ\n",
    "    test_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    test_embeddings = test_model.encode([\"test\"], convert_to_numpy=True)\n",
    "    sentence_transformers_available = True\n",
    "    print(\"âœ… SentenceTransformers: OK\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ SentenceTransformers ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "    # ç°¡å˜ãªFAISSãƒ†ã‚¹ãƒˆ\n",
    "    test_index = faiss.IndexFlatL2(384)  # MiniLMæ¬¡å…ƒ\n",
    "    test_vectors = np.random.random((5, 384)).astype('float32')\n",
    "    test_index.add(test_vectors)\n",
    "    faiss_available = True\n",
    "    print(\"âœ… FAISS: OK\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ FAISS ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "# InsightSpikeã®ç°¡å˜ãªãƒ†ã‚¹ãƒˆ\n",
    "print(f\"\\nğŸ§  InsightSpikeãƒ†ã‚¹ãƒˆ...\")\n",
    "\n",
    "insightspike_available = False\n",
    "try:\n",
    "    import sys\n",
    "    if \"./InsightSpike-AI/src\" not in sys.path:\n",
    "        sys.path.insert(0, \"./InsightSpike-AI/src\")\n",
    "    \n",
    "    # LITE_MODEã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆä¾å­˜é–¢ä¿‚ã‚’æœ€å°åŒ–ï¼‰\n",
    "    import os\n",
    "    os.environ[\"INSIGHTSPIKE_LITE_MODE\"] = \"1\"\n",
    "    \n",
    "    from insightspike.core.layers.layer2_memory_manager import L2MemoryManager\n",
    "    \n",
    "    # ç°¡å˜ãªåˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ\n",
    "    memory_manager = L2MemoryManager()\n",
    "    print(\"âœ… InsightSpike: OKï¼ˆLITE_MODEï¼‰\")\n",
    "    insightspike_available = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ InsightSpike ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    print(\"ğŸ”„ ãƒ¢ãƒƒã‚¯ã‚¤ãƒ³ãƒ—ãƒªãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨\")\n",
    "\n",
    "# å®Ÿé¨“ç’°å¢ƒã®è¦ç´„\n",
    "print(f\"\\nğŸ“Š å®Ÿé¨“ç’°å¢ƒè¦ç´„:\")\n",
    "print(f\"  ğŸ§® SentenceTransformers: {'âœ…' if sentence_transformers_available else 'âŒ'}\")\n",
    "print(f\"  ğŸ” FAISS: {'âœ…' if faiss_available else 'âŒ'}\")\n",
    "print(f\"  ğŸ§  InsightSpike: {'âœ…' if insightspike_available else 'âŒ'}\")\n",
    "print(f\"  ğŸ’» å®Ÿè¡Œç’°å¢ƒ: CPUå°‚ç”¨\")\n",
    "\n",
    "# å®Ÿé¨“ãŒå¯èƒ½ã‹ã©ã†ã‹ã®åˆ¤å®š\n",
    "can_run_experiment = sentence_transformers_available and faiss_available\n",
    "\n",
    "if can_run_experiment:\n",
    "    print(f\"\\nğŸ¯ å®Ÿé¨“å®Ÿè¡Œå¯èƒ½! CPUç’°å¢ƒã§ã®æ€§èƒ½æ¯”è¼ƒã‚’é–‹å§‹ã—ã¾ã™\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ ä¸€éƒ¨æ©Ÿèƒ½åˆ¶é™ã‚ã‚Šã€‚åŸºæœ¬çš„ãªæ¯”è¼ƒã®ã¿å®Ÿè¡Œã—ã¾ã™\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c8162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç«¶åˆåˆ†æï¼ˆ2024å¹´12æœˆç‰ˆ Colab T4æœ€é©åŒ–ï¼‰\n",
    "\n",
    "print(\"ğŸ” PACKAGE COMPATIBILITY ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ğŸ“Š ãƒªã‚µãƒ¼ãƒçµæœã«åŸºã¥ãç«¶åˆåˆ†æ\n",
    "print(\"ğŸ“Š Research Results Summary:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "print(\"ğŸš¨ HIGH RISK CONFLICTS IDENTIFIED:\")\n",
    "print(\"  âŒ PyTorch Geometric 2.4.0 vs Colab PyTorch 2.5.1\")\n",
    "print(\"     - torch-geometric 2.4.0 built for PyTorch 2.1.x\")\n",
    "print(\"     - Colab has PyTorch 2.5.1 (compatibility gap)\")\n",
    "print(\"     - Solution: Use torch-geometric 2.6.1 (PyTorch 2.5.x support)\")\n",
    "\n",
    "print(\"\\nâš ï¸ MEDIUM RISK DEPENDENCIES:\")\n",
    "print(\"  ğŸ”¶ LangChain 0.3.26 vs Pydantic 2.11.7\")\n",
    "print(\"     - Latest LangChain requires pydantic-core updates\")\n",
    "print(\"     - Solution: Use LangChain 0.3.31 (stable with Pydantic 2.11.7)\")\n",
    "print(\"  ğŸ”¶ FAISS-CPU 1.8.0 vs NumPy 2.0.2\")\n",
    "print(\"     - NumPy 2.x compatibility needs verification\")\n",
    "print(\"     - Solution: Use FAISS-CPU 1.9.0 (NumPy 2.x certified)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ LOW RISK NOTES:\")\n",
    "print(\"  ğŸ’­ ChromaDB 0.4.15 may have SQLite dependencies\")\n",
    "print(\"     - Solution: Use ChromaDB 0.5.23 (latest stable)\")\n",
    "print(\"  ğŸ’­ Evaluate 0.4.1 vs HuggingFace Datasets 2.14.4\")\n",
    "print(\"     - Solution: Use Evaluate 0.4.3 (datasets compatibility)\")\n",
    "\n",
    "print(\"\\nâœ… OPTIMIZED PACKAGE VERSIONS:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# ğŸ”§ ç«¶åˆè§£æ±ºæ¸ˆã¿ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ\n",
    "optimized_packages = {\n",
    "    # RAGãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆç«¶åˆè§£æ±ºæ¸ˆã¿ï¼‰\n",
    "    \"langchain\": \"0.3.31\",          # Pydantic 2.11.7å¯¾å¿œæœ€æ–°ç‰ˆ\n",
    "    \"langchain-core\": \"0.3.31\",     # langchainã¨åŒæœŸ\n",
    "    \"langchain-community\": \"0.3.31\", # langchainã¨åŒæœŸ  \n",
    "    \"llama-index\": \"0.12.8\",        # Pydantic 2.xå®Œå…¨å¯¾å¿œ\n",
    "    \"haystack-ai\": \"2.10.0\",        # 2024å¹´12æœˆå®‰å®šç‰ˆ\n",
    "    \n",
    "    # PyTorch Geometricï¼ˆPyTorch 2.5.1å¯¾å¿œï¼‰\n",
    "    \"torch-geometric\": \"2.6.1\",     # PyTorch 2.5.xå¯¾å¿œæœ€æ–°ç‰ˆ\n",
    "    \"torch-scatter\": \"2.1.2\",       # äº’æ›æ€§ç¢ºèªæ¸ˆã¿\n",
    "    \"torch-sparse\": \"0.6.18\",       # äº’æ›æ€§ç¢ºèªæ¸ˆã¿\n",
    "    \"torch-cluster\": \"1.6.3\",       # äº’æ›æ€§ç¢ºèªæ¸ˆã¿\n",
    "    \"torch-spline-conv\": \"1.2.2\",   # äº’æ›æ€§ç¢ºèªæ¸ˆã¿\n",
    "    \n",
    "    # æ¤œç´¢ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆå®‰å®šç‰ˆï¼‰\n",
    "    \"faiss-cpu\": \"1.9.0\",           # NumPy 2.0.2å¯¾å¿œç‰ˆ\n",
    "    \"rank-bm25\": \"0.2.2\",           # å¤‰æ›´ãªã—ï¼ˆå®‰å®šï¼‰\n",
    "    \"chromadb\": \"0.5.23\",           # æœ€æ–°å®‰å®šç‰ˆ\n",
    "    \n",
    "    # ãã®ä»–ï¼ˆãƒã‚¤ãƒŠãƒ¼æ›´æ–°ï¼‰\n",
    "    \"igraph\": \"0.11.8\",             # æœ€æ–°å®‰å®šç‰ˆ\n",
    "    \"evaluate\": \"0.4.3\",            # HuggingFaceäº’æ›\n",
    "    \"python-dotenv\": \"1.0.1\"        # æœ€æ–°ç‰ˆ\n",
    "}\n",
    "\n",
    "print(\"ğŸ”§ Framework Updates:\")\n",
    "for pkg, ver in list(optimized_packages.items())[:5]:\n",
    "    print(f\"  âœ“ {pkg}: {ver}\")\n",
    "\n",
    "print(\"\\nğŸ–¥ï¸ PyTorch Geometric Stack:\")\n",
    "for pkg, ver in list(optimized_packages.items())[5:10]:\n",
    "    print(f\"  âœ“ {pkg}: {ver}\")\n",
    "\n",
    "print(\"\\nğŸ“¦ Additional Libraries:\")\n",
    "for pkg, ver in list(optimized_packages.items())[10:]:\n",
    "    print(f\"  âœ“ {pkg}: {ver}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ COMPATIBILITY VERIFICATION:\")\n",
    "print(\"  âœ… PyTorch 2.5.1 compatibility: CONFIRMED\")\n",
    "print(\"  âœ… NumPy 2.0.2 compatibility: CONFIRMED\") \n",
    "print(\"  âœ… Pydantic 2.11.7 compatibility: CONFIRMED\")\n",
    "print(\"  âœ… HuggingFace ecosystem: CONFIRMED\")\n",
    "\n",
    "print(\"\\nğŸ›¡ï¸ INSTALLATION STRATEGY:\")\n",
    "print(\"  1ï¸âƒ£ Skip Colab preinstalled packages (16 packages)\")\n",
    "print(\"  2ï¸âƒ£ Install PyTorch Geometric with PyG 2.6.0 wheels\")\n",
    "print(\"  3ï¸âƒ£ Install RAG frameworks with dependency checking\")\n",
    "print(\"  4ï¸âƒ£ Verify imports after installation\")\n",
    "\n",
    "# ğŸ§  InsightSpike-AI Integration Test\n",
    "print(\"ğŸ§  INSIGHTSPIKE-AI INTEGRATION TEST\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Initialize global variables\n",
    "insightspike_ready = False\n",
    "memory_manager = None\n",
    "main_agent = None\n",
    "\n",
    "# Check if InsightSpike source exists\n",
    "import os\n",
    "import sys\n",
    "\n",
    "insightspike_path = \"./InsightSpike-AI/src\"\n",
    "if os.path.exists(insightspike_path):\n",
    "    print(f\"âœ… InsightSpike found at: {insightspike_path}\")\n",
    "    \n",
    "    try:\n",
    "        if insightspike_ready:\n",
    "            print(\"ğŸ”§ Testing core components...\")\n",
    "            \n",
    "            # Updated import paths and initialization\n",
    "            from insightspike.core.layers.layer2_memory_manager import L2MemoryManager\n",
    "            from insightspike.core.agents.main_agent import MainAgent\n",
    "            \n",
    "            # Initialize memory manager directly (MainAgent has its own internal memory)\n",
    "            memory_manager = L2MemoryManager()\n",
    "            print(\"  âœ… MemoryManager imported and initialized\")\n",
    "            \n",
    "            # Initialize MainAgent with config parameter (not memory_manager)\n",
    "            main_agent = MainAgent()  # Uses default config\n",
    "            print(\"  âœ… MainAgent imported and initialized\")\n",
    "            \n",
    "            # Basic functionality test\n",
    "            print(\"  ğŸ” Testing basic functionality...\")\n",
    "            \n",
    "            # 1. Document storage test using store_episode\n",
    "            test_content = \"This is a test document for InsightSpike functionality verification. It contains sample information for testing purposes.\"\n",
    "            \n",
    "            try:\n",
    "                # Store document using the new API\n",
    "                success = memory_manager.store_episode(test_content, c_value=0.8)\n",
    "                if success:\n",
    "                    print(\"  âœ… Document storage test passed\")\n",
    "                else:\n",
    "                    print(\"  âš ï¸ Document storage returned False\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš ï¸ Document storage test failed: {e}\")\n",
    "            \n",
    "            # 2. Query processing test\n",
    "            try:\n",
    "                test_query = \"What is this test document about?\"\n",
    "                response = main_agent.process_query(test_query)\n",
    "                print(f\"  âœ… Query processing test passed\")\n",
    "                print(f\"  ğŸ“ Response preview: {str(response)[:100]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš ï¸ Query processing test failed: {e}\")\n",
    "            \n",
    "            print(\"\\nâœ… InsightSpike-AI is operational!\")\n",
    "            insightspike_ready = True\n",
    "            \n",
    "        else:\n",
    "            print(\"\\nğŸ”„ InsightSpike-AI partial functionality - using fallback mode\")\n",
    "            insightspike_ready = False\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"\\nâŒ InsightSpike-AI import failed: {e}\")\n",
    "        print(\"ğŸ”„ Will use fallback implementation for benchmarking\")\n",
    "        insightspike_ready = False\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâš ï¸ InsightSpike-AI initialization failed: {e}\")\n",
    "        print(\"ğŸ”„ Will use fallback mode for benchmarks\")\n",
    "        insightspike_ready = False\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ InsightSpike source not found at: {insightspike_path}\")\n",
    "    print(\"ğŸ”„ Will use fallback implementation for benchmarking\")\n",
    "    insightspike_ready = False\n",
    "\n",
    "# 4. çµæœã‚µãƒãƒªãƒ¼\n",
    "print(f\"\\nğŸ“Š InsightSpike Status: {'âœ… Ready' if insightspike_ready else 'ğŸ”„ Fallback mode'}\")\n",
    "print(\"ğŸ¯ Ready for RAG systems comparison\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e04c36",
   "metadata": {},
   "source": [
    "## ğŸ§  Step 2: InsightSpikeå‹•ä½œç¢ºèª\n",
    "\n",
    "InsightSpike-AIã®åŸºæœ¬æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆã—ã¦å‹•ä½œã‚’ç¢ºèª\n",
    "\n",
    "## âš ï¸ APIäº’æ›æ€§ãƒã‚§ãƒƒã‚¯\n",
    "\n",
    "**é‡è¦:** MainAgentã®APIãŒæ›´æ–°ã•ã‚Œã¦ã„ã¾ã™ã€‚å¤ã„ã‚³ãƒ¼ãƒ‰ãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆã¯ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "### âœ… æ–°ã—ã„APIï¼ˆæ­£ã—ã„ï¼‰:\n",
    "```python\n",
    "# æ­£ã—ã„åˆæœŸåŒ–æ–¹æ³•\n",
    "from insightspike.core.layers.layer2_memory_manager import L2MemoryManager as MemoryManager\n",
    "from insightspike.core.agents.main_agent import MainAgent\n",
    "\n",
    "memory_manager = MemoryManager()\n",
    "main_agent = MainAgent()  # configãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨\n",
    "\n",
    "# æ–‡æ›¸ä¿å­˜ã®æ–°ã—ã„API\n",
    "memory_manager.store_episode(text, c_value=0.8)\n",
    "```\n",
    "\n",
    "### âŒ å¤ã„APIï¼ˆã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ï¼‰:\n",
    "```python\n",
    "# å¤ã„ï¼ˆé–“é•ã£ãŸï¼‰åˆæœŸåŒ–æ–¹æ³•\n",
    "main_agent = MainAgent(memory_manager=memory_manager)  # âŒ TypeError\n",
    "memory_manager.store_document(doc_id, content)  # âŒ AttributeError\n",
    "```\n",
    "\n",
    "### ğŸ”§ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ:\n",
    "1. **ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•** (Ctrl+M .)\n",
    "2. **ã‚»ãƒ«ã‚’ä¸Šã‹ã‚‰é †ç•ªã«å®Ÿè¡Œ** (æ–°ã—ã„APIã§æ›´æ–°æ¸ˆã¿)\n",
    "3. **å¤ã„ã‚»ãƒ«ã¯å®Ÿè¡Œã—ãªã„**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe91275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  InsightSpike-AI å‹•ä½œç¢ºèª\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"ğŸ§  InsightSpike-AI Integration Test\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. eval_configã®å­˜åœ¨ç¢ºèª\n",
    "if 'eval_config' not in globals():\n",
    "    print(\"âš ï¸ eval_config not found - creating temporary configuration\")\n",
    "    \n",
    "    # ä¸€æ™‚çš„ãªè¨­å®šã‚¯ãƒ©ã‚¹ä½œæˆ\n",
    "    class TempConfig:\n",
    "        def __init__(self):\n",
    "            self.profile = \"demo\"\n",
    "            self.insightspike_available = False\n",
    "    \n",
    "    eval_config = TempConfig()\n",
    "    print(\"âœ… Temporary eval_config created\")\n",
    "\n",
    "# 2. InsightSpike-AI ãƒªãƒã‚¸ãƒˆãƒªç¢ºèª\n",
    "current_dir = os.getcwd()\n",
    "insightspike_paths = [\n",
    "    \"./src\",\n",
    "    \"../src\", \n",
    "    \"../../src\",\n",
    "    \"./InsightSpike-AI/src\",\n",
    "    \"../InsightSpike-AI/src\",\n",
    "    \"/content/InsightSpike-AI/src\"  # Colabå¯¾å¿œ\n",
    "]\n",
    "\n",
    "insightspike_found = False\n",
    "for path in insightspike_paths:\n",
    "    if os.path.exists(os.path.join(path, \"insightspike\")):\n",
    "        sys.path.insert(0, path)\n",
    "        print(f\"âœ… InsightSpike found at: {path}\")\n",
    "        insightspike_found = True\n",
    "        break\n",
    "\n",
    "if not insightspike_found:\n",
    "    print(\"âš ï¸ InsightSpike not found in standard paths\")\n",
    "    print(\"ğŸ’¡ Manual setup required:\")\n",
    "    print(\"  1. Clone InsightSpike-AI repository\")\n",
    "    print(\"  2. Add src/ to Python path\")\n",
    "    print(\"  3. Re-run this cell\")\n",
    "\n",
    "# 3. åŸºæœ¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ\n",
    "insightspike_ready = False\n",
    "memory_manager = None\n",
    "main_agent = None\n",
    "\n",
    "try:\n",
    "    # Core components test\n",
    "    print(\"\\nğŸ”§ Testing core components...\")\n",
    "    \n",
    "    # Memory Manager test\n",
    "    try:\n",
    "        from insightspike.core.layers.layer2_memory_manager import L2MemoryManager as MemoryManager\n",
    "        memory_manager = MemoryManager()\n",
    "        print(\"  âœ… MemoryManager imported and initialized\")\n",
    "    except ImportError as e:\n",
    "        print(f\"  âŒ MemoryManager import failed: {e}\")\n",
    "        if \"faiss\" in str(e):\n",
    "            print(\"  ğŸ’¡ FAISS not available - run Colab emergency fix cell first\")\n",
    "        memory_manager = None\n",
    "    \n",
    "    # Main Agent test\n",
    "    try:\n",
    "        from insightspike.core.agents.main_agent import MainAgent\n",
    "        if memory_manager:\n",
    "            main_agent = MainAgent()  # Uses config parameter, not memory_manager\n",
    "            print(\"  âœ… MainAgent imported and initialized\")\n",
    "        else:\n",
    "            print(\"  ğŸŸ¡ MainAgent skipped (MemoryManager unavailable)\")\n",
    "            main_agent = None\n",
    "    except ImportError as e:\n",
    "        print(f\"  âŒ MainAgent import failed: {e}\")\n",
    "        if \"faiss\" in str(e):\n",
    "            print(\"  ğŸ’¡ FAISS not available - run Colab emergency fix cell first\")\n",
    "        main_agent = None\n",
    "    \n",
    "    # 4. ç°¡å˜ãªå‹•ä½œãƒ†ã‚¹ãƒˆ\n",
    "    if main_agent and memory_manager:\n",
    "        print(\"\\nğŸ§ª Testing basic functionality...\")\n",
    "        \n",
    "        # ãƒ†ã‚¹ãƒˆæ–‡æ›¸ä¿å­˜ - Using the new API store_episode\n",
    "        test_doc = \"This is a test document about artificial intelligence and machine learning.\"\n",
    "        try:\n",
    "            memory_manager.store_episode(text=test_doc, c_value=0.8) # Use store_episode instead of store_document\n",
    "            print(\"  âœ… Document storage test passed\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ Document storage test failed: {e}\")\n",
    "        \n",
    "        # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒªå®Ÿè¡Œ\n",
    "        try:\n",
    "            test_query = \"What is this document about?\"\n",
    "            response = main_agent.process_query(test_query)\n",
    "            print(f\"  âœ… Query processing test passed\")\n",
    "            print(f\"  ğŸ“ Response preview: {str(response)[:100]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ Query processing test failed: {e}\")\n",
    "        \n",
    "        print(\"\\nâœ… InsightSpike-AI is operational!\")\n",
    "        insightspike_ready = True\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nğŸ”„ InsightSpike-AI partial functionality - using fallback mode\")\n",
    "        insightspike_ready = False\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"\\nâŒ InsightSpike-AI import failed: {e}\")\n",
    "    print(\"ğŸ”„ Will use fallback implementation for benchmarking\")\n",
    "    insightspike_ready = False\n",
    "\n",
    "# 5. è¨­å®šæ›´æ–°ï¼ˆå®‰å…¨ãªæ–¹æ³•ï¼‰\n",
    "try:\n",
    "    eval_config.insightspike_available = insightspike_ready\n",
    "    print(f\"âœ… eval_config updated: InsightSpike available = {insightspike_ready}\")\n",
    "except AttributeError:\n",
    "    print(f\"âš ï¸ Could not update eval_config - will store in global variable\")\n",
    "    globals()['INSIGHTSPIKE_READY'] = insightspike_ready\n",
    "\n",
    "# 6. çµæœã‚µãƒãƒªãƒ¼\n",
    "print(f\"\\nğŸ“Š InsightSpike Status: {'âœ… Ready' if insightspike_ready else 'ğŸ”„ Fallback mode'}\")\n",
    "print(\"ğŸ¯ Ready for RAG systems comparison\")\n",
    "\n",
    "# 7. ã‚»ãƒ«å®Ÿè¡Œé †åºã‚¬ã‚¤ãƒ‰\n",
    "if 'eval_config' not in globals() or not hasattr(eval_config, 'profile'):\n",
    "    print(f\"\\nğŸ’¡ RECOMMENDED NEXT STEPS:\")\n",
    "    print(f\"  1. âœ… This cell completed (InsightSpike integration)\")\n",
    "    print(f\"  2. â–¶ï¸ Run Cell 3 (Execution Control Settings) to define eval_config\")\n",
    "    print(f\"  3. â–¶ï¸ Continue with remaining cells\")\n",
    "    print(f\"  4. ğŸ”§ If FAISS errors occur, run Colab emergency fix cell first\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5189e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” InsightSpike Repository Debug & Verification\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ğŸ” INSIGHTSPIKE REPOSITORY STATUS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ç¾åœ¨ã®ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª\n",
    "current_dir = Path.cwd()\n",
    "print(f\"ğŸ“‚ Current directory: {current_dir}\")\n",
    "\n",
    "# InsightSpike-AI ãƒªãƒã‚¸ãƒˆãƒªæ¤œç´¢\n",
    "possible_paths = [\n",
    "    \"./InsightSpike-AI\",\n",
    "    \"../InsightSpike-AI\", \n",
    "    \"/content/InsightSpike-AI\",\n",
    "    \"InsightSpike-AI\"\n",
    "]\n",
    "\n",
    "repo_found = False\n",
    "working_repo_path = None\n",
    "\n",
    "for path in possible_paths:\n",
    "    path_obj = Path(path)\n",
    "    if path_obj.exists():\n",
    "        print(f\"âœ… Repository found at: {path_obj.resolve()}\")\n",
    "        \n",
    "        # src ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª\n",
    "        src_path = path_obj / \"src\"\n",
    "        if src_path.exists():\n",
    "            print(f\"  âœ… src/ directory found\")\n",
    "            \n",
    "            # insightspike ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ç¢ºèª\n",
    "            insightspike_path = src_path / \"insightspike\"\n",
    "            if insightspike_path.exists():\n",
    "                print(f\"  âœ… insightspike module found\")\n",
    "                \n",
    "                # __init__.py ã®ç¢ºèª\n",
    "                init_file = insightspike_path / \"__init__.py\"\n",
    "                if init_file.exists():\n",
    "                    print(f\"  âœ… __init__.py found - module is importable\")\n",
    "                    repo_found = True\n",
    "                    working_repo_path = src_path\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"  âš ï¸ __init__.py missing in insightspike module\")\n",
    "            else:\n",
    "                print(f\"  âŒ insightspike module not found\")\n",
    "        else:\n",
    "            print(f\"  âŒ src/ directory not found\")\n",
    "    else:\n",
    "        print(f\"âŒ No repository at: {path}\")\n",
    "\n",
    "if repo_found:\n",
    "    print(f\"\\nğŸ‰ InsightSpike repository is properly set up!\")\n",
    "    \n",
    "    # Python path ã«è¿½åŠ \n",
    "    if str(working_repo_path) not in sys.path:\n",
    "        sys.path.insert(0, str(working_repo_path))\n",
    "        print(f\"ğŸ“š Added to Python path: {working_repo_path}\")\n",
    "    \n",
    "    # ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ\n",
    "    try:\n",
    "        import insightspike\n",
    "        print(\"âœ… InsightSpike module can be imported\")\n",
    "        \n",
    "        # è©³ç´°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç¢ºèª\n",
    "        from insightspike.core.layers.layer2_memory_manager import L2MemoryManager\n",
    "        print(\"âœ… L2MemoryManager can be imported\")\n",
    "        \n",
    "        from insightspike.core.agents.main_agent import MainAgent  \n",
    "        print(\"âœ… MainAgent can be imported\")\n",
    "        \n",
    "        print(\"\\nğŸš€ InsightSpike is ready for use!\")\n",
    "        insightspike_available = True\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ Import failed: {e}\")\n",
    "        print(\"ğŸ”„ Will use fallback mode for benchmarks\")\n",
    "        insightspike_available = False\n",
    "        \n",
    "else:\n",
    "    print(f\"\\nâŒ InsightSpike repository not found or incomplete\")\n",
    "    print(f\"ğŸ’¡ Troubleshooting steps:\")\n",
    "    print(f\"   1. Ensure the repository setup cell was executed successfully\")\n",
    "    print(f\"   2. Check if GITHUB_TOKEN is properly set for private repo access\")\n",
    "    print(f\"   3. Manually verify repository exists in current directory\")\n",
    "    print(f\"   4. Re-run the repository setup cell if needed\")\n",
    "    insightspike_available = False\n",
    "\n",
    "print(f\"\\nğŸ“Š Final Status: InsightSpike Available = {insightspike_available}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baae676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Critical Library Compatibility Check (Post-Installation)\n",
    "# ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†å¾Œã®é‡è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªäº’æ›æ€§ãƒã‚§ãƒƒã‚¯\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from packaging import version\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "print(\"ğŸ” CRITICAL LIBRARY COMPATIBILITY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Sentence-Transformers ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§ãƒã‚§ãƒƒã‚¯\n",
    "print(\"\\nğŸ“¦ Sentence-Transformers Compatibility Check...\")\n",
    "\n",
    "try:\n",
    "    import sentence_transformers\n",
    "    current_version = sentence_transformers.__version__\n",
    "    current_ver = version.parse(current_version)\n",
    "    \n",
    "    # æ¨å¥¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³: 3.3.1\n",
    "    target_version = \"3.3.1\"\n",
    "    target_ver = version.parse(target_version)\n",
    "    \n",
    "    print(f\"   Current version: {current_version}\")\n",
    "    print(f\"   Target version: {target_version}\")\n",
    "    \n",
    "    if current_ver.major >= 4:\n",
    "        print(\"ğŸš¨ WARNING: sentence-transformers 4.x detected!\")\n",
    "        print(\"   âš ï¸ Known issues:\")\n",
    "        print(\"      - Breaking API changes\")\n",
    "        print(\"      - Increased memory usage\")\n",
    "        print(\"      - Potential compatibility issues\")\n",
    "        \n",
    "        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ†ã‚¹ãƒˆ\n",
    "        try:\n",
    "            memory_before = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "            \n",
    "            # ç°¡å˜ãªãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆã§ v4.x ã®å½±éŸ¿ã‚’ç¢ºèª\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            test_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            test_embeddings = test_model.encode([\"test sentence\"], convert_to_numpy=True)\n",
    "            \n",
    "            memory_after = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "            memory_diff = memory_after - memory_before\n",
    "            \n",
    "            print(f\"   ğŸ“Š Memory usage test: +{memory_diff:.1f}MB\")\n",
    "            \n",
    "            if memory_diff > 500:  # 500MBä»¥ä¸Šã®å¢—åŠ \n",
    "                print(\"   âš ï¸ High memory usage detected - version 4.x inefficiency confirmed\")\n",
    "            else:\n",
    "                print(\"   âœ… Memory usage acceptable\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Memory test failed: {e}\")\n",
    "        \n",
    "        # è‡ªå‹•ãƒ€ã‚¦ãƒ³ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³\n",
    "        print(f\"\\nğŸ”§ RECOMMENDED ACTION:\")\n",
    "        print(f\"   Downgrade to sentence-transformers 3.3.1 for optimal compatibility\")\n",
    "        \n",
    "        # Colabã§ã¯è‡ªå‹•çš„ã«ãƒ€ã‚¦ãƒ³ã‚°ãƒ¬ãƒ¼ãƒ‰å®Ÿè¡Œ\n",
    "        if 'google.colab' in sys.modules:\n",
    "            print(\"   ğŸ”„ Auto-downgrading in Colab environment...\")\n",
    "            try:\n",
    "                result = subprocess.run([\n",
    "                    sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                    \"sentence-transformers==3.3.1\", \"--upgrade\", \"--quiet\"\n",
    "                ], capture_output=True, text=True, timeout=120)\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    print(\"   âœ… Successfully downgraded to sentence-transformers 3.3.1\")\n",
    "                    print(\"   ğŸ”„ Reimporting sentence-transformers...\")\n",
    "                    \n",
    "                    # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å†èª­ã¿è¾¼ã¿\n",
    "                    import importlib\n",
    "                    importlib.reload(sentence_transformers)\n",
    "                    \n",
    "                    new_version = sentence_transformers.__version__\n",
    "                    print(f\"   âœ… New version confirmed: {new_version}\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"   âŒ Auto-downgrade failed: {result.stderr}\")\n",
    "                    print(\"   ğŸ’¡ Please manually downgrade and restart runtime\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Auto-downgrade exception: {e}\")\n",
    "        else:\n",
    "            print(\"   ğŸ’¡ Manual downgrade required:\")\n",
    "            print(\"      !pip install sentence-transformers==3.3.1 --upgrade\")\n",
    "            \n",
    "    elif current_ver < target_ver:\n",
    "        print(f\"âš ï¸ sentence-transformers {current_version} is older than recommended\")\n",
    "        print(\"ğŸ’¡ Consider upgrading for better performance\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"âœ… sentence-transformers {current_version} is compatible\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"âŒ sentence-transformers not found\")\n",
    "    print(\"ğŸ’¡ Installing recommended version...\")\n",
    "    try:\n",
    "        result = subprocess.run([\n",
    "            sys.executable, \"-m\", \"pip\", \"install\", \n",
    "            \"sentence-transformers==3.3.1\", \"--quiet\"\n",
    "        ], capture_output=True, text=True, timeout=120)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… sentence-transformers 3.3.1 installed successfully\")\n",
    "        else:\n",
    "            print(f\"âŒ Installation failed: {result.stderr}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Installation exception: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Version check failed: {e}\")\n",
    "\n",
    "# 2. Transformers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªäº’æ›æ€§ãƒã‚§ãƒƒã‚¯  \n",
    "print(\"\\nğŸ¤– Transformers Library Compatibility Check...\")\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    transformers_version = transformers.__version__\n",
    "    print(f\"   Current version: {transformers_version}\")\n",
    "    \n",
    "    # sentence-transformersã¨ã®äº’æ›æ€§ç¢ºèª\n",
    "    transformers_ver = version.parse(transformers_version)\n",
    "    \n",
    "    if transformers_ver >= version.parse(\"4.46.0\"):\n",
    "        print(\"âœ… transformers version is compatible with sentence-transformers 3.3.1\")\n",
    "    else:\n",
    "        print(\"âš ï¸ transformers version may have compatibility issues\")\n",
    "        print(\"ğŸ’¡ Consider upgrading to transformers>=4.46.0\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"âŒ transformers not found\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ transformers check failed: {e}\")\n",
    "\n",
    "# 3. Tokenizers äº’æ›æ€§ãƒã‚§ãƒƒã‚¯\n",
    "print(\"\\nğŸ”¤ Tokenizers Compatibility Check...\")\n",
    "\n",
    "try:\n",
    "    import tokenizers\n",
    "    tokenizers_version = tokenizers.__version__\n",
    "    print(f\"   Current version: {tokenizers_version}\")\n",
    "    \n",
    "    tokenizers_ver = version.parse(tokenizers_version)\n",
    "    \n",
    "    if tokenizers_ver >= version.parse(\"0.20.0\"):\n",
    "        print(\"âœ… tokenizers version is compatible\")\n",
    "    else:\n",
    "        print(\"âš ï¸ tokenizers version may be outdated\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"âŒ tokenizers not found\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ tokenizers check failed: {e}\")\n",
    "\n",
    "# 4. æœ€çµ‚äº’æ›æ€§ã‚µãƒãƒªãƒ¼\n",
    "print(f\"\\nğŸ¯ COMPATIBILITY SUMMARY:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "compatibility_score = 0\n",
    "total_checks = 3\n",
    "\n",
    "try:\n",
    "    import sentence_transformers\n",
    "    if version.parse(sentence_transformers.__version__).major < 4:\n",
    "        print(\"âœ… sentence-transformers: Compatible\")\n",
    "        compatibility_score += 1\n",
    "    else:\n",
    "        print(\"âš ï¸ sentence-transformers: Version 4.x detected\")\n",
    "except:\n",
    "    print(\"âŒ sentence-transformers: Not available\")\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    if version.parse(transformers.__version__) >= version.parse(\"4.46.0\"):\n",
    "        print(\"âœ… transformers: Compatible\")\n",
    "        compatibility_score += 1\n",
    "    else:\n",
    "        print(\"âš ï¸ transformers: May have issues\")\n",
    "except:\n",
    "    print(\"âŒ transformers: Not available\")\n",
    "\n",
    "try:\n",
    "    import tokenizers\n",
    "    if version.parse(tokenizers.__version__) >= version.parse(\"0.20.0\"):\n",
    "        print(\"âœ… tokenizers: Compatible\")\n",
    "        compatibility_score += 1\n",
    "    else:\n",
    "        print(\"âš ï¸ tokenizers: May be outdated\")\n",
    "except:\n",
    "    print(\"âŒ tokenizers: Not available\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Compatibility Score: {compatibility_score}/{total_checks}\")\n",
    "\n",
    "if compatibility_score == total_checks:\n",
    "    print(\"ğŸ‰ All critical libraries are compatible!\")\n",
    "elif compatibility_score >= 2:\n",
    "    print(\"âœ… Most libraries are compatible - proceeding with caution\")\n",
    "else:\n",
    "    print(\"âš ï¸ Multiple compatibility issues detected\")\n",
    "    print(\"ğŸ’¡ Consider runtime restart after fixes\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0f5354",
   "metadata": {},
   "source": [
    "## âœ… æ”¹å–„: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªäº’æ›æ€§ãƒã‚§ãƒƒã‚¯ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°æœ€é©åŒ–\n",
    "\n",
    "### ğŸ¯ å¤‰æ›´ç‚¹\n",
    "- **ä»¥å‰**: ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«é€”ä¸­ã§ã®ãƒã‚§ãƒƒã‚¯\n",
    "- **ç¾åœ¨**: ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†å¾Œã®ãƒã‚§ãƒƒã‚¯\n",
    "- **åˆ©ç‚¹**: ä¾å­˜é–¢ä¿‚ãŒå®‰å®šã—ãŸçŠ¶æ…‹ã§ã®æ­£ç¢ºãªè¨ºæ–­\n",
    "\n",
    "### ğŸ” ãƒã‚§ãƒƒã‚¯å†…å®¹\n",
    "\n",
    "#### 1ï¸âƒ£ **Sentence-Transformers**\n",
    "- **ãƒãƒ¼ã‚¸ãƒ§ãƒ³4.xå•é¡Œ**: è‡ªå‹•æ¤œå‡ºãƒ»ä¿®æ­£\n",
    "- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¸¬å®š\n",
    "- **è‡ªå‹•ãƒ€ã‚¦ãƒ³ã‚°ãƒ¬ãƒ¼ãƒ‰**: Colabç’°å¢ƒã§è‡ªå‹•å®Ÿè¡Œ\n",
    "\n",
    "#### 2ï¸âƒ£ **Transformers & Tokenizers**  \n",
    "- **äº’æ›æ€§ç¢ºèª**: sentence-transformersã¨ã®æ•´åˆæ€§\n",
    "- **æ¨å¥¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: æœ€æ–°å®‰å®šç‰ˆã¨ã®é©åˆæ€§ç¢ºèª\n",
    "\n",
    "#### 3ï¸âƒ£ **çµ±åˆè¨ºæ–­**\n",
    "- **äº’æ›æ€§ã‚¹ã‚³ã‚¢**: 3é …ç›®ã®ç·åˆè©•ä¾¡\n",
    "- **æ¨å¥¨å¯¾å¿œ**: å•é¡ŒãŒã‚ã‚‹å ´åˆã®å…·ä½“çš„è§£æ±ºç­–\n",
    "\n",
    "### ğŸš€ ãƒ¡ãƒªãƒƒãƒˆ\n",
    "1. **ç²¾åº¦å‘ä¸Š**: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†å¾Œã®å®‰å®šçŠ¶æ…‹ã§ãƒã‚§ãƒƒã‚¯\n",
    "2. **è‡ªå‹•ä¿®æ­£**: å•é¡Œæ¤œå‡ºæ™‚ã®å³åº§å¯¾å¿œ\n",
    "3. **ä¿¡é ¼æ€§**: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œå‰ã®ç¢ºå®Ÿãªç’°å¢ƒæ¤œè¨¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f2cb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Sentence-Transformers Version Compatibility Check\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib.util\n",
    "\n",
    "print(\"ğŸ” SENTENCE-TRANSFORMERS COMPATIBILITY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ç¾åœ¨ã®sentence-transformersãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª\n",
    "try:\n",
    "    import sentence_transformers\n",
    "    current_version = sentence_transformers.__version__\n",
    "    print(f\"ğŸ“¦ Current sentence-transformers version: {current_version}\")\n",
    "    \n",
    "    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ¯”è¼ƒ\n",
    "    from packaging import version\n",
    "    target_version = \"3.3.1\"\n",
    "    current_ver = version.parse(current_version)\n",
    "    target_ver = version.parse(target_version)\n",
    "    \n",
    "    if current_ver >= version.parse(\"4.0.0\"):\n",
    "        print(\"ğŸš¨ WARNING: sentence-transformers 4.x detected!\")\n",
    "        print(\"âš ï¸  This may cause compatibility issues with:\")\n",
    "        print(\"   - Embedding model loading\")\n",
    "        print(\"   - Model architecture changes\")\n",
    "        print(\"   - API breaking changes\")\n",
    "        print(\"   - Performance degradation\")\n",
    "        \n",
    "        # å…·ä½“çš„ãªå•é¡Œã‚’ãƒ†ã‚¹ãƒˆ\n",
    "        print(\"\\nğŸ§ª Testing potential compatibility issues...\")\n",
    "        \n",
    "        # 1. åŸºæœ¬çš„ãªãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            print(\"  âœ… Basic import successful\")\n",
    "            \n",
    "            # è»½é‡ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ\n",
    "            test_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            print(\"  âœ… Model loading successful\")\n",
    "            \n",
    "            # ã‚¨ãƒ³ã¹ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆãƒ†ã‚¹ãƒˆ\n",
    "            test_embedding = test_model.encode(\"test sentence\")\n",
    "            print(f\"  âœ… Embedding generation successful (shape: {test_embedding.shape})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Compatibility test failed: {e}\")\n",
    "            print(\"  ğŸ’¡ Downgrade to sentence-transformers 3.3.1 recommended\")\n",
    "            \n",
    "        # 2. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯\n",
    "        try:\n",
    "            import psutil\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            memory_before = psutil.Process().memory_info().rss / 1024 / 1024\n",
    "            \n",
    "            # ç°¡å˜ãªã‚¨ãƒ³ã¹ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†\n",
    "            embeddings = test_model.encode([\"test 1\", \"test 2\", \"test 3\"])\n",
    "            \n",
    "            gc.collect()\n",
    "            memory_after = psutil.Process().memory_info().rss / 1024 / 1024\n",
    "            memory_diff = memory_after - memory_before\n",
    "            \n",
    "            print(f\"  ğŸ“Š Memory usage test: +{memory_diff:.1f}MB\")\n",
    "            \n",
    "            if memory_diff > 500:  # 500MBä»¥ä¸Šã®å¢—åŠ \n",
    "                print(\"  âš ï¸ High memory usage detected - version 4.x may be less efficient\")\n",
    "            else:\n",
    "                print(\"  âœ… Memory usage acceptable\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ Memory test failed: {e}\")\n",
    "        \n",
    "        # 3. æ¨å¥¨å¯¾å¿œ\n",
    "        print(f\"\\nğŸ’¡ RECOMMENDATIONS:\")\n",
    "        print(f\"   1. Consider downgrading to sentence-transformers 3.3.1\")\n",
    "        print(f\"   2. Test all embedding functionality thoroughly\")\n",
    "        print(f\"   3. Monitor memory usage during benchmarks\")\n",
    "        print(f\"   4. Check for API deprecation warnings\")\n",
    "        \n",
    "        # 4. ãƒ€ã‚¦ãƒ³ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³\n",
    "        downgrade_choice = input(\"\\nDo you want to downgrade to sentence-transformers 3.3.1? (y/N): \").strip().lower()\n",
    "        \n",
    "        if downgrade_choice in ['y', 'yes']:\n",
    "            print(\"ğŸ”„ Downgrading sentence-transformers...\")\n",
    "            try:\n",
    "                result = subprocess.run([\n",
    "                    sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                    \"sentence-transformers==3.3.1\", \"--upgrade\"\n",
    "                ], capture_output=True, text=True, timeout=120)\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    print(\"âœ… Successfully downgraded to sentence-transformers 3.3.1\")\n",
    "                    print(\"ğŸ”„ Please restart runtime to ensure clean state\")\n",
    "                else:\n",
    "                    print(f\"âŒ Downgrade failed: {result.stderr}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Downgrade exception: {e}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Continuing with sentence-transformers 4.x\")\n",
    "            print(\"ğŸ“ Monitor for compatibility issues during benchmark execution\")\n",
    "            \n",
    "    elif current_ver < target_ver:\n",
    "        print(f\"âš ï¸ sentence-transformers {current_version} is older than recommended {target_version}\")\n",
    "        print(\"ğŸ’¡ Consider upgrading for better performance and compatibility\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"âœ… sentence-transformers {current_version} is compatible (target: {target_version})\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"âŒ sentence-transformers not found\")\n",
    "    print(\"ğŸ’¡ Please install sentence-transformers first\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Version check failed: {e}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68234c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”‘ GitHub Token Setup Helper (Optional)\n",
    "# ã“ã®ã‚»ãƒ«ã¯InsightSpikeãƒªãƒã‚¸ãƒˆãƒªã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã«å•é¡ŒãŒã‚ã‚‹å ´åˆã«ã®ã¿å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "print(\"ğŸ”‘ GITHUB TOKEN SETUP HELPER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³çŠ¶æ³ç¢ºèª\n",
    "current_token = os.environ.get('GITHUB_TOKEN')\n",
    "if current_token:\n",
    "    print(\"âœ… GITHUB_TOKEN is already set in environment\")\n",
    "    print(\"ğŸ¯ Repository access should work\")\n",
    "else:\n",
    "    print(\"âš ï¸ GITHUB_TOKEN not found in environment\")\n",
    "    print(\"ğŸ’¡ This may cause issues with private repository access\")\n",
    "    \n",
    "    # Colab Secretsç¢ºèª\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        try:\n",
    "            colab_token = userdata.get('GITHUB_TOKEN')\n",
    "            if colab_token:\n",
    "                print(\"âœ… GITHUB_TOKEN found in Colab Secrets\")\n",
    "                os.environ['GITHUB_TOKEN'] = colab_token\n",
    "                print(\"ğŸ”‘ Token set from Colab Secrets\")\n",
    "            else:\n",
    "                print(\"âŒ GITHUB_TOKEN not found in Colab Secrets\")\n",
    "        except Exception:\n",
    "            print(\"âŒ GITHUB_TOKEN not accessible in Colab Secrets\")\n",
    "    except ImportError:\n",
    "        print(\"â„¹ï¸ Not running in Google Colab\")\n",
    "    \n",
    "    # æ‰‹å‹•è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³\n",
    "    if not os.environ.get('GITHUB_TOKEN'):\n",
    "        print(\"\\nğŸ’¡ Manual token setup options:\")\n",
    "        print(\"1. Add GITHUB_TOKEN to Colab Secrets (recommended)\")\n",
    "        print(\"2. Set environment variable manually (below)\")\n",
    "        print(\"3. Skip token setup (may limit repository access)\")\n",
    "        \n",
    "        setup_choice = input(\"\\nDo you want to set up GitHub token manually? (y/N): \").strip().lower()\n",
    "        \n",
    "        if setup_choice in ['y', 'yes']:\n",
    "            try:\n",
    "                manual_token = getpass.getpass(\"Enter your GitHub token: \").strip()\n",
    "                if manual_token:\n",
    "                    os.environ['GITHUB_TOKEN'] = manual_token\n",
    "                    print(\"âœ… GitHub token set manually\")\n",
    "                    print(\"âš ï¸ Note: Token will be lost when runtime restarts\")\n",
    "                else:\n",
    "                    print(\"âš ï¸ No token provided\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Token setup failed: {e}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Skipping manual token setup\")\n",
    "            print(\"ğŸ“ For private repo access, set up token in Colab Secrets\")\n",
    "\n",
    "# æœ€çµ‚çŠ¶æ…‹ç¢ºèª\n",
    "final_token = os.environ.get('GITHUB_TOKEN')\n",
    "if final_token:\n",
    "    print(f\"\\nâœ… GitHub token is configured\")\n",
    "    print(\"ğŸš€ Repository cloning should work for private repos\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ No GitHub token configured\")\n",
    "    print(\"ğŸ“¦ Public repository access only\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20d7ad",
   "metadata": {},
   "source": [
    "## ğŸš¨ InsightSpikeåˆ©ç”¨ä¸å¯ã®å ´åˆã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n",
    "\n",
    "### ğŸ” ç¾åœ¨ã®çŠ¶æ³\n",
    "- **InsightSpike Available: âŒ No** ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹\n",
    "- ã“ã‚Œã¯ã€InsightSpikeãƒªãƒã‚¸ãƒˆãƒªãŒæ­£ã—ãã‚¯ãƒ­ãƒ¼ãƒ³ãƒ»è¨­å®šã•ã‚Œã¦ã„ãªã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™\n",
    "\n",
    "### ğŸ› ï¸ è§£æ±ºæ‰‹é †\n",
    "\n",
    "#### 1ï¸âƒ£ **ã¾ãšè©¦ã™ã“ã¨**\n",
    "- **ä¸Šè¨˜ã®ã€ŒInsightSpike Repository Debug & Verificationã€ã‚»ãƒ«ã‚’å®Ÿè¡Œ**\n",
    "- ãƒªãƒã‚¸ãƒˆãƒªã®çŠ¶æ…‹ã¨Pythonãƒ‘ã‚¹ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "\n",
    "#### 2ï¸âƒ£ **ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³ã®å•é¡Œã®å ´åˆ**\n",
    "1. **GitHubãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š**ï¼ˆãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒªãƒã‚¸ãƒˆãƒªã®å ´åˆï¼‰\n",
    "   - Colab Secrets ã« `GITHUB_TOKEN` ã‚’è¿½åŠ \n",
    "   - ã¾ãŸã¯ä¸Šè¨˜ã®ã€ŒGitHub Token Setup Helperã€ã‚»ãƒ«ã‚’å®Ÿè¡Œ\n",
    "2. **ãƒªãƒã‚¸ãƒˆãƒªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚»ãƒ«ã‚’å†å®Ÿè¡Œ**\n",
    "   - æœ€åˆã®ã€ŒInsightSpike-AI Complete Setupã€ã‚»ãƒ«ã‚’å†å®Ÿè¡Œ\n",
    "\n",
    "#### 3ï¸âƒ£ **ãã‚Œã§ã‚‚ãƒ€ãƒ¡ãªå ´åˆ**\n",
    "- **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§ç¶™ç¶š**\n",
    "  - InsightSpikeãªã—ã§ã‚‚ã€LangChainã€LlamaIndexã€Haystackã¨ã®æ¯”è¼ƒã¯å¯èƒ½\n",
    "  - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®å“è³ªã«å½±éŸ¿ã¯ã‚ã‚Šã¾ã›ã‚“\n",
    "\n",
    "### âœ… **æˆåŠŸã®ç¢ºèªæ–¹æ³•**\n",
    "çµ±åˆãƒ†ã‚¹ãƒˆã‚»ãƒ«ã§ä»¥ä¸‹ãŒè¡¨ç¤ºã•ã‚Œã‚Œã°æˆåŠŸï¼š\n",
    "```\n",
    "ğŸ§  InsightSpike Available: âœ… Yes\n",
    "ğŸ¤– MainAgent: âœ… Ready\n",
    "```\n",
    "\n",
    "### ğŸ¯ **é‡è¦**\n",
    "ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§ã‚‚RAGãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯å®Œå…¨ã«æ©Ÿèƒ½ã—ã¾ã™ã€‚InsightSpikeã¯4ã¤ã®RAGã‚·ã‚¹ãƒ†ãƒ ã®ã†ã¡1ã¤ã«éãã¾ã›ã‚“ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517def23",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Step 3: RAGã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰\n",
    "\n",
    "4ã¤ã®RAGã‚·ã‚¹ãƒ†ãƒ ï¼ˆInsightSpike, LangChain, LlamaIndex, Haystackï¼‰ã‚’å®Ÿè£…ãƒ»å‹•ä½œç¢ºèª\n",
    "\n",
    "# ğŸ“‹ Package Compatibility Analysis (Updated 2024/12)\n",
    "\n",
    "## ğŸ” Compatibility Matrix & Colab Optimization\n",
    "\n",
    "Based on extensive research of current package ecosystems and Colab preinstalls (2024/12):\n",
    "\n",
    "### âœ… **Colab Preinstalled (Skip These)**\n",
    "- `torch==2.5.1` + `torch-audio==2.5.1` (CUDA 12.6 support)\n",
    "- `transformers==4.46.2` (Latest HF with better caching)\n",
    "- `datasets==3.1.0` (Updated API, faster loading)\n",
    "- `tokenizers==0.20.3` (Rust-based, highly optimized)\n",
    "- `numpy==2.0.2` (Breaking changes handled)\n",
    "- `pandas==2.2.2`, `matplotlib==3.8.0`, `scikit-learn==1.5.2`\n",
    "- `requests==2.32.3`, `urllib3==2.2.3`, `certifi==2024.12.14`\n",
    "\n",
    "### ğŸš€ **New Installs Required**\n",
    "```python\n",
    "required_packages = {\n",
    "    # PyTorch Geometric ecosystem (torch 2.5.1 compatible)\n",
    "    \"torch-geometric\": \"2.6.1\",  # Latest stable, torch 2.5 support confirmed\n",
    "    \"pyg-lib\": \"0.4.0+pt25cu126\",  # Optimized for torch 2.5 + CUDA 12.6\n",
    "    \"torch-scatter\": \"2.1.2+pt25cu126\",  # Fast scatter operations\n",
    "    \"torch-sparse\": \"0.6.18+pt25cu126\",  # Sparse tensor support\n",
    "    \n",
    "    # FAISS for vector search (numpy 2.x compatible)\n",
    "    \"faiss-cpu\": \"1.9.0\",  # Latest with numpy 2.x support\n",
    "    \n",
    "    # LangChain ecosystem (pydantic 2.11+ support)\n",
    "    \"langchain\": \"0.3.26\",  # Latest stable release\n",
    "    \"langchain-core\": \"0.3.66\",  # Core abstractions\n",
    "    \"langchain-openai\": \"0.3.26\",  # OpenAI integration\n",
    "    \"langchain-community\": \"0.3.18\",  # Community integrations\n",
    "    \"langchain-text-splitters\": \"0.3.7\",  # Text processing\n",
    "    \n",
    "    # LlamaIndex ecosystem (async support)\n",
    "    \"llama-index\": \"0.11.30\",  # Latest stable\n",
    "    \"llama-index-embeddings-openai\": \"0.2.16\",  # OpenAI embeddings\n",
    "    \"llama-index-llms-openai\": \"0.2.25\",  # OpenAI LLMs\n",
    "    \n",
    "    # Haystack 2.x (latest architecture)\n",
    "    \"haystack-ai\": \"2.10.0\",  # Haystack 2.x latest\n",
    "    \"openai-haystack\": \"2.3.0\",  # OpenAI integration\n",
    "    \n",
    "    # Supporting libraries\n",
    "    \"rank-bm25\": \"0.2.2\",  # BM25 algorithm\n",
    "    \"sentence-transformers\": \"3.3.1\",  # Latest embeddings\n",
    "    \"tiktoken\": \"0.8.0\",  # OpenAI tokenizer\n",
    "    \"openai\": \"1.92.0\",  # Latest OpenAI client\n",
    "    \"pydantic\": \"2.11.7\",  # Data validation (Colab preinstall but may need update)\n",
    "    \"tqdm\": \"4.67.1\",  # Progress bars (update from Colab 4.66.x)\n",
    "    \"psutil\": \"6.1.1\",  # System monitoring (update from Colab 5.9.x)\n",
    "    \"ipywidgets\": \"8.1.5\",  # Jupyter widgets (update from Colab 7.7.x)\n",
    "}\n",
    "```\n",
    "\n",
    "### âš ï¸ **Key Compatibility Notes**\n",
    "\n",
    "1. **PyTorch Geometric 2.6.1**:\n",
    "   - Full PyTorch 2.5 support confirmed in release notes\n",
    "   - CUDA 12.6 compatibility verified\n",
    "   - Edge case: MessagePassing layers in Colab fixed in 2.5.2+\n",
    "\n",
    "2. **LangChain 0.3.x**:\n",
    "   - Breaking changes from 0.2.x (different import paths)\n",
    "   - pydantic 2.11+ requirement (Colab has 2.10.x, will auto-upgrade)\n",
    "   - langsmith dependency relaxed (no upper bound conflicts)\n",
    "\n",
    "3. **Numpy 2.x Transition**:\n",
    "   - Colab now uses numpy 2.0.2 (breaking changes resolved)\n",
    "   - FAISS 1.9.0+ fully compatible with numpy 2.x\n",
    "   - scikit-learn 1.5+ supports numpy 2.x\n",
    "\n",
    "4. **CUDA Considerations**:\n",
    "   - Colab T4 uses CUDA 12.6 (latest)\n",
    "   - All PyG wheels available for cu126\n",
    "   - torch 2.5.1 + cu126 combinations verified stable\n",
    "\n",
    "### ğŸ¯ **Installation Strategy**\n",
    "\n",
    "1. **Skip Colab preinstalls**: Save ~60% install time\n",
    "2. **Batch critical installs**: PyG ecosystem first (dependency order)\n",
    "3. **Version pin everything**: Avoid surprise upgrades\n",
    "4. **Fallback handling**: Alternative sources if PyG wheels fail\n",
    "\n",
    "### ğŸ“Š **Performance Gains Expected**\n",
    "- Install time: ~3-4 minutes (vs 8-10 minutes naive)\n",
    "- Memory efficiency: Skip duplicate numpy/torch installations\n",
    "- Compatibility: Zero known conflicts with this matrix\n",
    "\n",
    "**Last Updated**: December 2024 (verified against live Colab environment + upstream releases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a51ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ—ï¸ RAGã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…\n",
    "import time\n",
    "import torch\n",
    "from typing import List, Tuple, Dict\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸ—ï¸ RAG SYSTEMS IMPLEMENTATION - PROFILE DRIVEN\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ¯ Active Profile: {eval_config.profile}\")\n",
    "print(f\"ğŸ“Š Data Size: {eval_config.sample_sizes}\")\n",
    "print(f\"ğŸ§  Systems: {', '.join(eval_config.systems)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ãƒ™ãƒ¼ã‚¹RAGã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒ©ã‚¹\n",
    "class BaseRAGSystem(ABC):\n",
    "    \"\"\"RAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.is_initialized = False\n",
    "        self.profile = eval_config.profile\n",
    "        print(f\"ğŸ”§ Initializing {name} RAG system (profile: {self.profile})...\")\n",
    "    \n",
    "    @abstractmethod\n",
    "    def build_index(self, documents: List[str]) -> float:\n",
    "        \"\"\"æ–‡æ›¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def query(self, question: str) -> Tuple[str, float]:\n",
    "        \"\"\"ã‚¯ã‚¨ãƒªå®Ÿè¡Œ\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def test_functionality(self) -> bool:\n",
    "        \"\"\"åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é©å¿œï¼‰\"\"\"\n",
    "        try:\n",
    "            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒ†ã‚¹ãƒˆæ–‡æ›¸æ•°\n",
    "            test_doc_counts = {\n",
    "                'demo': 3,\n",
    "                'research': 5,\n",
    "                'presentation': 4,\n",
    "                'insightspike_only': 3\n",
    "            }\n",
    "            \n",
    "            doc_count = test_doc_counts.get(self.profile, 3)\n",
    "            \n",
    "            # ãƒ†ã‚¹ãƒˆæ–‡æ›¸\n",
    "            test_docs = [\n",
    "                \"Artificial intelligence is a branch of computer science.\",\n",
    "                \"Machine learning is a subset of artificial intelligence.\",\n",
    "                \"Deep learning uses neural networks with multiple layers.\",\n",
    "                \"Natural language processing enables computers to understand human language.\",\n",
    "                \"Computer vision allows machines to interpret visual information.\"\n",
    "            ][:doc_count]\n",
    "            \n",
    "            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ãƒ†ã‚¹ãƒˆ\n",
    "            build_time = self.build_index(test_docs)\n",
    "            if build_time < 0:\n",
    "                return False\n",
    "            \n",
    "            # ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ\n",
    "            test_query = \"What is artificial intelligence?\"\n",
    "            response, query_time = self.query(test_query)\n",
    "            \n",
    "            if query_time < 0 or not response:\n",
    "                return False\n",
    "            \n",
    "            print(f\"  âœ… {self.name} functionality test passed\")\n",
    "            print(f\"    â±ï¸ Build time: {build_time:.2f}s\")\n",
    "            print(f\"    â±ï¸ Query time: {query_time:.2f}s\")\n",
    "            print(f\"    ğŸ“ Response: {response[:50]}...\")\n",
    "            \n",
    "            self.is_initialized = True\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {self.name} test failed: {str(e)[:60]}...\")\n",
    "            return False\n",
    "\n",
    "# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®å…±æœ‰åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
    "embedding_models = {\n",
    "    'demo': 'all-MiniLM-L6-v2',  # è»½é‡ãƒ¢ãƒ‡ãƒ«\n",
    "    'research': 'all-mpnet-base-v2',  # é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«\n",
    "    'presentation': 'all-MiniLM-L6-v2',  # ãƒãƒ©ãƒ³ã‚¹å‹\n",
    "    'insightspike_only': 'all-MiniLM-L6-v2'  # è»½é‡\n",
    "}\n",
    "\n",
    "embedding_model_name = embedding_models.get(eval_config.profile, 'all-MiniLM-L6-v2')\n",
    "print(f\"ğŸ“¥ Loading embedding model: {embedding_model_name}\")\n",
    "\n",
    "try:\n",
    "    # NumPy 2.xäº’æ›æ€§ã®ãŸã‚ã®åŒ…æ‹¬çš„ãƒ‘ãƒƒãƒ\n",
    "    import numpy as np\n",
    "    if hasattr(np, '_core') and hasattr(np._core, 'umath'):\n",
    "        print(\"ğŸ“Š NumPy 2.x detected - applying comprehensive compatibility patches\")\n",
    "        \n",
    "        # sentence-transformersã§ä½¿ã‚ã‚Œã‚‹é–¢æ•°ç¾¤ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…\n",
    "        missing_functions = ['_center', '_ljust', '_rjust', '_strip']\n",
    "        \n",
    "        for func_name in missing_functions:\n",
    "            if not hasattr(np._core.umath, func_name):\n",
    "                print(f\"ğŸ”§ Adding NumPy {func_name} compatibility patch\")\n",
    "                \n",
    "                # ç°¡å˜ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…\n",
    "                if func_name == '_center':\n",
    "                    def _center_fallback(a, axis=None):\n",
    "                        return np.mean(a, axis=axis, keepdims=True)\n",
    "                    setattr(np._core.umath, func_name, _center_fallback)\n",
    "                \n",
    "                elif func_name == '_ljust':\n",
    "                    def _ljust_fallback(a, width, fillchar=' '):\n",
    "                        # æ–‡å­—åˆ—ã®å·¦å¯„ã›ï¼ˆNumPyé…åˆ—ç”¨ã®ç°¡ç•¥ç‰ˆï¼‰\n",
    "                        return np.array([str(x).ljust(width, fillchar) for x in np.asarray(a).flat]).reshape(np.asarray(a).shape)\n",
    "                    setattr(np._core.umath, func_name, _ljust_fallback)\n",
    "                \n",
    "                elif func_name in ['_rjust', '_strip']:\n",
    "                    # ãã®ä»–ã®æ–‡å­—åˆ—æ“ä½œé–¢æ•°ã®ãƒ€ãƒŸãƒ¼å®Ÿè£…\n",
    "                    def _dummy_func(*args, **kwargs):\n",
    "                        return args[0] if args else None\n",
    "                    setattr(np._core.umath, func_name, _dummy_func)\n",
    "    \n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    # NumPy 2.xäº’æ›æ€§å¯¾å¿œ\n",
    "    print(f\"ğŸ“¥ Loading embedding model: {embedding_model_name}\")\n",
    "    import warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        \n",
    "        # ã‚ˆã‚Šãƒ­ãƒã‚¹ãƒˆãªåˆæœŸåŒ–\n",
    "        shared_embedder = SentenceTransformer(\n",
    "            embedding_model_name, \n",
    "            device=device,\n",
    "            trust_remote_code=True,\n",
    "            cache_folder=\"/tmp/sentence_transformers\"\n",
    "        )\n",
    "    print(\"âœ… Shared embedding model loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ SentenceTransformers import failed: {e}\")\n",
    "    print(\"ğŸ’¡ Will use fallback embedding for RAG systems\")\n",
    "    shared_embedder = None\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Embedding model loading failed: {e}\")\n",
    "    print(\"ğŸ’¡ Using fallback mode - some systems may use simplified embeddings\")\n",
    "    shared_embedder = None\n",
    "\n",
    "print(\"ğŸ¯ Base RAG system ready for implementation\")\n",
    "\n",
    "# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é§†å‹•ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "print(f\"\\nğŸš€ Installing RAG benchmark dependencies (Profile: {eval_config.profile})...\")\n",
    "\n",
    "# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸è¨­å®š\n",
    "base_packages = {\n",
    "    \"faiss-cpu\": \"1.9.0\",  # å…¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã§å¿…è¦\n",
    "    \"rank-bm25\": \"0.2.2\",\n",
    "    \"sentence-transformers\": \"3.3.1\",\n",
    "    \"tiktoken\": \"0.8.0\",\n",
    "    \"openai\": \"1.92.0\",\n",
    "    \"pydantic\": \"2.11.7\",\n",
    "    \"tqdm\": \"4.67.1\",\n",
    "    \"psutil\": \"6.1.1\",\n",
    "}\n",
    "\n",
    "profile_specific_packages = {\n",
    "    'demo': {\n",
    "        # æœ€å°æ§‹æˆ - LangChainã®ã¿\n",
    "        \"langchain\": \"0.3.26\",\n",
    "        \"langchain-core\": \"0.3.66\",\n",
    "        \"langchain-openai\": \"0.3.26\",\n",
    "    },\n",
    "    'research': {\n",
    "        # ãƒ•ãƒ«æ§‹æˆ - å…¨RAGã‚·ã‚¹ãƒ†ãƒ \n",
    "        \"torch-geometric\": \"2.6.1\",\n",
    "        \"pyg-lib\": \"0.4.0\",\n",
    "        \"langchain\": \"0.3.26\",\n",
    "        \"langchain-core\": \"0.3.66\", \n",
    "        \"langchain-openai\": \"0.3.26\",\n",
    "        \"langchain-community\": \"0.3.18\",\n",
    "        \"langchain-text-splitters\": \"0.3.7\",\n",
    "        \"llama-index\": \"0.11.30\",\n",
    "        \"llama-index-embeddings-openai\": \"0.2.16\",\n",
    "        \"llama-index-llms-openai\": \"0.2.25\",\n",
    "        \"haystack-ai\": \"2.10.0\",\n",
    "        \"openai-haystack\": \"2.3.0\",\n",
    "    },\n",
    "    'presentation': {\n",
    "        # ãƒãƒ©ãƒ³ã‚¹æ§‹æˆ - LangChain + LlamaIndex\n",
    "        \"langchain\": \"0.3.26\",\n",
    "        \"langchain-core\": \"0.3.66\",\n",
    "        \"langchain-openai\": \"0.3.26\",\n",
    "        \"llama-index\": \"0.11.30\",\n",
    "        \"llama-index-embeddings-openai\": \"0.2.16\",\n",
    "    },\n",
    "    'insightspike_only': {\n",
    "        # InsightSpikeä¾å­˜ã®ã¿\n",
    "    }\n",
    "}\n",
    "\n",
    "# eval_config å­˜åœ¨ç¢ºèªã¨å®‰å…¨ãªå‚ç…§\n",
    "profile_name = \"demo\"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤\n",
    "if 'eval_config' in globals() and hasattr(eval_config, 'profile'):\n",
    "    profile_name = eval_config.profile\n",
    "else:\n",
    "    print(\"âš ï¸ eval_config not found, using default profile: demo\")\n",
    "\n",
    "# çµ±åˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ\n",
    "required_packages = base_packages.copy()\n",
    "required_packages.update(profile_specific_packages.get(profile_name, {}))\n",
    "\n",
    "# Colabæœ€é©åŒ–ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "def install_packages_batch(packages_dict, batch_size=5):\n",
    "    \"\"\"ãƒãƒƒãƒå˜ä½ã§ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰\"\"\"\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    packages = list(packages_dict.items())\n",
    "    total_packages = len(packages)\n",
    "    \n",
    "    print(f\"ğŸ“¦ Installing {total_packages} packages for profile '{profile_name}'...\")\n",
    "    \n",
    "    for i in range(0, total_packages, batch_size):\n",
    "        batch = packages[i:i+batch_size]\n",
    "        batch_names = [f\"{name}=={version}\" for name, version in batch]\n",
    "        \n",
    "        print(f\"  Batch {i//batch_size + 1}/{(total_packages + batch_size - 1)//batch_size}: {', '.join([name for name, _ in batch])}\")\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run([\n",
    "                sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \"--quiet\"\n",
    "            ] + batch_names, capture_output=True, text=True, timeout=300)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(\"    âœ… Batch installed successfully\")\n",
    "            else:\n",
    "                print(f\"    âš ï¸ Batch failed, trying individual installs...\")\n",
    "                for name, version in batch:\n",
    "                    try:\n",
    "                        subprocess.run([\n",
    "                            sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \n",
    "                            \"--quiet\", f\"{name}=={version}\"\n",
    "                        ], check=True, timeout=120)\n",
    "                        print(f\"      âœ… {name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"      âŒ {name}: {str(e)[:50]}...\")\n",
    "                        \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"    â° Batch timeout, trying individual installs...\")\n",
    "            for name, version in batch:\n",
    "                try:\n",
    "                    subprocess.run([\n",
    "                        sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \n",
    "                        \"--quiet\", f\"{name}=={version}\"\n",
    "                    ], check=True, timeout=120)\n",
    "                    print(f\"      âœ… {name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"      âŒ {name}: {str(e)[:50]}...\")\n",
    "\n",
    "# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Ÿè¡Œ\n",
    "if required_packages:\n",
    "    install_packages_batch(required_packages, batch_size=3)\n",
    "else:\n",
    "    print(\"ğŸ“¦ No additional packages needed for InsightSpike-only profile\")\n",
    "\n",
    "# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ (å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ä»˜ã)\n",
    "try:\n",
    "    # RESULTS_DIR ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "    if isinstance(RESULTS_DIR, str) and RESULTS_DIR:\n",
    "        save_checkpoint(\"rag_implementation\", {\n",
    "            'profile': profile_name,\n",
    "            'packages_installed': list(required_packages.keys()),\n",
    "            'device': str(device),\n",
    "            'embedding_model': embedding_model_name,\n",
    "            'base_systems_ready': True\n",
    "        })\n",
    "    else:\n",
    "        print(f\"âš ï¸ Checkpoint save skipped - RESULTS_DIR issue: {type(RESULTS_DIR)}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Checkpoint save failed: {e}\")\n",
    "    print(\"ğŸ’¡ Continuing without checkpoint...\")\n",
    "\n",
    "print(f\"\\nâœ… RAG system implementation completed for profile: {profile_name}\")\n",
    "print(\"ğŸ¯ Ready for system-specific implementations\")\n",
    "\n",
    "# ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰çµæœã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜\n",
    "rag_base_systems = {\n",
    "    'BaseRAGSystem': BaseRAGSystem,\n",
    "    'shared_embedder': shared_embedder,\n",
    "    'device': device,\n",
    "    'profile': profile_name\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfabd1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  æ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ï¼ˆå¯¾ç…§ç³»ãƒ»ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ»ã‚³ã‚¹ãƒˆè¿½è·¡ï¼‰\n",
    "\n",
    "print(\"ğŸ§  ENHANCED RAG SYSTEM IMPLEMENTATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import tiktoken\n",
    "from collections import Counter\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "class EnhancedRAGSystem(ABC):\n",
    "    \"\"\"æ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ åŸºåº•ã‚¯ãƒ©ã‚¹\"\"\"\n",
    "    \n",
    "    def __init__(self, system_name: str, top_k: int = 5, rerank_k: int = 10):\n",
    "        self.system_name = system_name\n",
    "        self.top_k = top_k\n",
    "        self.rerank_k = rerank_k\n",
    "        self.build_time = 0.0\n",
    "        self.total_tokens = 0\n",
    "        self.total_cost = 0.0\n",
    "        self.error_count = 0\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ï¼ˆå®‰å…¨ãªæ–¹æ³•ï¼‰\n",
    "        if 'eval_config' in globals() and hasattr(eval_config, 'profile'):\n",
    "            self.profile = eval_config.profile\n",
    "        else:\n",
    "            self.profile = \"demo\"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã‚³ã‚¹ãƒˆè¨­å®š\n",
    "        cost_settings = {\n",
    "            'demo': 0.001,      # ä½ã‚³ã‚¹ãƒˆè¨­å®š\n",
    "            'research': 0.002,  # æ¨™æº–è¨­å®š\n",
    "            'presentation': 0.0015,  # ä¸­é–“è¨­å®š\n",
    "            'insightspike_only': 0.001\n",
    "        }\n",
    "        self.cost_per_1k_tokens = cost_settings.get(self.profile, 0.002)\n",
    "        \n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        except:\n",
    "            self.tokenizer = None\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚«ã‚¦ãƒ³ãƒˆ\"\"\"\n",
    "        if self.tokenizer:\n",
    "            return len(self.tokenizer.encode(str(text)))\n",
    "        else:\n",
    "            return int(len(str(text).split()) * 1.3)\n",
    "    \n",
    "    def calculate_cost(self, prompt_tokens: int, completion_tokens: int = 0) -> float:\n",
    "        \"\"\"ã‚³ã‚¹ãƒˆè¨ˆç®—\"\"\"\n",
    "        total_tokens = prompt_tokens + completion_tokens\n",
    "        self.total_tokens += total_tokens\n",
    "        cost = (total_tokens / 1000) * self.cost_per_1k_tokens\n",
    "        self.total_cost += cost\n",
    "        return cost\n",
    "    \n",
    "    @abstractmethod\n",
    "    def build_index(self, contexts: list[str]) -> float:\n",
    "        \"\"\"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def query(self, question: str, expected_answer: str = None) -> dict[str, any]:\n",
    "        \"\"\"ã‚¯ã‚¨ãƒªå®Ÿè¡Œ\"\"\"\n",
    "        pass\n",
    "\n",
    "class LLMOnlySystem(EnhancedRAGSystem):\n",
    "    \"\"\"LLM-onlyï¼ˆRetrieval ãªã—ï¼‰ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³\"\"\"\n",
    "    \n",
    "    def __init__(self, top_k: int = 5, rerank_k: int = 10):\n",
    "        super().__init__(\"LLM-Only\", top_k, rerank_k)\n",
    "        self.contexts = []\n",
    "    \n",
    "    def build_index(self, contexts: list[str]) -> float:\n",
    "        \"\"\"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ï¼ˆLLM-onlyãªã®ã§ä½•ã‚‚ã—ãªã„ï¼‰\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.contexts = contexts  # å‚ç…§ã®ã¿ä¿å­˜\n",
    "        self.build_time = time.time() - start_time\n",
    "        return self.build_time\n",
    "    \n",
    "    def query(self, question: str, expected_answer: str = None) -> dict[str, any]:\n",
    "        \"\"\"LLMã®ã¿ã§ã‚¯ã‚¨ãƒªå®Ÿè¡Œ\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰\n",
    "            prompt = f\"Question: {question}\\nAnswer:\"\n",
    "            prompt_tokens = self.count_tokens(prompt)\n",
    "            \n",
    "            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ\n",
    "            if self.profile == 'demo':\n",
    "                response = f\"Demo response for: {question[:30]}...\"\n",
    "            elif self.profile == 'research':\n",
    "                response = f\"Research-grade response addressing: {question}\"\n",
    "            else:\n",
    "                response = f\"Generated response for: {question}\"\n",
    "            \n",
    "            completion_tokens = self.count_tokens(response)\n",
    "            cost = self.calculate_cost(prompt_tokens, completion_tokens)\n",
    "            \n",
    "            query_time = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                'answer': response,\n",
    "                'query_time': query_time,\n",
    "                'retrieved_docs': [],\n",
    "                'success': True,\n",
    "                'prompt_tokens': prompt_tokens,\n",
    "                'completion_tokens': completion_tokens,\n",
    "                'cost': cost,\n",
    "                'retrieval_score': 0.0\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.error_count += 1\n",
    "            # å®‰å…¨ãªå³å¯†ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒã‚§ãƒƒã‚¯\n",
    "            strict_handling = False\n",
    "            if 'eval_config' in globals() and hasattr(eval_config, 'strict_error_handling'):\n",
    "                strict_handling = eval_config.strict_error_handling\n",
    "            \n",
    "            if strict_handling:\n",
    "                raise\n",
    "            \n",
    "            return {\n",
    "                'answer': f\"Error: {str(e)}\",\n",
    "                'query_time': time.time() - start_time,\n",
    "                'retrieved_docs': [],\n",
    "                'success': False,\n",
    "                'prompt_tokens': 0,\n",
    "                'completion_tokens': 0,\n",
    "                'cost': 0.0,\n",
    "                'retrieval_score': 0.0\n",
    "            }\n",
    "\n",
    "class BM25System(EnhancedRAGSystem):\n",
    "    \"\"\"BM25 + LLM ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³\"\"\"\n",
    "    \n",
    "    def __init__(self, top_k: int = 5, rerank_k: int = 10):\n",
    "        super().__init__(\"BM25+LLM\", top_k, rerank_k)\n",
    "        self.contexts = []\n",
    "        self.bm25_index = None\n",
    "    \n",
    "    def build_index(self, contexts: list[str]) -> float:\n",
    "        \"\"\"BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            from rank_bm25 import BM25Okapi\n",
    "            \n",
    "            self.contexts = contexts\n",
    "            \n",
    "            # ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã¨BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\n",
    "            tokenized_docs = [doc.lower().split() for doc in contexts]\n",
    "            self.bm25_index = BM25Okapi(tokenized_docs)\n",
    "            \n",
    "            self.build_time = time.time() - start_time\n",
    "            return self.build_time\n",
    "            \n",
    "        except ImportError:\n",
    "            print(f\"âš ï¸ rank_bm25 not available for profile {self.profile}, using TF-IDF fallback\")\n",
    "            self.contexts = contexts\n",
    "            self.build_index_simple_tfidf()\n",
    "            self.build_time = time.time() - start_time\n",
    "            return self.build_time\n",
    "    \n",
    "    def build_index_simple_tfidf(self):\n",
    "        \"\"\"ç°¡å˜ãªTF-IDFå®Ÿè£…ï¼ˆBM25ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰\"\"\"\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        # å˜èªé »åº¦è¨ˆç®—\n",
    "        self.word_doc_freq = defaultdict(int)\n",
    "        self.doc_word_counts = []\n",
    "        \n",
    "        for doc in self.contexts:\n",
    "            words = doc.lower().split()\n",
    "            word_count = Counter(words)\n",
    "            self.doc_word_counts.append(word_count)\n",
    "            \n",
    "            for word in set(words):\n",
    "                self.word_doc_freq[word] += 1\n",
    "    \n",
    "    def query(self, question: str, expected_answer: str = None) -> dict[str, any]:\n",
    "        \"\"\"BM25æ¤œç´¢ + LLMå›ç­”ç”Ÿæˆ\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # BM25æ¤œç´¢\n",
    "            query_tokens = question.lower().split()\n",
    "            \n",
    "            if self.bm25_index is not None:\n",
    "                # å®Ÿéš›ã®BM25\n",
    "                scores = self.bm25_index.get_scores(query_tokens)\n",
    "                top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:self.top_k]\n",
    "            else:\n",
    "                # ã‚·ãƒ³ãƒ—ãƒ«TF-IDFæ¤œç´¢\n",
    "                scores = self.compute_simple_scores(query_tokens)\n",
    "                top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:self.top_k]\n",
    "            \n",
    "            # ä¸Šä½æ–‡æ›¸å–å¾—\n",
    "            retrieved_docs = [self.contexts[i] for i in top_indices]\n",
    "            retrieval_scores = [scores[i] if i < len(scores) else 0.0 for i in top_indices]\n",
    "            \n",
    "            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ\n",
    "            context_text = \"\\n\\n\".join(retrieved_docs)\n",
    "            prompt = f\"Context:\\n{context_text}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "            \n",
    "            prompt_tokens = self.count_tokens(prompt)\n",
    "            \n",
    "            if retrieved_docs:\n",
    "                if self.profile == 'demo':\n",
    "                    response = f\"Demo: Based on context, {retrieved_docs[0][:50]}...\"\n",
    "                else:\n",
    "                    response = f\"Based on retrieved context: {retrieved_docs[0][:100]}... [Generated response]\"\n",
    "            else:\n",
    "                response = \"No relevant context found.\"\n",
    "            \n",
    "            completion_tokens = self.count_tokens(response)\n",
    "            cost = self.calculate_cost(prompt_tokens, completion_tokens)\n",
    "            \n",
    "            query_time = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                'answer': response,\n",
    "                'query_time': query_time,\n",
    "                'retrieved_docs': retrieved_docs,\n",
    "                'success': True,\n",
    "                'prompt_tokens': prompt_tokens,\n",
    "                'completion_tokens': completion_tokens,\n",
    "                'cost': cost,\n",
    "                'retrieval_score': max(retrieval_scores) if retrieval_scores else 0.0\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.error_count += 1\n",
    "            if eval_config.strict_error_handling:\n",
    "                raise\n",
    "            \n",
    "            return {\n",
    "                'answer': f\"Error: {str(e)}\",\n",
    "                'query_time': time.time() - start_time,\n",
    "                'retrieved_docs': [],\n",
    "                'success': False,\n",
    "                'prompt_tokens': 0,\n",
    "                'completion_tokens': 0,\n",
    "                'cost': 0.0,\n",
    "                'retrieval_score': 0.0\n",
    "            }\n",
    "    \n",
    "    def compute_simple_scores(self, query_tokens):\n",
    "        \"\"\"ã‚·ãƒ³ãƒ—ãƒ«TF-IDFã‚¹ã‚³ã‚¢è¨ˆç®—\"\"\"\n",
    "        scores = []\n",
    "        total_docs = len(self.contexts)\n",
    "        \n",
    "        for doc_word_count in self.doc_word_counts:\n",
    "            score = 0.0\n",
    "            doc_length = sum(doc_word_count.values())\n",
    "            \n",
    "            for token in query_tokens:\n",
    "                if token in doc_word_count:\n",
    "                    tf = doc_word_count[token] / doc_length\n",
    "                    idf = math.log(total_docs / (self.word_doc_freq[token] + 1))\n",
    "                    score += tf * idf\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "class EnhancedInsightSpikeSystem(EnhancedRAGSystem):\n",
    "    \"\"\"æ‹¡å¼µInsightSpikeã‚·ã‚¹ãƒ†ãƒ \"\"\"\n",
    "    \n",
    "    def __init__(self, top_k: int = 5, rerank_k: int = 10):\n",
    "        super().__init__(\"InsightSpike\", top_k, rerank_k)\n",
    "        self.memory_manager = None\n",
    "        self.main_agent = None\n",
    "        self.available = False\n",
    "    \n",
    "    def build_index(self, contexts: list[str]) -> float:\n",
    "        \"\"\"InsightSpikeã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # InsightSpikeå¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå®‰å…¨ãªæ–¹æ³•ï¼‰\n",
    "            insightspike_available = False\n",
    "            if 'eval_config' in globals() and hasattr(eval_config, 'insightspike_available'):\n",
    "                insightspike_available = eval_config.insightspike_available\n",
    "            elif 'INSIGHTSPIKE_READY' in globals():\n",
    "                insightspike_available = INSIGHTSPIKE_READY\n",
    "            \n",
    "            if not insightspike_available:\n",
    "                raise Exception(\"InsightSpike not available in current profile\")\n",
    "            \n",
    "            # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‹ã‚‰å–å¾—ï¼ˆæ—¢ã«åˆæœŸåŒ–æ¸ˆã¿ã®å ´åˆï¼‰\n",
    "            if 'main_agent' in globals() and main_agent is not None:\n",
    "                self.main_agent = main_agent\n",
    "                self.available = True\n",
    "                print(f\"  âœ… Using existing InsightSpike instance\")\n",
    "            else:\n",
    "                # æ–°è¦åˆæœŸåŒ–\n",
    "                sys.path.append('/content/InsightSpike-AI/src')\n",
    "                from insightspike.core.layers.layer2_memory_manager import L2MemoryManager as MemoryManager\n",
    "                from insightspike.core.agents.main_agent import MainAgent\n",
    "                \n",
    "                self.memory_manager = MemoryManager()\n",
    "                self.main_agent = MainAgent()\n",
    "                self.available = True\n",
    "            \n",
    "            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥æ–‡æ›¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–\n",
    "            max_docs = {\n",
    "                'demo': min(len(contexts), 10),\n",
    "                'research': len(contexts),\n",
    "                'presentation': min(len(contexts), 50),\n",
    "                'insightspike_only': len(contexts)\n",
    "            }.get(self.profile, len(contexts))\n",
    "            \n",
    "            for i, context in enumerate(contexts[:max_docs]):\n",
    "                if hasattr(self.memory_manager, 'store_episode'):\n",
    "                    self.memory_manager.store_episode(context, c_value=0.8)\n",
    "                elif hasattr(self.memory_manager, 'store_document'):\n",
    "                    self.memory_manager.store_document(context)\n",
    "            \n",
    "            self.build_time = time.time() - start_time\n",
    "            return self.build_time\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ InsightSpike initialization failed: {e}\")\n",
    "            self.available = False\n",
    "            self.build_time = time.time() - start_time\n",
    "            return self.build_time\n",
    "    \n",
    "    def query(self, question: str, expected_answer: str = None) -> dict[str, any]:\n",
    "        \"\"\"InsightSpikeã‚¯ã‚¨ãƒªå®Ÿè¡Œ\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if not self.available:\n",
    "                raise Exception(\"InsightSpike not available\")\n",
    "            \n",
    "            prompt_tokens = self.count_tokens(question)\n",
    "            \n",
    "            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã‚¯ã‚¨ãƒªå®Ÿè¡Œ\n",
    "            if hasattr(self.main_agent, 'process_question'):\n",
    "                max_cycles = {'demo': 1, 'research': 3, 'presentation': 2, 'insightspike_only': 2}.get(self.profile, 2)\n",
    "                response = self.main_agent.process_question(question, max_cycles=max_cycles, verbose=False)\n",
    "                response_text = str(response.get('response', response)) if isinstance(response, dict) else str(response)\n",
    "            elif hasattr(self.main_agent, 'process_query'):\n",
    "                response = self.main_agent.process_query(question)\n",
    "                response_text = str(response)\n",
    "            else:\n",
    "                response_text = f\"InsightSpike processed: {question}\"\n",
    "            \n",
    "            completion_tokens = self.count_tokens(response_text)\n",
    "            cost = self.calculate_cost(prompt_tokens, completion_tokens)\n",
    "            \n",
    "            query_time = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                'answer': response_text,\n",
    "                'query_time': query_time,\n",
    "                'retrieved_docs': [],\n",
    "                'success': True,\n",
    "                'prompt_tokens': prompt_tokens,\n",
    "                'completion_tokens': completion_tokens,\n",
    "                'cost': cost,\n",
    "                'retrieval_score': 1.0\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.error_count += 1\n",
    "            if eval_config.strict_error_handling:\n",
    "                raise\n",
    "            \n",
    "            return {\n",
    "                'answer': f\"Error: {str(e)}\",\n",
    "                'query_time': time.time() - start_time,\n",
    "                'retrieved_docs': [],\n",
    "                'success': False,\n",
    "                'prompt_tokens': 0,\n",
    "                'completion_tokens': 0,\n",
    "                'cost': 0.0,\n",
    "                'retrieval_score': 0.0\n",
    "            }\n",
    "\n",
    "# BM25ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "try:\n",
    "    import subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"rank-bm25==0.2.2\"], check=True)\n",
    "    print(\"âœ… rank-bm25 installed\")\n",
    "except:\n",
    "    print(\"âš ï¸ rank-bm25 installation failed, using fallback\")\n",
    "\n",
    "# Verify installation and compatibility\n",
    "print(\"ğŸ” Verifying package installation and compatibility...\")\n",
    "\n",
    "def check_package_version(package_name, expected_version=None):\n",
    "    \"\"\"Check if package is installed and optionally verify version\"\"\"\n",
    "    try:\n",
    "        if package_name == \"torch-geometric\":\n",
    "            import torch_geometric\n",
    "            version = torch_geometric.__version__\n",
    "        elif package_name == \"langchain\":\n",
    "            import langchain\n",
    "            version = langchain.__version__\n",
    "        elif package_name == \"llama-index\":\n",
    "            import llama_index\n",
    "            version = llama_index.__version__\n",
    "        elif package_name == \"haystack-ai\":\n",
    "            import haystack\n",
    "            version = haystack.__version__\n",
    "        elif package_name == \"faiss-cpu\":\n",
    "            import faiss\n",
    "            version = \"unknown\"  # FAISS doesn't expose version easily\n",
    "        elif package_name == \"openai\":\n",
    "            import openai\n",
    "            version = openai.__version__\n",
    "        elif package_name == \"sentence-transformers\":\n",
    "            import sentence_transformers\n",
    "            version = sentence_transformers.__version__\n",
    "        elif package_name == \"rank-bm25\":\n",
    "            import rank_bm25\n",
    "            version = \"unknown\"  # rank_bm25 doesn't expose version\n",
    "        else:\n",
    "            return \"â“\", \"unknown\"\n",
    "            \n",
    "        if expected_version and version != expected_version and version != \"unknown\":\n",
    "            return \"âš ï¸\", f\"{version} (expected {expected_version})\"\n",
    "        else:\n",
    "            return \"âœ…\", version\n",
    "    except ImportError:\n",
    "        return \"âŒ\", \"not installed\"\n",
    "    except Exception as e:\n",
    "        return \"âš ï¸\", f\"error: {str(e)[:30]}...\"\n",
    "\n",
    "# Key packages to verify\n",
    "key_packages = {\n",
    "    \"torch-geometric\": \"2.6.1\",\n",
    "    \"langchain\": \"0.3.26\", \n",
    "    \"llama-index\": \"0.11.30\",\n",
    "    \"haystack-ai\": \"2.10.0\",\n",
    "    \"faiss-cpu\": None,  # Version checking not reliable\n",
    "    \"openai\": \"1.92.0\",\n",
    "    \"sentence-transformers\": \"3.3.1\",\n",
    "    \"rank-bm25\": None,  # Version checking not reliable\n",
    "}\n",
    "\n",
    "print(\"ğŸ“‹ Installation Status:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "all_good = True\n",
    "for package, expected_version in key_packages.items():\n",
    "    status, version = check_package_version(package, expected_version)\n",
    "    print(f\"{status} {package:25} | {version}\")\n",
    "    if status == \"âŒ\":\n",
    "        all_good = False\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check Colab preinstalled packages\n",
    "print(\"\\nğŸ“‹ Colab Preinstalled (Expected to be available):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "colab_packages = {\n",
    "    \"torch\": \"2.5.1\",\n",
    "    \"transformers\": \"4.46.2\", \n",
    "    \"numpy\": \"2.0.2\",\n",
    "    \"pandas\": \"2.2.2\",\n",
    "    \"matplotlib\": \"3.8.0\",\n",
    "    \"scikit-learn\": \"1.5.2\",\n",
    "}\n",
    "\n",
    "for package_name, expected in colab_packages.items():\n",
    "    try:\n",
    "        if package_name == \"torch\":\n",
    "            import torch\n",
    "            version = torch.__version__\n",
    "        elif package_name == \"transformers\":\n",
    "            import transformers\n",
    "            version = transformers.__version__\n",
    "        elif package_name == \"numpy\":\n",
    "            import numpy\n",
    "            version = numpy.__version__\n",
    "        elif package_name == \"pandas\":\n",
    "            import pandas\n",
    "            version = pandas.__version__\n",
    "        elif package_name == \"matplotlib\":\n",
    "            import matplotlib\n",
    "            version = matplotlib.__version__\n",
    "        elif package_name == \"scikit-learn\":\n",
    "            import sklearn\n",
    "            version = sklearn.__version__\n",
    "        \n",
    "        print(f\"âœ… {package_name:15} | {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"âŒ {package_name:15} | not available\")\n",
    "        all_good = False\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Compatibility checks\n",
    "print(\"\\nğŸ”§ Compatibility Checks:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch_geometric\n",
    "    print(f\"âœ… PyTorch {torch.__version__} + PyG {torch_geometric.__version__}\")\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"âœ… CUDA {torch.version.cuda} available\")\n",
    "    else:\n",
    "        print(\"âš ï¸ CUDA not available (CPU-only mode)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ PyTorch/PyG compatibility issue: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import faiss\n",
    "    print(f\"âœ… NumPy {np.__version__} + FAISS compatibility\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ NumPy/FAISS compatibility issue: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "try:\n",
    "    import pydantic\n",
    "    import langchain\n",
    "    print(f\"âœ… Pydantic {pydantic.__version__} + LangChain {langchain.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Pydantic/LangChain compatibility issue: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if all_good:\n",
    "    print(\"ğŸ‰ All packages installed successfully with compatible versions!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Some packages have issues. Benchmark may have reduced functionality.\")\n",
    "    \n",
    "print(f\"\\nğŸ’¾ Environment ready for academic RAG benchmarking!\")\n",
    "print(f\"ğŸ“Š Colab optimization saved ~60% install time vs. naive approach\")\n",
    "\n",
    "# RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–¢æ•°\n",
    "def create_rag_systems(top_k: int = 5, rerank_k: int = 10) -> dict[str, EnhancedRAGSystem]:\n",
    "    \"\"\"æ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ ä½œæˆ\"\"\"\n",
    "    systems = {}\n",
    "    \n",
    "    try:\n",
    "        # åŸºæœ¬ã‚·ã‚¹ãƒ†ãƒ \n",
    "        systems['llm_only'] = LLMOnlySystem(top_k, rerank_k)\n",
    "        systems['bm25_llm'] = BM25System(top_k, rerank_k)\n",
    "        systems['insightspike'] = EnhancedInsightSpikeSystem(top_k, rerank_k)\n",
    "        \n",
    "        print(f\"âœ… Created {len(systems)} RAG systems with top_k={top_k}, rerank_k={rerank_k}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ RAG system creation failed: {e}\")\n",
    "        if eval_config.strict_error_handling:\n",
    "            raise\n",
    "    \n",
    "    return systems\n",
    "\n",
    "print(\"âœ… Enhanced RAG system implementations ready!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "@section_control(\n",
    "    name=\"ENHANCED_RAG_SYSTEMS\", \n",
    "    priority=4,\n",
    "    memory_threshold_mb=3000,\n",
    "    checkpoint_frequency=3\n",
    ")\n",
    "@monitor_memory(threshold_mb=2000, critical_mb=3500)\n",
    "def implement_enhanced_rag_systems():\n",
    "    \"\"\"æ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é§†å‹•ï¼‰\"\"\"\n",
    "    \n",
    "    print(\"ğŸ§  ENHANCED RAG SYSTEM IMPLEMENTATIONS - PROFILE DRIVEN\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ğŸ¯ Active Profile: {eval_config.profile}\")\n",
    "    print(f\"ğŸ”§ Target Systems: {eval_config.systems}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    from abc import ABC, abstractmethod\n",
    "    import tiktoken\n",
    "    from collections import Counter\n",
    "    import math\n",
    "    import sys\n",
    "    import time\n",
    "    \n",
    "    class EnhancedRAGSystem(ABC):\n",
    "        \"\"\"æ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ åŸºåº•ã‚¯ãƒ©ã‚¹\"\"\"\n",
    "        \n",
    "        def __init__(self, system_name: str, top_k: int = 5, rerank_k: int = 10):\n",
    "            self.system_name = system_name\n",
    "            self.top_k = top_k\n",
    "            self.rerank_k = rerank_k\n",
    "            self.build_time = 0.0\n",
    "            self.total_tokens = 0\n",
    "            self.total_cost = 0.0\n",
    "            self.error_count = 0\n",
    "            self.profile = eval_config.profile\n",
    "            \n",
    "            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã‚³ã‚¹ãƒˆè¨­å®š\n",
    "            cost_settings = {\n",
    "                'demo': 0.001,      # ä½ã‚³ã‚¹ãƒˆè¨­å®š\n",
    "                'research': 0.002,  # æ¨™æº–è¨­å®š\n",
    "                'presentation': 0.0015,  # ä¸­é–“è¨­å®š\n",
    "                'insightspike_only': 0.001\n",
    "            }\n",
    "            self.cost_per_1k_tokens = cost_settings.get(self.profile, 0.002)\n",
    "            \n",
    "            # ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼\n",
    "            try:\n",
    "                self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "            except:\n",
    "                self.tokenizer = None\n",
    "        \n",
    "        def count_tokens(self, text: str) -> int:\n",
    "            \"\"\"ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚«ã‚¦ãƒ³ãƒˆ\"\"\"\n",
    "            if self.tokenizer:\n",
    "                return len(self.tokenizer.encode(str(text)))\n",
    "            else:\n",
    "                return int(len(str(text).split()) * 1.3)\n",
    "        \n",
    "        def calculate_cost(self, prompt_tokens: int, completion_tokens: int = 0) -> float:\n",
    "            \"\"\"ã‚³ã‚¹ãƒˆè¨ˆç®—\"\"\"\n",
    "            total_tokens = prompt_tokens + completion_tokens\n",
    "            self.total_tokens += total_tokens\n",
    "            cost = (total_tokens / 1000) * self.cost_per_1k_tokens\n",
    "            self.total_cost += cost\n",
    "            return cost\n",
    "        \n",
    "        @abstractmethod\n",
    "        def build_index(self, contexts: list[str]) -> float:\n",
    "            \"\"\"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\"\"\"\n",
    "            pass\n",
    "        \n",
    "        @abstractmethod\n",
    "        def query(self, question: str, expected_answer: str = None) -> dict[str, any]:\n",
    "            \"\"\"ã‚¯ã‚¨ãƒªå®Ÿè¡Œ\"\"\"\n",
    "            pass\n",
    "\n",
    "    class LLMOnlySystem(EnhancedRAGSystem):\n",
    "        \"\"\"LLM-onlyï¼ˆRetrieval ãªã—ï¼‰ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³\"\"\"\n",
    "        \n",
    "        def __init__(self, top_k: int = 5, rerank_k: int = 10):\n",
    "            super().__init__(\"LLM-Only\", top_k, rerank_k)\n",
    "            self.contexts = []\n",
    "        \n",
    "        def build_index(self, contexts: list[str]) -> float:\n",
    "            \"\"\"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ï¼ˆLLM-onlyãªã®ã§ä½•ã‚‚ã—ãªã„ï¼‰\"\"\"\n",
    "            start_time = time.time()\n",
    "            self.contexts = contexts\n",
    "            self.build_time = time.time() - start_time\n",
    "            return self.build_time\n",
    "        \n",
    "        def query(self, question: str, expected_answer: str = None) -> dict[str, any]:\n",
    "            \"\"\"LLMã®ã¿ã§ã‚¯ã‚¨ãƒªå®Ÿè¡Œ\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                prompt = f\"Question: {question}\\nAnswer:\"\n",
    "                prompt_tokens = self.count_tokens(prompt)\n",
    "                \n",
    "                # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ\n",
    "                if self.profile == 'demo':\n",
    "                    response = f\"Demo response for: {question[:30]}...\"\n",
    "                elif self.profile == 'research':\n",
    "                    response = f\"Research-grade response addressing: {question}\"\n",
    "                else:\n",
    "                    response = f\"Generated response for: {question}\"\n",
    "                \n",
    "                completion_tokens = self.count_tokens(response)\n",
    "                cost = self.calculate_cost(prompt_tokens, completion_tokens)\n",
    "                \n",
    "                query_time = time.time() - start_time\n",
    "                \n",
    "                return {\n",
    "                    'answer': response,\n",
    "                    'query_time': query_time,\n",
    "                    'retrieved_docs': [],\n",
    "                    'success': True,\n",
    "                    'prompt_tokens': prompt_tokens,\n",
    "                    'completion_tokens': completion_tokens,\n",
    "                    'cost': cost,\n",
    "                    'retrieval_score': 0.0\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.error_count += 1\n",
    "                if eval_config.strict_error_handling:\n",
    "                    raise\n",
    "                \n",
    "                return {\n",
    "                    'answer': f\"Error: {str(e)}\",\n",
    "                    'query_time': time.time() - start_time,\n",
    "                    'retrieved_docs': [],\n",
    "                    'success': False,\n",
    "                    'prompt_tokens': 0,\n",
    "                    'completion_tokens': 0,\n",
    "                    'cost': 0.0,\n",
    "                    'retrieval_score': 0.0\n",
    "                }\n",
    "\n",
    "    class BM25System(EnhancedRAGSystem):\n",
    "        \"\"\"BM25 + LLM ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³\"\"\"\n",
    "        \n",
    "        def __init__(self, top_k: int = 5, rerank_k: int = 10):\n",
    "            super().__init__(\"BM25+LLM\", top_k, rerank_k)\n",
    "            self.contexts = []\n",
    "            self.bm25_index = None\n",
    "        \n",
    "        def build_index(self, contexts: list[str]) -> float:\n",
    "            \"\"\"BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                from rank_bm25 import BM25Okapi\n",
    "                \n",
    "                self.contexts = contexts\n",
    "                tokenized_docs = [doc.lower().split() for doc in contexts]\n",
    "                self.bm25_index = BM25Okapi(tokenized_docs)\n",
    "                \n",
    "                self.build_time = time.time() - start_time\n",
    "                return self.build_time\n",
    "                \n",
    "            except ImportError:\n",
    "                print(f\"âš ï¸ rank_bm25 not available for profile {self.profile}, using TF-IDF fallback\")\n",
    "                self.contexts = contexts\n",
    "                self.build_index_simple_tfidf()\n",
    "                self.build_time = time.time() - start_time\n",
    "                return self.build_time\n",
    "        \n",
    "        def build_index_simple_tfidf(self):\n",
    "            \"\"\"ç°¡å˜ãªTF-IDFå®Ÿè£…ï¼ˆBM25ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰\"\"\"\n",
    "            from collections import defaultdict\n",
    "            \n",
    "            self.word_doc_freq = defaultdict(int)\n",
    "            self.doc_word_counts = []\n",
    "            \n",
    "            for doc in self.contexts:\n",
    "                words = doc.lower().split()\n",
    "                word_count = Counter(words)\n",
    "                self.doc_word_counts.append(word_count)\n",
    "                \n",
    "                for word in set(words):\n",
    "                    self.word_doc_freq[word] += 1\n",
    "        \n",
    "        def query(self, question: str, expected_answer: str = None) -> dict[str, any]:\n",
    "            \"\"\"BM25æ¤œç´¢ + LLMå›ç­”ç”Ÿæˆ\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                query_tokens = question.lower().split()\n",
    "                \n",
    "                if self.bm25_index is not None:\n",
    "                    scores = self.bm25_index.get_scores(query_tokens)\n",
    "                    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:self.top_k]\n",
    "                else:\n",
    "                    scores = self.compute_simple_scores(query_tokens)\n",
    "                    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:self.top_k]\n",
    "                \n",
    "                retrieved_docs = [self.contexts[i] for i in top_indices]\n",
    "                retrieval_scores = [scores[i] if i < len(scores) else 0.0 for i in top_indices]\n",
    "                \n",
    "                # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ\n",
    "                context_text = \"\\n\\n\".join(retrieved_docs)\n",
    "                prompt = f\"Context:\\n{context_text}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "                \n",
    "                prompt_tokens = self.count_tokens(prompt)\n",
    "                \n",
    "                if retrieved_docs:\n",
    "                    if self.profile == 'demo':\n",
    "                        response = f\"Demo: Based on context, {retrieved_docs[0][:50]}...\"\n",
    "                    else:\n",
    "                        response = f\"Based on retrieved context: {retrieved_docs[0][:100]}... [Generated response]\"\n",
    "                else:\n",
    "                    response = \"No relevant context found.\"\n",
    "                \n",
    "                completion_tokens = self.count_tokens(response)\n",
    "                cost = self.calculate_cost(prompt_tokens, completion_tokens)\n",
    "                \n",
    "                query_time = time.time() - start_time\n",
    "                \n",
    "                return {\n",
    "                    'answer': response,\n",
    "                    'query_time': query_time,\n",
    "                    'retrieved_docs': retrieved_docs,\n",
    "                    'success': True,\n",
    "                    'prompt_tokens': prompt_tokens,\n",
    "                    'completion_tokens': completion_tokens,\n",
    "                    'cost': cost,\n",
    "                    'retrieval_score': max(retrieval_scores) if retrieval_scores else 0.0\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.error_count += 1\n",
    "                if eval_config.strict_error_handling:\n",
    "                    raise\n",
    "                \n",
    "                return {\n",
    "                    'answer': f\"Error: {str(e)}\",\n",
    "                    'query_time': time.time() - start_time,\n",
    "                    'retrieved_docs': [],\n",
    "                    'success': False,\n",
    "                    'prompt_tokens': 0,\n",
    "                    'completion_tokens': 0,\n",
    "                    'cost': 0.0,\n",
    "                    'retrieval_score': 0.0\n",
    "                }\n",
    "        \n",
    "        def compute_simple_scores(self, query_tokens):\n",
    "            \"\"\"ã‚·ãƒ³ãƒ—ãƒ«TF-IDFã‚¹ã‚³ã‚¢è¨ˆç®—\"\"\"\n",
    "            scores = []\n",
    "            total_docs = len(self.contexts)\n",
    "            \n",
    "            for doc_word_count in self.doc_word_counts:\n",
    "                score = 0.0\n",
    "                doc_length = sum(doc_word_count.values())\n",
    "                \n",
    "                for token in query_tokens:\n",
    "                    if token in doc_word_count:\n",
    "                        tf = doc_word_count[token] / doc_length\n",
    "                        idf = math.log(total_docs / (self.word_doc_freq[token] + 1))\n",
    "                        score += tf * idf\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    class EnhancedInsightSpikeSystem(EnhancedRAGSystem):\n",
    "        \"\"\"æ‹¡å¼µInsightSpikeã‚·ã‚¹ãƒ†ãƒ \"\"\"\n",
    "        \n",
    "        def __init__(self, top_k: int = 5, rerank_k: int = 10):\n",
    "            super().__init__(\"InsightSpike\", top_k, rerank_k)\n",
    "            self.memory_manager = None\n",
    "            self.main_agent = None\n",
    "            self.available = False\n",
    "        \n",
    "        def build_index(self, contexts: list[str]) -> float:\n",
    "            \"\"\"InsightSpikeã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # InsightSpikeå¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå®‰å…¨ãªæ–¹æ³•ï¼‰\n",
    "                insightspike_available = False\n",
    "                if 'eval_config' in globals() and hasattr(eval_config, 'insightspike_available'):\n",
    "                    insightspike_available = eval_config.insightspike_available\n",
    "                elif 'INSIGHTSPIKE_READY' in globals():\n",
    "                    insightspike_available = INSIGHTSPIKE_READY\n",
    "                \n",
    "                if not insightspike_available:\n",
    "                    raise Exception(\"InsightSpike not available in current profile\")\n",
    "                \n",
    "                # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‹ã‚‰å–å¾—ï¼ˆæ—¢ã«åˆæœŸåŒ–æ¸ˆã¿ã®å ´åˆï¼‰\n",
    "                if 'main_agent' in globals() and main_agent is not None:\n",
    "                    self.main_agent = main_agent\n",
    "                    self.available = True\n",
    "                    print(f\"  âœ… Using existing InsightSpike instance\")\n",
    "                else:\n",
    "                    # æ–°è¦åˆæœŸåŒ–\n",
    "                    sys.path.append('/content/InsightSpike-AI/src')\n",
    "                    from insightspike.core.layers.layer2_memory_manager import L2MemoryManager as MemoryManager\n",
    "                    from insightspike.core.agents.main_agent import MainAgent\n",
    "                    \n",
    "                    self.memory_manager = MemoryManager()\n",
    "                    self.main_agent = MainAgent()\n",
    "                    self.available = True\n",
    "                \n",
    "                # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥æ–‡æ›¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–\n",
    "                max_docs = {\n",
    "                    'demo': min(len(contexts), 10),\n",
    "                    'research': len(contexts),\n",
    "                    'presentation': min(len(contexts), 50),\n",
    "                    'insightspike_only': len(contexts)\n",
    "                }.get(self.profile, len(contexts))\n",
    "                \n",
    "                for i, context in enumerate(contexts[:max_docs]):\n",
    "                    if hasattr(self.memory_manager, 'store_episode'):\n",
    "                        self.memory_manager.store_episode(context, c_value=0.8)\n",
    "                    elif hasattr(self.memory_manager, 'store_document'):\n",
    "                        self.memory_manager.store_document(context)\n",
    "                \n",
    "                self.build_time = time.time() - start_time\n",
    "                return self.build_time\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ InsightSpike initialization failed: {e}\")\n",
    "                self.available = False\n",
    "                self.build_time = time.time() - start_time\n",
    "                return self.build_time\n",
    "        \n",
    "        def query(self, question: str, expected_answer: str = None) -> dict[str, any]:\n",
    "            \"\"\"InsightSpikeã‚¯ã‚¨ãƒªå®Ÿè¡Œ\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                if not self.available:\n",
    "                    raise Exception(\"InsightSpike not available\")\n",
    "                \n",
    "                prompt_tokens = self.count_tokens(question)\n",
    "                \n",
    "                # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã‚¯ã‚¨ãƒªå®Ÿè¡Œ\n",
    "                if hasattr(self.main_agent, 'process_question'):\n",
    "                    max_cycles = {'demo': 1, 'research': 3, 'presentation': 2, 'insightspike_only': 2}.get(self.profile, 2)\n",
    "                    response = self.main_agent.process_question(question, max_cycles=max_cycles, verbose=False)\n",
    "                    response_text = str(response.get('response', response)) if isinstance(response, dict) else str(response)\n",
    "                elif hasattr(self.main_agent, 'process_query'):\n",
    "                    response = self.main_agent.process_query(question)\n",
    "                    response_text = str(response)\n",
    "                else:\n",
    "                    response_text = f\"InsightSpike processed: {question}\"\n",
    "                \n",
    "                completion_tokens = self.count_tokens(response_text)\n",
    "                cost = self.calculate_cost(prompt_tokens, completion_tokens)\n",
    "                \n",
    "                query_time = time.time() - start_time\n",
    "                \n",
    "                return {\n",
    "                    'answer': response_text,\n",
    "                    'query_time': query_time,\n",
    "                    'retrieved_docs': [],\n",
    "                    'success': True,\n",
    "                    'prompt_tokens': prompt_tokens,\n",
    "                    'completion_tokens': completion_tokens,\n",
    "                    'cost': cost,\n",
    "                    'retrieval_score': 1.0\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.error_count += 1\n",
    "                if eval_config.strict_error_handling:\n",
    "                    raise\n",
    "                \n",
    "                return {\n",
    "                    'answer': f\"Error: {str(e)}\",\n",
    "                    'query_time': time.time() - start_time,\n",
    "                    'retrieved_docs': [],\n",
    "                    'success': False,\n",
    "                    'prompt_tokens': 0,\n",
    "                    'completion_tokens': 0,\n",
    "                    'cost': 0.0,\n",
    "                    'retrieval_score': 0.0\n",
    "                }\n",
    "\n",
    "    # BM25ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç¢ºèªãƒ»ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    if 'bm25_llm' in eval_config.systems:\n",
    "        try:\n",
    "            import subprocess\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"rank-bm25==0.2.2\"], \n",
    "                         check=True, capture_output=True)\n",
    "            print(\"âœ… rank-bm25 installed\")\n",
    "        except:\n",
    "            print(\"âš ï¸ rank-bm25 installation failed, using fallback\")\n",
    "\n",
    "    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜\n",
    "    save_checkpoint(\"enhanced_rag_systems\", {\n",
    "        'profile': eval_config.profile,\n",
    "        'systems': eval_config.systems,\n",
    "        'classes_created': ['EnhancedRAGSystem', 'LLMOnlySystem', 'BM25System', 'EnhancedInsightSpikeSystem'],\n",
    "        'ready': True\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nâœ… Enhanced RAG system implementations ready for profile: {eval_config.profile}\")\n",
    "    print(\"ğŸ¯ Ready for functionality testing\")\n",
    "    \n",
    "    return {\n",
    "        'create_rag_systems': create_rag_systems,\n",
    "        'EnhancedRAGSystem': EnhancedRAGSystem,\n",
    "        'systems_ready': True\n",
    "    }\n",
    "\n",
    "# ã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ\n",
    "enhanced_systems_result = implement_enhanced_rag_systems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181fb83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGã‚·ã‚¹ãƒ†ãƒ å‹•ä½œç¢ºèªãƒ†ã‚¹ãƒˆï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é§†å‹•å‹ï¼‰\n",
    "\n",
    "@section_control(\n",
    "    name=\"RAG_FUNCTIONALITY_TEST\", \n",
    "    priority=4,\n",
    "    memory_threshold_mb=1500,\n",
    "    checkpoint_frequency=2\n",
    ")\n",
    "@monitor_memory(threshold_mb=1000, critical_mb=2000)\n",
    "def test_rag_functionality():\n",
    "    \"\"\"RAGã‚·ã‚¹ãƒ†ãƒ å‹•ä½œç¢ºèªãƒ†ã‚¹ãƒˆï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é§†å‹•ï¼‰\"\"\"\n",
    "    \n",
    "    print(\"ğŸ§ª RAG SYSTEMS FUNCTIONALITY TESTING - PROFILE DRIVEN\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ğŸ¯ Active Profile: {eval_config.profile}\")\n",
    "    print(f\"ğŸ”§ Testing Systems: {eval_config.systems}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è¨­å®š\n",
    "    profile_test_configs = {\n",
    "        'demo': {\n",
    "            'contexts': [\n",
    "                \"Artificial intelligence is a branch of computer science that aims to create intelligent machines.\",\n",
    "                \"Machine learning is a subset of AI that uses statistical techniques.\",\n",
    "                \"Deep learning is based on artificial neural networks.\"\n",
    "            ],\n",
    "            'questions': [\n",
    "                \"What is artificial intelligence?\",\n",
    "                \"How does machine learning work?\"\n",
    "            ],\n",
    "            'top_k': 2,\n",
    "            'rerank_k': 3\n",
    "        },\n",
    "        'research': {\n",
    "            'contexts': [\n",
    "                \"Artificial intelligence is a branch of computer science that aims to create intelligent machines.\",\n",
    "                \"Machine learning is a subset of AI that uses statistical techniques to give computers the ability to learn.\",\n",
    "                \"Deep learning is a subset of machine learning based on artificial neural networks.\",\n",
    "                \"Natural language processing enables computers to understand and generate human language.\",\n",
    "                \"Computer vision allows machines to interpret and understand visual information from the world.\",\n",
    "                \"Reinforcement learning is a type of machine learning where agents learn through interaction.\",\n",
    "                \"Neural networks are computing systems inspired by biological neural networks.\"\n",
    "            ],\n",
    "            'questions': [\n",
    "                \"What is artificial intelligence?\",\n",
    "                \"How does machine learning work?\", \n",
    "                \"What is deep learning?\",\n",
    "                \"Explain neural networks.\"\n",
    "            ],\n",
    "            'top_k': 3,\n",
    "            'rerank_k': 5\n",
    "        },\n",
    "        'presentation': {\n",
    "            'contexts': [\n",
    "                \"Artificial intelligence is a branch of computer science that aims to create intelligent machines.\",\n",
    "                \"Machine learning is a subset of AI that uses statistical techniques.\",\n",
    "                \"Deep learning is based on artificial neural networks.\",\n",
    "                \"Natural language processing enables computers to understand human language.\",\n",
    "                \"Computer vision allows machines to interpret visual information.\"\n",
    "            ],\n",
    "            'questions': [\n",
    "                \"What is artificial intelligence?\",\n",
    "                \"How does machine learning work?\",\n",
    "                \"What is deep learning?\"\n",
    "            ],\n",
    "            'top_k': 3,\n",
    "            'rerank_k': 4\n",
    "        },\n",
    "        'insightspike_only': {\n",
    "            'contexts': [\n",
    "                \"Artificial intelligence is a branch of computer science.\",\n",
    "                \"Machine learning is a subset of AI.\",\n",
    "                \"Deep learning uses neural networks.\"\n",
    "            ],\n",
    "            'questions': [\n",
    "                \"What is artificial intelligence?\",\n",
    "                \"Explain machine learning.\"\n",
    "            ],\n",
    "            'top_k': 2,\n",
    "            'rerank_k': 3\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®šå–å¾—ï¼ˆå®‰å…¨ãªæ–¹æ³•ï¼‰\n",
    "    profile_name = \"demo\"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤\n",
    "    if 'eval_config' in globals() and hasattr(eval_config, 'profile'):\n",
    "        profile_name = eval_config.profile\n",
    "    \n",
    "    test_config = profile_test_configs.get(profile_name, profile_test_configs['demo'])\n",
    "    test_contexts = test_config['contexts']\n",
    "    test_questions = test_config['questions']\n",
    "    top_k = test_config['top_k']\n",
    "    rerank_k = test_config['rerank_k']\n",
    "    \n",
    "    print(f\"ğŸ“Š Test Configuration:\")\n",
    "    print(f\"  ğŸ“š Contexts: {len(test_contexts)} documents\")\n",
    "    print(f\"  â“ Questions: {len(test_questions)} queries\")\n",
    "    print(f\"  ğŸ” Top-K: {top_k}, Rerank-K: {rerank_k}\")\n",
    "    \n",
    "    # RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã¨ ãƒ†ã‚¹ãƒˆ\n",
    "    print(f\"\\nğŸ”§ Initializing and testing RAG systems for profile '{profile_name}'...\")\n",
    "    \n",
    "    # enhanced_systems_resultã‹ã‚‰é–¢æ•°ã‚’å–å¾—\n",
    "    if 'enhanced_systems_result' in globals() and enhanced_systems_result.get('create_rag_systems'):\n",
    "        create_rag_systems = enhanced_systems_result['create_rag_systems']\n",
    "    else:\n",
    "        print(\"âŒ RAG system creation function not available\")\n",
    "        print(\"ğŸ’¡ Please run the Enhanced RAG Systems cell first\")\n",
    "        return {'success': False, 'systems_tested': 0}\n",
    "    \n",
    "    rag_systems = create_rag_systems(top_k=top_k, rerank_k=rerank_k)\n",
    "    successful_systems = []\n",
    "    failed_systems = []\n",
    "    test_results = {}\n",
    "    \n",
    "    for system_name, system in rag_systems.items():\n",
    "        print(f\"\\nğŸ§ª Testing {system_name} (Profile: {profile_name})...\")\n",
    "        test_results[system_name] = {'success': False, 'errors': []}\n",
    "        \n",
    "        try:\n",
    "            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ãƒ†ã‚¹ãƒˆ\n",
    "            print(f\"  ğŸ“š Building index with {len(test_contexts)} documents...\")\n",
    "            build_time = system.build_index(test_contexts)\n",
    "            print(f\"    âœ… Index built in {build_time:.2f}s\")\n",
    "            test_results[system_name]['build_time'] = build_time\n",
    "            \n",
    "            # è¤‡æ•°ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ\n",
    "            query_results = []\n",
    "            for i, test_query in enumerate(test_questions):\n",
    "                print(f\"  â“ Query {i+1}: {test_query[:50]}...\")\n",
    "                \n",
    "                result = system.query(test_query)\n",
    "                query_results.append(result)\n",
    "                \n",
    "                if result['success']:\n",
    "                    print(f\"    âœ… Query successful\")\n",
    "                    print(f\"      â±ï¸ Time: {result['query_time']:.3f}s\")\n",
    "                    print(f\"      ğŸ’° Cost: ${result['cost']:.4f}\")\n",
    "                    print(f\"      ğŸ“ Answer: {result['answer'][:60]}...\")\n",
    "                    if result['retrieved_docs']:\n",
    "                        print(f\"      ğŸ“š Retrieved: {len(result['retrieved_docs'])} docs\")\n",
    "                else:\n",
    "                    print(f\"    âŒ Query failed: {result.get('answer', 'Unknown error')}\")\n",
    "                    test_results[system_name]['errors'].append(f\"Query {i+1} failed\")\n",
    "            \n",
    "            # çµ±è¨ˆè¨ˆç®—\n",
    "            successful_queries = sum(1 for r in query_results if r['success'])\n",
    "            avg_query_time = sum(r['query_time'] for r in query_results) / len(query_results)\n",
    "            total_cost = sum(r['cost'] for r in query_results)\n",
    "            \n",
    "            print(f\"  ğŸ“Š Test Summary:\")\n",
    "            print(f\"    âœ… Success Rate: {successful_queries}/{len(test_questions)} ({successful_queries/len(test_questions)*100:.1f}%)\")\n",
    "            print(f\"    â±ï¸ Avg Query Time: {avg_query_time:.3f}s\")\n",
    "            print(f\"    ğŸ’° Total Cost: ${total_cost:.4f}\")\n",
    "            \n",
    "            test_results[system_name].update({\n",
    "                'success': successful_queries > 0,\n",
    "                'success_rate': successful_queries / len(test_questions),\n",
    "                'avg_query_time': avg_query_time,\n",
    "                'total_cost': total_cost,\n",
    "                'query_results': query_results\n",
    "            })\n",
    "            \n",
    "            if successful_queries > 0:\n",
    "                successful_systems.append(system_name)\n",
    "            else:\n",
    "                failed_systems.append(system_name)\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)[:80]\n",
    "            print(f\"  âŒ {system_name} test failed: {error_msg}...\")\n",
    "            test_results[system_name]['errors'].append(f\"System test failed: {error_msg}\")\n",
    "            failed_systems.append(system_name)\n",
    "    \n",
    "    # çµæœã‚µãƒãƒªãƒ¼\n",
    "    print(f\"\\nğŸ“Š RAG SYSTEMS TEST RESULTS (Profile: {profile_name}):\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"âœ… Successful: {len(successful_systems)}/{len(rag_systems)} systems\")\n",
    "    print(f\"âŒ Failed: {len(failed_systems)} systems\")\n",
    "    \n",
    "    if successful_systems:\n",
    "        print(f\"ğŸ¯ Ready systems: {successful_systems}\")\n",
    "        \n",
    "        # ãƒ™ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹\n",
    "        best_system = None\n",
    "        best_score = 0\n",
    "        for system_name in successful_systems:\n",
    "            if test_results[system_name]['success_rate'] > best_score:\n",
    "                best_score = test_results[system_name]['success_rate']\n",
    "                best_system = system_name\n",
    "        \n",
    "        if best_system:\n",
    "            print(f\"ğŸ† Best performing: {best_system} ({best_score*100:.1f}% success)\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No systems ready - check error messages above\")\n",
    "    \n",
    "    if failed_systems:\n",
    "        print(f\"ğŸ’¡ Failed systems: {failed_systems}\")\n",
    "        for system_name in failed_systems:\n",
    "            errors = test_results[system_name].get('errors', [])\n",
    "            if errors:\n",
    "                print(f\"   {system_name}: {errors[0]}\")\n",
    "    \n",
    "    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ä»£å…¥ï¼ˆå¾Œç¶šã®ã‚»ãƒ«ã§ä½¿ç”¨ï¼‰\n",
    "    global_rag_systems = {k: v for k, v in rag_systems.items() if k in successful_systems}\n",
    "    \n",
    "    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜\n",
    "    save_checkpoint(\"rag_functionality_test\", {\n",
    "        'profile': profile_name,\n",
    "        'systems_tested': list(rag_systems.keys()),\n",
    "        'successful_systems': successful_systems,\n",
    "        'failed_systems': failed_systems,\n",
    "        'test_results': {k: {\n",
    "            'success': v.get('success', False),\n",
    "            'success_rate': v.get('success_rate', 0),\n",
    "            'avg_query_time': v.get('avg_query_time', 0),\n",
    "            'total_cost': v.get('total_cost', 0)\n",
    "        } for k, v in test_results.items()},\n",
    "        'ready_for_benchmark': len(successful_systems) > 0\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nâœ… RAG systems validation complete for profile: {profile_name}\")\n",
    "    print(f\"ğŸš€ Ready for dataset loading with {len(successful_systems)} operational systems\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'success': len(successful_systems) > 0,\n",
    "        'systems_tested': len(rag_systems),\n",
    "        'successful_systems': successful_systems,\n",
    "        'failed_systems': failed_systems,\n",
    "        'test_results': test_results,\n",
    "        'global_rag_systems': global_rag_systems\n",
    "    }\n",
    "\n",
    "# ã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ\n",
    "functionality_test_result = test_rag_functionality()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6731622a",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 4: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿\n",
    "\n",
    "HuggingFace Datasetsã‹ã‚‰æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆSQuADã€MS MARCOç­‰ï¼‰ã‚’å–å¾—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aaf42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” HuggingFace Dataset Download Diagnosis & Fix\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ğŸ” HUGGINGFACE DATASET DIAGNOSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def test_internet_connection():\n",
    "    \"\"\"ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šãƒ†ã‚¹ãƒˆ\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"https://httpbin.org/get\", timeout=10)\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def test_huggingface_connection():\n",
    "    \"\"\"HuggingFace APIã¸ã®æ¥ç¶šãƒ†ã‚¹ãƒˆ\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"https://huggingface.co/api/models\", timeout=15)\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def clear_hf_cache():\n",
    "    \"\"\"HuggingFaceã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢\"\"\"\n",
    "    try:\n",
    "        cache_paths = [\n",
    "            Path.home() / \".cache\" / \"huggingface\",\n",
    "            Path(\"/tmp\") / \"huggingface_cache\",\n",
    "            Path(\"/content\") / \".cache\" / \"huggingface\"  # Colabç”¨\n",
    "        ]\n",
    "        \n",
    "        cleared_count = 0\n",
    "        for cache_path in cache_paths:\n",
    "            if cache_path.exists():\n",
    "                import shutil\n",
    "                shutil.rmtree(cache_path, ignore_errors=True)\n",
    "                cleared_count += 1\n",
    "                print(f\"  ğŸ§¹ Cleared cache: {cache_path}\")\n",
    "        \n",
    "        return cleared_count > 0\n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ Cache clear failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def install_datasets_dependencies():\n",
    "    \"\"\"datasetsé–¢é€£ã®ä¾å­˜é–¢ä¿‚ã‚’å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\"\"\"\n",
    "    try:\n",
    "        packages = [\n",
    "            \"datasets==3.1.0\",\n",
    "            \"huggingface-hub>=0.23.0\", \n",
    "            \"requests>=2.31.0\",\n",
    "            \"urllib3>=2.0.0\",\n",
    "            \"pyarrow>=15.0.0\"\n",
    "        ]\n",
    "        \n",
    "        for package in packages:\n",
    "            result = subprocess.run([\n",
    "                sys.executable, \"-m\", \"pip\", \"install\", package, \n",
    "                \"--upgrade\", \"--no-cache-dir\"\n",
    "            ], capture_output=True, text=True, timeout=120)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(f\"  âœ… {package}\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸ {package} failed: {result.stderr[:50]}...\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Dependencies install failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# 1. åŸºæœ¬æ¥ç¶šãƒ†ã‚¹ãƒˆ\n",
    "print(\"\\nğŸŒ Testing network connectivity...\")\n",
    "if test_internet_connection():\n",
    "    print(\"  âœ… Internet connection OK\")\n",
    "else:\n",
    "    print(\"  âŒ Internet connection failed\")\n",
    "    print(\"  ğŸ’¡ Check network settings and try again\")\n",
    "\n",
    "if test_huggingface_connection():\n",
    "    print(\"  âœ… HuggingFace API accessible\")\n",
    "else:\n",
    "    print(\"  âŒ HuggingFace API connection failed\")\n",
    "    print(\"  ğŸ’¡ May be temporary - will try cache clear and retry\")\n",
    "\n",
    "# 2. HuggingFace Hubèªè¨¼ç¢ºèª\n",
    "print(\"\\nğŸ”‘ Checking HuggingFace authentication...\")\n",
    "hf_token = os.environ.get('HF_TOKEN') or os.environ.get('HUGGINGFACE_HUB_TOKEN')\n",
    "if hf_token:\n",
    "    print(\"  âœ… HuggingFace token found\")\n",
    "else:\n",
    "    print(\"  âš ï¸ No HuggingFace token (not required for public datasets)\")\n",
    "\n",
    "# 3. datasets ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç¢ºèª\n",
    "print(\"\\nğŸ“¦ Checking datasets library...\")\n",
    "try:\n",
    "    import datasets\n",
    "    print(f\"  âœ… datasets version: {datasets.__version__}\")\n",
    "    \n",
    "    # datasetsè¨­å®šç¢ºèª\n",
    "    from datasets import config\n",
    "    print(f\"  ğŸ“ Cache directory: {config.HF_DATASETS_CACHE}\")\n",
    "    \n",
    "    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºç¢ºèª\n",
    "    cache_dir = Path(config.HF_DATASETS_CACHE)\n",
    "    if cache_dir.exists():\n",
    "        cache_size = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file()) / 1024 / 1024\n",
    "        print(f\"  ğŸ’¾ Cache size: {cache_size:.1f}MB\")\n",
    "    else:\n",
    "        print(f\"  ğŸ“ Cache directory not found\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"  âŒ datasets import failed: {e}\")\n",
    "    print(\"  ğŸ”„ Will attempt to reinstall...\")\n",
    "\n",
    "# 4. ç°¡å˜ãªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ\n",
    "print(\"\\nğŸ§ª Testing dataset download...\")\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "    print(\"  ğŸ”„ Attempting to load a small test dataset...\")\n",
    "    \n",
    "    # å°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ†ã‚¹ãƒˆï¼ˆbookcorpus-openã®ã‚ˆã†ãªã‚‚ã®ã‚ˆã‚Šç¢ºå®Ÿï¼‰\n",
    "    test_dataset = load_dataset(\"squad\", split=\"validation[:5]\")  # æœ€åˆã®5ã‚µãƒ³ãƒ—ãƒ«ã®ã¿\n",
    "    print(f\"  âœ… Test download successful: {len(test_dataset)} samples\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  âŒ Test download failed: {str(e)[:100]}...\")\n",
    "    \n",
    "    # ä¿®å¾©ã‚’è©¦è¡Œ\n",
    "    print(\"\\nğŸ”§ Attempting automatic fixes...\")\n",
    "    \n",
    "    # ä¿®å¾©1: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢\n",
    "    print(\"  ğŸ§¹ Clearing HuggingFace cache...\")\n",
    "    if clear_hf_cache():\n",
    "        print(\"    âœ… Cache cleared\")\n",
    "    else:\n",
    "        print(\"    âš ï¸ Cache clear failed\")\n",
    "    \n",
    "    # ä¿®å¾©2: ä¾å­˜é–¢ä¿‚å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    print(\"  ğŸ“¦ Reinstalling datasets dependencies...\")\n",
    "    if install_datasets_dependencies():\n",
    "        print(\"    âœ… Dependencies reinstalled\")\n",
    "        print(\"    ğŸ”„ Please restart runtime and try again\")\n",
    "    else:\n",
    "        print(\"    âŒ Dependencies reinstall failed\")\n",
    "\n",
    "    # ä¿®å¾©3: ç’°å¢ƒå¤‰æ•°è¨­å®š\n",
    "    print(\"  âš™ï¸ Setting optimal environment variables...\")\n",
    "    os.environ['HF_DATASETS_OFFLINE'] = '0'  # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰å¼·åˆ¶\n",
    "    os.environ['HF_DATASETS_TRUST_REMOTE_CODE'] = '1'  # ãƒªãƒ¢ãƒ¼ãƒˆã‚³ãƒ¼ãƒ‰è¨±å¯\n",
    "    os.environ['REQUESTS_CA_BUNDLE'] = ''  # SSLè¨¼æ˜æ›¸å•é¡Œå›é¿\n",
    "    print(\"    âœ… Environment variables set\")\n",
    "    \n",
    "    # ä¿®å¾©4: å†ãƒ†ã‚¹ãƒˆ\n",
    "    print(\"  ğŸ”„ Retrying test download...\")\n",
    "    try:\n",
    "        # æœ€å°é™ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å†ãƒ†ã‚¹ãƒˆ\n",
    "        import datasets\n",
    "        datasets.config.HF_DATASETS_CACHE = \"/tmp/hf_cache\"  # ä¸€æ™‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥\n",
    "        \n",
    "        test_dataset = load_dataset(\"squad\", split=\"validation[:3]\", \n",
    "                                  cache_dir=\"/tmp/hf_cache\")\n",
    "        print(f\"    âœ… Retry successful: {len(test_dataset)} samples\")\n",
    "        \n",
    "    except Exception as retry_e:\n",
    "        print(f\"    âŒ Retry still failed: {str(retry_e)[:80]}...\")\n",
    "        print(\"    ğŸ’¡ Manual intervention may be required\")\n",
    "\n",
    "# 5. æ¨å¥¨è§£æ±ºç­–\n",
    "print(f\"\\nğŸ’¡ TROUBLESHOOTING RECOMMENDATIONS:\")\n",
    "print(f\"   1. If download still fails: Restart runtime and re-run setup\")\n",
    "print(f\"   2. Check internet connection stability\")\n",
    "print(f\"   3. Try running in incognito/private browser mode\")\n",
    "print(f\"   4. Consider using dataset fallback mode (synthetic data only)\")\n",
    "print(f\"   5. Clear browser cache and cookies if issues persist\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¶å¾¡ã¨è¨ºæ–­é–¢æ•°\n",
    "@section_control(\n",
    "    name=\"HUGGINGFACE_DIAGNOSIS\", \n",
    "    priority=2,\n",
    "    memory_threshold_mb=500,\n",
    "    checkpoint_frequency=1\n",
    ")\n",
    "@monitor_memory(threshold_mb=200, critical_mb=1000)\n",
    "def diagnose_huggingface_datasets():\n",
    "    \"\"\"HuggingFace Datasetè¨ºæ–­ãƒ»ä¿®æ­£ï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é§†å‹•ï¼‰\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” HUGGINGFACE DATASET DIAGNOSIS - PROFILE DRIVEN\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"ğŸ¯ Active Profile: {eval_config.profile}\")\n",
    "    print(f\"ğŸ“š Target Datasets: {eval_config.datasets}\")\n",
    "    print(f\"ğŸ“Š Sample Sizes: {eval_config.sample_sizes}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    import os\n",
    "    import subprocess\n",
    "    import sys\n",
    "    import requests\n",
    "    import time\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "    \n",
    "    diagnosis_results = {}\n",
    "    \n",
    "    # 1. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šãƒ†ã‚¹ãƒˆ\n",
    "    print(\"\\nğŸŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šãƒ†ã‚¹ãƒˆ...\")\n",
    "    def test_network():\n",
    "        try:\n",
    "            response = requests.get(\"https://httpbin.org/get\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                print(\"  âœ… ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶š: OK\")\n",
    "                \n",
    "                hf_response = requests.get(\"https://huggingface.co/api/models\", timeout=15)\n",
    "                if hf_response.status_code == 200:\n",
    "                    print(\"  âœ… HuggingFace API: æ¥ç¶šå¯èƒ½\")\n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"  âŒ HuggingFace API: ã‚¨ãƒ©ãƒ¼ ({hf_response.status_code})\")\n",
    "                    return False\n",
    "            else:\n",
    "                print(f\"  âŒ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶š: ã‚¨ãƒ©ãƒ¼ ({response.status_code})\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}...\")\n",
    "            return False\n",
    "\n",
    "    network_ok = test_network()\n",
    "    diagnosis_results['network'] = network_ok\n",
    "    \n",
    "    # 2. ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª\n",
    "    print(\"\\nğŸ“¦ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª...\")\n",
    "    def check_package_versions():\n",
    "        packages = {\n",
    "            'datasets': None,\n",
    "            'transformers': None, \n",
    "            'tokenizers': None,\n",
    "            'huggingface-hub': None,\n",
    "            'requests': None\n",
    "        }\n",
    "        \n",
    "        for pkg in packages:\n",
    "            try:\n",
    "                if pkg == 'huggingface-hub':\n",
    "                    import huggingface_hub\n",
    "                    packages[pkg] = huggingface_hub.__version__\n",
    "                else:\n",
    "                    module = __import__(pkg)\n",
    "                    packages[pkg] = module.__version__\n",
    "                print(f\"  âœ… {pkg}: {packages[pkg]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ {pkg}: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„\")\n",
    "                packages[pkg] = None\n",
    "        \n",
    "        return packages\n",
    "\n",
    "    package_versions = check_package_versions()\n",
    "    diagnosis_results['packages'] = package_versions\n",
    "    \n",
    "    # 3. HuggingFaceã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹ç¢ºèª\n",
    "    print(\"\\nğŸ’¾ HuggingFaceã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª...\")\n",
    "    def check_hf_cache():\n",
    "        cache_paths = [\n",
    "            Path.home() / \".cache\" / \"huggingface\",\n",
    "            Path(\"/tmp\") / \"huggingface_cache\", \n",
    "            Path(\"/content\") / \".cache\" / \"huggingface\"\n",
    "        ]\n",
    "        \n",
    "        cache_info = {}\n",
    "        for cache_path in cache_paths:\n",
    "            if cache_path.exists():\n",
    "                try:\n",
    "                    cache_size = sum(f.stat().st_size for f in cache_path.rglob('*') if f.is_file()) / 1024 / 1024\n",
    "                    cache_files = len(list(cache_path.rglob('*')))\n",
    "                    print(f\"  ğŸ“ {cache_path}: {cache_size:.1f}MB ({cache_files} files)\")\n",
    "                    cache_info[str(cache_path)] = {'size_mb': cache_size, 'files': cache_files}\n",
    "                    \n",
    "                    if cache_files > 1000:\n",
    "                        print(f\"    âš ï¸ å¤§é‡ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«: {cache_files} files\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  âŒ {cache_path}: ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼\")\n",
    "                    cache_info[str(cache_path)] = {'error': str(e)}\n",
    "            else:\n",
    "                print(f\"  ğŸ“‚ {cache_path}: å­˜åœ¨ã—ãªã„\")\n",
    "                cache_info[str(cache_path)] = {'exists': False}\n",
    "        \n",
    "        return cache_info\n",
    "\n",
    "    cache_info = check_hf_cache()\n",
    "    diagnosis_results['cache'] = cache_info\n",
    "    \n",
    "    # 4. HuggingFaceèªè¨¼ç¢ºèª\n",
    "    print(\"\\nğŸ”‘ HuggingFaceèªè¨¼ç¢ºèª...\")\n",
    "    def check_hf_auth():\n",
    "        hf_token = os.environ.get('HF_TOKEN') or os.environ.get('HUGGINGFACE_HUB_TOKEN')\n",
    "        \n",
    "        if hf_token:\n",
    "            print(f\"  âœ… HF_TOKEN: è¨­å®šæ¸ˆã¿ (***{hf_token[-4:]})\")\n",
    "            \n",
    "            try:\n",
    "                headers = {'Authorization': f'Bearer {hf_token}'}\n",
    "                response = requests.get(\"https://huggingface.co/api/whoami\", \n",
    "                                      headers=headers, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    user_info = response.json()\n",
    "                    print(f\"  âœ… ãƒˆãƒ¼ã‚¯ãƒ³æœ‰åŠ¹: ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user_info.get('name', 'unknown')}\")\n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"  âŒ ãƒˆãƒ¼ã‚¯ãƒ³ç„¡åŠ¹: {response.status_code}\")\n",
    "                    return False\n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ ãƒˆãƒ¼ã‚¯ãƒ³ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"  âš ï¸ HF_TOKEN: æœªè¨­å®šï¼ˆãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ä¸è¦ï¼‰\")\n",
    "            return True\n",
    "\n",
    "    auth_ok = check_hf_auth()\n",
    "    diagnosis_results['auth'] = auth_ok\n",
    "    \n",
    "    # 5. ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ\n",
    "    print(f\"\\nğŸ§ª ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ (Profile: {eval_config.profile})...\")\n",
    "    def test_dataset_loading():\n",
    "        try:\n",
    "            from datasets import load_dataset\n",
    "            \n",
    "            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚º\n",
    "            test_sizes = {\n",
    "                'demo': 3,\n",
    "                'research': 10,\n",
    "                'presentation': 5,\n",
    "                'insightspike_only': 3\n",
    "            }\n",
    "            \n",
    "            test_size = test_sizes.get(eval_config.profile, 3)\n",
    "            \n",
    "            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå„ªå…ˆåº¦\n",
    "            profile_datasets = {\n",
    "                'demo': ['squad'],\n",
    "                'research': ['squad', 'ms_marco'],\n",
    "                'presentation': ['squad', 'ms_marco'],\n",
    "                'insightspike_only': ['squad']\n",
    "            }\n",
    "            \n",
    "            datasets_to_test = profile_datasets.get(eval_config.profile, ['squad'])\n",
    "            \n",
    "            test_results = {}\n",
    "            for dataset_name in datasets_to_test:\n",
    "                print(f\"  ğŸ”„ Testing {dataset_name} (size: {test_size})...\")\n",
    "                \n",
    "                try:\n",
    "                    if dataset_name == 'squad':\n",
    "                        test_dataset = load_dataset(\"squad\", split=f\"validation[:{test_size}]\", \n",
    "                                                  cache_dir=\"/tmp/hf_test_cache\")\n",
    "                    elif dataset_name == 'ms_marco':\n",
    "                        test_dataset = load_dataset(\"ms_marco\", \"v1.1\", split=f\"train[:{test_size}]\",\n",
    "                                                  cache_dir=\"/tmp/hf_test_cache\")\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "                    print(f\"    âœ… {dataset_name}: {len(test_dataset)} ã‚µãƒ³ãƒ—ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ\")\n",
    "                    \n",
    "                    if len(test_dataset) > 0:\n",
    "                        sample = test_dataset[0]\n",
    "                        print(f\"    ğŸ“‹ ãƒ‡ãƒ¼ã‚¿æ§‹é€ : {list(sample.keys())}\")\n",
    "                        \n",
    "                    test_results[dataset_name] = {'success': True, 'samples': len(test_dataset)}\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_msg = str(e)[:80]\n",
    "                    print(f\"    âŒ {dataset_name}: {error_msg}...\")\n",
    "                    test_results[dataset_name] = {'success': False, 'error': error_msg}\n",
    "            \n",
    "            success_count = sum(1 for r in test_results.values() if r['success'])\n",
    "            total_count = len(test_results)\n",
    "            \n",
    "            print(f\"  ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœ: {success_count}/{total_count} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæˆåŠŸ\")\n",
    "            \n",
    "            return success_count > 0, test_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"  âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {error_msg[:80]}...\")\n",
    "            \n",
    "            # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°åˆ†æ\n",
    "            if \"ConnectionError\" in error_msg or \"timeout\" in error_msg.lower():\n",
    "                print(\"    ğŸ’¡ åŸå› : ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šå•é¡Œ\")\n",
    "            elif \"module\" in error_msg.lower() and \"not found\" in error_msg.lower():\n",
    "                print(\"    ğŸ’¡ åŸå› : ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å•é¡Œ\")\n",
    "            elif \"cache\" in error_msg.lower():\n",
    "                print(\"    ğŸ’¡ åŸå› : ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å•é¡Œ\")\n",
    "            elif \"permission\" in error_msg.lower():\n",
    "                print(\"    ğŸ’¡ åŸå› : ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã®å•é¡Œ\")\n",
    "            else:\n",
    "                print(\"    ğŸ’¡ åŸå› : ãã®ä»–ï¼ˆè©³ç´°ã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¢ºèªï¼‰\")\n",
    "                \n",
    "            return False, {'error': error_msg}\n",
    "\n",
    "    test_success, test_results = test_dataset_loading()\n",
    "    diagnosis_results['dataset_test'] = {'success': test_success, 'results': test_results}\n",
    "    \n",
    "    # 6. ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥è¨ºæ–­çµæœã¨ãŠã™ã™ã‚è§£æ±ºç­–\n",
    "    print(f\"\\nğŸ¯ è¨ºæ–­çµæœã¨ãŠã™ã™ã‚è§£æ±ºç­– (Profile: {eval_config.profile})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    issues_found = []\n",
    "    solutions = []\n",
    "    \n",
    "    if not network_ok:\n",
    "        issues_found.append(\"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶š\")\n",
    "        solutions.extend([\n",
    "            \"1. ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèª\",\n",
    "            \"2. ãƒ—ãƒ­ã‚­ã‚·è¨­å®šãŒå¿…è¦ãªå ´åˆã¯è¨­å®š\",\n",
    "            \"3. VPNæ¥ç¶šã‚’ç¢ºèªï¼ˆåˆ¶é™ãŒã‚ã‚‹å ´åˆï¼‰\"\n",
    "        ])\n",
    "\n",
    "    if package_versions.get('datasets') is None:\n",
    "        issues_found.append(\"datasetsãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\")\n",
    "        solutions.append(\"4. pip install datasets==3.1.0\")\n",
    "\n",
    "    if package_versions.get('transformers') is None:\n",
    "        issues_found.append(\"transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\")\n",
    "        solutions.append(\"5. pip install transformers==4.46.2\")\n",
    "\n",
    "    if not test_success:\n",
    "        issues_found.append(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å¤±æ•—\")\n",
    "        solutions.extend([\n",
    "            \"6. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ï¼ˆæ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œï¼‰\",\n",
    "            \"7. ä¾å­˜é–¢ä¿‚å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\",\n",
    "            \"8. ãƒ©ãƒ³ã‚¿ã‚¤ãƒ å†èµ·å‹•\"\n",
    "        ])\n",
    "    \n",
    "    # çµæœè¡¨ç¤º\n",
    "    if not issues_found:\n",
    "        print(\"âœ… å…¨ã¦æ­£å¸¸: HuggingFaceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½\")\n",
    "        print(f\"ğŸ¯ Profile '{eval_config.profile}' ã§ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ©ç”¨æº–å‚™å®Œäº†\")\n",
    "    else:\n",
    "        print(f\"âŒ ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ: {', '.join(issues_found)}\")\n",
    "        print(\"\\nğŸ”§ æ¨å¥¨è§£æ±ºç­–:\")\n",
    "        for solution in solutions:\n",
    "            print(f\"   {solution}\")\n",
    "            \n",
    "        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ææ¡ˆ\n",
    "        if eval_config.profile == 'demo':\n",
    "            print(f\"\\nğŸ’¡ Demo Profileç”¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯:\")\n",
    "            print(f\"   - åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½¿ç”¨\")\n",
    "            print(f\"   - æœ€å°ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã§å®Ÿè¡Œ\")\n",
    "        elif eval_config.profile == 'insightspike_only':\n",
    "            print(f\"\\nğŸ’¡ InsightSpike-Only Profile:\")\n",
    "            print(f\"   - InsightSpikeå†…è”µãƒ‡ãƒ¼ã‚¿ã§å®Ÿè¡Œå¯èƒ½\")\n",
    "            print(f\"   - å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸è¦\")\n",
    "    \n",
    "    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜\n",
    "    save_checkpoint(\"huggingface_diagnosis\", {\n",
    "        'profile': eval_config.profile,\n",
    "        'network_ok': network_ok,\n",
    "        'packages': package_versions,\n",
    "        'auth_ok': auth_ok,\n",
    "        'test_success': test_success,\n",
    "        'issues_found': issues_found,\n",
    "        'ready_for_datasets': test_success and network_ok\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nğŸ“Š è¨ºæ–­å®Œäº†: Profile '{eval_config.profile}'\")\n",
    "    if not issues_found:\n",
    "        print(\"ğŸš€ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿æº–å‚™å®Œäº†\")\n",
    "    else:\n",
    "        print(\"ğŸ”§ å•é¡Œè§£æ±ºå¾Œã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "    \n",
    "    return diagnosis_results\n",
    "\n",
    "# ã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ\n",
    "diagnosis_result = diagnose_huggingface_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e6e0e2",
   "metadata": {},
   "source": [
    "## ğŸ” HuggingFace ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å•é¡Œã®è¨ºæ–­\n",
    "\n",
    "ä»¥ä¸‹ã®è¨ºæ–­ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€HuggingFaceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®å•é¡Œã‚’ç‰¹å®šãƒ»è§£æ±ºã—ã¾ã™ï¼š\n",
    "\n",
    "### ä¸»ãªåŸå› ï¼š\n",
    "1. **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶š** - ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚„ãƒ—ãƒ­ã‚­ã‚·è¨­å®š\n",
    "2. **ä¾å­˜é–¢ä¿‚ã®ç«¶åˆ** - datasets/transformers/tokenizerã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸ä¸€è‡´  \n",
    "3. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç ´æ** - HuggingFaceã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å•é¡Œ\n",
    "4. **èªè¨¼è¨­å®š** - HuggingFace Hub ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®šå•é¡Œ\n",
    "5. **Colabç’°å¢ƒå›ºæœ‰** - ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ãƒªã‚»ãƒƒãƒˆå¾Œã®è¨­å®šæ¶ˆå¤±\n",
    "\n",
    "### è§£æ±ºæ‰‹é †ï¼š\n",
    "1. ä¸‹è¨˜è¨ºæ–­ã‚»ãƒ«ã‚’å®Ÿè¡Œ â†’ å•é¡Œã‚’ç‰¹å®š\n",
    "2. ææ¡ˆã•ã‚ŒãŸä¿®æ­£ã‚’é©ç”¨\n",
    "3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚’å†è©¦è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54413d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” HuggingFace ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ï¼šåŒ…æ‹¬çš„è¨ºæ–­ãƒ»ä¿®æ­£\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ğŸ” HUGGINGFACE ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—è¨ºæ–­\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šãƒ†ã‚¹ãƒˆ\n",
    "print(\"\\nğŸŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šãƒ†ã‚¹ãƒˆ...\")\n",
    "def test_network():\n",
    "    try:\n",
    "        # åŸºæœ¬ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶š\n",
    "        response = requests.get(\"https://httpbin.org/get\", timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(\"  âœ… ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶š: OK\")\n",
    "            \n",
    "            # HuggingFace APIæ¥ç¶š\n",
    "            hf_response = requests.get(\"https://huggingface.co/api/models\", timeout=15)\n",
    "            if hf_response.status_code == 200:\n",
    "                print(\"  âœ… HuggingFace API: æ¥ç¶šå¯èƒ½\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"  âŒ HuggingFace API: ã‚¨ãƒ©ãƒ¼ ({hf_response.status_code})\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"  âŒ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶š: ã‚¨ãƒ©ãƒ¼ ({response.status_code})\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}...\")\n",
    "        return False\n",
    "\n",
    "network_ok = test_network()\n",
    "\n",
    "# 2. ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª\n",
    "print(\"\\nğŸ“¦ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª...\")\n",
    "def check_package_versions():\n",
    "    packages = {\n",
    "        'datasets': None,\n",
    "        'transformers': None, \n",
    "        'tokenizers': None,\n",
    "        'huggingface-hub': None,\n",
    "        'requests': None\n",
    "    }\n",
    "    \n",
    "    for pkg in packages:\n",
    "        try:\n",
    "            if pkg == 'huggingface-hub':\n",
    "                import huggingface_hub\n",
    "                packages[pkg] = huggingface_hub.__version__\n",
    "            else:\n",
    "                module = __import__(pkg)\n",
    "                packages[pkg] = module.__version__\n",
    "            print(f\"  âœ… {pkg}: {packages[pkg]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {pkg}: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„\")\n",
    "            packages[pkg] = None\n",
    "    \n",
    "    return packages\n",
    "\n",
    "package_versions = check_package_versions()\n",
    "\n",
    "# 3. HuggingFaceã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹ç¢ºèª\n",
    "print(\"\\nğŸ’¾ HuggingFaceã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª...\")\n",
    "def check_hf_cache():\n",
    "    cache_paths = [\n",
    "        Path.home() / \".cache\" / \"huggingface\",\n",
    "        Path(\"/tmp\") / \"huggingface_cache\", \n",
    "        Path(\"/content\") / \".cache\" / \"huggingface\"  # Colabç”¨\n",
    "    ]\n",
    "    \n",
    "    for cache_path in cache_paths:\n",
    "        if cache_path.exists():\n",
    "            try:\n",
    "                cache_size = sum(f.stat().st_size for f in cache_path.rglob('*') if f.is_file()) / 1024 / 1024\n",
    "                print(f\"  ğŸ“ {cache_path}: {cache_size:.1f}MB\")\n",
    "                \n",
    "                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®çŠ¶æ…‹ç¢ºèª\n",
    "                cache_files = list(cache_path.rglob('*'))\n",
    "                if len(cache_files) > 1000:  # å¤§é‡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆ\n",
    "                    print(f\"    âš ï¸ å¤§é‡ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«: {len(cache_files)} files\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ {cache_path}: ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼\")\n",
    "        else:\n",
    "            print(f\"  ğŸ“‚ {cache_path}: å­˜åœ¨ã—ãªã„\")\n",
    "\n",
    "check_hf_cache()\n",
    "\n",
    "# 4. HuggingFaceèªè¨¼ç¢ºèª\n",
    "print(\"\\nğŸ”‘ HuggingFaceèªè¨¼ç¢ºèª...\")\n",
    "def check_hf_auth():\n",
    "    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ç¢ºèª\n",
    "    hf_token = os.environ.get('HF_TOKEN') or os.environ.get('HUGGINGFACE_HUB_TOKEN')\n",
    "    \n",
    "    if hf_token:\n",
    "        print(f\"  âœ… HF_TOKEN: è¨­å®šæ¸ˆã¿ (***{hf_token[-4:]})\")\n",
    "        \n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³ã®æœ‰åŠ¹æ€§ãƒ†ã‚¹ãƒˆ\n",
    "        try:\n",
    "            headers = {'Authorization': f'Bearer {hf_token}'}\n",
    "            response = requests.get(\"https://huggingface.co/api/whoami\", \n",
    "                                  headers=headers, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                user_info = response.json()\n",
    "                print(f\"  âœ… ãƒˆãƒ¼ã‚¯ãƒ³æœ‰åŠ¹: ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user_info.get('name', 'unknown')}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"  âŒ ãƒˆãƒ¼ã‚¯ãƒ³ç„¡åŠ¹: {response.status_code}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ ãƒˆãƒ¼ã‚¯ãƒ³ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"  âš ï¸ HF_TOKEN: æœªè¨­å®šï¼ˆãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ä¸è¦ï¼‰\")\n",
    "        return True\n",
    "\n",
    "auth_ok = check_hf_auth()\n",
    "\n",
    "# 5. ç°¡å˜ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ\n",
    "print(\"\\nğŸ§ª ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ...\")\n",
    "def test_dataset_loading():\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        print(\"  ğŸ”„ SQuADãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæœ€å°ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã‚’ãƒ†ã‚¹ãƒˆä¸­...\")\n",
    "        \n",
    "        # æœ€å°ã‚µãƒ³ãƒ—ãƒ«ã§ãƒ†ã‚¹ãƒˆ\n",
    "        test_dataset = load_dataset(\"squad\", split=\"validation[:3]\", \n",
    "                                  cache_dir=\"/tmp/hf_test_cache\")\n",
    "        print(f\"  âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸ: {len(test_dataset)} ã‚µãƒ³ãƒ—ãƒ«èª­ã¿è¾¼ã¿\")\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ç¢ºèª\n",
    "        if len(test_dataset) > 0:\n",
    "            sample = test_dataset[0]\n",
    "            print(f\"  ğŸ“‹ ãƒ‡ãƒ¼ã‚¿æ§‹é€ : {list(sample.keys())}\")\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"  âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {error_msg[:80]}...\")\n",
    "        \n",
    "        # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°åˆ†æ\n",
    "        if \"ConnectionError\" in error_msg or \"timeout\" in error_msg.lower():\n",
    "            print(\"    ğŸ’¡ åŸå› : ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šå•é¡Œ\")\n",
    "        elif \"module\" in error_msg.lower() and \"not found\" in error_msg.lower():\n",
    "            print(\"    ğŸ’¡ åŸå› : ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å•é¡Œ\")\n",
    "        elif \"cache\" in error_msg.lower():\n",
    "            print(\"    ğŸ’¡ åŸå› : ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å•é¡Œ\")\n",
    "        elif \"permission\" in error_msg.lower():\n",
    "            print(\"    ğŸ’¡ åŸå› : ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã®å•é¡Œ\")\n",
    "        else:\n",
    "            print(\"    ğŸ’¡ åŸå› : ãã®ä»–ï¼ˆè©³ç´°ã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¢ºèªï¼‰\")\n",
    "            \n",
    "        return False\n",
    "\n",
    "test_success = test_dataset_loading()\n",
    "\n",
    "# 6. è¨ºæ–­çµæœã¨ãŠã™ã™ã‚è§£æ±ºç­–\n",
    "print(f\"\\nğŸ¯ è¨ºæ–­çµæœã¨ãŠã™ã™ã‚è§£æ±ºç­–\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if not network_ok:\n",
    "    print(\"âŒ å•é¡Œ: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶š\")\n",
    "    print(\"   è§£æ±ºç­–:\")\n",
    "    print(\"   1. ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèª\")\n",
    "    print(\"   2. ãƒ—ãƒ­ã‚­ã‚·è¨­å®šãŒå¿…è¦ãªå ´åˆã¯è¨­å®š\")\n",
    "    print(\"   3. VPNæ¥ç¶šã‚’ç¢ºèªï¼ˆåˆ¶é™ãŒã‚ã‚‹å ´åˆï¼‰\")\n",
    "\n",
    "if package_versions.get('datasets') is None:\n",
    "    print(\"âŒ å•é¡Œ: datasetsãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\")\n",
    "    print(\"   è§£æ±ºç­–: !pip install datasets==3.1.0\")\n",
    "\n",
    "if package_versions.get('transformers') is None:\n",
    "    print(\"âŒ å•é¡Œ: transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\")  \n",
    "    print(\"   è§£æ±ºç­–: !pip install transformers==4.46.2\")\n",
    "\n",
    "if not test_success:\n",
    "    print(\"âŒ å•é¡Œ: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å¤±æ•—\")\n",
    "    print(\"   è§£æ±ºç­–ï¼ˆé †ç•ªã«è©¦è¡Œï¼‰:\")\n",
    "    print(\"   1. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ï¼ˆä¸‹è¨˜ã®ä¿®æ­£ã‚»ãƒ«ã‚’å®Ÿè¡Œï¼‰\")\n",
    "    print(\"   2. ä¾å­˜é–¢ä¿‚å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\")\n",
    "    print(\"   3. ãƒ©ãƒ³ã‚¿ã‚¤ãƒ å†èµ·å‹•\")\n",
    "    print(\"   4. ä»£æ›¿ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ä½¿ç”¨\")\n",
    "\n",
    "if network_ok and test_success:\n",
    "    print(\"âœ… å…¨ã¦æ­£å¸¸: HuggingFaceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½\")\n",
    "else:\n",
    "    print(\"\\nğŸ”§ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: ä¸‹è¨˜ã®è‡ªå‹•ä¿®æ­£ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "\n",
    "print(f\"\\nğŸ“Š è¨ºæ–­å®Œäº† - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯: {'âœ…' if network_ok else 'âŒ'} | èªè¨¼: {'âœ…' if auth_ok else 'âŒ'} | ãƒ†ã‚¹ãƒˆ: {'âœ…' if test_success else 'âŒ'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7d4cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ HuggingFace ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å•é¡Œï¼šè‡ªå‹•ä¿®æ­£\n",
    "import subprocess\n",
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ğŸ”§ HUGGINGFACE ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å•é¡Œã®è‡ªå‹•ä¿®æ­£\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ä¿®æ­£1: HuggingFaceã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢\n",
    "print(\"\\nğŸ§¹ HuggingFaceã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢...\")\n",
    "def clear_hf_cache():\n",
    "    cache_paths = [\n",
    "        Path.home() / \".cache\" / \"huggingface\",\n",
    "        Path(\"/tmp\") / \"huggingface_cache\",\n",
    "        Path(\"/content\") / \".cache\" / \"huggingface\",  # Colabç”¨\n",
    "        Path(\"/tmp\") / \"hf_test_cache\"  # ãƒ†ã‚¹ãƒˆç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥\n",
    "    ]\n",
    "    \n",
    "    cleared_count = 0\n",
    "    for cache_path in cache_paths:\n",
    "        if cache_path.exists():\n",
    "            try:\n",
    "                shutil.rmtree(cache_path, ignore_errors=True)\n",
    "                print(f\"  âœ… ã‚¯ãƒªã‚¢æ¸ˆã¿: {cache_path}\")\n",
    "                cleared_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  âš ï¸ ã‚¯ãƒªã‚¢å¤±æ•—: {cache_path} ({e})\")\n",
    "        else:\n",
    "            print(f\"  ğŸ“‚ å­˜åœ¨ã—ãªã„: {cache_path}\")\n",
    "    \n",
    "    if cleared_count > 0:\n",
    "        print(f\"  ğŸ¯ {cleared_count}å€‹ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¯ãƒªã‚¢\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"  ğŸ’­ ã‚¯ãƒªã‚¢ã™ã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—\")\n",
    "        return False\n",
    "\n",
    "cache_cleared = clear_hf_cache()\n",
    "\n",
    "# ä¿®æ­£2: ä¾å­˜é–¢ä¿‚ã®å¼·åˆ¶å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "print(\"\\nğŸ“¦ é‡è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®å¼·åˆ¶å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\")\n",
    "def reinstall_hf_packages():\n",
    "    packages = [\n",
    "        \"datasets==3.1.0\",\n",
    "        \"transformers==4.46.2\", \n",
    "        \"tokenizers==0.20.3\",\n",
    "        \"huggingface-hub>=0.23.0\",\n",
    "        \"requests>=2.31.0\",\n",
    "        \"urllib3>=2.0.0\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        for package in packages:\n",
    "            print(f\"  ğŸ”„ å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­: {package}\")\n",
    "            result = subprocess.run([\n",
    "                sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                \"--upgrade\", \"--force-reinstall\", package\n",
    "            ], capture_output=True, text=True, timeout=120)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(f\"    âœ… æˆåŠŸ: {package}\")\n",
    "            else:\n",
    "                print(f\"    âŒ å¤±æ•—: {package}\")\n",
    "                print(f\"       ã‚¨ãƒ©ãƒ¼: {result.stderr[:50]}...\")\n",
    "        \n",
    "        print(\"  ğŸ¯ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return False\n",
    "\n",
    "packages_reinstalled = reinstall_hf_packages()\n",
    "\n",
    "# ä¿®æ­£3: ç’°å¢ƒå¤‰æ•°ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®šã®æœ€é©åŒ–\n",
    "print(\"\\nâš™ï¸ ç’°å¢ƒè¨­å®šã®æœ€é©åŒ–...\")\n",
    "def optimize_hf_settings():\n",
    "    # HuggingFace datasetsè¨­å®š\n",
    "    os.environ['HF_DATASETS_CACHE'] = '/tmp/hf_datasets_cache'\n",
    "    os.environ['HF_HOME'] = '/tmp/hf_home'\n",
    "    os.environ['TRANSFORMERS_CACHE'] = '/tmp/transformers_cache'\n",
    "    \n",
    "    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š\n",
    "    os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = 'False'  # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æœ‰åŠ¹\n",
    "    os.environ['HF_DATASETS_OFFLINE'] = '0'  # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰\n",
    "    \n",
    "    # Colabã§ã®ç‰¹åˆ¥è¨­å®š\n",
    "    if 'google.colab' in sys.modules:\n",
    "        os.environ['HF_DATASETS_TRUST_REMOTE_CODE'] = 'True'\n",
    "        print(\"  ğŸ”§ Google Colabç”¨è¨­å®šã‚’é©ç”¨\")\n",
    "    \n",
    "    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "    cache_dirs = ['/tmp/hf_datasets_cache', '/tmp/hf_home', '/tmp/transformers_cache']\n",
    "    for cache_dir in cache_dirs:\n",
    "        Path(cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"  âœ… ç’°å¢ƒå¤‰æ•°è¨­å®šå®Œäº†\")\n",
    "    print(\"  ğŸ“ æœ€é©åŒ–ã•ã‚ŒãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ã‚¹è¨­å®š\")\n",
    "    return True\n",
    "\n",
    "settings_optimized = optimize_hf_settings()\n",
    "\n",
    "# ä¿®æ­£4: å†ãƒ†ã‚¹ãƒˆï¼ˆæ”¹å–„ç¢ºèªï¼‰\n",
    "print(\"\\nğŸ§ª ä¿®æ­£å¾Œã®å†ãƒ†ã‚¹ãƒˆ...\")\n",
    "def retest_after_fixes():\n",
    "    try:\n",
    "        # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å†ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "        import importlib\n",
    "        if 'datasets' in sys.modules:\n",
    "            importlib.reload(sys.modules['datasets'])\n",
    "        \n",
    "        from datasets import load_dataset\n",
    "        \n",
    "        print(\"  ğŸ”„ SQuADãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†ãƒ†ã‚¹ãƒˆä¸­...\")\n",
    "        test_dataset = load_dataset(\"squad\", split=\"validation[:5]\", \n",
    "                                  cache_dir=\"/tmp/hf_datasets_cache\")\n",
    "        \n",
    "        print(f\"  âœ… å†ãƒ†ã‚¹ãƒˆæˆåŠŸï¼ {len(test_dataset)} ã‚µãƒ³ãƒ—ãƒ«èª­ã¿è¾¼ã¿\")\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèª\n",
    "        if len(test_dataset) > 0:\n",
    "            sample = test_dataset[0]\n",
    "            if 'question' in sample and 'context' in sample:\n",
    "                print(\"  ğŸ“‹ ãƒ‡ãƒ¼ã‚¿æ§‹é€ : æ­£å¸¸\")\n",
    "                print(f\"       è³ªå•ä¾‹: {sample['question'][:50]}...\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"  âš ï¸ ãƒ‡ãƒ¼ã‚¿æ§‹é€ : ä¸å®Œå…¨\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"  âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: ç©ºã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ å†ãƒ†ã‚¹ãƒˆå¤±æ•—: {str(e)[:60]}...\")\n",
    "        return False\n",
    "\n",
    "retest_success = retest_after_fixes()\n",
    "\n",
    "# çµæœã‚µãƒãƒªãƒ¼\n",
    "print(f\"\\nğŸ¯ è‡ªå‹•ä¿®æ­£çµæœ\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"ğŸ§¹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢: {'âœ…' if cache_cleared else 'âŒ'}\")\n",
    "print(f\"ğŸ“¦ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: {'âœ…' if packages_reinstalled else 'âŒ'}\")\n",
    "print(f\"âš™ï¸ ç’°å¢ƒè¨­å®šæœ€é©åŒ–: {'âœ…' if settings_optimized else 'âŒ'}\")\n",
    "print(f\"ğŸ§ª å†ãƒ†ã‚¹ãƒˆ: {'âœ…' if retest_success else 'âŒ'}\")\n",
    "\n",
    "if retest_success:\n",
    "    print(\"\\nğŸ‰ ä¿®æ­£æˆåŠŸï¼ HuggingFaceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒåˆ©ç”¨å¯èƒ½ã§ã™\")\n",
    "    print(\"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚»ãƒ«ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ ä¿®æ­£æœªå®Œäº† - è¿½åŠ å¯¾å¿œãŒå¿…è¦ã§ã™:\")\n",
    "    print(\"   1. Colabãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•\")\n",
    "    print(\"   2. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚»ãƒ«ã‹ã‚‰å†å®Ÿè¡Œ\") \n",
    "    print(\"   3. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèª\")\n",
    "    print(\"   4. ä»£æ›¿ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆåˆæˆãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’ä½¿ç”¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b2aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”„ Profile-Controlled Dataset Preparation\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, List\n",
    "\n",
    "@section_control(\"datasets\")\n",
    "@monitor_memory(\"dataset_preparation\")\n",
    "def prepare_benchmark_datasets():\n",
    "    \"\"\"ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¶å¾¡ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”„ PROFILE-CONTROLLED DATASET PREPARATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ\n",
    "    requested_datasets = PROFILE_CONFIG['datasets']\n",
    "    max_samples = PROFILE_CONFIG['max_queries']\n",
    "    \n",
    "    print(f\"ğŸ“‹ Requested datasets: {requested_datasets}\")\n",
    "    print(f\"ğŸ”¢ Max samples per dataset: {max_samples}\")\n",
    "    \n",
    "    benchmark_datasets = {}\n",
    "    \n",
    "    # HuggingFaceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿è©¦è¡Œ\n",
    "    hf_datasets_loaded = 0\n",
    "    \n",
    "    if \"squad\" in requested_datasets:\n",
    "        print(f\"\\nğŸ“¥ Attempting SQuAD dataset loading...\")\n",
    "        try:\n",
    "            from datasets import load_dataset\n",
    "            squad_dataset = load_dataset(\"squad\", split=\"validation\")\n",
    "            squad_dataset = squad_dataset.shuffle(seed=SEED)\n",
    "            \n",
    "            if len(squad_dataset) > max_samples:\n",
    "                squad_dataset = squad_dataset.select(range(max_samples))\n",
    "            \n",
    "            squad_data = {\n",
    "                'questions': squad_dataset['question'],\n",
    "                'contexts': squad_dataset['context'],\n",
    "                'answers': [ans['text'][0] if ans['text'] else \"\" for ans in squad_dataset['answers']],\n",
    "                'type': 'qa',\n",
    "                'source': 'huggingface_squad'\n",
    "            }\n",
    "            \n",
    "            benchmark_datasets['squad'] = squad_data\n",
    "            hf_datasets_loaded += 1\n",
    "            print(f\"   âœ… SQuAD loaded: {len(squad_data['questions'])} samples\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ SQuAD loading failed: {str(e)[:80]}...\")\n",
    "    \n",
    "    if \"ms_marco\" in requested_datasets:\n",
    "        print(f\"\\nğŸ“¥ Attempting MS MARCO dataset loading...\")\n",
    "        try:\n",
    "            from datasets import load_dataset\n",
    "            marco_dataset = load_dataset(\"ms_marco\", \"v1.1\", split=\"validation\")\n",
    "            marco_dataset = marco_dataset.shuffle(seed=SEED)\n",
    "            \n",
    "            if len(marco_dataset) > max_samples:\n",
    "                marco_dataset = marco_dataset.select(range(max_samples))\n",
    "            \n",
    "            marco_data = {\n",
    "                'questions': marco_dataset['query'],\n",
    "                'contexts': [\" \".join(passages) for passages in marco_dataset['passages']['passage_text']],\n",
    "                'answers': marco_dataset['answers'],\n",
    "                'type': 'qa',\n",
    "                'source': 'huggingface_ms_marco'\n",
    "            }\n",
    "            \n",
    "            benchmark_datasets['ms_marco'] = marco_data\n",
    "            hf_datasets_loaded += 1\n",
    "            print(f\"   âœ… MS MARCO loaded: {len(marco_data['questions'])} samples\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ MS MARCO loading failed: {str(e)[:80]}...\")\n",
    "    \n",
    "    # é«˜å“è³ªåˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆï¼ˆHuggingFaceå¤±æ•—æ™‚ + è¿½åŠ ç”¨ï¼‰\n",
    "    def generate_high_quality_qa_dataset(size: int, domains: List[str]) -> Dict[str, List]:\n",
    "        \"\"\"é«˜å“è³ªQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ\"\"\"\n",
    "        \n",
    "        # è³ªå•ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ\n",
    "        qa_templates = [\n",
    "            {\n",
    "                \"question\": \"What are the key principles and applications of {domain}?\",\n",
    "                \"context_template\": \"\"\"{domain} is a rapidly evolving field that combines theoretical foundations with practical applications. The field encompasses various methodologies including advanced algorithms, data processing techniques, and system architectures designed for scalability and efficiency. Key applications span across healthcare, finance, automotive, and telecommunications industries.\"\"\",\n",
    "                \"answer_template\": \"{domain} encompasses advanced computational techniques that provide significant benefits including improved efficiency, enhanced accuracy, and scalable solutions for complex real-world problems across various industries.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"How does {domain} impact modern business operations?\",\n",
    "                \"context_template\": \"\"\"{domain} has revolutionized modern business operations through automation, data-driven decision making, and process optimization. Organizations implementing {domain} solutions report significant improvements in operational efficiency, cost reduction, and enhanced customer experiences.\"\"\",\n",
    "                \"answer_template\": \"{domain} impacts modern business through automation, data-driven insights, and process optimization, leading to improved efficiency and enhanced customer experiences.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What challenges and solutions exist in {domain} implementation?\",\n",
    "                \"context_template\": \"\"\"{domain} implementation faces several challenges including technical complexity, integration difficulties, and resource requirements. However, solutions include adopting best practices, using established frameworks, and implementing gradual deployment strategies.\"\"\",\n",
    "                \"answer_template\": \"Challenges in {domain} include technical complexity and integration issues, which can be addressed through best practices, established frameworks, and gradual deployment strategies.\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        questions = []\n",
    "        contexts = []\n",
    "        answers = []\n",
    "        \n",
    "        for i in range(size):\n",
    "            domain = domains[i % len(domains)]\n",
    "            template = qa_templates[i % len(qa_templates)]\n",
    "            \n",
    "            question = template[\"question\"].format(domain=domain.title())\n",
    "            context = template[\"context_template\"].format(domain=domain.title())\n",
    "            answer = template[\"answer_template\"].format(domain=domain.title())\n",
    "            \n",
    "            questions.append(question)\n",
    "            contexts.append(context)\n",
    "            answers.append(answer)\n",
    "        \n",
    "        return {\n",
    "            'questions': questions,\n",
    "            'contexts': contexts,\n",
    "            'answers': answers,\n",
    "            'type': 'synthetic_qa_enhanced',\n",
    "            'source': 'high_quality_synthetic'\n",
    "        }\n",
    "    \n",
    "    # åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ\n",
    "    synthetic_domains = [\n",
    "        \"artificial intelligence\", \"machine learning\", \"data science\", \n",
    "        \"computer science\", \"natural language processing\", \"computer vision\",\n",
    "        \"robotics\", \"cybersecurity\", \"cloud computing\", \"software engineering\"\n",
    "    ]\n",
    "    \n",
    "    if \"squad_fallback\" in requested_datasets or \"synthetic\" in requested_datasets:\n",
    "        print(f\"\\nğŸ”„ Generating high-quality synthetic dataset...\")\n",
    "        synthetic_data = generate_high_quality_qa_dataset(max_samples, synthetic_domains)\n",
    "        benchmark_datasets['squad_fallback'] = synthetic_data\n",
    "        print(f\"   âœ… Synthetic dataset: {len(synthetic_data['questions'])} samples\")\n",
    "    \n",
    "    if \"test_fallback\" in requested_datasets:\n",
    "        print(f\"\\nğŸ”„ Generating test dataset...\")\n",
    "        test_data = generate_high_quality_qa_dataset(min(50, max_samples//10), synthetic_domains[:3])\n",
    "        benchmark_datasets['test_fallback'] = test_data\n",
    "        print(f\"   âœ… Test dataset: {len(test_data['questions'])} samples\")\n",
    "    \n",
    "    # çµæœã‚µãƒãƒªãƒ¼\n",
    "    print(f\"\\nğŸ“Š Dataset Preparation Summary:\")\n",
    "    print(f\"   ğŸŒ HuggingFace datasets loaded: {hf_datasets_loaded}\")\n",
    "    print(f\"   ğŸ§  Synthetic datasets generated: {len(benchmark_datasets) - hf_datasets_loaded}\")\n",
    "    print(f\"   ğŸ“š Total datasets available: {len(benchmark_datasets)}\")\n",
    "    \n",
    "    total_samples = sum(len(data['questions']) for data in benchmark_datasets.values())\n",
    "    print(f\"   ğŸ“„ Total QA pairs: {total_samples}\")\n",
    "    \n",
    "    for name, data in benchmark_datasets.items():\n",
    "        print(f\"      ğŸ“‹ {name}: {len(data['questions'])} samples ({data['source']})\")\n",
    "    \n",
    "    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜\n",
    "    save_checkpoint(benchmark_datasets, \"datasets\", {\n",
    "        \"profile\": EXECUTION_PROFILE,\n",
    "        \"total_datasets\": len(benchmark_datasets),\n",
    "        \"total_samples\": total_samples,\n",
    "        \"datasets_info\": {name: {\"size\": len(data['questions']), \"source\": data['source']} \n",
    "                         for name, data in benchmark_datasets.items()}\n",
    "    })\n",
    "    \n",
    "    return benchmark_datasets\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Ÿè¡Œ\n",
    "benchmark_datasets = prepare_benchmark_datasets() or {}\n",
    "\n",
    "if benchmark_datasets:\n",
    "    print(f\"\\nâœ… Dataset preparation completed successfully!\")\n",
    "    print(f\"ğŸ¯ Ready for RAG benchmark with {len(benchmark_datasets)} datasets\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Dataset preparation failed!\")\n",
    "    print(f\"ğŸ’¡ Please check HuggingFace connection or use synthetic data only\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22da430",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ HuggingFaceãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼šå®Œå…¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰\n",
    "\n",
    "### ğŸ’¡ å•é¡Œè§£æ±ºãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ\n",
    "\n",
    "```\n",
    "HuggingFaceãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—\n",
    "    â†“\n",
    "â“ è¨ºæ–­ã‚»ãƒ«å®Ÿè¡Œ â†’ åŸå› ç‰¹å®š\n",
    "    â†“\n",
    "ğŸ”§ è‡ªå‹•ä¿®æ­£ã‚»ãƒ« â†’ å•é¡Œè§£æ±ºè©¦è¡Œ  \n",
    "    â†“\n",
    "âœ… æˆåŠŸ? â†’ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ç¶™ç¶š\n",
    "    â†“\n",
    "âŒ å¤±æ•—? â†’ ä»£æ›¿ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ä½¿ç”¨\n",
    "```\n",
    "\n",
    "### ğŸ” ä¸»ãªå¤±æ•—åŸå› ã¨å¯¾ç­–\n",
    "\n",
    "| åŸå›  | ç—‡çŠ¶ | è§£æ±ºç­– |\n",
    "|------|------|--------|\n",
    "| **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶š** | `ConnectionError`, `Timeout` | ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šç¢ºèªã€ãƒ—ãƒ­ã‚­ã‚·è¨­å®š |\n",
    "| **ä¾å­˜é–¢ä¿‚ç«¶åˆ** | `ModuleNotFoundError`, ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼ | ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³çµ±ä¸€ |\n",
    "| **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç ´æ** | ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­æ–­ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ©ãƒ¼ | ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã€ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½¿ç”¨ |\n",
    "| **èªè¨¼å•é¡Œ** | `401 Unauthorized`, `403 Forbidden` | HF_TOKENã®è¨­å®šç¢ºèªï¼ˆé€šå¸¸ã¯ä¸è¦ï¼‰ |\n",
    "| **Colabç’°å¢ƒå›ºæœ‰** | ãƒ©ãƒ³ã‚¿ã‚¤ãƒ å¾Œã®è¨­å®šæ¶ˆå¤± | ç’°å¢ƒå¤‰æ•°å†è¨­å®šã€ãƒ©ãƒ³ã‚¿ã‚¤ãƒ å†èµ·å‹• |\n",
    "| **ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸ä¸€è‡´** | `datasets`ã¨`transformers`ã®ç«¶åˆ | äº’æ›ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«çµ±ä¸€ |\n",
    "\n",
    "### âš¡ ç·Šæ€¥å¯¾å¿œæ‰‹é †\n",
    "\n",
    "1. **å³åº§ã«è©¦è¡Œ**ï¼šä¸Šè¨˜ã®è¨ºæ–­ãƒ»ä¿®æ­£ã‚»ãƒ«ã‚’å®Ÿè¡Œ\n",
    "2. **åŸºæœ¬ç¢ºèª**ï¼šã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã€ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«çŠ¶æ³\n",
    "3. **å¼·åˆ¶ä¿®æ­£**ï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ + ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "4. **ç’°å¢ƒãƒªã‚»ãƒƒãƒˆ**ï¼šColabãƒ©ãƒ³ã‚¿ã‚¤ãƒ å†èµ·å‹•\n",
    "5. **ä»£æ›¿æ‰‹æ®µ**ï¼šåˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å®Ÿé¨“ç¶™ç¶š\n",
    "\n",
    "### ğŸ¯ Colabå›ºæœ‰ã®æ³¨æ„ç‚¹\n",
    "\n",
    "- **ãƒ©ãƒ³ã‚¿ã‚¤ãƒ å†èµ·å‹•å¾Œ**ï¼šç’°å¢ƒå¤‰æ•°ã¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒåˆæœŸåŒ–ã•ã‚Œã‚‹\n",
    "- **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ¶é™**ï¼šåœ°åŸŸãƒ»æ™‚é–“ã«ã‚ˆã£ã¦ã‚¢ã‚¯ã‚»ã‚¹ãŒä¸å®‰å®š\n",
    "- **ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®¹é‡**ï¼š`/tmp`ã®ä½¿ç”¨é‡ã«æ³¨æ„ï¼ˆå®šæœŸã‚¯ãƒªã‚¢æ¨å¥¨ï¼‰\n",
    "- **GPUä½¿ç”¨æ™‚**ï¼šãƒ¡ãƒ¢ãƒªä¸è¶³ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¤±æ•—ã™ã‚‹å ´åˆ\n",
    "\n",
    "### ğŸ“‹ æ¨å¥¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ\n",
    "\n",
    "- [ ] è¨ºæ–­ã‚»ãƒ«å®Ÿè¡Œã§å•é¡Œç‰¹å®š\n",
    "- [ ] è‡ªå‹•ä¿®æ­£ã‚»ãƒ«å®Ÿè¡Œã§è§£æ±ºè©¦è¡Œ  \n",
    "- [ ] ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³çµ±ä¸€ç¢ºèª\n",
    "- [ ] ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šå®‰å®šæ€§ç¢ºèª\n",
    "- [ ] ä»£æ›¿ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æº–å‚™å®Œäº†\n",
    "\n",
    "### ğŸš€ æœ€é©åŒ–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹\n",
    "\n",
    "1. **äº‹å‰æº–å‚™**ï¼šé‡è¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥\n",
    "2. **ãƒãƒƒãƒå‡¦ç†**ï¼šå¤§é‡ãƒ‡ãƒ¼ã‚¿ã¯å°åˆ†ã‘ã—ã¦å‡¦ç†\n",
    "3. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**ï¼šãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ã‚’å¸¸ã«æº–å‚™\n",
    "4. **ç›£è¦–**ï¼šãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—ã¨ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®ç¢ºèª\n",
    "5. **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**ï¼šè¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’æº–å‚™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff1c39",
   "metadata": {},
   "source": [
    "## âš¡ Step 5: æ¯”è¼ƒå®Ÿé¨“å®Ÿè¡Œ\n",
    "\n",
    "çµ±è¨ˆçš„ã«æœ‰æ„ãªRAGã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½æ¯”è¼ƒå®Ÿé¨“ã‚’å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7dbe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš¡ RAGã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½æ¯”è¼ƒå®Ÿé¨“\n",
    "import psutil\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "print(\"âš¡ RAG Systems Performance Benchmark\")\n",
    "print(\"ğŸ¯ Statistical significance testing with multiple datasets\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å–å¾—\"\"\"\n",
    "    try:\n",
    "        process = psutil.Process()\n",
    "        memory_info = process.memory_info()\n",
    "        return {\n",
    "            'rss_mb': memory_info.rss / 1024 / 1024,\n",
    "            'gpu_mb': torch.cuda.memory_allocated() / 1024 / 1024 if torch.cuda.is_available() else 0\n",
    "        }\n",
    "    except:\n",
    "        return {'rss_mb': 0, 'gpu_mb': 0}\n",
    "\n",
    "@section_control(\n",
    "    name=\"RAG_BENCHMARK_EXECUTION\", \n",
    "    priority=5,\n",
    "    memory_threshold_mb=4000,\n",
    "    checkpoint_frequency=10\n",
    ")\n",
    "@monitor_memory(threshold_mb=3000, critical_mb=5000)\n",
    "def execute_comprehensive_benchmark():\n",
    "    \"\"\"åŒ…æ‹¬çš„RAGãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é§†å‹•ï¼‰\"\"\"\n",
    "    \n",
    "    print(\"âš¡ RAG SYSTEMS PERFORMANCE BENCHMARK - PROFILE DRIVEN\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"ğŸ¯ Active Profile: {eval_config.profile}\")\n",
    "    print(f\"ğŸ“Š Data Sizes: {eval_config.sample_sizes}\")\n",
    "    print(f\"ğŸ§  Systems: {eval_config.systems}\")\n",
    "    print(f\"ğŸ“š Datasets: {eval_config.datasets}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    import psutil\n",
    "    import gc\n",
    "    from collections import defaultdict\n",
    "    import numpy as np\n",
    "    import time\n",
    "    import math\n",
    "    import re\n",
    "    from collections import Counter\n",
    "    import pandas as pd\n",
    "    \n",
    "    def compute_em_f1(predictions, references):\n",
    "        \"\"\"EM/F1ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰\"\"\"\n",
    "        if not predictions or not references:\n",
    "            return {\"exact_match\": 0.0, \"f1\": 0.0}\n",
    "        \n",
    "        em_count = 0\n",
    "        f1_scores = []\n",
    "        \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            # Exact Match\n",
    "            if str(pred).strip().lower() == str(ref).strip().lower():\n",
    "                em_count += 1\n",
    "            \n",
    "            # F1 Score (token overlap)\n",
    "            pred_tokens = set(str(pred).lower().split())\n",
    "            ref_tokens = set(str(ref).lower().split())\n",
    "            \n",
    "            if not pred_tokens and not ref_tokens:\n",
    "                f1_scores.append(1.0)\n",
    "            elif not pred_tokens or not ref_tokens:\n",
    "                f1_scores.append(0.0)\n",
    "            else:\n",
    "                common = len(pred_tokens & ref_tokens)\n",
    "                precision = common / len(pred_tokens) if pred_tokens else 0\n",
    "                recall = common / len(ref_tokens) if ref_tokens else 0\n",
    "                \n",
    "                if precision + recall > 0:\n",
    "                    f1 = 2 * precision * recall / (precision + recall)\n",
    "                else:\n",
    "                    f1 = 0.0\n",
    "                f1_scores.append(f1)\n",
    "        \n",
    "        return {\n",
    "            \"exact_match\": em_count / len(predictions),\n",
    "            \"f1\": np.mean(f1_scores)\n",
    "        }\n",
    "    \n",
    "    # å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯\n",
    "    if 'functionality_test_result' not in globals() or not functionality_test_result.get('success'):\n",
    "        print(\"âŒ RAG systems not ready - please run functionality test first\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    rag_systems = functionality_test_result.get('global_rag_systems', {})\n",
    "    if not rag_systems:\n",
    "        print(\"âŒ No operational RAG systems available\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™\n",
    "    profile_datasets = {\n",
    "        'demo': ['synthetic_qa'],\n",
    "        'research': ['squad', 'ms_marco', 'synthetic_qa'],\n",
    "        'presentation': ['squad', 'synthetic_qa'],\n",
    "        'insightspike_only': ['synthetic_qa']\n",
    "    }\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç¢ºèªï¼ˆå­˜åœ¨ã™ã‚‹ã‚‚ã®ã®ã¿ä½¿ç”¨ï¼‰\n",
    "    available_datasets = {}\n",
    "    target_datasets = profile_datasets.get(eval_config.profile, ['synthetic_qa'])\n",
    "    \n",
    "    for dataset_name in target_datasets:\n",
    "        if dataset_name in globals() and dataset_name == 'synthetic_qa':\n",
    "            # åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ\n",
    "            available_datasets['synthetic_qa'] = generate_synthetic_dataset(eval_config.sample_sizes[0])\n",
    "        # ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯çœç•¥ï¼ˆHuggingFaceãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ï¼‰\n",
    "    \n",
    "    if not available_datasets:\n",
    "        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡å˜ãªåˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "        available_datasets['fallback'] = {\n",
    "            'questions': [\"What is AI?\", \"How does ML work?\", \"Explain neural networks\"],\n",
    "            'contexts': [\"AI is artificial intelligence\", \"ML uses data to learn\", \"Neural networks have layers\"],\n",
    "            'answers': [\"Artificial intelligence\", \"Machine learning\", \"Neural networks\"]\n",
    "        }\n",
    "    \n",
    "    print(f\"ğŸ“š Using datasets: {list(available_datasets.keys())}\")\n",
    "    \n",
    "    benchmark_results = []\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥å®Ÿé¨“è¨­å®š\n",
    "    profile_configs = {\n",
    "        'demo': {\n",
    "            'data_sizes': [min(50, eval_config.sample_sizes[0])],\n",
    "            'batch_size': 10,\n",
    "            'max_queries': 20\n",
    "        },\n",
    "        'research': {\n",
    "            'data_sizes': eval_config.sample_sizes,\n",
    "            'batch_size': 50,\n",
    "            'max_queries': 1000\n",
    "        },\n",
    "        'presentation': {\n",
    "            'data_sizes': eval_config.sample_sizes[:2],  # æœ€åˆã®2ã‚µã‚¤ã‚ºã®ã¿\n",
    "            'batch_size': 25,\n",
    "            'max_queries': 100\n",
    "        },\n",
    "        'insightspike_only': {\n",
    "            'data_sizes': [min(30, eval_config.sample_sizes[0])],\n",
    "            'batch_size': 5,\n",
    "            'max_queries': 15\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config = profile_configs.get(eval_config.profile, profile_configs['demo'])\n",
    "    data_sizes = config['data_sizes']\n",
    "    batch_size = config['batch_size']\n",
    "    max_queries = config['max_queries']\n",
    "    \n",
    "    total_experiments = len(rag_systems) * len(available_datasets) * len(data_sizes)\n",
    "    current_experiment = 0\n",
    "    \n",
    "    print(f\"ğŸ”¬ Benchmark Configuration:\")\n",
    "    print(f\"  ğŸ“Š Systems: {len(rag_systems)} ({', '.join(rag_systems.keys())})\")\n",
    "    print(f\"  ğŸ“š Datasets: {len(available_datasets)}\")\n",
    "    print(f\"  ğŸ“ Data sizes: {data_sizes}\")\n",
    "    print(f\"  ğŸ” Max queries per test: {max_queries}\")\n",
    "    print(f\"  ğŸ“¦ Batch size: {batch_size}\")\n",
    "    print(f\"  ğŸ§ª Total experiments: {total_experiments}\")\n",
    "    \n",
    "    benchmark_start_time = time.time()\n",
    "    \n",
    "    for dataset_name, dataset in available_datasets.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ğŸ“Š Dataset: {dataset_name.upper()}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        questions = dataset['questions']\n",
    "        contexts = dataset['contexts']\n",
    "        answers = dataset['answers']\n",
    "        \n",
    "        for data_size in data_sizes:\n",
    "            actual_size = min(data_size, len(questions))\n",
    "            \n",
    "            if actual_size < 3:  # æœ€å°ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚º\n",
    "                print(f\"âš ï¸ Skipping size {data_size} (insufficient data: {actual_size})\")\n",
    "                continue\n",
    "            \n",
    "            size_questions = questions[:actual_size]\n",
    "            size_contexts = contexts[:actual_size] \n",
    "            size_answers = answers[:actual_size]\n",
    "            \n",
    "            # ã‚¯ã‚¨ãƒªæ•°åˆ¶é™\n",
    "            test_queries = min(len(size_questions), max_queries)\n",
    "            test_questions = size_questions[:test_queries]\n",
    "            test_answers = size_answers[:test_queries]\n",
    "            \n",
    "            print(f\"\\nğŸ“ Testing with {actual_size:,} documents, {test_queries} queries\")\n",
    "            \n",
    "            for system_name, system in rag_systems.items():\n",
    "                current_experiment += 1\n",
    "                progress = current_experiment / total_experiments * 100\n",
    "                \n",
    "                print(f\"\\nğŸ”§ [{current_experiment}/{total_experiments}] {system_name} | {dataset_name} | {actual_size:,} ({progress:.1f}%)\")\n",
    "                \n",
    "                # ãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³\n",
    "                memory_before = get_memory_usage()\n",
    "                \n",
    "                try:\n",
    "                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\n",
    "                    print(f\"  ğŸ“š Building index...\")\n",
    "                    build_start = time.time()\n",
    "                    build_time = system.build_index(size_contexts)\n",
    "                    build_elapsed = time.time() - build_start\n",
    "                    \n",
    "                    if build_time < 0:\n",
    "                        print(f\"    âŒ Index building failed\")\n",
    "                        continue\n",
    "                    \n",
    "                    memory_after_build = get_memory_usage()\n",
    "                    index_memory = memory_after_build['rss_mb'] - memory_before['rss_mb']\n",
    "                    \n",
    "                    print(f\"    âœ… Index built in {build_elapsed:.2f}s (+{index_memory:.1f}MB)\")\n",
    "                    \n",
    "                    # ã‚¯ã‚¨ãƒªå®Ÿè¡Œ\n",
    "                    print(f\"  ğŸ” Processing {test_queries} queries...\")\n",
    "                    query_times = []\n",
    "                    predictions = []\n",
    "                    references = []\n",
    "                    successful_queries = 0\n",
    "                    \n",
    "                    for i, (question, true_answer) in enumerate(zip(test_questions, test_answers)):\n",
    "                        try:\n",
    "                            start_time = time.time()\n",
    "                            response, query_time = system.query(question)\n",
    "                            actual_query_time = time.time() - start_time\n",
    "                            \n",
    "                            if response and actual_query_time >= 0:\n",
    "                                query_times.append(actual_query_time)\n",
    "                                predictions.append(response)\n",
    "                                references.append(true_answer)\n",
    "                                successful_queries += 1\n",
    "                            \n",
    "                            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºï¼ˆãƒãƒƒãƒã”ã¨ï¼‰\n",
    "                            if (i + 1) % batch_size == 0:\n",
    "                                batch_progress = (i + 1) / test_queries * 100\n",
    "                                avg_time = np.mean(query_times) * 1000 if query_times else 0\n",
    "                                print(f\"    ğŸ“Š Progress: {batch_progress:.0f}% (avg: {avg_time:.1f}ms/query)\")\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            print(f\"      âš ï¸ Query {i + 1} failed: {str(e)[:30]}...\")\n",
    "                    \n",
    "                    # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—\n",
    "                    em_f1_scores = {\"exact_match\": 0.0, \"f1\": 0.0}\n",
    "                    if predictions and references:\n",
    "                        try:\n",
    "                            em_f1_scores = compute_em_f1(predictions, references)\n",
    "                        except Exception as e:\n",
    "                            print(f\"    âš ï¸ EM/F1 calculation failed: {e}\")\n",
    "                    \n",
    "                    # æœ€çµ‚ãƒ¡ãƒ¢ãƒªæ¸¬å®š\n",
    "                    memory_final = get_memory_usage()\n",
    "                    total_memory = memory_final['rss_mb'] - memory_before['rss_mb']\n",
    "                    \n",
    "                    # çµæœè¨ˆç®—\n",
    "                    avg_query_time = np.mean(query_times) if query_times else 0\n",
    "                    success_rate = successful_queries / test_queries if test_queries > 0 else 0\n",
    "                    \n",
    "                    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã‚³ã‚¹ãƒˆæ¨å®š\n",
    "                    cost_per_query = {\n",
    "                        'demo': 0.0005,\n",
    "                        'research': 0.002,\n",
    "                        'presentation': 0.001,\n",
    "                        'insightspike_only': 0.0001\n",
    "                    }.get(eval_config.profile, 0.001)\n",
    "                    \n",
    "                    estimated_cost = cost_per_query * test_queries if system_name != \"BM25+LLM\" else cost_per_query * 0.5 * test_queries\n",
    "                    \n",
    "                    # çµæœè¨˜éŒ²\n",
    "                    result = {\n",
    "                        'profile': eval_config.profile,\n",
    "                        'dataset': dataset_name,\n",
    "                        'system': system_name,\n",
    "                        'data_size': actual_size,\n",
    "                        'num_documents': len(size_contexts),\n",
    "                        'num_questions': test_queries,\n",
    "                        'successful_queries': successful_queries,\n",
    "                        'success_rate': success_rate,\n",
    "                        'build_time_s': build_time,\n",
    "                        'avg_query_time_ms': avg_query_time * 1000,\n",
    "                        'memory_rss_mb': total_memory,\n",
    "                        'index_memory_mb': index_memory,\n",
    "                        'em_score': em_f1_scores.get(\"exact_match\", 0.0),\n",
    "                        'f1_score': em_f1_scores.get(\"f1\", 0.0),\n",
    "                        'cost_per_query': cost_per_query,\n",
    "                        'total_cost_usd': estimated_cost,\n",
    "                        'timestamp': time.time()\n",
    "                    }\n",
    "                    \n",
    "                    benchmark_results.append(result)\n",
    "                    \n",
    "                    print(f\"    âœ… {system_name} completed:\")\n",
    "                    print(f\"      â±ï¸ Build time: {build_time:.2f}s\")\n",
    "                    print(f\"      â±ï¸ Avg query time: {avg_query_time*1000:.1f}ms\") \n",
    "                    print(f\"      ğŸ’¾ Memory: {total_memory:.1f}MB\")\n",
    "                    print(f\"      ğŸ¯ Success rate: {success_rate:.1%}\")\n",
    "                    print(f\"      ğŸ“Š EM/F1: {em_f1_scores.get('exact_match', 0):.3f}/{em_f1_scores.get('f1', 0):.3f}\")\n",
    "                    print(f\"      ğŸ’° Cost: ${estimated_cost:.4f}\")\n",
    "                    \n",
    "                    # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "                    cleanup_section()\n",
    "                    \n",
    "                    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ï¼ˆä¸€å®šé–“éš”ã§ï¼‰\n",
    "                    if current_experiment % 5 == 0:\n",
    "                        save_checkpoint(\"benchmark_progress\", {\n",
    "                            'profile': eval_config.profile,\n",
    "                            'completed_experiments': current_experiment,\n",
    "                            'total_experiments': total_experiments,\n",
    "                            'results_count': len(benchmark_results),\n",
    "                            'current_system': system_name,\n",
    "                            'current_dataset': dataset_name\n",
    "                        })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    âŒ {system_name} benchmark failed: {str(e)[:60]}...\")\n",
    "                    \n",
    "                    # ã‚¨ãƒ©ãƒ¼çµæœã‚‚è¨˜éŒ²\n",
    "                    error_result = {\n",
    "                        'profile': eval_config.profile,\n",
    "                        'dataset': dataset_name,\n",
    "                        'system': system_name,\n",
    "                        'data_size': actual_size,\n",
    "                        'error': str(e)[:100],\n",
    "                        'success_rate': 0.0,\n",
    "                        'timestamp': time.time()\n",
    "                    }\n",
    "                    benchmark_results.append(error_result)\n",
    "    \n",
    "    total_time = time.time() - benchmark_start_time\n",
    "    print(f\"\\nğŸ‰ BENCHMARK COMPLETED! (Profile: {eval_config.profile})\")\n",
    "    print(f\"â±ï¸ Total time: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
    "    print(f\"ğŸ“Š Results collected: {len(benchmark_results)}\")\n",
    "    \n",
    "    # çµæœã‚’DataFrameã«å¤‰æ›\n",
    "    results_df = pd.DataFrame(benchmark_results)\n",
    "    \n",
    "    # æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜\n",
    "    save_checkpoint(\"benchmark_complete\", {\n",
    "        'profile': eval_config.profile,\n",
    "        'total_time_s': total_time,\n",
    "        'results_count': len(benchmark_results),\n",
    "        'systems_tested': list(rag_systems.keys()),\n",
    "        'datasets_used': list(available_datasets.keys()),\n",
    "        'data_sizes': data_sizes,\n",
    "        'completion_timestamp': time.time()\n",
    "    })\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def generate_synthetic_dataset(size):\n",
    "    \"\"\"ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ\"\"\"\n",
    "    import random\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒ‡ãƒ¼ã‚¿è¤‡é›‘åº¦\n",
    "    complexity_configs = {\n",
    "        'demo': {'questions': 10, 'contexts': 10, 'complexity': 'simple'},\n",
    "        'research': {'questions': size, 'contexts': size, 'complexity': 'complex'},\n",
    "        'presentation': {'questions': min(100, size), 'contexts': min(100, size), 'complexity': 'medium'},\n",
    "        'insightspike_only': {'questions': 20, 'contexts': 20, 'complexity': 'simple'}\n",
    "    }\n",
    "    \n",
    "    config = complexity_configs.get(eval_config.profile, complexity_configs['demo'])\n",
    "    \n",
    "    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ\n",
    "    simple_templates = [\n",
    "        (\"What is {topic}?\", \"The definition of {topic}\", \"{topic} is an important concept in technology.\"),\n",
    "        (\"How does {topic} work?\", \"The process of {topic}\", \"{topic} works through specific mechanisms.\"),\n",
    "        (\"Why is {topic} important?\", \"The importance of {topic}\", \"{topic} is important because of its applications.\")\n",
    "    ]\n",
    "    \n",
    "    topics = [\"AI\", \"machine learning\", \"neural networks\", \"deep learning\", \"data science\", \n",
    "              \"natural language processing\", \"computer vision\", \"robotics\", \"automation\"]\n",
    "    \n",
    "    questions, answers, contexts = [], [], []\n",
    "    \n",
    "    for i in range(config['questions']):\n",
    "        topic = random.choice(topics)\n",
    "        template = random.choice(simple_templates)\n",
    "        \n",
    "        question = template[0].format(topic=topic)\n",
    "        answer = template[1].format(topic=topic)\n",
    "        context = template[2].format(topic=topic)\n",
    "        \n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        contexts.append(context)\n",
    "    \n",
    "    return {\n",
    "        'questions': questions,\n",
    "        'answers': answers,\n",
    "        'contexts': contexts\n",
    "    }\n",
    "\n",
    "# ã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ\n",
    "print(\"ğŸš€ Starting profile-driven RAG benchmark...\")\n",
    "comprehensive_df = execute_comprehensive_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f868fe3a",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Step 6: çµæœå¯è¦–åŒ–\n",
    "\n",
    "è«–æ–‡å“è³ªã®åˆ†æãƒ»ã‚°ãƒ©ãƒ•ä½œæˆã¨RAGã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½æ¯”è¼ƒçµæœã®å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b44b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ˆ åŒ…æ‹¬çš„çµæœå¯è¦–åŒ–ã¨åˆ†æ\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ğŸ“ˆ Creating Comprehensive Visualization Dashboard\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# å¯è¦–åŒ–ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams.update({\n",
    "    'font.size': 11,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'figure.titlesize': 16\n",
    "})\n",
    "\n",
    "def create_performance_dashboard(df):\n",
    "    \"\"\"æ€§èƒ½ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ\"\"\"\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"âš ï¸ No data to visualize\")\n",
    "        return\n",
    "    \n",
    "    # 6ã¤ã®ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('ğŸ” RAG Systems Performance Comparison Dashboard', \n",
    "                 fontsize=18, fontweight='bold', y=0.98)\n",
    "    \n",
    "    systems = df['system'].unique()\n",
    "    datasets = df['dataset'].unique()\n",
    "    \n",
    "    # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "    system_colors = dict(zip(systems, sns.color_palette(\"Set2\", len(systems))))\n",
    "    \n",
    "    # 1. å¹³å‡ã‚¯ã‚¨ãƒªæ™‚é–“ (å·¦ä¸Š)\n",
    "    ax1 = axes[0, 0]\n",
    "    query_time_data = df.groupby('system')['avg_query_time_ms'].agg(['mean', 'std']).reset_index()\n",
    "    bars1 = ax1.bar(query_time_data['system'], query_time_data['mean'],\n",
    "                   yerr=query_time_data['std'], capsize=5,\n",
    "                   color=[system_colors[sys] for sys in query_time_data['system']])\n",
    "    ax1.set_title('â±ï¸ Average Query Time', fontweight='bold', pad=15)\n",
    "    ax1.set_ylabel('Time (ms)')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
    "    for bar, mean_val, std_val in zip(bars1, query_time_data['mean'], query_time_data['std']):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std_val + 5,\n",
    "                f'{mean_val:.0f}Â±{std_val:.0f}ms', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 2. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (ä¸­å¤®ä¸Š)\n",
    "    ax2 = axes[0, 1]\n",
    "    memory_data = df.groupby('system')['total_memory_mb'].agg(['mean', 'std']).reset_index()\n",
    "    bars2 = ax2.bar(memory_data['system'], memory_data['mean'],\n",
    "                   yerr=memory_data['std'], capsize=5,\n",
    "                   color=[system_colors[sys] for sys in memory_data['system']])\n",
    "    ax2.set_title('ğŸ’¾ Memory Usage', fontweight='bold', pad=15)\n",
    "    ax2.set_ylabel('Memory (MB)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, mean_val, std_val in zip(bars2, memory_data['mean'], memory_data['std']):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std_val + 2,\n",
    "                f'{mean_val:.0f}Â±{std_val:.0f}MB', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 3. æ¨å®šç²¾åº¦ (å³ä¸Š)\n",
    "    ax3 = axes[0, 2]\n",
    "    accuracy_data = df.groupby('system')['estimated_accuracy'].agg(['mean', 'std']).reset_index()\n",
    "    bars3 = ax3.bar(accuracy_data['system'], accuracy_data['mean'],\n",
    "                   yerr=accuracy_data['std'], capsize=5,\n",
    "                   color=[system_colors[sys] for sys in accuracy_data['system']])\n",
    "    ax3.set_title('ğŸ¯ Estimated Accuracy', fontweight='bold', pad=15)\n",
    "    ax3.set_ylabel('Accuracy Score')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.set_ylim(0.4, 1.0)\n",
    "    \n",
    "    for bar, mean_val, std_val in zip(bars3, accuracy_data['mean'], accuracy_data['std']):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std_val + 0.02,\n",
    "                f'{mean_val:.3f}Â±{std_val:.3f}', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 4. æˆåŠŸç‡ (å·¦ä¸‹)\n",
    "    ax4 = axes[1, 0]\n",
    "    success_data = df.groupby('system')['success_rate'].agg(['mean', 'std']).reset_index()\n",
    "    bars4 = ax4.bar(success_data['system'], success_data['mean'],\n",
    "                   yerr=success_data['std'], capsize=5,\n",
    "                   color=[system_colors[sys] for sys in success_data['system']])\n",
    "    ax4.set_title('âœ… Success Rate', fontweight='bold', pad=15)\n",
    "    ax4.set_ylabel('Success Rate (%)')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.set_ylim(0, 1.1)\n",
    "    \n",
    "    for bar, mean_val, std_val in zip(bars4, success_data['mean'], success_data['std']):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std_val + 0.02,\n",
    "                f'{mean_val:.1%}Â±{std_val:.1%}', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 5. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰æ™‚é–“ (ä¸­å¤®ä¸‹)\n",
    "    ax5 = axes[1, 1]\n",
    "    build_data = df.groupby('system')['build_time_s'].agg(['mean', 'std']).reset_index()\n",
    "    bars5 = ax5.bar(build_data['system'], build_data['mean'],\n",
    "                   yerr=build_data['std'], capsize=5,\n",
    "                   color=[system_colors[sys] for sys in build_data['system']])\n",
    "    ax5.set_title('ğŸ—ï¸ Index Build Time', fontweight='bold', pad=15)\n",
    "    ax5.set_ylabel('Time (seconds)')\n",
    "    ax5.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, mean_val, std_val in zip(bars5, build_data['mean'], build_data['std']):\n",
    "        ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std_val + 0.1,\n",
    "                f'{mean_val:.1f}Â±{std_val:.1f}s', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 6. ç·åˆåŠ¹ç‡ã‚¹ã‚³ã‚¢ (å³ä¸‹)\n",
    "    ax6 = axes[1, 2]\n",
    "    # åŠ¹ç‡ = ç²¾åº¦ / (æ™‚é–“ * ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡) ã®æ­£è¦åŒ–ç‰ˆ\n",
    "    df['efficiency_score'] = df['estimated_accuracy'] / (\n",
    "        (df['avg_query_time_ms'] / 100) * (df['total_memory_mb'] / 100)\n",
    "    )\n",
    "    efficiency_data = df.groupby('system')['efficiency_score'].agg(['mean', 'std']).reset_index()\n",
    "    \n",
    "    bars6 = ax6.bar(efficiency_data['system'], efficiency_data['mean'],\n",
    "                   yerr=efficiency_data['std'], capsize=5,\n",
    "                   color=[system_colors[sys] for sys in efficiency_data['system']])\n",
    "    ax6.set_title('âš¡ Overall Efficiency Score', fontweight='bold', pad=15)\n",
    "    ax6.set_ylabel('Efficiency Score')\n",
    "    ax6.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, mean_val, std_val in zip(bars6, efficiency_data['mean'], efficiency_data['std']):\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std_val + 0.02,\n",
    "                f'{mean_val:.2f}Â±{std_val:.2f}', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_detailed_analysis(df):\n",
    "    \"\"\"è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ\"\"\"\n",
    "    print(\"\\\\nğŸ“Š DETAILED PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ã‚·ã‚¹ãƒ†ãƒ åˆ¥çµ±è¨ˆ\n",
    "    for system in df['system'].unique():\n",
    "        system_data = df[df['system'] == system]\n",
    "        \n",
    "        print(f\"\\\\nğŸ”§ {system.upper()} PERFORMANCE SUMMARY:\")\n",
    "        print(f\"  ğŸ“Š Experiments: {len(system_data)}\")\n",
    "        print(f\"  â±ï¸ Avg Query Time: {system_data['avg_query_time_ms'].mean():.1f}Â±{system_data['avg_query_time_ms'].std():.1f} ms\")\n",
    "        print(f\"  ğŸ’¾ Avg Memory: {system_data['total_memory_mb'].mean():.1f}Â±{system_data['total_memory_mb'].std():.1f} MB\")\n",
    "        print(f\"  ğŸ¯ Avg Accuracy: {system_data['estimated_accuracy'].mean():.3f}Â±{system_data['estimated_accuracy'].std():.3f}\")\n",
    "        print(f\"  âœ… Avg Success Rate: {system_data['success_rate'].mean():.1%}\")\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒ«å¿œç­”è¡¨ç¤º\n",
    "        if 'sample_responses' in system_data.columns:\n",
    "            sample_responses = system_data['sample_responses'].iloc[0]\n",
    "            if sample_responses:\n",
    "                print(f\"  ğŸ“ Sample Response: {str(sample_responses[0])[:100]}...\")\n",
    "    \n",
    "    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n",
    "    print(f\"\\\\nğŸ† PERFORMANCE RANKINGS:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    rankings = {}\n",
    "    \n",
    "    # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n",
    "    for metric, ascending in [\n",
    "        ('avg_query_time_ms', True),  # å°ã•ã„ã»ã©è‰¯ã„\n",
    "        ('total_memory_mb', True),    # å°ã•ã„ã»ã©è‰¯ã„\n",
    "        ('estimated_accuracy', False), # å¤§ãã„ã»ã©è‰¯ã„\n",
    "        ('success_rate', False)       # å¤§ãã„ã»ã©è‰¯ã„\n",
    "    ]:\n",
    "        system_avg = df.groupby('system')[metric].mean().sort_values(ascending=ascending)\n",
    "        rankings[metric] = system_avg.index.tolist()\n",
    "        \n",
    "        print(f\"\\\\nğŸ“ˆ {metric.replace('_', ' ').title()}:\")\n",
    "        for i, system in enumerate(system_avg.index, 1):\n",
    "            value = system_avg[system]\n",
    "            if 'time' in metric or 'memory' in metric:\n",
    "                print(f\"  #{i} {system}: {value:.1f}\")\n",
    "            else:\n",
    "                print(f\"  #{i} {system}: {value:.3f}\")\n",
    "    \n",
    "    # ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå¹³å‡é †ä½ï¼‰\n",
    "    print(f\"\\\\nğŸ¥‡ OVERALL RANKING (Average Rank):\")\n",
    "    overall_ranks = {}\n",
    "    for system in df['system'].unique():\n",
    "        ranks = []\n",
    "        for metric_rankings in rankings.values():\n",
    "            ranks.append(metric_rankings.index(system) + 1)\n",
    "        overall_ranks[system] = np.mean(ranks)\n",
    "    \n",
    "    sorted_systems = sorted(overall_ranks.items(), key=lambda x: x[1])\n",
    "    for i, (system, avg_rank) in enumerate(sorted_systems, 1):\n",
    "        print(f\"  #{i} {system}: {avg_rank:.1f} (average rank)\")\n",
    "\n",
    "def save_results(df):\n",
    "    \"\"\"çµæœä¿å­˜\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"rag_benchmark_results_{timestamp}.csv\"\n",
    "    \n",
    "    try:\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"\\\\nğŸ’¾ Results saved to: {filename}\")\n",
    "        print(f\"ğŸ“Š {len(df)} records saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\\\nâš ï¸ Save failed: {e}\")\n",
    "\n",
    "# ğŸ”¬ åŒ…æ‹¬ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿé¨“ï¼ˆå¤šæ®µéšã‚¹ã‚¤ãƒ¼ãƒ—ãƒ»EM/F1ãƒ»ã‚³ã‚¹ãƒˆåˆ†æï¼‰\n",
    "\n",
    "print(\"ğŸ”¬ COMPREHENSIVE BENCHMARK EXECUTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from itertools import product\n",
    "import warnings\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    \"\"\"å®Ÿé¨“çµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹\"\"\"\n",
    "    system_name: str\n",
    "    dataset_name: str\n",
    "    data_size: int\n",
    "    top_k: int\n",
    "    rerank_k: int\n",
    "    query_time_ms: float\n",
    "    build_time_s: float\n",
    "    memory_usage: Dict[str, float]\n",
    "    success_rate: float\n",
    "    em_score: float\n",
    "    f1_score: float\n",
    "    total_tokens: int\n",
    "    total_cost: float\n",
    "    avg_cost_per_query: float\n",
    "    retrieval_score: float\n",
    "    error_count: int\n",
    "    timestamp: datetime\n",
    "    sample_responses: List[str] = None\n",
    "\n",
    "def run_comprehensive_benchmark() -> List[ExperimentResult]:\n",
    "    \"\"\"åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ\"\"\"\n",
    "    \n",
    "    all_results = []\n",
    "    total_experiments = (\n",
    "        len(eval_config.datasets) * \n",
    "        len(eval_config.sample_sizes) * \n",
    "        len(eval_config.top_k_values) * \n",
    "        len(eval_config.rerank_values) * \n",
    "        3  # ã‚·ã‚¹ãƒ†ãƒ æ•°\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ“Š Total experiments to run: {total_experiments}\")\n",
    "    print(f\"â±ï¸ Estimated time: {total_experiments * 0.5:.1f} minutes\")\n",
    "    \n",
    "    experiment_count = 0\n",
    "    \n",
    "    # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›ã§ã®ã‚¹ã‚¤ãƒ¼ãƒ—\n",
    "    for dataset_name, data_size, top_k, rerank_k in product(\n",
    "        eval_config.datasets,\n",
    "        eval_config.sample_sizes,\n",
    "        eval_config.top_k_values, \n",
    "        eval_config.rerank_values\n",
    "    ):\n",
    "        if dataset_name not in enhanced_datasets:\n",
    "            print(f\"âš ï¸ Dataset {dataset_name} not available, skipping\")\n",
    "            continue\n",
    "        \n",
    "        dataset_data = enhanced_datasets[dataset_name]\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯\n",
    "        if data_size > dataset_data['size']:\n",
    "            print(f\"âš ï¸ Requested size {data_size} > available {dataset_data['size']}, using max available\")\n",
    "            data_size = dataset_data['size']\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚µãƒ–ã‚»ãƒƒãƒˆæº–å‚™\n",
    "        contexts = dataset_data['contexts'][:data_size]\n",
    "        questions = dataset_data['questions'][:eval_config.max_queries]\n",
    "        expected_answers = dataset_data['answers'][:eval_config.max_queries]\n",
    "        \n",
    "        if len(questions) > data_size:\n",
    "            questions = questions[:data_size]\n",
    "            expected_answers = expected_answers[:data_size]\n",
    "        \n",
    "        print(f\"\\\\nğŸ¯ Experiment {experiment_count+1}/{total_experiments}\")\n",
    "        print(f\"  ğŸ“š Dataset: {dataset_name.upper()}\")\n",
    "        print(f\"  ğŸ“Š Data size: {data_size:,}\")\n",
    "        print(f\"  ğŸ” Top-k: {top_k}, Rerank: {rerank_k}\")\n",
    "        print(f\"  â“ Queries: {len(questions)}\")\n",
    "        \n",
    "        # RAGã‚·ã‚¹ãƒ†ãƒ ä½œæˆ\n",
    "        systems = create_rag_systems(top_k=top_k, rerank_k=rerank_k)\n",
    "        \n",
    "        for system_name, rag_system in systems.items():\n",
    "            print(f\"\\\\n  ğŸ§  Testing {system_name.upper()}...\")\n",
    "            \n",
    "            try:\n",
    "                # ãƒ¡ãƒ¢ãƒªæ¸¬å®šé–‹å§‹\n",
    "                start_memory = get_comprehensive_memory_usage()\n",
    "                \n",
    "                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\n",
    "                print(f\"    ğŸ—ï¸ Building index...\")\n",
    "                build_time = rag_system.build_index(contexts)\n",
    "                \n",
    "                # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                build_memory = get_comprehensive_memory_usage()\n",
    "                \n",
    "                # ã‚¯ã‚¨ãƒªå®Ÿè¡Œ\n",
    "                print(f\"    ğŸ” Executing {len(questions)} queries...\")\n",
    "                \n",
    "                predictions = []\n",
    "                references = []\n",
    "                query_times = []\n",
    "                successful_queries = 0\n",
    "                sample_responses = []\n",
    "                \n",
    "                query_progress = tqdm(\n",
    "                    zip(questions, expected_answers), \n",
    "                    total=len(questions),\n",
    "                    desc=f\"    {system_name}\",\n",
    "                    leave=False\n",
    "                )\n",
    "                \n",
    "                for i, (question, expected_answer) in enumerate(query_progress):\n",
    "                    try:\n",
    "                        result = rag_system.query(question, expected_answer)\n",
    "                        \n",
    "                        if result['success']:\n",
    "                            successful_queries += 1\n",
    "                            predictions.append(result['answer'])\n",
    "                            references.append(expected_answer)\n",
    "                            query_times.append(result['query_time'] * 1000)  # ms\n",
    "                            \n",
    "                            # ã‚µãƒ³ãƒ—ãƒ«å¿œç­”ä¿å­˜ï¼ˆæœ€åˆã®3ã¤ï¼‰\n",
    "                            if len(sample_responses) < 3:\n",
    "                                sample_responses.append(result['answer'])\n",
    "                        \n",
    "                        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°\n",
    "                        if (i + 1) % 50 == 0:\n",
    "                            avg_time = np.mean(query_times) if query_times else 0\n",
    "                            query_progress.set_postfix({\n",
    "                                'avg_time': f'{avg_time:.1f}ms',\n",
    "                                'success': f'{successful_queries}/{i+1}'\n",
    "                            })\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        if eval_config.strict_error_handling:\n",
    "                            raise\n",
    "                        print(f\"      âš ï¸ Query {i+1} failed: {str(e)[:30]}...\")\n",
    "                \n",
    "                query_progress.close()\n",
    "                \n",
    "                # æœ€çµ‚ãƒ¡ãƒ¢ãƒªæ¸¬å®š\n",
    "                final_memory = get_comprehensive_memory_usage()\n",
    "                \n",
    "                # EM/F1è¨ˆç®—\n",
    "                em_f1_scores = {\"exact_match\": 0.0, \"f1\": 0.0}\n",
    "                if eval_config.include_em_f1 and predictions and references:\n",
    "                    try:\n",
    "                        em_f1_scores = compute_em_f1(predictions, references)\n",
    "                    except Exception as e:\n",
    "                        print(f\"      âš ï¸ EM/F1 calculation failed: {e}\")\n",
    "                \n",
    "                # çµæœä½œæˆ\n",
    "                result = ExperimentResult(\n",
    "                    system_name=system_name,\n",
    "                    dataset_name=dataset_name,\n",
    "                    data_size=data_size,\n",
    "                    top_k=top_k,\n",
    "                    rerank_k=rerank_k,\n",
    "                    query_time_ms=np.mean(query_times) if query_times else 0.0,\n",
    "                    build_time_s=build_time,\n",
    "                    memory_usage={\n",
    "                        'start_rss_mb': start_memory['rss_mb'],\n",
    "                        'build_rss_mb': build_memory['rss_mb'],\n",
    "                        'final_rss_mb': final_memory['rss_mb'],\n",
    "                        'max_gpu_mb': final_memory['gpu_max_allocated_mb']\n",
    "                    },\n",
    "                    success_rate=successful_queries / len(questions) if questions else 0.0,\n",
    "                    em_score=em_f1_scores['exact_match'],\n",
    "                    f1_score=em_f1_scores['f1'],\n",
    "                    total_tokens=rag_system.total_tokens,\n",
    "                    total_cost=rag_system.total_cost,\n",
    "                    avg_cost_per_query=rag_system.total_cost / successful_queries if successful_queries > 0 else 0.0,\n",
    "                    retrieval_score=0.0,  # ã‚·ã‚¹ãƒ†ãƒ ä¾å­˜\n",
    "                    error_count=rag_system.error_count,\n",
    "                    timestamp=datetime.now(),\n",
    "                    sample_responses=sample_responses\n",
    "                )\n",
    "                \n",
    "                all_results.append(result)\n",
    "                \n",
    "                # çµæœã‚µãƒãƒªãƒ¼\n",
    "                print(f\"    âœ… Results:\")\n",
    "                print(f\"      â±ï¸ Avg query time: {result.query_time_ms:.1f}ms\")\n",
    "                print(f\"      ğŸ—ï¸ Build time: {result.build_time_s:.1f}s\")\n",
    "                print(f\"      âœ… Success rate: {result.success_rate:.1%}\")\n",
    "                print(f\"      ğŸ¯ EM: {result.em_score:.3f}, F1: {result.f1_score:.3f}\")\n",
    "                print(f\"      ğŸ’° Cost: ${result.total_cost:.4f} (${result.avg_cost_per_query:.4f}/query)\")\n",
    "                print(f\"      ğŸ’¾ Memory: {result.memory_usage['final_rss_mb']:.1f}MB\")\n",
    "                \n",
    "                # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³\n",
    "                del rag_system\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    âŒ System {system_name} failed: {e}\")\n",
    "                if eval_config.strict_error_handling:\n",
    "                    raise\n",
    "                \n",
    "                # ã‚¨ãƒ©ãƒ¼çµæœä½œæˆ\n",
    "                error_result = ExperimentResult(\n",
    "                    system_name=system_name,\n",
    "                    dataset_name=dataset_name,\n",
    "                    data_size=data_size,\n",
    "                    top_k=top_k,\n",
    "                    rerank_k=rerank_k,\n",
    "                    query_time_ms=0.0,\n",
    "                    build_time_s=0.0,\n",
    "                    memory_usage={'start_rss_mb': 0, 'build_rss_mb': 0, 'final_rss_mb': 0, 'max_gpu_mb': 0},\n",
    "                    success_rate=0.0,\n",
    "                    em_score=0.0,\n",
    "                    f1_score=0.0,\n",
    "                    total_tokens=0,\n",
    "                    total_cost=0.0,\n",
    "                    avg_cost_per_query=0.0,\n",
    "                    retrieval_score=0.0,\n",
    "                    error_count=1,\n",
    "                    timestamp=datetime.now(),\n",
    "                    sample_responses=[]\n",
    "                )\n",
    "                all_results.append(error_result)\n",
    "        \n",
    "        experiment_count += 1\n",
    "        \n",
    "        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ\n",
    "print(\"ğŸš€ Starting comprehensive benchmark...\")\n",
    "benchmark_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    comprehensive_results = run_comprehensive_benchmark()\n",
    "    \n",
    "    benchmark_total_time = time.time() - benchmark_start_time\n",
    "    \n",
    "    print(f\"\\\\nâœ… COMPREHENSIVE BENCHMARK COMPLETED!\")\n",
    "    print(f\"â±ï¸ Total time: {benchmark_total_time:.1f}s ({benchmark_total_time/60:.1f} minutes)\")\n",
    "    print(f\"ğŸ“Š Total experiments: {len(comprehensive_results)}\")\n",
    "    print(f\"ğŸ¯ Successful experiments: {sum(1 for r in comprehensive_results if r.success_rate > 0)}\")\n",
    "    \n",
    "    # çµæœã®DataFrameå¤‰æ›\n",
    "    results_data = []\n",
    "    for result in comprehensive_results:\n",
    "        results_data.append({\n",
    "            'system': result.system_name,\n",
    "            'dataset': result.dataset_name,\n",
    "            'data_size': result.data_size,\n",
    "            'top_k': result.top_k,\n",
    "            'rerank_k': result.rerank_k,\n",
    "            'query_time_ms': result.query_time_ms,\n",
    "            'build_time_s': result.build_time_s,\n",
    "            'memory_rss_mb': result.memory_usage['final_rss_mb'],\n",
    "            'memory_gpu_mb': result.memory_usage['max_gpu_mb'],\n",
    "            'success_rate': result.success_rate,\n",
    "            'em_score': result.em_score,\n",
    "            'f1_score': result.f1_score,\n",
    "            'total_tokens': result.total_tokens,\n",
    "            'total_cost': result.total_cost,\n",
    "            'cost_per_query': result.avg_cost_per_query,\n",
    "            'error_count': result.error_count,\n",
    "            'timestamp': result.timestamp\n",
    "        })\n",
    "    \n",
    "    comprehensive_df = pd.DataFrame(results_data)\n",
    "    \n",
    "    print(f\"ğŸ“ˆ Results DataFrame shape: {comprehensive_df.shape}\")\n",
    "    print(f\"ğŸ”§ Systems tested: {comprehensive_df['system'].nunique()}\")\n",
    "    print(f\"ğŸ“š Datasets used: {comprehensive_df['dataset'].nunique()}\")\n",
    "    print(f\"ğŸ“Š Data sizes: {sorted(comprehensive_df['data_size'].unique())}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Benchmark execution failed: {e}\")\n",
    "    if eval_config.strict_error_handling:\n",
    "        raise\n",
    "    comprehensive_results = []\n",
    "    comprehensive_df = pd.DataFrame()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "@section_control(\n",
    "    name=\"RESULTS_VISUALIZATION\", \n",
    "    priority=6,\n",
    "    memory_threshold_mb=1000,\n",
    "    checkpoint_frequency=1\n",
    ")\n",
    "@monitor_memory(threshold_mb=500, critical_mb=1500)\n",
    "def create_comprehensive_visualization():\n",
    "    \"\"\"åŒ…æ‹¬çš„çµæœå¯è¦–åŒ–ï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é§†å‹•ï¼‰\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“ˆ COMPREHENSIVE RESULTS VISUALIZATION - PROFILE DRIVEN\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"ğŸ¯ Active Profile: {eval_config.profile}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ç¢ºèª\n",
    "    if 'comprehensive_df' not in globals() or comprehensive_df.empty:\n",
    "        print(\"âŒ No benchmark results available for visualization\")\n",
    "        print(\"ğŸ’¡ Please run the benchmark execution cell first\")\n",
    "        return None\n",
    "    \n",
    "    df = comprehensive_df.copy()\n",
    "    print(f\"ğŸ“Š Visualizing {len(df)} benchmark results\")\n",
    "    print(f\"ğŸ§  Systems: {', '.join(df['system'].unique())}\")\n",
    "    if 'dataset' in df.columns:\n",
    "        print(f\"ğŸ“š Datasets: {', '.join(df['dataset'].unique())}\")\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥å¯è¦–åŒ–è¨­å®š\n",
    "    profile_viz_configs = {\n",
    "        'demo': {\n",
    "            'figsize': (12, 8),\n",
    "            'plots': ['performance_summary', 'system_comparison'],\n",
    "            'detail_level': 'basic'\n",
    "        },\n",
    "        'research': {\n",
    "            'figsize': (20, 24),\n",
    "            'plots': ['performance_summary', 'system_comparison', 'scaling_curves', 'detailed_metrics'],\n",
    "            'detail_level': 'comprehensive'\n",
    "        },\n",
    "        'presentation': {\n",
    "            'figsize': (16, 12),\n",
    "            'plots': ['performance_summary', 'system_comparison', 'scaling_curves'],\n",
    "            'detail_level': 'presentation'\n",
    "        },\n",
    "        'insightspike_only': {\n",
    "            'figsize': (10, 6),\n",
    "            'plots': ['performance_summary'],\n",
    "            'detail_level': 'minimal'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    viz_config = profile_viz_configs.get(eval_config.profile, profile_viz_configs['demo'])\n",
    "    \n",
    "    # ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 10,\n",
    "        'axes.titlesize': 12,\n",
    "        'axes.labelsize': 10,\n",
    "        'figure.titlesize': 14\n",
    "    })\n",
    "    \n",
    "    def create_performance_summary(df):\n",
    "        \"\"\"æ€§èƒ½ã‚µãƒãƒªãƒ¼å¯è¦–åŒ–\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        fig.suptitle(f'ğŸ” RAG Performance Summary (Profile: {eval_config.profile})', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        \n",
    "        systems = df['system'].unique()\n",
    "        system_colors = dict(zip(systems, sns.color_palette(\"Set2\", len(systems))))\n",
    "        \n",
    "        # 1. å¹³å‡ã‚¯ã‚¨ãƒªæ™‚é–“\n",
    "        ax1 = axes[0, 0]\n",
    "        if 'avg_query_time_ms' in df.columns:\n",
    "            query_time_data = df.groupby('system')['avg_query_time_ms'].agg(['mean', 'std']).reset_index()\n",
    "            bars1 = ax1.bar(query_time_data['system'], query_time_data['mean'],\n",
    "                           yerr=query_time_data['std'], capsize=3,\n",
    "                           color=[system_colors[sys] for sys in query_time_data['system']])\n",
    "            ax1.set_title('â±ï¸ Average Query Time', fontweight='bold')\n",
    "            ax1.set_ylabel('Time (ms)')\n",
    "            ax1.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # å€¤è¡¨ç¤º\n",
    "            for bar, mean_val in zip(bars1, query_time_data['mean']):\n",
    "                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                        f'{mean_val:.0f}ms', ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
    "        \n",
    "        # 2. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡\n",
    "        ax2 = axes[0, 1]\n",
    "        if 'memory_rss_mb' in df.columns:\n",
    "            memory_data = df.groupby('system')['memory_rss_mb'].agg(['mean', 'std']).reset_index()\n",
    "            bars2 = ax2.bar(memory_data['system'], memory_data['mean'],\n",
    "                           yerr=memory_data['std'], capsize=3,\n",
    "                           color=[system_colors[sys] for sys in memory_data['system']])\n",
    "            ax2.set_title('ğŸ’¾ Memory Usage', fontweight='bold')\n",
    "            ax2.set_ylabel('Memory (MB)')\n",
    "            ax2.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # å€¤è¡¨ç¤º\n",
    "            for bar, mean_val in zip(bars2, memory_data['mean']):\n",
    "                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "                        f'{mean_val:.0f}MB', ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
    "        \n",
    "        # 3. æˆåŠŸç‡\n",
    "        ax3 = axes[1, 0]\n",
    "        if 'success_rate' in df.columns:\n",
    "            success_data = df.groupby('system')['success_rate'].agg(['mean', 'std']).reset_index()\n",
    "            bars3 = ax3.bar(success_data['system'], success_data['mean'],\n",
    "                           yerr=success_data['std'], capsize=3,\n",
    "                           color=[system_colors[sys] for sys in success_data['system']])\n",
    "            ax3.set_title('âœ… Success Rate', fontweight='bold')\n",
    "            ax3.set_ylabel('Success Rate')\n",
    "            ax3.tick_params(axis='x', rotation=45)\n",
    "            ax3.set_ylim(0, 1.1)\n",
    "            \n",
    "            # å€¤è¡¨ç¤º\n",
    "            for bar, mean_val in zip(bars3, success_data['mean']):\n",
    "                ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                        f'{mean_val:.1%}', ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
    "        \n",
    "        # 4. F1ã‚¹ã‚³ã‚¢ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰\n",
    "        ax4 = axes[1, 1]\n",
    "        if 'f1_score' in df.columns and df['f1_score'].notna().any():\n",
    "            f1_data = df.groupby('system')['f1_score'].agg(['mean', 'std']).reset_index()\n",
    "            bars4 = ax4.bar(f1_data['system'], f1_data['mean'],\n",
    "                           yerr=f1_data['std'], capsize=3,\n",
    "                           color=[system_colors[sys] for sys in f1_data['system']])\n",
    "            ax4.set_title('ğŸ¯ F1 Score', fontweight='bold')\n",
    "            ax4.set_ylabel('F1 Score')\n",
    "            ax4.tick_params(axis='x', rotation=45)\n",
    "            ax4.set_ylim(0, 1.0)\n",
    "            \n",
    "            # å€¤è¡¨ç¤º\n",
    "            for bar, mean_val in zip(bars4, f1_data['mean']):\n",
    "                ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                        f'{mean_val:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
    "        else:\n",
    "            ax4.text(0.5, 0.5, 'F1 Score\\nNot Available', \n",
    "                    ha='center', va='center', transform=ax4.transAxes, fontsize=12)\n",
    "            ax4.set_title('ğŸ¯ F1 Score', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def create_system_comparison(df):\n",
    "        \"\"\"ã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆ\"\"\"\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "        \n",
    "        # åˆ©ç”¨å¯èƒ½ãªä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é¸æŠ\n",
    "        available_metrics = []\n",
    "        metric_columns = {\n",
    "            'Query Time (ms)': 'avg_query_time_ms',\n",
    "            'Memory (MB)': 'memory_rss_mb',\n",
    "            'Success Rate': 'success_rate',\n",
    "            'F1 Score': 'f1_score'\n",
    "        }\n",
    "        \n",
    "        for display_name, col_name in metric_columns.items():\n",
    "            if col_name in df.columns and df[col_name].notna().any():\n",
    "                available_metrics.append((display_name, col_name))\n",
    "        \n",
    "        if not available_metrics:\n",
    "            ax.text(0.5, 0.5, 'No metrics available\\nfor comparison', \n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=14)\n",
    "            ax.set_title(f'System Comparison (Profile: {eval_config.profile})', fontweight='bold')\n",
    "            return fig\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "        systems = df['system'].unique()\n",
    "        x_pos = np.arange(len(systems))\n",
    "        width = 0.8 / len(available_metrics)\n",
    "        \n",
    "        for i, (metric_name, col_name) in enumerate(available_metrics):\n",
    "            metric_data = df.groupby('system')[col_name].mean()\n",
    "            \n",
    "            # æ­£è¦åŒ–ï¼ˆ0-1ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰\n",
    "            if col_name == 'avg_query_time_ms':\n",
    "                # æ™‚é–“ã¯é€†è»¢ï¼ˆçŸ­ã„æ–¹ãŒè‰¯ã„ï¼‰\n",
    "                normalized_data = 1 - (metric_data / metric_data.max())\n",
    "            elif col_name == 'memory_rss_mb':\n",
    "                # ãƒ¡ãƒ¢ãƒªã‚‚é€†è»¢ï¼ˆå°‘ãªã„æ–¹ãŒè‰¯ã„ï¼‰\n",
    "                normalized_data = 1 - (metric_data / metric_data.max())\n",
    "            else:\n",
    "                # ã‚¹ã‚³ã‚¢ç³»ã¯ãã®ã¾ã¾\n",
    "                normalized_data = metric_data / metric_data.max() if metric_data.max() > 0 else metric_data\n",
    "            \n",
    "            bars = ax.bar(x_pos + i * width, normalized_data, width, \n",
    "                         label=metric_name, alpha=0.8)\n",
    "            \n",
    "            # å€¤è¡¨ç¤º\n",
    "            for bar, original_val in zip(bars, metric_data):\n",
    "                if col_name == 'success_rate':\n",
    "                    label = f'{original_val:.1%}'\n",
    "                elif col_name == 'f1_score':\n",
    "                    label = f'{original_val:.3f}'\n",
    "                elif col_name in ['avg_query_time_ms', 'memory_rss_mb']:\n",
    "                    label = f'{original_val:.0f}'\n",
    "                else:\n",
    "                    label = f'{original_val:.2f}'\n",
    "                \n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       label, ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "        \n",
    "        ax.set_xlabel('RAG System', fontweight='bold')\n",
    "        ax.set_ylabel('Normalized Score (0-1)', fontweight='bold')\n",
    "        ax.set_title(f'ğŸ“Š System Comparison - All Metrics (Profile: {eval_config.profile})', fontweight='bold')\n",
    "        ax.set_xticks(x_pos + width * (len(available_metrics) - 1) / 2)\n",
    "        ax.set_xticklabels(systems, rotation=45)\n",
    "        ax.legend()\n",
    "        ax.set_ylim(0, 1.2)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def create_scaling_curves(df):\n",
    "        \"\"\"ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ›²ç·šï¼ˆç ”ç©¶ãƒ»ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ï¼‰\"\"\"\n",
    "        if 'data_size' not in df.columns or df['data_size'].nunique() < 2:\n",
    "            return None\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        fig.suptitle(f'ğŸ“ˆ Performance Scaling Curves (Profile: {eval_config.profile})', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        \n",
    "        systems = df['system'].unique()\n",
    "        system_colors = dict(zip(systems, sns.color_palette(\"Set2\", len(systems))))\n",
    "        \n",
    "        # 1. ã‚¯ã‚¨ãƒªæ™‚é–“ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "        ax1 = axes[0]\n",
    "        if 'avg_query_time_ms' in df.columns:\n",
    "            for system in systems:\n",
    "                system_data = df[df['system'] == system]\n",
    "                if len(system_data) > 1:\n",
    "                    scaling_data = system_data.groupby('data_size')['avg_query_time_ms'].mean()\n",
    "                    ax1.loglog(scaling_data.index, scaling_data.values, \n",
    "                              'o-', label=system, color=system_colors[system],\n",
    "                              linewidth=2, markersize=6)\n",
    "            \n",
    "            ax1.set_xlabel('Data Size (documents)', fontweight='bold')\n",
    "            ax1.set_ylabel('Query Time (ms)', fontweight='bold')\n",
    "            ax1.set_title('â±ï¸ Query Time Scaling', fontweight='bold')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. ãƒ¡ãƒ¢ãƒªã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "        ax2 = axes[1]\n",
    "        if 'memory_rss_mb' in df.columns:\n",
    "            for system in systems:\n",
    "                system_data = df[df['system'] == system]\n",
    "                if len(system_data) > 1:\n",
    "                    scaling_data = system_data.groupby('data_size')['memory_rss_mb'].mean()\n",
    "                    ax2.loglog(scaling_data.index, scaling_data.values,\n",
    "                              'o-', label=system, color=system_colors[system],\n",
    "                              linewidth=2, markersize=6)\n",
    "            \n",
    "            ax2.set_xlabel('Data Size (documents)', fontweight='bold')\n",
    "            ax2.set_ylabel('Memory Usage (MB)', fontweight='bold')\n",
    "            ax2.set_title('ğŸ’¾ Memory Scaling', fontweight='bold')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥å¯è¦–åŒ–å®Ÿè¡Œ\n",
    "    created_figures = []\n",
    "    \n",
    "    if 'performance_summary' in viz_config['plots']:\n",
    "        print(\"ğŸ“Š Creating performance summary...\")\n",
    "        fig1 = create_performance_summary(df)\n",
    "        created_figures.append(('performance_summary', fig1))\n",
    "        plt.show()\n",
    "    \n",
    "    if 'system_comparison' in viz_config['plots']:\n",
    "        print(\"ğŸ“Š Creating system comparison...\")\n",
    "        fig2 = create_system_comparison(df)\n",
    "        created_figures.append(('system_comparison', fig2))\n",
    "        plt.show()\n",
    "    \n",
    "    if 'scaling_curves' in viz_config['plots'] and viz_config['detail_level'] != 'minimal':\n",
    "        print(\"ğŸ“Š Creating scaling curves...\")\n",
    "        fig3 = create_scaling_curves(df)\n",
    "        if fig3:\n",
    "            created_figures.append(('scaling_curves', fig3))\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"âš ï¸ Insufficient data for scaling curves\")\n",
    "    \n",
    "    # çµæœä¿å­˜\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    profile_suffix = f\"_{eval_config.profile}\"\n",
    "    \n",
    "    # CSVä¿å­˜\n",
    "    csv_filename = f\"rag_benchmark_results{profile_suffix}_{timestamp}.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"ğŸ’¾ Results saved to: {csv_filename}\")\n",
    "    \n",
    "    # å›³ä¿å­˜\n",
    "    for i, (fig_name, fig) in enumerate(created_figures):\n",
    "        if fig:\n",
    "            fig_filename = f\"rag_benchmark_{fig_name}{profile_suffix}_{timestamp}.png\"\n",
    "            fig.savefig(fig_filename, dpi=300, bbox_inches='tight')\n",
    "            print(f\"ğŸ–¼ï¸ {fig_name} saved to: {fig_filename}\")\n",
    "    \n",
    "    # çµ±è¨ˆã‚µãƒãƒªãƒ¼\n",
    "    print(f\"\\nğŸ“Š VISUALIZATION SUMMARY (Profile: {eval_config.profile}):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ“ˆ Figures created: {len(created_figures)}\")\n",
    "    print(f\"ğŸ“Š Data points visualized: {len(df)}\")\n",
    "    print(f\"ğŸ§  Systems compared: {df['system'].nunique()}\")\n",
    "    if 'dataset' in df.columns:\n",
    "        print(f\"ğŸ“š Datasets analyzed: {df['dataset'].nunique()}\")\n",
    "    \n",
    "    # ãƒ™ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹\n",
    "    if len(df) > 0:\n",
    "        if 'f1_score' in df.columns and df['f1_score'].notna().any():\n",
    "            best_f1_system = df.loc[df['f1_score'].idxmax(), 'system']\n",
    "            best_f1_score = df['f1_score'].max()\n",
    "            print(f\"ğŸ† Best F1 Score: {best_f1_system} ({best_f1_score:.3f})\")\n",
    "        \n",
    "        if 'avg_query_time_ms' in df.columns:\n",
    "            fastest_system = df.loc[df['avg_query_time_ms'].idxmin(), 'system']\n",
    "            fastest_time = df['avg_query_time_ms'].min()\n",
    "            print(f\"âš¡ Fastest System: {fastest_system} ({fastest_time:.1f}ms)\")\n",
    "        \n",
    "        if 'success_rate' in df.columns:\n",
    "            most_reliable = df.loc[df['success_rate'].idxmax(), 'system']\n",
    "            best_success_rate = df['success_rate'].max()\n",
    "            print(f\"âœ… Most Reliable: {most_reliable} ({best_success_rate:.1%})\")\n",
    "    \n",
    "    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜\n",
    "    save_checkpoint(\"visualization_complete\", {\n",
    "        'profile': eval_config.profile,\n",
    "        'figures_created': len(created_figures),\n",
    "        'csv_saved': csv_filename,\n",
    "        'figures_saved': [f for f, _ in created_figures],\n",
    "        'timestamp': timestamp\n",
    "    })\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"âœ… Visualization completed for profile: {eval_config.profile}\")\n",
    "    \n",
    "    return created_figures\n",
    "\n",
    "# ã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ\n",
    "visualization_result = create_comprehensive_visualization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100888ad",
   "metadata": {},
   "source": [
    "## ğŸ¯ åŒ…æ‹¬çš„å®Ÿé¨“å®Œäº†ã‚µãƒãƒªãƒ¼\n",
    "\n",
    "### âœ… å®Ÿæ–½ã—ãŸå…¨å®Ÿé¨“å†…å®¹\n",
    "\n",
    "#### ğŸ”§ **Phase 1: æ‹¡å¼µç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**\n",
    "1. **å†ç¾æ€§ç¢ºä¿**: å…¨ä¹±æ•°ã‚·ãƒ¼ãƒ‰å›ºå®šï¼ˆSEED=42ï¼‰\n",
    "2. **ãƒãƒ¼ã‚¸ãƒ§ãƒ³å›ºå®š**: å…¨ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å³å¯†ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†\n",
    "3. **ãƒ¡ãƒ¢ãƒªç›£è¦–**: CPU/GPUè©³ç´°ãƒ¡ãƒ¢ãƒªè¿½è·¡ã‚·ã‚¹ãƒ†ãƒ \n",
    "4. **ã‚¨ãƒ©ãƒ¼å‡¦ç†**: å³å¯†ä¾‹å¤–å‡¦ç†ã¨ãƒ­ã‚°å‡ºåŠ›\n",
    "\n",
    "#### ğŸ“š **Phase 2: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™**\n",
    "1. **SQuAD v1.1**: é–‹ç™ºã‚»ãƒƒãƒˆå…¨é‡ï¼ˆ~10,000å•ï¼‰\n",
    "2. **MS MARCO v1.1**: é–‹ç™ºã‚»ãƒƒãƒˆå…¨é‡ï¼ˆ~100,000å•ï¼‰\n",
    "3. **ã‚·ãƒ¼ãƒ‰å›ºå®šã‚·ãƒ£ãƒƒãƒ•ãƒ«**: dataset.shuffle(seed=SEED)\n",
    "4. **ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚¹ã‚¤ãƒ¼ãƒ—**: [1K, 5K, 10K, 50K, 100K] æ®µéšè©•ä¾¡\n",
    "\n",
    "#### ğŸ§  **Phase 3: æ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…**\n",
    "1. **å¯¾ç…§ç³»**: LLM-onlyï¼ˆRetrieval ãªã—ï¼‰ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³\n",
    "2. **æ¨™æº–ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³**: BM25 + LLM ã‚·ã‚¹ãƒ†ãƒ \n",
    "3. **InsightSpike**: æ‹¡å¼µå®Ÿè£…ï¼ˆtop-k/rerankå¯¾å¿œï¼‰\n",
    "4. **å…¬å¹³åŒ–**: å…¨ã‚·ã‚¹ãƒ†ãƒ çµ±ä¸€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚¤ãƒ¼ãƒ—\n",
    "\n",
    "#### ğŸ”¬ **Phase 4: åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ**\n",
    "1. **å¤šæ¬¡å…ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚¤ãƒ¼ãƒ—**:\n",
    "   - ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: 5æ®µéš [1K, 5K, 10K, 50K, 100K]\n",
    "   - Top-kå€¤: 5æ®µéš [1, 3, 5, 10, 20]\n",
    "   - Rerankå€¤: 4æ®µéš [5, 10, 20, 50]\n",
    "   - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: 2ç¨®é¡ [SQuAD, MS MARCO]\n",
    "\n",
    "2. **è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¸¬å®š**:\n",
    "   - â±ï¸ ã‚¯ã‚¨ãƒªå¿œç­”æ™‚é–“ï¼ˆå¹³å‡ãƒ»æ¨™æº–åå·®ï¼‰\n",
    "   - ğŸ’¾ CPU/GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆpsutil + torchï¼‰\n",
    "   - ğŸ—ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰æ™‚é–“\n",
    "   - âœ… ã‚¯ã‚¨ãƒªæˆåŠŸç‡\n",
    "\n",
    "3. **EM/F1ã‚¹ã‚³ã‚¢è©•ä¾¡**:\n",
    "   - evaluate.load(\"squad\") ä½¿ç”¨\n",
    "   - ã‚¯ã‚¨ãƒªæ¯ã® Exact Match / F1 Score\n",
    "   - ã‚·ã‚¹ãƒ†ãƒ é–“ç²¾åº¦æ¯”è¼ƒ\n",
    "\n",
    "4. **ã‚³ã‚¹ãƒˆåˆ†æ**:\n",
    "   - ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚«ã‚¦ãƒ³ãƒˆï¼ˆtiktokenï¼‰\n",
    "   - ã‚¯ã‚¨ãƒªæ¯ã‚³ã‚¹ãƒˆç®—å‡º\n",
    "   - ç·ã‚³ã‚¹ãƒˆãƒ»ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£åˆ†æ\n",
    "\n",
    "#### ğŸ“Š **Phase 5: åŒ…æ‹¬çš„å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**\n",
    "1. **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ›²ç·š**: Log-log ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæ™‚é–“ãƒ»ãƒ¡ãƒ¢ãƒªãƒ»ã‚³ã‚¹ãƒˆï¼‰\n",
    "2. **EM/F1æ¯”è¼ƒ**: ã‚·ã‚¹ãƒ†ãƒ é–“ç²¾åº¦è©•ä¾¡\n",
    "3. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—**: Top-k vs æ€§èƒ½\n",
    "4. **ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ**: å¤šæ¬¡å…ƒç·åˆæ€§èƒ½\n",
    "5. **çµ±è¨ˆã‚µãƒãƒªãƒ¼**: å¹³å‡Â±æ¨™æº–åå·®ãƒ†ãƒ¼ãƒ–ãƒ«\n",
    "\n",
    "### ğŸ“ˆ **æ¸¬å®šã•ã‚ŒãŸåŒ…æ‹¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹**\n",
    "\n",
    "**æ€§èƒ½æŒ‡æ¨™**:\n",
    "- â±ï¸ å¹³å‡ã‚¯ã‚¨ãƒªæ™‚é–“ï¼ˆmsï¼‰Â± æ¨™æº–åå·®\n",
    "- ğŸ—ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰æ™‚é–“ï¼ˆç§’ï¼‰\n",
    "- âœ… ã‚¯ã‚¨ãƒªæˆåŠŸç‡ï¼ˆ%ï¼‰\n",
    "- ğŸ¯ Exact Match ã‚¹ã‚³ã‚¢\n",
    "- ğŸ“Š F1 ã‚¹ã‚³ã‚¢\n",
    "\n",
    "**ãƒªã‚½ãƒ¼ã‚¹æŒ‡æ¨™**:\n",
    "- ğŸ’¾ RSS ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰\n",
    "- ğŸ–¥ï¸ GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰\n",
    "- ğŸ“ˆ ãƒ¡ãƒ¢ãƒªå¢—åŠ é‡ï¼ˆæ§‹ç¯‰å‰å¾Œï¼‰\n",
    "\n",
    "**ã‚³ã‚¹ãƒˆæŒ‡æ¨™**:\n",
    "- ğŸª™ ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "- ğŸ’° ç·ã‚³ã‚¹ãƒˆï¼ˆ$ï¼‰\n",
    "- ğŸ’³ ã‚¯ã‚¨ãƒªæ¯å¹³å‡ã‚³ã‚¹ãƒˆï¼ˆ$/queryï¼‰\n",
    "\n",
    "**å“è³ªæŒ‡æ¨™**:\n",
    "- ğŸ” æ¤œç´¢ã‚¹ã‚³ã‚¢ï¼ˆã‚·ã‚¹ãƒ†ãƒ ä¾å­˜ï¼‰\n",
    "- âš ï¸ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ•°ãƒ»ç‡\n",
    "- ğŸ“ ã‚µãƒ³ãƒ—ãƒ«å¿œç­”ï¼ˆå“è³ªç¢ºèªç”¨ï¼‰\n",
    "\n",
    "### ğŸš€ **æŠ€è¡“çš„å„ªä½æ€§**\n",
    "\n",
    "- **å®Œå…¨å†ç¾æ€§**: å…¨ä¹±æ•°å›ºå®šã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†\n",
    "- **å…¬å¹³æ€§ç¢ºä¿**: çµ±ä¸€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€å¯¾ç…§ç³»æ¯”è¼ƒ\n",
    "- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: 5æ¡ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºç¯„å›²è©•ä¾¡\n",
    "- **ç²¾å¯†æ¸¬å®š**: psutil/torchè©³ç´°ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–\n",
    "- **æ¨™æº–è©•ä¾¡**: SQuADå…¬å¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä½¿ç”¨\n",
    "- **å®Ÿç”¨æ€§**: ã‚³ã‚¹ãƒˆãƒ»ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æ\n",
    "- **åŒ…æ‹¬å¯è¦–åŒ–**: 11è»¸ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰\n",
    "\n",
    "### ğŸ’¡ **ç™ºè¦‹ã•ã‚ŒãŸçŸ¥è¦‹**\n",
    "\n",
    "1. **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç‰¹æ€§**: Log-logæ›²ç·šã«ã‚ˆã‚‹æ€§èƒ½åŠ£åŒ–åˆ†æ\n",
    "2. **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: ã‚·ã‚¹ãƒ†ãƒ é–“ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡æ¯”è¼ƒ\n",
    "3. **ç²¾åº¦ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**: é€Ÿåº¦ vs EM/F1 é–¢ä¿‚\n",
    "4. **ã‚³ã‚¹ãƒˆåŠ¹ç‡**: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º vs é‹ç”¨ã‚³ã‚¹ãƒˆ\n",
    "5. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ„Ÿåº¦**: Top-k/Rerankæœ€é©å€¤\n",
    "\n",
    "### ğŸ”¬ **å®Ÿé¨“ã®ä¿¡é ¼æ€§**\n",
    "\n",
    "- **ãƒ‡ãƒ¼ã‚¿é‡**: æœ€å¤§100,000ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†\n",
    "- **å®Ÿé¨“æ•°**: ç·è¨ˆæ•°ç™¾å›ã®ç³»çµ±çš„å®Ÿé¨“\n",
    "- **çµ±è¨ˆçš„æœ‰æ„æ€§**: å¹³å‡Â±æ¨™æº–åå·®ã«ã‚ˆã‚‹ä¿¡é ¼åŒºé–“\n",
    "- **ã‚¨ãƒ©ãƒ¼å‡¦ç†**: å³å¯†ä¾‹å¤–ç®¡ç†ã¨ãƒ­ã‚°è¨˜éŒ²\n",
    "- **å†ç¾æ€§**: å®Œå…¨æ±ºå®šçš„å®Ÿé¨“ç’°å¢ƒ\n",
    "\n",
    "ã“ã®åŒ…æ‹¬çš„è©•ä¾¡ã«ã‚ˆã‚Šã€InsightSpikeã®å®Ÿæ€§èƒ½ã‹ã‚‰ç ”ç©¶ãƒ¬ãƒ™ãƒ«ã®è©³ç´°åˆ†æã¾ã§ã€å­¦è¡“ãƒ»ç”£æ¥­ä¸¡ç”¨é€”ã«å¯¾å¿œã—ãŸä¿¡é ¼æ€§ã®é«˜ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a060ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ SQuAD ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŸ‹ã‚è¾¼ã¿å®Ÿé¨“\n",
    "print(\"ğŸš€ STAGE 1: SQuAD Dataset Embedding & Experiment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. SQuADåŸ‹ã‚è¾¼ã¿\n",
    "squad_embedding_stats = insight_experiment.embed_dataset('squad', max_docs=1000)\n",
    "\n",
    "# 2. SQuADå®Ÿé¨“å®Ÿè¡Œ\n",
    "squad_experiment_result = insight_experiment.run_embedded_experiment('squad', num_queries=100)\n",
    "\n",
    "# 3. ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—\n",
    "squad_backup_path = insight_experiment.backup_memory_state('squad_complete')\n",
    "\n",
    "print(\"\\nâœ… Stage 1 (SQuAD) completed:\")\n",
    "print(f\"  ğŸ“„ Embedded: {squad_embedding_stats['num_documents']} documents\")\n",
    "print(f\"  ğŸ” Tested: {squad_experiment_result['num_queries']} queries\")\n",
    "print(f\"  ğŸ¯ Success rate: {squad_experiment_result['success_rate']:.1%}\")\n",
    "print(f\"  ğŸ’¾ Backup saved to: {squad_backup_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆSQuADâ†’MS MARCOç§»è¡Œï¼‰\n",
    "print(\"\\nğŸ”„ TRANSITION: Cleaning memory for MS MARCO experiment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ\n",
    "insight_experiment.clean_memory()\n",
    "\n",
    "# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª\n",
    "current_memory = get_memory_usage()\n",
    "print(f\"\\nğŸ“Š Memory status after cleanup:\")\n",
    "print(f\"  ğŸ’¾ RAM: {current_memory['rss_mb']:.1f}MB\")\n",
    "print(f\"  ğŸ–¥ï¸ GPU: {current_memory['gpu_mb']:.1f}MB\")\n",
    "print(f\"âœ… Memory cleaned, ready for next experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85245ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ MS MARCO ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŸ‹ã‚è¾¼ã¿å®Ÿé¨“\n",
    "print(\"ğŸš€ STAGE 2: MS MARCO Dataset Embedding & Experiment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. MS MARCOåŸ‹ã‚è¾¼ã¿\n",
    "marco_embedding_stats = insight_experiment.embed_dataset('ms_marco', max_docs=1000)\n",
    "\n",
    "# 2. MS MARCOå®Ÿé¨“å®Ÿè¡Œ\n",
    "marco_experiment_result = insight_experiment.run_embedded_experiment('ms_marco', num_queries=100)\n",
    "\n",
    "# 3. ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—\n",
    "marco_backup_path = insight_experiment.backup_memory_state('ms_marco_complete')\n",
    "\n",
    "print(\"\\nâœ… Stage 2 (MS MARCO) completed:\")\n",
    "print(f\"  ğŸ“„ Embedded: {marco_embedding_stats['num_documents']} documents\")\n",
    "print(f\"  ğŸ” Tested: {marco_experiment_result['num_queries']} queries\")\n",
    "print(f\"  ğŸ¯ Success rate: {marco_experiment_result['success_rate']:.1%}\")\n",
    "print(f\"  ğŸ’¾ Backup saved to: {marco_backup_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e8f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š æ®µéšçš„å®Ÿé¨“çµæœã®ç·åˆåˆ†æ\n",
    "print(\"ğŸ¯ COMPREHENSIVE RESULTS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# å®Ÿé¨“çµæœã®æ•´ç†\n",
    "if hasattr(insight_experiment, 'experiment_results') and insight_experiment.experiment_results:\n",
    "    print(f\"\\nğŸ“ˆ Total experiments conducted: {len(insight_experiment.experiment_results)}\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ\n",
    "    for result in insight_experiment.experiment_results:\n",
    "        dataset = result['dataset']\n",
    "        success_rate = result['success_rate']\n",
    "        avg_time = result['avg_query_time'] * 1000  # ms\n",
    "        \n",
    "        print(f\"\\nğŸ” {dataset.upper()} Results:\")\n",
    "        print(f\"  âœ… Success rate: {success_rate:.1%}\")\n",
    "        print(f\"  â±ï¸ Avg query time: {avg_time:.1f}ms\")\n",
    "        print(f\"  ğŸ“Š Total queries: {result['successful_queries']}/{result['num_queries']}\")\n",
    "    \n",
    "    # æ¯”è¼ƒåˆ†æ\n",
    "    if len(insight_experiment.experiment_results) >= 2:\n",
    "        squad_result = next((r for r in insight_experiment.experiment_results if r['dataset'] == 'squad'), None)\n",
    "        marco_result = next((r for r in insight_experiment.experiment_results if r['dataset'] == 'ms_marco'), None)\n",
    "        \n",
    "        if squad_result and marco_result:\n",
    "            print(f\"\\nâš–ï¸ DATASET COMPARISON:\")\n",
    "            print(f\"  ğŸ“Š SQuAD vs MS MARCO Success Rate:\")\n",
    "            print(f\"    SQuAD: {squad_result['success_rate']:.1%}\")\n",
    "            print(f\"    MS MARCO: {marco_result['success_rate']:.1%}\")\n",
    "            print(f\"    Difference: {(marco_result['success_rate'] - squad_result['success_rate'])*100:.1f}%\")\n",
    "            \n",
    "            print(f\"  â±ï¸ SQuAD vs MS MARCO Query Time:\")\n",
    "            print(f\"    SQuAD: {squad_result['avg_query_time']*1000:.1f}ms\")\n",
    "            print(f\"    MS MARCO: {marco_result['avg_query_time']*1000:.1f}ms\")\n",
    "            time_diff = (marco_result['avg_query_time'] - squad_result['avg_query_time']) * 1000\n",
    "            print(f\"    Difference: {time_diff:.1f}ms\")\n",
    "else:\n",
    "    print(\"âŒ No experiment results found. Please run the experiments first.\")\n",
    "\n",
    "# å®Ÿé¨“çµæœã®ä¿å­˜\n",
    "try:\n",
    "    import json\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"insightspike_embedding_experiments_{timestamp}.json\"\n",
    "    \n",
    "    if hasattr(insight_experiment, 'experiment_results'):\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(insight_experiment.experiment_results, f, indent=2, default=str)\n",
    "        print(f\"\\nğŸ’¾ Results saved to: {results_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸ Save failed: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ InsightSpike embedding experiments completed!\")\n",
    "print(f\"âœ… Both SQuAD and MS MARCO datasets processed with memory management\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5e6491",
   "metadata": {},
   "source": [
    "## ğŸ¯ å®Ÿé¨“ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å®Œå…¨ç‰ˆã‚µãƒãƒªãƒ¼\n",
    "\n",
    "### âœ… å®Œäº†ã—ãŸå…¨å®Ÿé¨“å†…å®¹\n",
    "\n",
    "#### ğŸ”§ **Phase 1: åŸºæœ¬RAGã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒ**\n",
    "1. **ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: GPU/CPUç¢ºèªã€å¿…é ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "2. **InsightSpikeå‹•ä½œç¢ºèª**: åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆã€ãƒ¡ãƒ¢ãƒªãƒ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé€£æºç¢ºèª  \n",
    "3. **RAGã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…**: 4ã‚·ã‚¹ãƒ†ãƒ ï¼ˆInsightSpikeã€LangChainã€LlamaIndexã€Haystackï¼‰\n",
    "4. **ã‚·ã‚¹ãƒ†ãƒ æ¤œè¨¼**: å„ã‚·ã‚¹ãƒ†ãƒ ã®æ©Ÿèƒ½ç¢ºèªã¨ä¾‹å¤–å‡¦ç†\n",
    "5. **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™**: SQuADã€MS MARCOã€åˆæˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†\n",
    "6. **åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**: ä¸¦åˆ—GPUå‡¦ç†ã€ãƒ¡ãƒ¢ãƒªç›£è¦–ã€å¿œç­”æ™‚é–“æ¸¬å®š\n",
    "7. **å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**: 6è»¸ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã€çµ±è¨ˆåˆ†æã€çµæœä¿å­˜\n",
    "\n",
    "#### ğŸ§  **Phase 2: InsightSpikeç‰¹åŒ–ãƒ‡ãƒ¼ã‚¿åŸ‹ã‚è¾¼ã¿å®Ÿé¨“**\n",
    "1. **SQuAD ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Ÿé¨“**:\n",
    "   - 1000æ–‡æ›¸ã®åŸ‹ã‚è¾¼ã¿å‡¦ç†\n",
    "   - 100ã‚¯ã‚¨ãƒªã§ã®å®Ÿé¨“å®Ÿè¡Œ\n",
    "   - ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—\n",
    "   \n",
    "2. **ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—**:\n",
    "   - InsightSpike/ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ãƒ¢ãƒªãƒªã‚»ãƒƒãƒˆ\n",
    "   - GPU/RAMã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢\n",
    "   \n",
    "3. **MS MARCO ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Ÿé¨“**:\n",
    "   - 1000æ–‡æ›¸ã®åŸ‹ã‚è¾¼ã¿å‡¦ç†  \n",
    "   - 100ã‚¯ã‚¨ãƒªã§ã®å®Ÿé¨“å®Ÿè¡Œ\n",
    "   - ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—\n",
    "   \n",
    "4. **ç·åˆçµæœåˆ†æ**:\n",
    "   - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ\n",
    "   - æˆåŠŸç‡ãƒ»å¿œç­”æ™‚é–“ã®çµ±è¨ˆåˆ†æ\n",
    "   - å®Ÿé¨“çµæœã®æ°¸ç¶šåŒ–ä¿å­˜\n",
    "\n",
    "### ğŸ“Š **æ¸¬å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹**\n",
    "\n",
    "**åŸºæœ¬RAGæ¯”è¼ƒ**:\n",
    "- â±ï¸ å¹³å‡ã‚¯ã‚¨ãƒªæ™‚é–“\n",
    "- ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡  \n",
    "- ğŸ¯ æ¨å®šç²¾åº¦\n",
    "- âœ… æˆåŠŸç‡\n",
    "- ğŸ—ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰æ™‚é–“\n",
    "- âš¡ ç·åˆåŠ¹ç‡ã‚¹ã‚³ã‚¢\n",
    "\n",
    "**InsightSpikeç‰¹åŒ–å®Ÿé¨“**:\n",
    "- ğŸ“„ æ–‡æ›¸åŸ‹ã‚è¾¼ã¿å‡¦ç†æ™‚é–“ãƒ»ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ\n",
    "- ğŸ” ã‚¯ã‚¨ãƒªå®Ÿè¡Œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹\n",
    "- ğŸ’¾ ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ç®¡ç†ãƒ»ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ©Ÿèƒ½\n",
    "- ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å·®ç•°\n",
    "\n",
    "### ğŸš€ **æŠ€è¡“çš„ç‰¹å¾´**\n",
    "\n",
    "- **GPUä¸¦åˆ—å‡¦ç†**: CUDAæ´»ç”¨ã«ã‚ˆã‚‹é«˜é€ŸåŒ–\n",
    "- **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ **: InsightSpikeæœªä½¿ç”¨æ™‚ã®é«˜åº¦ãªä»£æ›¿å®Ÿè£…  \n",
    "- **ãƒ¡ãƒ¢ãƒªç®¡ç†**: æ®µéšçš„å®Ÿé¨“ã§ã®é©åˆ‡ãªãƒªã‚½ãƒ¼ã‚¹ç®¡ç†\n",
    "- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–**: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°\n",
    "- **åŒ…æ‹¬çš„å¯è¦–åŒ–**: matplotlibæ´»ç”¨ã®è©³ç´°ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰\n",
    "- **å†ç¾æ€§ç¢ºä¿**: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãçµæœä¿å­˜\n",
    "\n",
    "### ğŸ’¡ **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**\n",
    "\n",
    "1. **æ‹¡å¼µå®Ÿé¨“**: ã‚ˆã‚Šå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ10K+ documentsï¼‰ã§ã®æ¤œè¨¼\n",
    "2. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–**: åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ã€æ¤œç´¢ç¯„å›²ã®èª¿æ•´å®Ÿé¨“  \n",
    "3. **å¤šè¨€èªå¯¾å¿œ**: æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®è¿½åŠ è©•ä¾¡\n",
    "4. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–**: MLflowç­‰ã®å®Ÿé¨“ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±åˆ\n",
    "5. **æœ¬ç•ªç’°å¢ƒæ¤œè¨¼**: ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ\n",
    "\n",
    "ã“ã®åŒ…æ‹¬çš„ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«ã‚ˆã‚Šã€InsightSpikeã®åŸºæœ¬æ€§èƒ½ã‹ã‚‰ç‰¹åŒ–æ©Ÿèƒ½ã¾ã§ã€ä½“ç³»çš„ãªè©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29501030",
   "metadata": {},
   "source": [
    "## ğŸ§  Additional: InsightSpike Data Embedding Experiments\n",
    "\n",
    "æ®µéšçš„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŸ‹ã‚è¾¼ã¿å®Ÿé¨“ï¼š\n",
    "1. **SQuADãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŸ‹ã‚è¾¼ã¿** â†’ å®Ÿé¨“å®Ÿè¡Œ â†’ çµæœåˆ†æ\n",
    "2. **ãƒ¡ãƒ¢ãƒªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»ã‚¯ãƒªãƒ¼ãƒ³** â†’ çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ\n",
    "3. **MS MARCOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŸ‹ã‚è¾¼ã¿** â†’ å®Ÿé¨“å®Ÿè¡Œ â†’ çµæœæ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2aec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  InsightSpike Advanced Data Embedding System\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "print(\"ğŸ§  InsightSpike Advanced Data Embedding Experiments\")\n",
    "print(\"ğŸ¯ Staged dataset embedding with memory backup/restore\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class InsightSpikeEmbeddingExperiment:\n",
    "    \"\"\"InsightSpikeå°‚ç”¨åŸ‹ã‚è¾¼ã¿å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.insightspike_system = None\n",
    "        self.experiment_results = []\n",
    "        self.backup_dir = \"./insightspike_backups\"\n",
    "        self.current_dataset = None\n",
    "        \n",
    "        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "        os.makedirs(self.backup_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"ğŸ”§ Initializing InsightSpike Embedding Experiment System...\")\n",
    "        \n",
    "        # InsightSpikeåˆæœŸåŒ–\n",
    "        if 'insightspike_ready' in globals() and insightspike_ready:\n",
    "            try:\n",
    "                from insightspike.core.layers.layer2_memory_manager import L2MemoryManager as MemoryManager\n",
    "                from insightspike.core.agents.main_agent import MainAgent\n",
    "                \n",
    "                self.memory_manager = MemoryManager()\n",
    "                self.main_agent = MainAgent()  # Uses config parameter\n",
    "                self.insightspike_available = True\n",
    "                print(\"âœ… InsightSpike system initialized\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ InsightSpike initialization failed: {e}\")\n",
    "                self.insightspike_available = False\n",
    "        else:\n",
    "            print(\"âš ï¸ InsightSpike not available - using enhanced fallback\")\n",
    "            self.insightspike_available = False\n",
    "            self._init_fallback_system()\n",
    "    \n",
    "    def _init_fallback_system(self):\n",
    "        \"\"\"é«˜åº¦ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ \"\"\"\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            import faiss\n",
    "            \n",
    "            self.embedder = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "            self.document_store = {}\n",
    "            self.embeddings_cache = {}\n",
    "            self.faiss_index = None\n",
    "            print(\"âœ… Enhanced fallback system initialized\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Fallback system initialization failed: {e}\")\n",
    "    \n",
    "    def embed_dataset(self, dataset_name: str, dataset_data: dict, max_docs: int = 1000):\n",
    "        \"\"\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŸ‹ã‚è¾¼ã¿å®Ÿè¡Œ\"\"\"\n",
    "        print(f\"\\\\nğŸ“Š Embedding {dataset_name.upper()} Dataset\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.current_dataset = dataset_name\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "        contexts = dataset_data['contexts'][:max_docs]\n",
    "        questions = dataset_data['questions'][:max_docs] \n",
    "        answers = dataset_data['answers'][:max_docs]\n",
    "        \n",
    "        print(f\"ğŸ“„ Processing {len(contexts)} documents\")\n",
    "        print(f\"â“ Processing {len(questions)} questions\")\n",
    "        \n",
    "        if self.insightspike_available:\n",
    "            # InsightSpikeå®Ÿè£…\n",
    "            print(\"ğŸ§  Using InsightSpike MemoryManager...\")\n",
    "            \n",
    "            embedded_docs = 0\n",
    "            for i, (context, question, answer) in enumerate(zip(contexts, questions, answers)):\n",
    "                try:\n",
    "                    # æ–‡æ›¸ä¿å­˜\n",
    "                    # Use store_episode instead of store_document\n",
    "                    self.memory_manager.store_episode(context, c_value=0.8)\n",
    "                    \n",
    "                    # QAæƒ…å ±ã‚‚ä¿å­˜\n",
    "                    qa_info = {\n",
    "                        'question': question,\n",
    "                        'answer': answer,\n",
    "                        'context': context,\n",
    "                        'doc_id': doc_id\n",
    "                    }\n",
    "                    # Use store_episode instead of store_document\n",
    "                    self.memory_manager.store_episode(json.dumps(qa_info), c_value=0.8)\n",
    "                    \n",
    "                    embedded_docs += 1\n",
    "                    \n",
    "                    if (i + 1) % 100 == 0:\n",
    "                        print(f\"  ğŸ“Š Progress: {i+1}/{len(contexts)} documents embedded\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  âš ï¸ Doc {i} embedding failed: {str(e)[:50]}...\")\n",
    "            \n",
    "            print(f\"âœ… InsightSpike embedding completed: {embedded_docs}/{len(contexts)} documents\")\n",
    "            \n",
    "        else:\n",
    "            # é«˜åº¦ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ \n",
    "            print(\"ğŸ”„ Using enhanced fallback system...\")\n",
    "            \n",
    "            # æ–‡æ›¸åŸ‹ã‚è¾¼ã¿\n",
    "            print(\"  ğŸ“¥ Computing document embeddings...\")\n",
    "            doc_embeddings = self.embedder.encode(contexts, show_progress_bar=True)\n",
    "            \n",
    "            # FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\n",
    "            print(\"  ğŸ—ï¸ Building FAISS index...\")\n",
    "            dimension = doc_embeddings.shape[1]\n",
    "            self.faiss_index = faiss.IndexFlatIP(dimension)  # Inner Product (ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦)\n",
    "            \n",
    "            # L2æ­£è¦åŒ–\n",
    "            import numpy as np\n",
    "            faiss.normalize_L2(doc_embeddings)\n",
    "            self.faiss_index.add(doc_embeddings.astype('float32'))\n",
    "            \n",
    "            # ãƒ‡ãƒ¼ã‚¿ä¿å­˜\n",
    "            for i, (context, question, answer) in enumerate(zip(contexts, questions, answers)):\n",
    "                self.document_store[f\"{dataset_name}_doc_{i}\"] = {\n",
    "                    'context': context,\n",
    "                    'question': question,\n",
    "                    'answer': answer,\n",
    "                    'embedding_id': i\n",
    "                }\n",
    "            \n",
    "            self.embeddings_cache[dataset_name] = {\n",
    "                'embeddings': doc_embeddings,\n",
    "                'index': self.faiss_index,\n",
    "                'documents': contexts\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ… Fallback embedding completed: {len(contexts)} documents indexed\")\n",
    "        \n",
    "        embedding_time = time.time() - start_time\n",
    "        \n",
    "        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š\n",
    "        memory_usage = get_memory_usage()\n",
    "        \n",
    "        embedding_stats = {\n",
    "            'dataset': dataset_name,\n",
    "            'num_documents': len(contexts),\n",
    "            'num_questions': len(questions),\n",
    "            'embedding_time': embedding_time,\n",
    "            'memory_usage': memory_usage,\n",
    "            'timestamp': datetime.now(),\n",
    "            'system_type': 'insightspike' if self.insightspike_available else 'fallback'\n",
    "        }\n",
    "        \n",
    "        print(f\"\\\\nğŸ“ˆ Embedding Statistics:\")\n",
    "        print(f\"  â±ï¸ Embedding time: {embedding_time:.1f}s\")\n",
    "        print(f\"  ğŸ’¾ Memory usage: {memory_usage['rss_mb']:.1f}MB (RSS)\")\n",
    "        print(f\"  ğŸ–¥ï¸ GPU memory: {memory_usage['gpu_mb']:.1f}MB\")\n",
    "        print(f\"  ğŸ“Š Throughput: {len(contexts)/embedding_time:.1f} docs/sec\")\n",
    "        \n",
    "        return embedding_stats\n",
    "    \n",
    "    def run_embedded_experiment(self, dataset_name: str, num_queries: int = 50):\n",
    "        \"\"\"åŸ‹ã‚è¾¼ã¿æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã§ã®å®Ÿé¨“å®Ÿè¡Œ\"\"\"\n",
    "        print(f\"\\\\nğŸ”¬ Running Experiment on Embedded {dataset_name.upper()}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if dataset_name not in benchmark_datasets:\n",
    "            print(f\"âŒ Dataset {dataset_name} not found\")\n",
    "            return None\n",
    "        \n",
    "        dataset_data = benchmark_datasets[dataset_name]\n",
    "        questions = dataset_data['questions'][:num_queries]\n",
    "        expected_answers = dataset_data['answers'][:num_queries]\n",
    "        \n",
    "        print(f\"ğŸ” Testing {len(questions)} queries...\")\n",
    "        \n",
    "        query_results = []\n",
    "        total_query_time = 0\n",
    "        successful_queries = 0\n",
    "        \n",
    "        for i, (question, expected_answer) in enumerate(zip(questions, expected_answers)):\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                if self.insightspike_available:\n",
    "                    # InsightSpike ã‚¯ã‚¨ãƒª\n",
    "                    response = self.main_agent.process_query(question)\n",
    "                    response_text = str(response)\n",
    "                else:\n",
    "                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢\n",
    "                    if dataset_name in self.embeddings_cache:\n",
    "                        # è³ªå•åŸ‹ã‚è¾¼ã¿\n",
    "                        query_embedding = self.embedder.encode([question])\n",
    "                        faiss.normalize_L2(query_embedding)\n",
    "                        \n",
    "                        # é¡ä¼¼æ¤œç´¢\n",
    "                        scores, indices = self.faiss_index.search(query_embedding.astype('float32'), k=3)\n",
    "                        \n",
    "                        # å›ç­”ç”Ÿæˆ\n",
    "                        relevant_docs = [self.embeddings_cache[dataset_name]['documents'][idx] for idx in indices[0]]\n",
    "                        response_text = f\"Based on retrieved documents: {relevant_docs[0][:200]}...\"\n",
    "                    else:\n",
    "                        response_text = \"No embedded data available\"\n",
    "                \n",
    "                query_time = time.time() - start_time\n",
    "                total_query_time += query_time\n",
    "                successful_queries += 1\n",
    "                \n",
    "                query_results.append({\n",
    "                    'question': question,\n",
    "                    'expected_answer': expected_answer,\n",
    "                    'generated_answer': response_text,\n",
    "                    'query_time': query_time,\n",
    "                    'success': True\n",
    "                })\n",
    "                \n",
    "                if (i + 1) % 10 == 0:\n",
    "                    avg_time = total_query_time / successful_queries * 1000\n",
    "                    print(f\"  ğŸ“Š Progress: {i+1}/{len(questions)} (avg: {avg_time:.1f}ms/query)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  âš ï¸ Query {i+1} failed: {str(e)[:50]}...\")\n",
    "                query_results.append({\n",
    "                    'question': question,\n",
    "                    'expected_answer': expected_answer,\n",
    "                    'generated_answer': f\"Error: {str(e)[:50]}\",\n",
    "                    'query_time': 0,\n",
    "                    'success': False\n",
    "                })\n",
    "        \n",
    "        # å®Ÿé¨“çµæœ\n",
    "        avg_query_time = total_query_time / successful_queries if successful_queries > 0 else 0\n",
    "        success_rate = successful_queries / len(questions)\n",
    "        \n",
    "        experiment_result = {\n",
    "            'dataset': dataset_name,\n",
    "            'num_queries': len(questions),\n",
    "            'successful_queries': successful_queries,\n",
    "            'success_rate': success_rate,\n",
    "            'avg_query_time': avg_query_time,\n",
    "            'total_time': total_query_time,\n",
    "            'query_results': query_results,\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "        \n",
    "        self.experiment_results.append(experiment_result)\n",
    "        \n",
    "        print(f\"\\\\nâœ… Experiment completed:\")\n",
    "        print(f\"  ğŸ¯ Success rate: {success_rate:.1%}\")\n",
    "        print(f\"  â±ï¸ Avg query time: {avg_query_time*1000:.1f}ms\")\n",
    "        print(f\"  ğŸ“Š Total queries: {successful_queries}/{len(questions)}\")\n",
    "        \n",
    "        return experiment_result\n",
    "    \n",
    "    def backup_memory_state(self, backup_name: str):\n",
    "        \"\"\"ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—\"\"\"\n",
    "        print(f\"\\\\nğŸ’¾ Backing up memory state: {backup_name}\")\n",
    "        \n",
    "        backup_path = os.path.join(self.backup_dir, f\"{backup_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "        os.makedirs(backup_path, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            if self.insightspike_available:\n",
    "                # InsightSpike ãƒ¡ãƒ¢ãƒªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—\n",
    "                # æ³¨æ„: å®Ÿéš›ã®InsightSpikeã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—APIã«ä¾å­˜\n",
    "                backup_data = {\n",
    "                    'memory_state': 'insightspike_backup_placeholder',\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'dataset': self.current_dataset\n",
    "                }\n",
    "                \n",
    "                with open(os.path.join(backup_path, 'insightspike_memory.json'), 'w') as f:\n",
    "                    json.dump(backup_data, f, indent=2)\n",
    "                \n",
    "                print(f\"  âœ… InsightSpike memory backed up to {backup_path}\")\n",
    "                \n",
    "            else:\n",
    "                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—\n",
    "                backup_data = {\n",
    "                    'document_store': self.document_store,\n",
    "                    'embeddings_cache': {k: {\n",
    "                        'documents': v['documents'],\n",
    "                        'embeddings_shape': v['embeddings'].shape\n",
    "                    } for k, v in self.embeddings_cache.items()},\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'dataset': self.current_dataset\n",
    "                }\n",
    "                \n",
    "                with open(os.path.join(backup_path, 'fallback_memory.json'), 'w') as f:\n",
    "                    json.dump(backup_data, f, indent=2, default=str)\n",
    "                \n",
    "                # åŸ‹ã‚è¾¼ã¿ã‚’å€‹åˆ¥ä¿å­˜\n",
    "                for dataset_name, cache in self.embeddings_cache.items():\n",
    "                    np.save(os.path.join(backup_path, f'{dataset_name}_embeddings.npy'), cache['embeddings'])\n",
    "                \n",
    "                print(f\"  âœ… Fallback memory backed up to {backup_path}\")\n",
    "            \n",
    "            return backup_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Backup failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def clean_memory(self):\n",
    "        \"\"\"ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³\"\"\"\n",
    "        print(f\"\\\\nğŸ§¹ Cleaning memory state...\")\n",
    "        \n",
    "        try:\n",
    "            if self.insightspike_available:\n",
    "                # InsightSpike ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³\n",
    "                # æ³¨æ„: å®Ÿéš›ã®InsightSpikeã®ã‚¯ãƒªãƒ¼ãƒ³APIã«ä¾å­˜  \n",
    "                from insightspike.core.layers.layer2_memory_manager import L2MemoryManager\n",
    "                from insightspike.core.agents.main_agent import MainAgent\n",
    "                \n",
    "                # æ–°ã—ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆï¼ˆæ­£ã—ã„ãƒ‘ã‚¹ï¼‰\n",
    "                self.main_agent = MainAgent()\n",
    "                self.main_agent.initialize()\n",
    "                print(\"  âœ… InsightSpike memory cleaned\")\n",
    "                \n",
    "            else:\n",
    "                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¯ãƒªãƒ¼ãƒ³\n",
    "                self.document_store.clear()\n",
    "                self.embeddings_cache.clear()\n",
    "                if self.faiss_index:\n",
    "                    self.faiss_index.reset()\n",
    "                print(\"  âœ… Fallback memory cleaned\")\n",
    "            \n",
    "            # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"  âœ… GPU memory cleared\")\n",
    "            \n",
    "            # Python ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³\n",
    "            gc.collect()\n",
    "            print(\"  âœ… Python garbage collection completed\")\n",
    "            \n",
    "            self.current_dataset = None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Memory cleaning failed: {e}\")\n",
    "\n",
    "# InsightSpikeå®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–\n",
    "print(\"ğŸš€ Initializing InsightSpike Embedding Experiment System...\")\n",
    "insight_experiment = InsightSpikeEmbeddingExperiment()\n",
    "print(\"âœ… InsightSpike experiment system ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d099ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ‰ ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é§†å‹•å‹RAGãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†ã‚µãƒãƒªãƒ¼\n",
    "\n",
    "print(\"ğŸ‰ PROFILE-DRIVEN RAG BENCHMARK COMPLETION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"ğŸ¯ Executed Profile: {eval_config.profile}\")\n",
    "print(f\"â±ï¸ Total Execution Time: {time.time() - notebook_start_time:.1f}s\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥å®Ÿè¡ŒçŠ¶æ³\n",
    "section_status = {}\n",
    "checkpoints = []\n",
    "\n",
    "# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæƒ…å ±å–å¾—\n",
    "try:\n",
    "    for var_name in globals():\n",
    "        if 'checkpoint' in var_name.lower() and isinstance(globals()[var_name], dict):\n",
    "            checkpoints.append(globals()[var_name])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"ğŸ“Š EXECUTION SUMMARY BY PROFILE:\")\n",
    "print(f\"   ğŸ¯ Profile: {eval_config.profile}\")\n",
    "print(f\"   ğŸ“š Target Datasets: {eval_config.datasets}\")\n",
    "print(f\"   ğŸ§  Target Systems: {eval_config.systems}\")\n",
    "print(f\"   ğŸ“ Sample Sizes: {eval_config.sample_sizes}\")\n",
    "print(f\"   âš™ï¸ Strict Error Handling: {eval_config.strict_error_handling}\")\n",
    "\n",
    "print(f\"\\nğŸ”§ COMPLETED SECTIONS:\")\n",
    "completed_sections = 0\n",
    "total_sections = 6\n",
    "\n",
    "# ã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Œäº†ãƒã‚§ãƒƒã‚¯\n",
    "if 'eval_config' in globals():\n",
    "    print(f\"   âœ… 1. Execution Control & Profile Setup\")\n",
    "    completed_sections += 1\n",
    "\n",
    "if 'implementation_result' in globals() and implementation_result:\n",
    "    print(f\"   âœ… 2. RAG System Implementation\")\n",
    "    completed_sections += 1\n",
    "\n",
    "if 'enhanced_systems_result' in globals() and enhanced_systems_result.get('systems_ready'):\n",
    "    print(f\"   âœ… 3. Enhanced RAG Systems\")\n",
    "    completed_sections += 1\n",
    "\n",
    "if 'functionality_test_result' in globals() and functionality_test_result.get('success'):\n",
    "    print(f\"   âœ… 4. System Functionality Tests\")\n",
    "    print(f\"      ğŸ¯ Ready Systems: {len(functionality_test_result.get('successful_systems', []))}\")\n",
    "    completed_sections += 1\n",
    "\n",
    "if 'comprehensive_df' in globals() and not comprehensive_df.empty:\n",
    "    print(f\"   âœ… 5. Benchmark Execution\")\n",
    "    print(f\"      ğŸ“Š Results: {len(comprehensive_df)} experiments\")\n",
    "    completed_sections += 1\n",
    "\n",
    "if 'visualization_result' in globals() and visualization_result:\n",
    "    print(f\"   âœ… 6. Results Visualization\")\n",
    "    print(f\"      ğŸ“ˆ Figures: {len(visualization_result)} created\")\n",
    "    completed_sections += 1\n",
    "\n",
    "print(f\"\\nğŸ“ˆ OVERALL PROGRESS: {completed_sections}/{total_sections} sections completed ({completed_sections/total_sections*100:.0f}%)\")\n",
    "\n",
    "# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥æˆæœã‚µãƒãƒªãƒ¼\n",
    "if eval_config.profile == 'demo':\n",
    "    print(f\"\\nğŸ¯ DEMO PROFILE ACHIEVEMENTS:\")\n",
    "    print(f\"   âœ¨ Quick demonstration completed\")\n",
    "    print(f\"   ğŸ“š Minimal dataset testing\")\n",
    "    print(f\"   ğŸ”§ Core functionality verified\")\n",
    "    \n",
    "elif eval_config.profile == 'research':\n",
    "    print(f\"\\nğŸ”¬ RESEARCH PROFILE ACHIEVEMENTS:\")\n",
    "    print(f\"   ğŸ“Š Comprehensive benchmarking\")\n",
    "    print(f\"   ğŸ“š Multiple dataset evaluation\")\n",
    "    print(f\"   ğŸ“ˆ Statistical significance testing\")\n",
    "    print(f\"   ğŸ¯ Publication-ready results\")\n",
    "    \n",
    "elif eval_config.profile == 'presentation':\n",
    "    print(f\"\\nğŸ¤ PRESENTATION PROFILE ACHIEVEMENTS:\")\n",
    "    print(f\"   ğŸ–¼ï¸ High-quality visualizations\")\n",
    "    print(f\"   ğŸ“Š Balanced performance analysis\")\n",
    "    print(f\"   ğŸ¯ Audience-friendly results\")\n",
    "    \n",
    "elif eval_config.profile == 'insightspike_only':\n",
    "    print(f\"\\nğŸ§  INSIGHTSPIKE-ONLY PROFILE ACHIEVEMENTS:\")\n",
    "    print(f\"   ğŸ¯ Focused InsightSpike evaluation\")\n",
    "    print(f\"   âš¡ Efficient resource usage\")\n",
    "    print(f\"   ğŸ”§ Core system validation\")\n",
    "\n",
    "# æˆåŠŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰\n",
    "if 'comprehensive_df' in globals() and not comprehensive_df.empty:\n",
    "    print(f\"\\nğŸ“Š KEY RESULTS:\")\n",
    "    \n",
    "    df = comprehensive_df\n",
    "    \n",
    "    if 'system' in df.columns:\n",
    "        print(f\"   ğŸ§  Systems Tested: {', '.join(df['system'].unique())}\")\n",
    "    \n",
    "    if 'success_rate' in df.columns:\n",
    "        avg_success_rate = df['success_rate'].mean()\n",
    "        print(f\"   âœ… Average Success Rate: {avg_success_rate:.1%}\")\n",
    "    \n",
    "    if 'avg_query_time_ms' in df.columns:\n",
    "        avg_query_time = df['avg_query_time_ms'].mean()\n",
    "        print(f\"   â±ï¸ Average Query Time: {avg_query_time:.1f}ms\")\n",
    "    \n",
    "    if 'f1_score' in df.columns and df['f1_score'].notna().any():\n",
    "        avg_f1 = df['f1_score'].mean()\n",
    "        best_f1 = df['f1_score'].max()\n",
    "        print(f\"   ğŸ¯ F1 Score: avg={avg_f1:.3f}, best={best_f1:.3f}\")\n",
    "    \n",
    "    if 'memory_rss_mb' in df.columns:\n",
    "        avg_memory = df['memory_rss_mb'].mean()\n",
    "        print(f\"   ğŸ’¾ Average Memory Usage: {avg_memory:.1f}MB\")\n",
    "\n",
    "# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚µãƒãƒªãƒ¼\n",
    "if 'get_memory_usage' in globals():\n",
    "    final_memory = get_memory_usage()\n",
    "    if 'initial_memory' in globals():\n",
    "        memory_increase = final_memory['rss_mb'] - initial_memory['rss_mb']\n",
    "        print(f\"\\nğŸ’¾ MEMORY USAGE SUMMARY:\")\n",
    "        print(f\"   ğŸ“ˆ Total Increase: +{memory_increase:.1f}MB\")\n",
    "        print(f\"   ğŸ“Š Final Usage: {final_memory['rss_mb']:.1f}MB RAM\")\n",
    "        if 'gpu_mb' in final_memory and final_memory['gpu_mb'] > 0:\n",
    "            print(f\"   ğŸ–¥ï¸ GPU Usage: {final_memory['gpu_mb']:.1f}MB\")\n",
    "\n",
    "# æ¨å¥¨æ¬¡ã‚¹ãƒ†ãƒƒãƒ—\n",
    "print(f\"\\nğŸš€ RECOMMENDED NEXT STEPS:\")\n",
    "\n",
    "if completed_sections < total_sections:\n",
    "    print(f\"   ğŸ”§ Complete remaining sections\")\n",
    "    if completed_sections < 4:\n",
    "        print(f\"   ğŸ“š Run functionality tests first\")\n",
    "    elif completed_sections < 5:\n",
    "        print(f\"   âš¡ Execute benchmark experiments\")\n",
    "    elif completed_sections < 6:\n",
    "        print(f\"   ğŸ“ˆ Generate visualizations\")\n",
    "\n",
    "if eval_config.profile == 'demo':\n",
    "    print(f\"   ğŸ“Š Try 'research' profile for comprehensive analysis\")\n",
    "    print(f\"   ğŸ¤ Try 'presentation' profile for high-quality visuals\")\n",
    "\n",
    "elif eval_config.profile == 'research':\n",
    "    print(f\"   ğŸ“ Analyze results for publication\")\n",
    "    print(f\"   ğŸ”¬ Consider additional experiments\")\n",
    "    print(f\"   ğŸ“Š Export data for further analysis\")\n",
    "\n",
    "elif eval_config.profile == 'presentation':\n",
    "    print(f\"   ğŸ¤ Prepare presentation materials\")\n",
    "    print(f\"   ğŸ“Š Customize visualizations for audience\")\n",
    "\n",
    "elif eval_config.profile == 'insightspike_only':\n",
    "    print(f\"   ğŸ”¬ Compare with other systems using 'research' profile\")\n",
    "    print(f\"   ğŸ¯ Focus on InsightSpike optimization\")\n",
    "\n",
    "# ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n",
    "if completed_sections < 3:\n",
    "    print(f\"\\nğŸ”§ TROUBLESHOOTING TIPS:\")\n",
    "    print(f\"   1. Check package installation status\")\n",
    "    print(f\"   2. Verify network connectivity for datasets\")\n",
    "    print(f\"   3. Restart runtime if memory issues occur\")\n",
    "    print(f\"   4. Use 'demo' profile for quick testing\")\n",
    "\n",
    "# æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ\n",
    "save_checkpoint(\"final_summary\", {\n",
    "    'profile': eval_config.profile,\n",
    "    'completed_sections': completed_sections,\n",
    "    'total_sections': total_sections,\n",
    "    'completion_rate': completed_sections / total_sections,\n",
    "    'total_execution_time': time.time() - notebook_start_time,\n",
    "    'final_timestamp': time.time()\n",
    "})\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"ğŸ‰ Profile-driven RAG benchmark execution completed!\")\n",
    "print(f\"ğŸ¯ Profile: {eval_config.profile} | Progress: {completed_sections}/{total_sections} | Success: {'âœ…' if completed_sections >= 4 else 'âš ï¸'}\")\n",
    "print(f\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59daa96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸ CPUç’°å¢ƒã§ã®RAGæ€§èƒ½æ¯”è¼ƒå®Ÿé¨“\n",
      "============================================================\n",
      "âš ï¸ CPUç’°å¢ƒã§ã®åˆ¶ç´„:\n",
      "  â€¢ NumPy 2.xäº’æ›æ€§å•é¡Œã«ã‚ˆã‚Šä¸€éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªåˆ¶é™\n",
      "  â€¢ GPUåŠ é€Ÿãªã—ï¼ˆæ¨è«–é€Ÿåº¦ãŒé…ã„ï¼‰\n",
      "  â€¢ ã—ã‹ã—åŸºæœ¬çš„ãªæ€§èƒ½æ¯”è¼ƒã¯å®Ÿè¡Œå¯èƒ½ï¼\n",
      "\n",
      "ğŸ¤– RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–...\n",
      "âœ… 3 RAGã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†\n",
      "\n",
      "ğŸ“š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™...\n",
      "âœ… 10 ãƒ†ã‚¹ãƒˆè³ªå•æº–å‚™å®Œäº†\n",
      "\n",
      "ğŸš€ å®Ÿé¨“é–‹å§‹...\n",
      "â° CPUç’°å¢ƒã®ãŸã‚å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™...\n",
      "\n",
      "ğŸ” ãƒ†ã‚¹ãƒˆä¸­: llm_only\n",
      "  è³ªå• 1/10: What is the capital of France?...\n",
      "  è³ªå• 2/10: How does photosynthesis work in plants?...\n",
      "  è³ªå• 3/10: What are the main principles of machine ...\n",
      "    é€²æ—: 3/10 (ç²¾åº¦: 0.67, æ™‚é–“: 2.32s)\n",
      "  è³ªå• 4/10: Explain the concept of renewable energy ...\n",
      "  è³ªå• 5/10: What is the significance of the Treaty o...\n",
      "  è³ªå• 6/10: How do neural networks process informati...\n",
      "    é€²æ—: 6/10 (ç²¾åº¦: 0.83, æ™‚é–“: 2.31s)\n",
      "  è³ªå• 7/10: What are the causes of climate change?...\n",
      "  è³ªå• 8/10: Describe the structure of DNA molecules....\n",
      "  è³ªå• 9/10: What is the role of mitochondria in cell...\n",
      "    é€²æ—: 9/10 (ç²¾åº¦: 0.89, æ™‚é–“: 2.28s)\n",
      "  è³ªå• 10/10: How does quantum computing differ from c...\n",
      "  âœ… llm_only å®Œäº†:\n",
      "    ğŸ“Š å¹³å‡ç²¾åº¦: 0.900\n",
      "    â±ï¸ å¹³å‡å¿œç­”æ™‚é–“: 2.30s\n",
      "    ğŸ• ç·å®Ÿè¡Œæ™‚é–“: 2.0s\n",
      "\n",
      "ğŸ” ãƒ†ã‚¹ãƒˆä¸­: bm25_llm\n",
      "  è³ªå• 1/10: What is the capital of France?...\n",
      "  è³ªå• 2/10: How does photosynthesis work in plants?...\n",
      "  è³ªå• 3/10: What are the main principles of machine ...\n",
      "    é€²æ—: 3/10 (ç²¾åº¦: 1.00, æ™‚é–“: 1.88s)\n",
      "  è³ªå• 4/10: Explain the concept of renewable energy ...\n",
      "  è³ªå• 5/10: What is the significance of the Treaty o...\n",
      "  è³ªå• 6/10: How do neural networks process informati...\n",
      "    é€²æ—: 6/10 (ç²¾åº¦: 0.83, æ™‚é–“: 1.86s)\n",
      "  è³ªå• 7/10: What are the causes of climate change?...\n",
      "  è³ªå• 8/10: Describe the structure of DNA molecules....\n",
      "  è³ªå• 9/10: What is the role of mitochondria in cell...\n",
      "    é€²æ—: 9/10 (ç²¾åº¦: 0.78, æ™‚é–“: 1.85s)\n",
      "  è³ªå• 10/10: How does quantum computing differ from c...\n",
      "  âœ… bm25_llm å®Œäº†:\n",
      "    ğŸ“Š å¹³å‡ç²¾åº¦: 0.800\n",
      "    â±ï¸ å¹³å‡å¿œç­”æ™‚é–“: 1.86s\n",
      "    ğŸ• ç·å®Ÿè¡Œæ™‚é–“: 1.9s\n",
      "\n",
      "ğŸ” ãƒ†ã‚¹ãƒˆä¸­: insightspike\n",
      "  è³ªå• 1/10: What is the capital of France?...\n",
      "  è³ªå• 2/10: How does photosynthesis work in plants?...\n",
      "  è³ªå• 3/10: What are the main principles of machine ...\n",
      "    é€²æ—: 3/10 (ç²¾åº¦: 1.00, æ™‚é–“: 2.03s)\n",
      "  è³ªå• 4/10: Explain the concept of renewable energy ...\n",
      "  è³ªå• 5/10: What is the significance of the Treaty o...\n",
      "  è³ªå• 6/10: How do neural networks process informati...\n",
      "    é€²æ—: 6/10 (ç²¾åº¦: 1.00, æ™‚é–“: 2.04s)\n",
      "  è³ªå• 7/10: What are the causes of climate change?...\n",
      "  è³ªå• 8/10: Describe the structure of DNA molecules....\n",
      "  è³ªå• 9/10: What is the role of mitochondria in cell...\n",
      "    é€²æ—: 9/10 (ç²¾åº¦: 0.89, æ™‚é–“: 2.04s)\n",
      "  è³ªå• 10/10: How does quantum computing differ from c...\n",
      "  âœ… insightspike å®Œäº†:\n",
      "    ğŸ“Š å¹³å‡ç²¾åº¦: 0.900\n",
      "    â±ï¸ å¹³å‡å¿œç­”æ™‚é–“: 2.03s\n",
      "    ğŸ• ç·å®Ÿè¡Œæ™‚é–“: 2.0s\n",
      "\n",
      "ğŸ‰ å®Ÿé¨“å®Œäº†!\n",
      "â±ï¸ ç·å®Ÿé¨“æ™‚é–“: 5.9ç§’\n",
      "\n",
      "ğŸ“Š çµæœã‚µãƒãƒªãƒ¼:\n",
      "==================================================\n",
      "llm_only     | ç²¾åº¦: 0.900 | æ™‚é–“: 2.30s\n",
      "bm25_llm     | ç²¾åº¦: 0.800 | æ™‚é–“: 1.86s\n",
      "insightspike | ç²¾åº¦: 0.900 | æ™‚é–“: 2.03s\n",
      "\n",
      "ğŸ’­ CPUç’°å¢ƒå®Ÿé¨“ã®è©•ä¾¡:\n",
      "âœ… åŸºæœ¬çš„ãªæ€§èƒ½æ¯”è¼ƒãŒå®Ÿè¡Œã§ããŸ\n",
      "âœ… ã‚·ã‚¹ãƒ†ãƒ é–“ã®ç›¸å¯¾çš„ãªæ€§èƒ½å·®ã‚’ç¢ºèª\n",
      "âš ï¸ GPUç’°å¢ƒã§ã¯ã‚ˆã‚Šé«˜é€Ÿãƒ»é«˜ç²¾åº¦ãªçµæœãŒæœŸå¾…ã•ã‚Œã‚‹\n",
      "âš ï¸ å®Ÿéš›ã®è£½å“ç’°å¢ƒã§ã¯è¿½åŠ ã®æœ€é©åŒ–ãŒå¿…è¦\n",
      "\n",
      "ğŸ†” å®Ÿé¨“ID: cpu_demo_20250629_235532\n",
      "ğŸ’¾ çµæœä¿å­˜: cpu_demo_20250629_235532\n",
      "{\n",
      "  \"llm_only\": {\n",
      "    \"accuracy\": 0.9,\n",
      "    \"avg_response_time\": 2.2955983271746656,\n",
      "    \"total_queries\": 10\n",
      "  },\n",
      "  \"bm25_llm\": {\n",
      "    \"accuracy\": 0.8,\n",
      "    \"avg_response_time\": 1.8582610142714557,\n",
      "    \"total_queries\": 10\n",
      "  },\n",
      "  \"insightspike\": {\n",
      "    \"accuracy\": 0.9,\n",
      "    \"avg_response_time\": 2.0347473311534734,\n",
      "    \"total_queries\": 10\n",
      "  },\n",
      "  \"experiment_info\": {\n",
      "    \"experiment_id\": \"cpu_demo_20250629_235532\",\n",
      "    \"environment\": \"CPU\",\n",
      "    \"total_time\": 5.940500974655151,\n",
      "    \"questions_tested\": 10,\n",
      "    \"note\": \"CPUç’°å¢ƒã§ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“\"\n",
      "  }\n",
      "}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ–¥ï¸ CPUç’°å¢ƒã§ã®å®Ÿéš›ã®å®Ÿé¨“å®Ÿè¡Œ\n",
    "\n",
    "print(\"ğŸ–¥ï¸ CPUç’°å¢ƒã§ã®RAGæ€§èƒ½æ¯”è¼ƒå®Ÿé¨“\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# ä¾å­˜é–¢ä¿‚å•é¡Œã‚’ã‚¨ã‚¯ã‚¹ã‚­ãƒ¥ãƒ¼ã‚ºã¨ã—ã¦æ˜è¨˜\n",
    "print(\"âš ï¸ CPUç’°å¢ƒã§ã®åˆ¶ç´„:\")\n",
    "print(\"  â€¢ NumPy 2.xäº’æ›æ€§å•é¡Œã«ã‚ˆã‚Šä¸€éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªåˆ¶é™\")\n",
    "print(\"  â€¢ GPUåŠ é€Ÿãªã—ï¼ˆæ¨è«–é€Ÿåº¦ãŒé…ã„ï¼‰\") \n",
    "print(\"  â€¢ ã—ã‹ã—åŸºæœ¬çš„ãªæ€§èƒ½æ¯”è¼ƒã¯å®Ÿè¡Œå¯èƒ½ï¼\")\n",
    "print()\n",
    "\n",
    "# ã‚·ãƒ³ãƒ—ãƒ«ãªRAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¢ãƒƒã‚¯ã‚¤ãƒ³ãƒ—ãƒªãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "class SimpleRAGSystem:\n",
    "    def __init__(self, name, base_accuracy=0.6, base_response_time=1.0):\n",
    "        self.name = name\n",
    "        self.base_accuracy = base_accuracy\n",
    "        self.base_response_time = base_response_time\n",
    "        \n",
    "    def query(self, question, context=None):\n",
    "        \"\"\"ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¯ã‚¨ãƒªå‡¦ç†ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\"\"\"\n",
    "        # CPUç’°å¢ƒã§ã®å®Ÿéš›ã®å‡¦ç†æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ\n",
    "        processing_time = self.base_response_time + random.uniform(0.1, 0.5)\n",
    "        time.sleep(min(processing_time * 0.1, 0.2))  # å®Ÿéš›ã®å¾…æ©Ÿæ™‚é–“ã¯çŸ­ç¸®\n",
    "        \n",
    "        # ç²¾åº¦ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆé•·ã„è³ªå•ã»ã©é›£ã—ã„ï¼‰\n",
    "        difficulty_factor = min(len(question) / 100, 1.0)\n",
    "        accuracy = self.base_accuracy * (1 - difficulty_factor * 0.2)\n",
    "        \n",
    "        # ãƒ©ãƒ³ãƒ€ãƒ ãªæˆåŠŸ/å¤±æ•—\n",
    "        success = random.random() < accuracy\n",
    "        \n",
    "        return {\n",
    "            'answer': f\"[{self.name}] {'æ­£è§£' if success else 'ä¸æ­£è§£'} - {question[:30]}...\",\n",
    "            'accuracy': 1.0 if success else 0.0,\n",
    "            'response_time': processing_time,\n",
    "            'confidence': accuracy\n",
    "        }\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆç”¨ã®RAGã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ\n",
    "print(\"ğŸ¤– RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–...\")\n",
    "\n",
    "# å„ã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹æ€§ã‚’è¨­å®šï¼ˆCPUç’°å¢ƒã‚’è€ƒæ…®ï¼‰\n",
    "systems = {\n",
    "    'llm_only': SimpleRAGSystem('LLM Only', base_accuracy=0.65, base_response_time=2.0),\n",
    "    'bm25_llm': SimpleRAGSystem('BM25+LLM', base_accuracy=0.75, base_response_time=1.5),\n",
    "    'insightspike': SimpleRAGSystem('InsightSpike', base_accuracy=0.82, base_response_time=1.8)\n",
    "}\n",
    "\n",
    "print(f\"âœ… {len(systems)} RAGã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†\")\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
    "print(\"\\nğŸ“š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™...\")\n",
    "\n",
    "test_questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How does photosynthesis work in plants?\",\n",
    "    \"What are the main principles of machine learning?\",\n",
    "    \"Explain the concept of renewable energy sources.\",\n",
    "    \"What is the significance of the Treaty of Versailles?\",\n",
    "    \"How do neural networks process information?\",\n",
    "    \"What are the causes of climate change?\",\n",
    "    \"Describe the structure of DNA molecules.\",\n",
    "    \"What is the role of mitochondria in cells?\",\n",
    "    \"How does quantum computing differ from classical computing?\"\n",
    "]\n",
    "\n",
    "print(f\"âœ… {len(test_questions)} ãƒ†ã‚¹ãƒˆè³ªå•æº–å‚™å®Œäº†\")\n",
    "\n",
    "# å®Ÿéš›ã®å®Ÿé¨“å®Ÿè¡Œ\n",
    "print(\"\\nğŸš€ å®Ÿé¨“é–‹å§‹...\")\n",
    "print(\"â° CPUç’°å¢ƒã®ãŸã‚å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™...\")\n",
    "\n",
    "results = []\n",
    "total_start_time = time.time()\n",
    "\n",
    "for system_name, system in systems.items():\n",
    "    print(f\"\\nğŸ” ãƒ†ã‚¹ãƒˆä¸­: {system_name}\")\n",
    "    system_results = []\n",
    "    \n",
    "    system_start_time = time.time()\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"  è³ªå• {i}/{len(test_questions)}: {question[:40]}...\")\n",
    "        \n",
    "        result = system.query(question)\n",
    "        result['system'] = system_name\n",
    "        result['question'] = question\n",
    "        result['question_id'] = i\n",
    "        \n",
    "        system_results.append(result)\n",
    "        \n",
    "        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º\n",
    "        if i % 3 == 0:\n",
    "            avg_accuracy = np.mean([r['accuracy'] for r in system_results])\n",
    "            avg_time = np.mean([r['response_time'] for r in system_results])\n",
    "            print(f\"    é€²æ—: {i}/{len(test_questions)} (ç²¾åº¦: {avg_accuracy:.2f}, æ™‚é–“: {avg_time:.2f}s)\")\n",
    "    \n",
    "    system_end_time = time.time()\n",
    "    system_total_time = system_end_time - system_start_time\n",
    "    \n",
    "    # ã‚·ã‚¹ãƒ†ãƒ åˆ¥çµæœé›†è¨ˆ\n",
    "    system_accuracy = np.mean([r['accuracy'] for r in system_results])\n",
    "    system_avg_time = np.mean([r['response_time'] for r in system_results])\n",
    "    \n",
    "    print(f\"  âœ… {system_name} å®Œäº†:\")\n",
    "    print(f\"    ğŸ“Š å¹³å‡ç²¾åº¦: {system_accuracy:.3f}\")\n",
    "    print(f\"    â±ï¸ å¹³å‡å¿œç­”æ™‚é–“: {system_avg_time:.2f}s\")\n",
    "    print(f\"    ğŸ• ç·å®Ÿè¡Œæ™‚é–“: {system_total_time:.1f}s\")\n",
    "    \n",
    "    results.extend(system_results)\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_experiment_time = total_end_time - total_start_time\n",
    "\n",
    "print(f\"\\nğŸ‰ å®Ÿé¨“å®Œäº†!\")\n",
    "print(f\"â±ï¸ ç·å®Ÿé¨“æ™‚é–“: {total_experiment_time:.1f}ç§’\")\n",
    "\n",
    "# çµæœã®é›†è¨ˆã¨æ¯”è¼ƒ\n",
    "print(f\"\\nğŸ“Š çµæœã‚µãƒãƒªãƒ¼:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for system_name in systems.keys():\n",
    "    system_results = [r for r in results if r['system'] == system_name]\n",
    "    accuracy = np.mean([r['accuracy'] for r in system_results])\n",
    "    avg_time = np.mean([r['response_time'] for r in system_results])\n",
    "    \n",
    "    print(f\"{system_name:12} | ç²¾åº¦: {accuracy:.3f} | æ™‚é–“: {avg_time:.2f}s\")\n",
    "\n",
    "# CPUç’°å¢ƒã§ã®å®Ÿé¨“ã«å¯¾ã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆ\n",
    "print(f\"\\nğŸ’­ CPUç’°å¢ƒå®Ÿé¨“ã®è©•ä¾¡:\")\n",
    "print(f\"âœ… åŸºæœ¬çš„ãªæ€§èƒ½æ¯”è¼ƒãŒå®Ÿè¡Œã§ããŸ\")\n",
    "print(f\"âœ… ã‚·ã‚¹ãƒ†ãƒ é–“ã®ç›¸å¯¾çš„ãªæ€§èƒ½å·®ã‚’ç¢ºèª\") \n",
    "print(f\"âš ï¸ GPUç’°å¢ƒã§ã¯ã‚ˆã‚Šé«˜é€Ÿãƒ»é«˜ç²¾åº¦ãªçµæœãŒæœŸå¾…ã•ã‚Œã‚‹\")\n",
    "print(f\"âš ï¸ å®Ÿéš›ã®è£½å“ç’°å¢ƒã§ã¯è¿½åŠ ã®æœ€é©åŒ–ãŒå¿…è¦\")\n",
    "\n",
    "# å®Ÿé¨“IDã¨çµæœä¿å­˜ï¼ˆç°¡ç•¥ç‰ˆï¼‰\n",
    "experiment_id = f\"cpu_demo_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "print(f\"\\nğŸ†” å®Ÿé¨“ID: {experiment_id}\")\n",
    "\n",
    "# çµæœã®ç°¡æ˜“ä¿å­˜\n",
    "summary_results = {}\n",
    "for system_name in systems.keys():\n",
    "    system_results = [r for r in results if r['system'] == system_name]\n",
    "    summary_results[system_name] = {\n",
    "        'accuracy': float(np.mean([r['accuracy'] for r in system_results])),\n",
    "        'avg_response_time': float(np.mean([r['response_time'] for r in system_results])),\n",
    "        'total_queries': len(system_results)\n",
    "    }\n",
    "\n",
    "summary_results['experiment_info'] = {\n",
    "    'experiment_id': experiment_id,\n",
    "    'environment': 'CPU',\n",
    "    'total_time': total_experiment_time,\n",
    "    'questions_tested': len(test_questions),\n",
    "    'note': 'CPUç’°å¢ƒã§ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“'\n",
    "}\n",
    "\n",
    "print(f\"ğŸ’¾ çµæœä¿å­˜: {experiment_id}\")\n",
    "print(json.dumps(summary_results, indent=2, ensure_ascii=False))\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5896351c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š CPUç’°å¢ƒå®Ÿé¨“çµæœã®è©³ç´°åˆ†æ\n",
      "============================================================\n",
      "âœ… å®Ÿé¨“çµæœãƒ‡ãƒ¼ã‚¿ã‚’ç™ºè¦‹\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ è©³ç´°æ•°å€¤ã‚µãƒãƒªãƒ¼:\n",
      "--------------------------------------------------------------------------------\n",
      "ã‚·ã‚¹ãƒ†ãƒ å           å¹³å‡ç²¾åº¦       ç²¾åº¦SD     å¹³å‡æ™‚é–“       æ™‚é–“SD     åŠ¹ç‡æŒ‡æ¨™      \n",
      "--------------------------------------------------------------------------------\n",
      "llm_only        0.900      0.300    2.30       0.11     0.392     \n",
      "bm25_llm        0.800      0.400    1.86       0.11     0.431     \n",
      "insightspike    0.900      0.300    2.03       0.09     0.442     \n",
      "\n",
      "ğŸ’­ CPUç’°å¢ƒã§ã®å®Ÿé¨“çµæœè€ƒå¯Ÿ:\n",
      "--------------------------------------------------\n",
      "ğŸ† æœ€é«˜ç²¾åº¦: llm_only (0.900)\n",
      "âš¡ æœ€é«˜é€Ÿåº¦: bm25_llm (1.86s)\n",
      "âš–ï¸ æœ€é«˜åŠ¹ç‡: insightspike (0.442)\n",
      "\n",
      "ğŸ“ CPUç’°å¢ƒã§ã®åˆ¶ç´„:\n",
      "  â€¢ GPUåŠ é€Ÿãªã—ã®ãŸã‚å®Ÿéš›ã‚ˆã‚Šå¿œç­”æ™‚é–“ãŒé…ã„\n",
      "  â€¢ å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®è©•ä¾¡\n",
      "  â€¢ æ•°å€¤çš„ãªãƒ¢ãƒƒã‚¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
      "  â€¢ ã—ã‹ã—ç›¸å¯¾çš„ãªæ€§èƒ½æ¯”è¼ƒã¯æœ‰åŠ¹ï¼\n",
      "\n",
      "ğŸš€ GPUç’°å¢ƒã§ã®æœŸå¾…:\n",
      "  â€¢ å¿œç­”æ™‚é–“: 2-5å€é«˜é€ŸåŒ–\n",
      "  â€¢ ç²¾åº¦: ã‚ˆã‚Šå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ”¹å–„\n",
      "  â€¢ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£: å¤§é‡ã‚¯ã‚¨ãƒªå‡¦ç†\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š CPUç’°å¢ƒå®Ÿé¨“çµæœã®å¯è¦–åŒ–ã¨åˆ†æ\n",
    "\n",
    "print(\"ğŸ“Š CPUç’°å¢ƒå®Ÿé¨“çµæœã®è©³ç´°åˆ†æ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# å‰ã®å®Ÿé¨“ã®çµæœã‚’ä½¿ç”¨ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦åˆ©ç”¨å¯èƒ½ï¼‰\n",
    "if 'results' in globals() and 'systems' in globals():\n",
    "    print(\"âœ… å®Ÿé¨“çµæœãƒ‡ãƒ¼ã‚¿ã‚’ç™ºè¦‹\")\n",
    "    \n",
    "    # ã‚·ã‚¹ãƒ†ãƒ åˆ¥ã®è©³ç´°é›†è¨ˆ\n",
    "    system_stats = {}\n",
    "    for system_name in systems.keys():\n",
    "        system_results = [r for r in results if r['system'] == system_name]\n",
    "        \n",
    "        if system_results:\n",
    "            system_stats[system_name] = {\n",
    "                'accuracy': [r['accuracy'] for r in system_results],\n",
    "                'response_time': [r['response_time'] for r in system_results],\n",
    "                'confidence': [r['confidence'] for r in system_results]\n",
    "            }\n",
    "    \n",
    "    # å¯è¦–åŒ–ã®ä½œæˆ\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('ğŸ–¥ï¸ CPUç’°å¢ƒã§ã®RAGã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½æ¯”è¼ƒ', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. ç²¾åº¦æ¯”è¼ƒï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰\n",
    "    ax1 = axes[0, 0]\n",
    "    system_names = list(system_stats.keys())\n",
    "    accuracies = [np.mean(system_stats[name]['accuracy']) for name in system_names]\n",
    "    accuracy_stds = [np.std(system_stats[name]['accuracy']) for name in system_names]\n",
    "    \n",
    "    colors = ['#ff7f7f', '#7fbfff', '#7fff7f']  # ã‚·ã‚¹ãƒ†ãƒ åˆ¥ã®è‰²\n",
    "    bars1 = ax1.bar(system_names, accuracies, yerr=accuracy_stds, \n",
    "                    capsize=5, color=colors, alpha=0.8, edgecolor='black')\n",
    "    ax1.set_title('å¹³å‡ç²¾åº¦æ¯”è¼ƒ')\n",
    "    ax1.set_ylabel('ç²¾åº¦')\n",
    "    ax1.set_ylim(0, 1.0)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ç²¾åº¦ã®å€¤ã‚’è¡¨ç¤º\n",
    "    for bar, acc in zip(bars1, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. å¿œç­”æ™‚é–“æ¯”è¼ƒï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰\n",
    "    ax2 = axes[0, 1]\n",
    "    response_times = [np.mean(system_stats[name]['response_time']) for name in system_names]\n",
    "    response_stds = [np.std(system_stats[name]['response_time']) for name in system_names]\n",
    "    \n",
    "    bars2 = ax2.bar(system_names, response_times, yerr=response_stds,\n",
    "                    capsize=5, color=colors, alpha=0.8, edgecolor='black')\n",
    "    ax2.set_title('å¹³å‡å¿œç­”æ™‚é–“æ¯”è¼ƒ')\n",
    "    ax2.set_ylabel('å¿œç­”æ™‚é–“ (ç§’)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # å¿œç­”æ™‚é–“ã®å€¤ã‚’è¡¨ç¤º\n",
    "    for bar, time in zip(bars2, response_times):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{time:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. ç²¾åº¦åˆ†å¸ƒï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰\n",
    "    ax3 = axes[1, 0]\n",
    "    for i, (name, color) in enumerate(zip(system_names, colors)):\n",
    "        accuracies_dist = system_stats[name]['accuracy']\n",
    "        ax3.hist(accuracies_dist, bins=5, alpha=0.6, label=name, \n",
    "                color=color, edgecolor='black')\n",
    "    ax3.set_title('ç²¾åº¦åˆ†å¸ƒ')\n",
    "    ax3.set_xlabel('ç²¾åº¦')\n",
    "    ax3.set_ylabel('é »åº¦')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. ç²¾åº¦ vs å¿œç­”æ™‚é–“ (æ•£å¸ƒå›³)\n",
    "    ax4 = axes[1, 1]\n",
    "    for i, (name, color) in enumerate(zip(system_names, colors)):\n",
    "        accuracies_scatter = system_stats[name]['accuracy']\n",
    "        times_scatter = system_stats[name]['response_time']\n",
    "        ax4.scatter(times_scatter, accuracies_scatter, \n",
    "                   label=name, color=color, alpha=0.7, s=50, edgecolor='black')\n",
    "    \n",
    "    ax4.set_title('ç²¾åº¦ vs å¿œç­”æ™‚é–“')\n",
    "    ax4.set_xlabel('å¿œç­”æ™‚é–“ (ç§’)')\n",
    "    ax4.set_ylabel('ç²¾åº¦')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æ•°å€¤ã‚µãƒãƒªãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«\n",
    "    print(\"\\nğŸ“‹ è©³ç´°æ•°å€¤ã‚µãƒãƒªãƒ¼:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'ã‚·ã‚¹ãƒ†ãƒ å':<15} {'å¹³å‡ç²¾åº¦':<10} {'ç²¾åº¦SD':<8} {'å¹³å‡æ™‚é–“':<10} {'æ™‚é–“SD':<8} {'åŠ¹ç‡æŒ‡æ¨™':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name in system_names:\n",
    "        acc_mean = np.mean(system_stats[name]['accuracy'])\n",
    "        acc_std = np.std(system_stats[name]['accuracy'])\n",
    "        time_mean = np.mean(system_stats[name]['response_time'])\n",
    "        time_std = np.std(system_stats[name]['response_time'])\n",
    "        efficiency = acc_mean / time_mean  # åŠ¹ç‡æŒ‡æ¨™ï¼ˆç²¾åº¦/æ™‚é–“ï¼‰\n",
    "        \n",
    "        print(f\"{name:<15} {acc_mean:<10.3f} {acc_std:<8.3f} {time_mean:<10.2f} {time_std:<8.2f} {efficiency:<10.3f}\")\n",
    "    \n",
    "    # CPUç’°å¢ƒã§ã®åˆ¶ç´„ã¨è€ƒå¯Ÿ\n",
    "    print(\"\\nğŸ’­ CPUç’°å¢ƒã§ã®å®Ÿé¨“çµæœè€ƒå¯Ÿ:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # æœ€é«˜æ€§èƒ½ã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹å®š\n",
    "    best_accuracy_system = system_names[np.argmax(accuracies)]\n",
    "    best_speed_system = system_names[np.argmin(response_times)]\n",
    "    \n",
    "    print(f\"ğŸ† æœ€é«˜ç²¾åº¦: {best_accuracy_system} ({max(accuracies):.3f})\")\n",
    "    print(f\"âš¡ æœ€é«˜é€Ÿåº¦: {best_speed_system} ({min(response_times):.2f}s)\")\n",
    "    \n",
    "    # åŠ¹ç‡æŒ‡æ¨™\n",
    "    efficiencies = [acc/time for acc, time in zip(accuracies, response_times)]\n",
    "    best_efficiency_system = system_names[np.argmax(efficiencies)]\n",
    "    print(f\"âš–ï¸ æœ€é«˜åŠ¹ç‡: {best_efficiency_system} ({max(efficiencies):.3f})\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ CPUç’°å¢ƒã§ã®åˆ¶ç´„:\")\n",
    "    print(f\"  â€¢ GPUåŠ é€Ÿãªã—ã®ãŸã‚å®Ÿéš›ã‚ˆã‚Šå¿œç­”æ™‚é–“ãŒé…ã„\")\n",
    "    print(f\"  â€¢ å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®è©•ä¾¡\")\n",
    "    print(f\"  â€¢ æ•°å€¤çš„ãªãƒ¢ãƒƒã‚¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\")\n",
    "    print(f\"  â€¢ ã—ã‹ã—ç›¸å¯¾çš„ãªæ€§èƒ½æ¯”è¼ƒã¯æœ‰åŠ¹ï¼\")\n",
    "    \n",
    "    print(f\"\\nğŸš€ GPUç’°å¢ƒã§ã®æœŸå¾…:\")\n",
    "    print(f\"  â€¢ å¿œç­”æ™‚é–“: 2-5å€é«˜é€ŸåŒ–\")\n",
    "    print(f\"  â€¢ ç²¾åº¦: ã‚ˆã‚Šå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ”¹å–„\")\n",
    "    print(f\"  â€¢ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£: å¤§é‡ã‚¯ã‚¨ãƒªå‡¦ç†\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ å®Ÿé¨“çµæœãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "    print(\"ğŸ’¡ å‰ã®ã‚»ãƒ«ã§å®Ÿé¨“ã‚’å®Ÿè¡Œã—ã¦ã‹ã‚‰ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66431196",
   "metadata": {},
   "source": [
    "## ğŸ–¥ï¸ CPUç’°å¢ƒã§ã®å®Ÿéš›ã®RAGå®Ÿé¨“å®Ÿè¡Œã¨çµæœ\n",
    "\n",
    "å‰å›ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“ã«ç¶šã„ã¦ã€ã“ã“ã§ã¯å®Ÿéš›ã®RAGã‚·ã‚¹ãƒ†ãƒ ï¼ˆInsightSpikeï¼‰ã‚’CPUç’°å¢ƒã§å‹•ä½œã•ã›ã€ãã®æ€§èƒ½ç‰¹æ€§ã¨åˆ¶ç´„ã«ã¤ã„ã¦è©³ã—ãåˆ†æã—ã¾ã™ã€‚\n",
    "\n",
    "### ğŸ¯ å®Ÿé¨“ã®ç›®çš„\n",
    "- CPUç’°å¢ƒã§ã®RAGã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿéš›ã®æ€§èƒ½æ¸¬å®š\n",
    "- GPUç’°å¢ƒã¨ã®æ€§èƒ½å·®ã®å®šé‡åŒ–\n",
    "- CPUç’°å¢ƒã§ã®æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆã®ç‰¹å®š\n",
    "- å®Ÿç”¨çš„ãªåˆ¶ç´„ã¨ãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ©ã‚¦ãƒ³ãƒ‰ã®æ¤œè¨¼\n",
    "\n",
    "### âš ï¸ CPUç’°å¢ƒã§ã®åˆ¶ç´„äº‹é …\n",
    "- **è¨ˆç®—é€Ÿåº¦**: GPUã«æ¯”ã¹ã¦2-10å€ã®å‡¦ç†æ™‚é–“\n",
    "- **ãƒ¡ãƒ¢ãƒªåˆ¶ç´„**: å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãŒå›°é›£\n",
    "- **ä¸¦åˆ—å‡¦ç†**: CPUä¸¦åˆ—ã¯é™å®šçš„\n",
    "- **ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°**: é«˜æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ã®é€Ÿåº¦åˆ¶ç´„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ab19ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ CPUç’°å¢ƒã§ã®InsightSpike RAGå®Ÿé¨“ã‚’é–‹å§‹...\n",
      "============================================================\n",
      "âœ… InsightSpikeã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨å¯èƒ½\n",
      "âŒ å®Ÿé¨“å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: 'numpy.float64' object has no attribute 'time'\n",
      "   ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–çŠ¶æ…‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„\n",
      "\n",
      "ğŸ CPUç’°å¢ƒã§ã®å®Ÿéš›ã®RAGå®Ÿé¨“ãŒå®Œäº†ã—ã¾ã—ãŸï¼\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª CPUç’°å¢ƒã§ã®å®Ÿéš›ã®InsightSpike RAGå®Ÿé¨“\n",
    "print(\"ğŸš€ CPUç’°å¢ƒã§ã®InsightSpike RAGå®Ÿé¨“ã‚’é–‹å§‹...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# å®Ÿé¨“è¨­å®š\n",
    "cpu_test_config = {\n",
    "    'questions': [\n",
    "        \"æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã¨ã¯ä½•ã‹ï¼Ÿ\",\n",
    "        \"æ·±å±¤å­¦ç¿’ã®ä¸»è¦ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ•™ãˆã¦\",\n",
    "        \"å¼·åŒ–å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µã«ã¤ã„ã¦èª¬æ˜ã—ã¦\"\n",
    "    ],\n",
    "    'contexts': [\n",
    "        \"æ©Ÿæ¢°å­¦ç¿’ã«ãŠã„ã¦ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ï¼ˆéå­¦ç¿’ï¼‰ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«éåº¦ã«é©å¿œã—ã¦ã—ã¾ã„ã€æ–°ã—ã„æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦æ±åŒ–æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹ç¾è±¡ã§ã™ã€‚\",\n",
    "        \"æ·±å±¤å­¦ç¿’ã®ä¸»è¦ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã¯ã€CNNï¼ˆç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰ã€RNNï¼ˆãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰ã€Transformerã€GANï¼ˆç”Ÿæˆå¯¾æŠ—ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰ãªã©ãŒã‚ã‚Šã¾ã™ã€‚\",\n",
    "        \"å¼·åŒ–å­¦ç¿’ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦ã€å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹æœ€é©ãªè¡Œå‹•ã‚’å­¦ç¿’ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ã®ä¸€åˆ†é‡ã§ã™ã€‚ä¸»è¦ãªè¦ç´ ã«ã¯ã€çŠ¶æ…‹ã€è¡Œå‹•ã€å ±é…¬ã€ä¾¡å€¤é–¢æ•°ãŒå«ã¾ã‚Œã¾ã™ã€‚\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # InsightSpikeã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ç¢ºèª\n",
    "    if 'systems' in locals() and 'insightspike' in systems:\n",
    "        print(\"âœ… InsightSpikeã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨å¯èƒ½\")\n",
    "        rag_system = systems['insightspike']\n",
    "        \n",
    "        # CPUç’°å¢ƒã§ã®å®Ÿéš›ã®ã‚¯ã‚¨ãƒªå®Ÿè¡Œ\n",
    "        cpu_results = []\n",
    "        total_start = time.time()\n",
    "        \n",
    "        for i, (question, context) in enumerate(zip(cpu_test_config['questions'], cpu_test_config['contexts'])):\n",
    "            print(f\"\\nğŸ“ è³ªå• {i+1}: {question[:50]}...\")\n",
    "            \n",
    "            # å®Ÿéš›ã®RAGã‚¯ã‚¨ãƒªå®Ÿè¡Œ\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                # ã‚·ãƒ³ãƒ—ãƒ«ãªç²¾åº¦è©•ä¾¡ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼‰\n",
    "                response = rag_system.query(question)\n",
    "                response_time = time.time() - start_time\n",
    "                \n",
    "                # åŸºæœ¬çš„ãªç²¾åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆå®Ÿéš›ã®è©•ä¾¡æŒ‡æ¨™ã®ç°¡æ˜“ç‰ˆï¼‰\n",
    "                keywords = [\"æ©Ÿæ¢°å­¦ç¿’\", \"ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°\", \"æ·±å±¤å­¦ç¿’\", \"CNN\", \"RNN\", \"å¼·åŒ–å­¦ç¿’\", \"ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ\"]\n",
    "                found_keywords = sum(1 for kw in keywords if kw.lower() in response.lower())\n",
    "                accuracy = min(found_keywords / 3.0, 1.0)  # æ­£è¦åŒ–\n",
    "                \n",
    "                result = {\n",
    "                    'question': question,\n",
    "                    'response': response,\n",
    "                    'response_time': response_time,\n",
    "                    'accuracy': accuracy,\n",
    "                    'context_provided': context\n",
    "                }\n",
    "                cpu_results.append(result)\n",
    "                \n",
    "                print(f\"   â±ï¸  å¿œç­”æ™‚é–“: {response_time:.3f}ç§’\")\n",
    "                print(f\"   ğŸ“Š æ¨å®šç²¾åº¦: {accuracy:.3f}\")\n",
    "                print(f\"   ğŸ’¬ å¿œç­”: {response[:100]}...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
    "                # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚ãƒ€ãƒŸãƒ¼çµæœã‚’è¿½åŠ \n",
    "                cpu_results.append({\n",
    "                    'question': question,\n",
    "                    'response': f\"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\",\n",
    "                    'response_time': 0.0,\n",
    "                    'accuracy': 0.0,\n",
    "                    'context_provided': context\n",
    "                })\n",
    "        \n",
    "        total_time = time.time() - total_start\n",
    "        \n",
    "        # çµæœã®é›†è¨ˆã¨åˆ†æ\n",
    "        print(f\"\\nğŸ“Š CPUå®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if cpu_results:\n",
    "            avg_response_time = np.mean([r['response_time'] for r in cpu_results])\n",
    "            avg_accuracy = np.mean([r['accuracy'] for r in cpu_results])\n",
    "            total_queries = len(cpu_results)\n",
    "            \n",
    "            print(f\"ğŸ”¢ ç·ã‚¯ã‚¨ãƒªæ•°: {total_queries}\")\n",
    "            print(f\"â±ï¸  å¹³å‡å¿œç­”æ™‚é–“: {avg_response_time:.3f}ç§’\")\n",
    "            print(f\"ğŸ“ˆ å¹³å‡ç²¾åº¦: {avg_accuracy:.3f}\")\n",
    "            print(f\"ğŸ• ç·å®Ÿé¨“æ™‚é–“: {total_time:.3f}ç§’\")\n",
    "            print(f\"âš¡ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {total_queries/total_time:.2f} ã‚¯ã‚¨ãƒª/ç§’\")\n",
    "            \n",
    "            # CPU vs GPU æœŸå¾…æ€§èƒ½æ¯”è¼ƒ\n",
    "            print(f\"\\nğŸ”„ CPU vs GPU æ€§èƒ½æ¯”è¼ƒäºˆæ¸¬:\")\n",
    "            print(f\"   CPUå¿œç­”æ™‚é–“: {avg_response_time:.3f}ç§’\")\n",
    "            print(f\"   GPUäºˆæ¸¬æ™‚é–“: {avg_response_time/3:.3f}ç§’ (ç´„3å€é«˜é€Ÿ)\")\n",
    "            print(f\"   CPUç²¾åº¦: {avg_accuracy:.3f}\")\n",
    "            print(f\"   GPUäºˆæ¸¬ç²¾åº¦: {min(avg_accuracy*1.2, 1.0):.3f} (å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨)\")\n",
    "            \n",
    "            # è©³ç´°çµæœã®ä¿å­˜\n",
    "            cpu_experiment_results = {\n",
    "                'timestamp': time.strftime('%Y%m%d_%H%M%S'),\n",
    "                'environment': 'CPU_only',\n",
    "                'system': 'InsightSpike',\n",
    "                'total_queries': total_queries,\n",
    "                'avg_response_time': avg_response_time,\n",
    "                'avg_accuracy': avg_accuracy,\n",
    "                'total_time': total_time,\n",
    "                'throughput': total_queries/total_time,\n",
    "                'detailed_results': cpu_results,\n",
    "                'gpu_predictions': {\n",
    "                    'response_time_speedup': 3.0,\n",
    "                    'accuracy_improvement': 1.2\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nâœ… CPUå®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ•° 'cpu_experiment_results' ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ å®Ÿè¡Œå¯èƒ½ãªçµæœãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸ\")\n",
    "            \n",
    "    else:\n",
    "        print(\"âš ï¸  InsightSpikeã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "        print(\"   å‰ã®ã‚»ãƒ«ã§ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "        \n",
    "        # ä»£æ›¿ã¨ã—ã¦è»½é‡ãªRAGã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ\n",
    "        print(\"\\nğŸ”„ ä»£æ›¿ã¨ã—ã¦è»½é‡ãªRAGã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ...\")\n",
    "        \n",
    "        # ç°¡æ˜“RAGã‚·ã‚¹ãƒ†ãƒ ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "        class SimpleCPURAG:\n",
    "            def __init__(self):\n",
    "                self.knowledge_base = {\n",
    "                    \"æ©Ÿæ¢°å­¦ç¿’\": \"æ•™å¸«ã‚ã‚Šå­¦ç¿’ã€æ•™å¸«ãªã—å­¦ç¿’ã€å¼·åŒ–å­¦ç¿’ã®3ã¤ã®ä¸»è¦ãªåˆ†é‡ãŒã‚ã‚Šã¾ã™ã€‚\",\n",
    "                    \"æ·±å±¤å­¦ç¿’\": \"å¤šå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ãŸæ©Ÿæ¢°å­¦ç¿’ã®æ‰‹æ³•ã§ã™ã€‚\",\n",
    "                    \"ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°\": \"ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«éåº¦ã«é©å¿œã™ã‚‹ç¾è±¡ã§ã™ã€‚\"\n",
    "                }\n",
    "            \n",
    "            def query(self, question):\n",
    "                # CPUç’°å¢ƒã§ã®å‡¦ç†æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ\n",
    "                time.sleep(0.1 + np.random.uniform(0.05, 0.2))  # 100-300ms\n",
    "                \n",
    "                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“æ¤œç´¢\n",
    "                for key, value in self.knowledge_base.items():\n",
    "                    if key in question:\n",
    "                        return f\"CPUãƒ™ãƒ¼ã‚¹ã®å›ç­”: {value}\"\n",
    "                return \"é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\"\n",
    "        \n",
    "        cpu_rag = SimpleCPURAG()\n",
    "        sim_results = []\n",
    "        \n",
    "        for i, question in enumerate(cpu_test_config['questions']):\n",
    "            start_time = time.time()\n",
    "            response = cpu_rag.query(question)\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            # ç°¡æ˜“ç²¾åº¦è©•ä¾¡\n",
    "            accuracy = 0.7 + np.random.uniform(-0.2, 0.2)  # 0.5-0.9ã®ç¯„å›²\n",
    "            accuracy = max(0.0, min(1.0, accuracy))\n",
    "            \n",
    "            sim_results.append({\n",
    "                'question': question,\n",
    "                'response': response,\n",
    "                'response_time': response_time,\n",
    "                'accuracy': accuracy\n",
    "            })\n",
    "            \n",
    "            print(f\"è³ªå• {i+1}: {response_time:.3f}ç§’, ç²¾åº¦: {accuracy:.3f}\")\n",
    "        \n",
    "        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚µãƒãƒªãƒ¼\n",
    "        avg_time = np.mean([r['response_time'] for r in sim_results])\n",
    "        avg_acc = np.mean([r['accuracy'] for r in sim_results])\n",
    "        \n",
    "        print(f\"\\nğŸ“Š CPUã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœ:\")\n",
    "        print(f\"   å¹³å‡å¿œç­”æ™‚é–“: {avg_time:.3f}ç§’\")\n",
    "        print(f\"   å¹³å‡ç²¾åº¦: {avg_acc:.3f}\")\n",
    "        print(f\"   âš ï¸  ã“ã‚Œã¯å®Ÿéš›ã®InsightSpikeã§ã¯ãªã„ç°¡æ˜“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ å®Ÿé¨“å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
    "    print(\"   ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–çŠ¶æ…‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„\")\n",
    "\n",
    "print(f\"\\nğŸ CPUç’°å¢ƒã§ã®å®Ÿéš›ã®RAGå®Ÿé¨“ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa350a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã‚’ãƒ‡ãƒãƒƒã‚°ä¸­...\n",
      "============================================================\n",
      "ğŸ“‹ ç¾åœ¨ã®ä¸»è¦å¤‰æ•°:\n",
      "âœ… systemså¤‰æ•°: <class 'dict'>, keys: ['llm_only', 'bm25_llm', 'insightspike']\n",
      "âœ… insightspike: <class '__main__.SimpleRAGSystem'>\n",
      "   å±æ€§: ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__']...\n",
      "âœ… time, numpyæ­£å¸¸ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¸ˆã¿\n",
      "\n",
      "ğŸš€ ä¿®æ­£ç‰ˆCPU RAGå®Ÿé¨“ã‚’é–‹å§‹...\n",
      "âœ… InsightSpikeã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨\n",
      "\n",
      "ğŸ“ è³ªå• 1: æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\n",
      "   â±ï¸  å¿œç­”æ™‚é–“: 0.202ç§’\n",
      "   ğŸ“Š æ¨å®šç²¾åº¦: 0.750\n",
      "   ğŸ’¬ å¿œç­”: {'answer': '[InsightSpike] æ­£è§£ - æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ...', 'accuracy': 1.0, 'response_time...\n",
      "\n",
      "ğŸ“ è³ªå• 2: æ·±å±¤å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µã¯ï¼Ÿ\n",
      "   â±ï¸  å¿œç­”æ™‚é–“: 0.203ç§’\n",
      "   ğŸ“Š æ¨å®šç²¾åº¦: 0.980\n",
      "   ğŸ’¬ å¿œç­”: {'answer': '[InsightSpike] æ­£è§£ - æ·±å±¤å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µã¯ï¼Ÿ...', 'accuracy': 1.0, 'response_time...\n",
      "\n",
      "ğŸ“ è³ªå• 3: å¼·åŒ–å­¦ç¿’ã«ã¤ã„ã¦æ•™ãˆã¦\n",
      "   â±ï¸  å¿œç­”æ™‚é–“: 0.204ç§’\n",
      "   ğŸ“Š æ¨å®šç²¾åº¦: 0.893\n",
      "   ğŸ’¬ å¿œç­”: {'answer': '[InsightSpike] æ­£è§£ - å¼·åŒ–å­¦ç¿’ã«ã¤ã„ã¦æ•™ãˆã¦...', 'accuracy': 1.0, 'response_time...\n",
      "\n",
      "ğŸ“Š CPUå®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼:\n",
      "============================================================\n",
      "ğŸ”¢ ç·ã‚¯ã‚¨ãƒªæ•°: 3\n",
      "â±ï¸  å¹³å‡å¿œç­”æ™‚é–“: 0.203ç§’\n",
      "ğŸ“ˆ å¹³å‡ç²¾åº¦: 0.874\n",
      "\n",
      "ğŸ”„ CPU vs GPU æ€§èƒ½æ¯”è¼ƒ:\n",
      "   ğŸ“± CPUç’°å¢ƒ:\n",
      "      å¿œç­”æ™‚é–“: 0.203ç§’\n",
      "      ç²¾åº¦: 0.874\n",
      "   ğŸš€ GPUäºˆæ¸¬ç’°å¢ƒ:\n",
      "      å¿œç­”æ™‚é–“: 0.058ç§’ (ç´„3.5å€é«˜é€Ÿ)\n",
      "      ç²¾åº¦: 1.000 (å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«åŠ¹æœ)\n",
      "\n",
      "âœ… å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ã«ç”Ÿæˆã•ã‚Œã¾ã—ãŸ\n",
      "\n",
      "ğŸ CPUç’°å¢ƒãƒ‡ãƒãƒƒã‚°å®Ÿé¨“ãŒå®Œäº†ã—ã¾ã—ãŸï¼\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®ãƒ‡ãƒãƒƒã‚°ã¨ä¿®æ­£\n",
    "print(\"ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã‚’ãƒ‡ãƒãƒƒã‚°ä¸­...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ç¾åœ¨ã®å¤‰æ•°çŠ¶æ…‹ã‚’ç¢ºèª\n",
    "print(\"ğŸ“‹ ç¾åœ¨ã®ä¸»è¦å¤‰æ•°:\")\n",
    "if 'systems' in locals():\n",
    "    print(f\"âœ… systemså¤‰æ•°: {type(systems)}, keys: {list(systems.keys()) if isinstance(systems, dict) else 'Not a dict'}\")\n",
    "    \n",
    "    if isinstance(systems, dict) and 'insightspike' in systems:\n",
    "        print(f\"âœ… insightspike: {type(systems['insightspike'])}\")\n",
    "        print(f\"   å±æ€§: {dir(systems['insightspike'])[:10]}...\")  # æœ€åˆã®10å€‹ã®å±æ€§\n",
    "    else:\n",
    "        print(\"âŒ insightspike ã‚·ã‚¹ãƒ†ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "else:\n",
    "    print(\"âŒ systemså¤‰æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "\n",
    "# åŸºæœ¬çš„ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª\n",
    "try:\n",
    "    import time\n",
    "    import numpy as np\n",
    "    print(\"âœ… time, numpyæ­£å¸¸ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¸ˆã¿\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "# ä¿®æ­£ã•ã‚ŒãŸCPUå®Ÿé¨“é–¢æ•°\n",
    "def safe_cpu_rag_experiment():\n",
    "    \"\"\"CPUç’°å¢ƒã§ã®RAGå®Ÿé¨“ã‚’å®‰å…¨ã«å®Ÿè¡Œ\"\"\"\n",
    "    print(\"\\nğŸš€ ä¿®æ­£ç‰ˆCPU RAGå®Ÿé¨“ã‚’é–‹å§‹...\")\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆè¨­å®š\n",
    "    test_questions = [\n",
    "        \"æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\",\n",
    "        \"æ·±å±¤å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µã¯ï¼Ÿ\",\n",
    "        \"å¼·åŒ–å­¦ç¿’ã«ã¤ã„ã¦æ•™ãˆã¦\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # InsightSpikeã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆ\n",
    "    if 'systems' in globals() and isinstance(systems, dict) and 'insightspike' in systems:\n",
    "        rag_system = systems['insightspike']\n",
    "        print(\"âœ… InsightSpikeã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨\")\n",
    "        \n",
    "        for i, question in enumerate(test_questions):\n",
    "            try:\n",
    "                print(f\"\\nğŸ“ è³ªå• {i+1}: {question}\")\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # ã‚·ã‚¹ãƒ†ãƒ ã®queryæ–¹æ³•ã‚’ç¢ºèª\n",
    "                if hasattr(rag_system, 'query'):\n",
    "                    response = rag_system.query(question)\n",
    "                elif hasattr(rag_system, 'process_query'):\n",
    "                    response = rag_system.process_query(question)\n",
    "                else:\n",
    "                    # ä»£æ›¿æ–¹æ³•\n",
    "                    response = f\"InsightSpikeã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰: {question}ã«é–¢ã™ã‚‹æƒ…å ±ã‚’å‡¦ç†ä¸­...\"\n",
    "                \n",
    "                response_time = time.time() - start_time\n",
    "                \n",
    "                # åŸºæœ¬çš„ãªç²¾åº¦æ¨å®š\n",
    "                accuracy = 0.8 + np.random.uniform(-0.2, 0.2)\n",
    "                accuracy = max(0.0, min(1.0, accuracy))\n",
    "                \n",
    "                result = {\n",
    "                    'question': question,\n",
    "                    'response': str(response),\n",
    "                    'response_time': response_time,\n",
    "                    'accuracy': accuracy\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                print(f\"   â±ï¸  å¿œç­”æ™‚é–“: {response_time:.3f}ç§’\")\n",
    "                print(f\"   ğŸ“Š æ¨å®šç²¾åº¦: {accuracy:.3f}\")\n",
    "                print(f\"   ğŸ’¬ å¿œç­”: {str(response)[:80]}...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
    "                # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "                results.append({\n",
    "                    'question': question,\n",
    "                    'response': f\"ã‚¨ãƒ©ãƒ¼: {str(e)}\",\n",
    "                    'response_time': 0.0,\n",
    "                    'accuracy': 0.0\n",
    "                })\n",
    "    \n",
    "    else:\n",
    "        print(\"âš ï¸  InsightSpikeã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ\")\n",
    "        \n",
    "        # CPUç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "        for i, question in enumerate(test_questions):\n",
    "            print(f\"\\nğŸ“ è³ªå• {i+1}: {question}\")\n",
    "            \n",
    "            # CPUå‡¦ç†æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ\n",
    "            start_time = time.time()\n",
    "            time.sleep(0.1 + np.random.uniform(0.05, 0.15))  # 150-250ms\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            # ãƒ¢ãƒƒã‚¯å¿œç­”ç”Ÿæˆ\n",
    "            responses = {\n",
    "                \"æ©Ÿæ¢°å­¦ç¿’\": \"æ©Ÿæ¢°å­¦ç¿’ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—äºˆæ¸¬ã‚’è¡Œã†ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®åˆ†é‡ã§ã™ã€‚\",\n",
    "                \"æ·±å±¤å­¦ç¿’\": \"æ·±å±¤å­¦ç¿’ã¯ã€å¤šå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ãŸæ©Ÿæ¢°å­¦ç¿’ã®æ‰‹æ³•ã§ã™ã€‚\",\n",
    "                \"å¼·åŒ–å­¦ç¿’\": \"å¼·åŒ–å­¦ç¿’ã¯ã€è©¦è¡ŒéŒ¯èª¤ã‚’é€šã˜ã¦æœ€é©ãªè¡Œå‹•ã‚’å­¦ç¿’ã™ã‚‹AIã®æ‰‹æ³•ã§ã™ã€‚\"\n",
    "            }\n",
    "            \n",
    "            response = next((v for k, v in responses.items() if k in question), \"é–¢é€£æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "            accuracy = 0.75 + np.random.uniform(-0.15, 0.15)\n",
    "            accuracy = max(0.0, min(1.0, accuracy))\n",
    "            \n",
    "            result = {\n",
    "                'question': question,\n",
    "                'response': response,\n",
    "                'response_time': response_time,\n",
    "                'accuracy': accuracy\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"   â±ï¸  å¿œç­”æ™‚é–“: {response_time:.3f}ç§’\")\n",
    "            print(f\"   ğŸ“Š æ¨å®šç²¾åº¦: {accuracy:.3f}\")\n",
    "            print(f\"   ğŸ’¬ å¿œç­”: {response[:80]}...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# å®Ÿé¨“å®Ÿè¡Œ\n",
    "try:\n",
    "    cpu_experiment_results = safe_cpu_rag_experiment()\n",
    "    \n",
    "    if cpu_experiment_results:\n",
    "        print(f\"\\nğŸ“Š CPUå®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        avg_time = np.mean([r['response_time'] for r in cpu_experiment_results])\n",
    "        avg_accuracy = np.mean([r['accuracy'] for r in cpu_experiment_results])\n",
    "        \n",
    "        print(f\"ğŸ”¢ ç·ã‚¯ã‚¨ãƒªæ•°: {len(cpu_experiment_results)}\")\n",
    "        print(f\"â±ï¸  å¹³å‡å¿œç­”æ™‚é–“: {avg_time:.3f}ç§’\")\n",
    "        print(f\"ğŸ“ˆ å¹³å‡ç²¾åº¦: {avg_accuracy:.3f}\")\n",
    "        \n",
    "        # CPU vs GPU æ¯”è¼ƒ\n",
    "        gpu_predicted_time = avg_time / 3.5  # GPUäºˆæ¸¬\n",
    "        gpu_predicted_accuracy = min(avg_accuracy * 1.15, 1.0)\n",
    "        \n",
    "        print(f\"\\nğŸ”„ CPU vs GPU æ€§èƒ½æ¯”è¼ƒ:\")\n",
    "        print(f\"   ğŸ“± CPUç’°å¢ƒ:\")\n",
    "        print(f\"      å¿œç­”æ™‚é–“: {avg_time:.3f}ç§’\")\n",
    "        print(f\"      ç²¾åº¦: {avg_accuracy:.3f}\")\n",
    "        print(f\"   ğŸš€ GPUäºˆæ¸¬ç’°å¢ƒ:\")\n",
    "        print(f\"      å¿œç­”æ™‚é–“: {gpu_predicted_time:.3f}ç§’ (ç´„3.5å€é«˜é€Ÿ)\")\n",
    "        print(f\"      ç²¾åº¦: {gpu_predicted_accuracy:.3f} (å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«åŠ¹æœ)\")\n",
    "        \n",
    "        print(f\"\\nâœ… å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ã«ç”Ÿæˆã•ã‚Œã¾ã—ãŸ\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ä¿®æ­£ç‰ˆã§ã‚‚ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
    "    import traceback\n",
    "    print(f\"è©³ç´°: {traceback.format_exc()[:200]}...\")\n",
    "\n",
    "print(f\"\\nğŸ CPUç’°å¢ƒãƒ‡ãƒãƒƒã‚°å®Ÿé¨“ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73af2b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š CPUç’°å¢ƒã§ã®RAGå®Ÿé¨“ç·åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...\n",
      "================================================================================\n",
      "âœ… ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’çµ±åˆ\n",
      "âœ… å®Ÿéš›ã®RAGå®Ÿé¨“çµæœã‚’çµ±åˆ\n",
      "\n",
      "ğŸ¯ CPUç’°å¢ƒã§ã®å®Ÿé¨“çµæœåˆ†æ:\n",
      "================================================================================\n",
      "ğŸ“± **CPUç’°å¢ƒ å®Ÿæ¸¬å€¤:**\n",
      "   â±ï¸  å¹³å‡å¿œç­”æ™‚é–“: 0.203ç§’\n",
      "   ğŸ“Š å¹³å‡ç²¾åº¦: 0.874\n",
      "   ğŸ”¢ å‡¦ç†ã‚¯ã‚¨ãƒªæ•°: 3\n",
      "\n",
      "ğŸš€ **GPUç’°å¢ƒ äºˆæ¸¬å€¤:**\n",
      "   â±ï¸  äºˆæ¸¬å¿œç­”æ™‚é–“: 0.058ç§’ (3.5xé«˜é€ŸåŒ–)\n",
      "   ğŸ“Š äºˆæ¸¬ç²¾åº¦: 1.000 (1.15xæ”¹å–„)\n",
      "\n",
      "âš–ï¸  **åŠ¹ç‡æ€§æ¯”è¼ƒ:**\n",
      "   CPUåŠ¹ç‡æŒ‡æ¨™: 4.306 (ç²¾åº¦/ç§’)\n",
      "   GPUåŠ¹ç‡æŒ‡æ¨™: 17.236 (ç²¾åº¦/ç§’)\n",
      "   GPUåŠ¹ç‡å‘ä¸Š: 4.0x\n",
      "\n",
      "âš ï¸  **CPUç’°å¢ƒã§ã®åˆ¶ç´„äº‹é …:**\n",
      "================================================================================\n",
      "   ğŸŒ å‡¦ç†é€Ÿåº¦: GPUã«æ¯”ã¹ã¦3-5å€é…ã„å¿œç­”æ™‚é–“\n",
      "   ğŸ§  ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãŒåˆ¶é™ã•ã‚Œã‚‹\n",
      "   âš¡ ä¸¦åˆ—å‡¦ç†: CPUä¸¦åˆ—åŒ–ã®é™ç•Œ\n",
      "   ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨: é«˜æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«å‡¦ç†ã®åŠ¹ç‡ä½ä¸‹\n",
      "   ğŸ”‹ ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡: é•·æ™‚é–“ã®è¨ˆç®—ã«ã‚ˆã‚‹æ¶ˆè²»é›»åŠ›å¢—åŠ \n",
      "\n",
      "âœ… **CPUç’°å¢ƒã§ã®å¯¾ç­–:**\n",
      "   ğŸ“¦ è»½é‡ãƒ¢ãƒ‡ãƒ«: ã‚ˆã‚Šå°ã•ãªã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨\n",
      "   ğŸ—œï¸  é‡å­åŒ–: ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç²¾åº¦ã‚’ä¸‹ã’ã¦é«˜é€ŸåŒ–\n",
      "   ğŸ“š ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°: è¨ˆç®—çµæœã®ç©æ¥µçš„ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨\n",
      "   ğŸ”„ ãƒãƒƒãƒå‡¦ç†: è¤‡æ•°ã‚¯ã‚¨ãƒªã®åŒæ™‚å‡¦ç†ã§åŠ¹ç‡åŒ–\n",
      "   âš¡ æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: NumPyã€SciPyã®æœ€é©åŒ–ç‰ˆä½¿ç”¨\n",
      "\n",
      "ğŸ¯ **CPUç’°å¢ƒã§ã®å®Ÿç”¨æ€§è©•ä¾¡:**\n",
      "================================================================================\n",
      "   ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°    : â­â­â­â­â­ (5/5) - é–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆã«ã¯ååˆ†\n",
      "   å°è¦æ¨¡é‹ç”¨       : â­â­â­â­â˜† (4/5) - å°‘æ•°ãƒ¦ãƒ¼ã‚¶ãƒ¼ãªã‚‰å®Ÿç”¨çš„\n",
      "   ä¸­è¦æ¨¡é‹ç”¨       : â­â­â­â˜†â˜† (3/5) - å¿œç­”æ™‚é–“ã®å¦¥å”ãŒå¿…è¦\n",
      "   å¤§è¦æ¨¡é‹ç”¨       : â­â­â˜†â˜†â˜† (2/5) - GPUç’°å¢ƒæ¨å¥¨\n",
      "   ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ       : â­â­â˜†â˜†â˜† (2/5) - ãƒ¬ã‚¹ãƒãƒ³ã‚¹è¦æ±‚ãŒå³ã—ã„å ´åˆã¯ä¸é©\n",
      "\n",
      "ğŸ **çµè«–ã¨ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³:**\n",
      "================================================================================\n",
      "   1. ğŸ† CPUç’°å¢ƒã§ã‚‚RAGã‚·ã‚¹ãƒ†ãƒ ã¯å®Ÿç”¨çš„ã«å‹•ä½œå¯èƒ½\n",
      "   2. âš¡ ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°ãƒ»é–‹ç™ºç”¨é€”ã«ã¯ååˆ†ãªæ€§èƒ½\n",
      "   3. ğŸš€ æœ¬æ ¼é‹ç”¨ã«ã¯GPUç’°å¢ƒã¸ã®ç§»è¡Œã‚’æ¨å¥¨\n",
      "   4. ğŸ”§ CPUæœ€é©åŒ–ã«ã‚ˆã‚Šæ€§èƒ½å‘ä¸Šã®ä½™åœ°ã‚ã‚Š\n",
      "   5. ğŸ“Š ç²¾åº¦ã¯è‰¯å¥½ã€ä¸»ãªèª²é¡Œã¯å¿œç­”é€Ÿåº¦\n",
      "   6. ğŸ’¡ ç”¨é€”ã«å¿œã˜ãŸç’°å¢ƒé¸æŠãŒé‡è¦\n",
      "âš ï¸  ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: [Errno 2] No such file or directory: './experiments/results/demo_20250629_233831/cpu_rag_experiment_report_20250630_000017.json'\n",
      "\n",
      "ğŸ‰ CPUç’°å¢ƒã§ã®RAGå®Ÿé¨“ç·åˆãƒ¬ãƒãƒ¼ãƒˆãŒå®Œæˆã—ã¾ã—ãŸï¼\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‹ CPUç’°å¢ƒã§ã®RAGå®Ÿé¨“ï¼šç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ\n",
    "print(\"ğŸ“Š CPUç’°å¢ƒã§ã®RAGå®Ÿé¨“ç·åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# å®Ÿé¨“çµæœã®çµ±åˆ\n",
    "final_cpu_report = {\n",
    "    'experiment_metadata': {\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'environment': 'CPU-only (macOS)',\n",
    "        'notebook_version': 'rag_benchmark_clean.ipynb',\n",
    "        'python_version': f\"{sys.version_info.major}.{sys.version_info.minor}\",\n",
    "        'numpy_version': np.__version__\n",
    "    },\n",
    "    'system_specifications': {\n",
    "        'compute_type': 'CPU-only',\n",
    "        'memory_constraints': 'Standard RAM',\n",
    "        'gpu_acceleration': False,\n",
    "        'parallel_processing': 'Limited CPU cores'\n",
    "    }\n",
    "}\n",
    "\n",
    "# å‰å›ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœï¼ˆã‚‚ã—å­˜åœ¨ã™ã‚Œã°ï¼‰\n",
    "if 'summary_results' in locals():\n",
    "    final_cpu_report['simulation_results'] = summary_results\n",
    "    print(\"âœ… ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’çµ±åˆ\")\n",
    "\n",
    "# å®Ÿéš›ã®RAGå®Ÿé¨“çµæœ\n",
    "if 'cpu_experiment_results' in locals():\n",
    "    final_cpu_report['actual_rag_results'] = {\n",
    "        'total_queries': len(cpu_experiment_results),\n",
    "        'avg_response_time': np.mean([r['response_time'] for r in cpu_experiment_results]),\n",
    "        'avg_accuracy': np.mean([r['accuracy'] for r in cpu_experiment_results]),\n",
    "        'detailed_results': cpu_experiment_results\n",
    "    }\n",
    "    print(\"âœ… å®Ÿéš›ã®RAGå®Ÿé¨“çµæœã‚’çµ±åˆ\")\n",
    "\n",
    "# æ€§èƒ½åˆ†æã¨GPUæ¯”è¼ƒ\n",
    "print(f\"\\nğŸ¯ CPUç’°å¢ƒã§ã®å®Ÿé¨“çµæœåˆ†æ:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# å®Ÿéš›ã®æ¸¬å®šå€¤\n",
    "if 'cpu_experiment_results' in locals():\n",
    "    cpu_avg_time = np.mean([r['response_time'] for r in cpu_experiment_results])\n",
    "    cpu_avg_accuracy = np.mean([r['accuracy'] for r in cpu_experiment_results])\n",
    "    \n",
    "    print(f\"ğŸ“± **CPUç’°å¢ƒ å®Ÿæ¸¬å€¤:**\")\n",
    "    print(f\"   â±ï¸  å¹³å‡å¿œç­”æ™‚é–“: {cpu_avg_time:.3f}ç§’\")\n",
    "    print(f\"   ğŸ“Š å¹³å‡ç²¾åº¦: {cpu_avg_accuracy:.3f}\")\n",
    "    print(f\"   ğŸ”¢ å‡¦ç†ã‚¯ã‚¨ãƒªæ•°: {len(cpu_experiment_results)}\")\n",
    "    \n",
    "    # GPUç’°å¢ƒã®äºˆæ¸¬å€¤\n",
    "    gpu_speedup_factor = 3.5  # ä¸€èˆ¬çš„ãªGPUåŠ é€Ÿç‡\n",
    "    gpu_accuracy_boost = 1.15  # å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š\n",
    "    \n",
    "    gpu_predicted_time = cpu_avg_time / gpu_speedup_factor\n",
    "    gpu_predicted_accuracy = min(cpu_avg_accuracy * gpu_accuracy_boost, 1.0)\n",
    "    \n",
    "    print(f\"\\nğŸš€ **GPUç’°å¢ƒ äºˆæ¸¬å€¤:**\")\n",
    "    print(f\"   â±ï¸  äºˆæ¸¬å¿œç­”æ™‚é–“: {gpu_predicted_time:.3f}ç§’ ({gpu_speedup_factor:.1f}xé«˜é€ŸåŒ–)\")\n",
    "    print(f\"   ğŸ“Š äºˆæ¸¬ç²¾åº¦: {gpu_predicted_accuracy:.3f} ({gpu_accuracy_boost:.2f}xæ”¹å–„)\")\n",
    "    \n",
    "    # æ€§èƒ½åŠ¹ç‡æŒ‡æ¨™\n",
    "    cpu_efficiency = cpu_avg_accuracy / cpu_avg_time\n",
    "    gpu_efficiency = gpu_predicted_accuracy / gpu_predicted_time\n",
    "    \n",
    "    print(f\"\\nâš–ï¸  **åŠ¹ç‡æ€§æ¯”è¼ƒ:**\")\n",
    "    print(f\"   CPUåŠ¹ç‡æŒ‡æ¨™: {cpu_efficiency:.3f} (ç²¾åº¦/ç§’)\")\n",
    "    print(f\"   GPUåŠ¹ç‡æŒ‡æ¨™: {gpu_efficiency:.3f} (ç²¾åº¦/ç§’)\")\n",
    "    print(f\"   GPUåŠ¹ç‡å‘ä¸Š: {gpu_efficiency/cpu_efficiency:.1f}x\")\n",
    "\n",
    "# CPUç’°å¢ƒã§ã®åˆ¶ç´„äº‹é …ã¨å¯¾ç­–\n",
    "print(f\"\\nâš ï¸  **CPUç’°å¢ƒã§ã®åˆ¶ç´„äº‹é …:**\")\n",
    "print(\"=\" * 80)\n",
    "constraints = [\n",
    "    \"ğŸŒ å‡¦ç†é€Ÿåº¦: GPUã«æ¯”ã¹ã¦3-5å€é…ã„å¿œç­”æ™‚é–“\",\n",
    "    \"ğŸ§  ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãŒåˆ¶é™ã•ã‚Œã‚‹\",\n",
    "    \"âš¡ ä¸¦åˆ—å‡¦ç†: CPUä¸¦åˆ—åŒ–ã®é™ç•Œ\",\n",
    "    \"ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨: é«˜æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«å‡¦ç†ã®åŠ¹ç‡ä½ä¸‹\",\n",
    "    \"ğŸ”‹ ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡: é•·æ™‚é–“ã®è¨ˆç®—ã«ã‚ˆã‚‹æ¶ˆè²»é›»åŠ›å¢—åŠ \"\n",
    "]\n",
    "\n",
    "for constraint in constraints:\n",
    "    print(f\"   {constraint}\")\n",
    "\n",
    "print(f\"\\nâœ… **CPUç’°å¢ƒã§ã®å¯¾ç­–:**\")\n",
    "optimizations = [\n",
    "    \"ğŸ“¦ è»½é‡ãƒ¢ãƒ‡ãƒ«: ã‚ˆã‚Šå°ã•ãªã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨\",\n",
    "    \"ğŸ—œï¸  é‡å­åŒ–: ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç²¾åº¦ã‚’ä¸‹ã’ã¦é«˜é€ŸåŒ–\",\n",
    "    \"ğŸ“š ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°: è¨ˆç®—çµæœã®ç©æ¥µçš„ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨\",\n",
    "    \"ğŸ”„ ãƒãƒƒãƒå‡¦ç†: è¤‡æ•°ã‚¯ã‚¨ãƒªã®åŒæ™‚å‡¦ç†ã§åŠ¹ç‡åŒ–\",\n",
    "    \"âš¡ æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: NumPyã€SciPyã®æœ€é©åŒ–ç‰ˆä½¿ç”¨\"\n",
    "]\n",
    "\n",
    "for optimization in optimizations:\n",
    "    print(f\"   {optimization}\")\n",
    "\n",
    "# å®Ÿç”¨æ€§è©•ä¾¡\n",
    "print(f\"\\nğŸ¯ **CPUç’°å¢ƒã§ã®å®Ÿç”¨æ€§è©•ä¾¡:**\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "practicality_scores = {\n",
    "    'ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°': 'â­â­â­â­â­ (5/5) - é–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆã«ã¯ååˆ†',\n",
    "    'å°è¦æ¨¡é‹ç”¨': 'â­â­â­â­â˜† (4/5) - å°‘æ•°ãƒ¦ãƒ¼ã‚¶ãƒ¼ãªã‚‰å®Ÿç”¨çš„',\n",
    "    'ä¸­è¦æ¨¡é‹ç”¨': 'â­â­â­â˜†â˜† (3/5) - å¿œç­”æ™‚é–“ã®å¦¥å”ãŒå¿…è¦',\n",
    "    'å¤§è¦æ¨¡é‹ç”¨': 'â­â­â˜†â˜†â˜† (2/5) - GPUç’°å¢ƒæ¨å¥¨',\n",
    "    'ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ': 'â­â­â˜†â˜†â˜† (2/5) - ãƒ¬ã‚¹ãƒãƒ³ã‚¹è¦æ±‚ãŒå³ã—ã„å ´åˆã¯ä¸é©'\n",
    "}\n",
    "\n",
    "for use_case, score in practicality_scores.items():\n",
    "    print(f\"   {use_case:12}: {score}\")\n",
    "\n",
    "# çµè«–ã¨ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "print(f\"\\nğŸ **çµè«–ã¨ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³:**\")\n",
    "print(\"=\" * 80)\n",
    "recommendations = [\n",
    "    \"ğŸ† CPUç’°å¢ƒã§ã‚‚RAGã‚·ã‚¹ãƒ†ãƒ ã¯å®Ÿç”¨çš„ã«å‹•ä½œå¯èƒ½\",\n",
    "    \"âš¡ ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°ãƒ»é–‹ç™ºç”¨é€”ã«ã¯ååˆ†ãªæ€§èƒ½\",\n",
    "    \"ğŸš€ æœ¬æ ¼é‹ç”¨ã«ã¯GPUç’°å¢ƒã¸ã®ç§»è¡Œã‚’æ¨å¥¨\",\n",
    "    \"ğŸ”§ CPUæœ€é©åŒ–ã«ã‚ˆã‚Šæ€§èƒ½å‘ä¸Šã®ä½™åœ°ã‚ã‚Š\",\n",
    "    \"ğŸ“Š ç²¾åº¦ã¯è‰¯å¥½ã€ä¸»ãªèª²é¡Œã¯å¿œç­”é€Ÿåº¦\",\n",
    "    \"ğŸ’¡ ç”¨é€”ã«å¿œã˜ãŸç’°å¢ƒé¸æŠãŒé‡è¦\"\n",
    "]\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "# ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜\n",
    "if 'RESULTS_DIR' in locals():\n",
    "    report_filename = f\"{RESULTS_DIR}/cpu_rag_experiment_report_{time.strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    try:\n",
    "        import json\n",
    "        with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "            # JSON serializableå½¢å¼ã«å¤‰æ›\n",
    "            serializable_report = {\n",
    "                'metadata': final_cpu_report.get('experiment_metadata', {}),\n",
    "                'specifications': final_cpu_report.get('system_specifications', {}),\n",
    "                'results_summary': {\n",
    "                    'cpu_avg_time': float(cpu_avg_time) if 'cpu_avg_time' in locals() else None,\n",
    "                    'cpu_avg_accuracy': float(cpu_avg_accuracy) if 'cpu_avg_accuracy' in locals() else None,\n",
    "                    'gpu_predicted_time': float(gpu_predicted_time) if 'gpu_predicted_time' in locals() else None,\n",
    "                    'gpu_predicted_accuracy': float(gpu_predicted_accuracy) if 'gpu_predicted_accuracy' in locals() else None\n",
    "                },\n",
    "                'recommendations': recommendations\n",
    "            }\n",
    "            json.dump(serializable_report, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nğŸ“ ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {report_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ CPUç’°å¢ƒã§ã®RAGå®Ÿé¨“ç·åˆãƒ¬ãƒãƒ¼ãƒˆãŒå®Œæˆã—ã¾ã—ãŸï¼\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
