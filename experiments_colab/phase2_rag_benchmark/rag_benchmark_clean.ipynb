{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d03dbf",
   "metadata": {},
   "source": [
    "# ğŸ” Phase 2: RAG Systems Benchmark - Clean Edition\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯InsightSpikeã®æ–°ã—ã„API (L2MemoryManager, process_question) ã«åˆã‚ã›ã¦æ›´æ–°ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "## ğŸ“‹ å®Ÿé¨“è¨­è¨ˆ\n",
    "- **4ã¤ã®RAGã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒ**: InsightSpike vs LangChain vs LlamaIndex vs Haystack\n",
    "- **GPUä¸¦åˆ—å‡¦ç†**: é«˜é€Ÿé¡ä¼¼åº¦æ¤œç´¢ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯\n",
    "- **å®Ÿãƒ‡ãƒ¼ã‚¿è©•ä¾¡**: SQuADã€MS MARCOç­‰ã®æ¨™æº–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "\n",
    "## ğŸš€ å®Ÿé¨“ãƒ•ãƒ­ãƒ¼\n",
    "1. **ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: ç’°å¢ƒæ§‹ç¯‰ãƒ»ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "2. **InsightSpikeå‹•ä½œç¢ºèª**: åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ\n",
    "3. **RAGã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰**: 4ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ãƒ»å‹•ä½œç¢ºèª\n",
    "4. **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿**: HuggingFaceçµŒç”±ã§SQuADç­‰å–å¾—\n",
    "5. **æ¯”è¼ƒå®Ÿé¨“å®Ÿè¡Œ**: çµ±è¨ˆçš„ã«æœ‰æ„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯\n",
    "6. **çµæœå¯è¦–åŒ–**: è«–æ–‡å“è³ªã®åˆ†æãƒ»ã‚°ãƒ©ãƒ•ä½œæˆ\n",
    "\n",
    "---\n",
    "**å®Ÿè¡Œç’°å¢ƒ**: Google Colab GPU (T4/V100) | **æ¨å®šæ™‚é–“**: 20-30åˆ†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba45f762",
   "metadata": {},
   "source": [
    "## ğŸš€ ç‹¬ç«‹ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼šãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³ & ç’°å¢ƒæ§‹ç¯‰\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ **ç‹¬ç«‹ã—ã¦å®Ÿè¡Œå¯èƒ½** ã§ã™ã€‚InsightSpike-AIãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ã‹ã‚‰å…¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’è‡ªå‹•åŒ–ã•ã‚Œã¦ã„ã¾ã™\n",
    "\n",
    "### ğŸ“‹ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…å®¹\n",
    "1. **ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³**: InsightSpike-AI ã®æœ€æ–°ç‰ˆã‚’å–å¾—\n",
    "2. **ç’°å¢ƒæ§‹ç¯‰**: Pythonä¾å­˜é–¢ä¿‚ã®å®Œå…¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« \n",
    "3. **å†ç¾æ€§ç¢ºä¿**: ã‚·ãƒ¼ãƒ‰å›ºå®šã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†\n",
    "4. **æ¤œè¨¼**: å…¨ã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œç¢ºèª\n",
    "\n",
    "**âš¡ å®Ÿè¡Œæ–¹æ³•**: ä¸Šã‹ã‚‰é †ç•ªã«ã‚»ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã ã‘ã§å®Œäº†ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4356a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ Step 1: ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³ & ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ğŸš€ INSIGHTSPIKE-AI COMPLETE SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª\n",
    "current_dir = Path.cwd()\n",
    "print(f\"ğŸ“ Current directory: {current_dir}\")\n",
    "\n",
    "# InsightSpike-AIãƒªãƒã‚¸ãƒˆãƒªã®ç¢ºèªãƒ»ã‚¯ãƒ­ãƒ¼ãƒ³\n",
    "repo_name = \"InsightSpike-AI\"\n",
    "repo_url = \"https://github.com/miyauchikazuyoshi/InsightSpike-AI.git\"\n",
    "repo_path = current_dir / repo_name\n",
    "\n",
    "if repo_path.exists():\n",
    "    print(f\"ğŸ“‚ Repository already exists at {repo_path}\")\n",
    "    \n",
    "    # æ—¢å­˜ãƒªãƒã‚¸ãƒˆãƒªã®æ›´æ–°\n",
    "    try:\n",
    "        print(\"ğŸ”„ Updating existing repository...\")\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"pull\", \"origin\", \"main\"], \n",
    "            cwd=repo_path, \n",
    "            capture_output=True, \n",
    "            text=True,\n",
    "            timeout=60\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… Repository updated successfully\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Git pull warning: {result.stderr}\")\n",
    "            print(\"ğŸ“¦ Continuing with existing repository...\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Update failed: {e}\")\n",
    "        print(\"ğŸ“¦ Using existing repository...\")\n",
    "\n",
    "else:\n",
    "    # æ–°è¦ã‚¯ãƒ­ãƒ¼ãƒ³\n",
    "    try:\n",
    "        print(f\"ğŸ“¥ Cloning repository from {repo_url}...\")\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"clone\", repo_url, str(repo_path)], \n",
    "            capture_output=True, \n",
    "            text=True,\n",
    "            timeout=300\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… Repository cloned successfully\")\n",
    "        else:\n",
    "            raise Exception(f\"Git clone failed: {result.stderr}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Repository cloning failed: {e}\")\n",
    "        print(\"ğŸ”„ Creating fallback directory structure...\")\n",
    "        \n",
    "        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šåŸºæœ¬ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ä½œæˆ\n",
    "        repo_path.mkdir(exist_ok=True)\n",
    "        (repo_path / \"src\").mkdir(exist_ok=True)\n",
    "        (repo_path / \"src\" / \"insightspike\").mkdir(exist_ok=True)\n",
    "        \n",
    "        print(\"âš ï¸ Fallback mode: Some InsightSpike features may not be available\")\n",
    "\n",
    "# Pythonãƒ‘ã‚¹ã«è¿½åŠ \n",
    "insightspike_src = repo_path / \"src\"\n",
    "if str(insightspike_src) not in sys.path:\n",
    "    sys.path.insert(0, str(insightspike_src))\n",
    "    print(f\"ğŸ“š Added to Python path: {insightspike_src}\")\n",
    "\n",
    "# ç’°å¢ƒå¤‰æ•°è¨­å®š\n",
    "os.environ['INSIGHTSPIKE_ROOT'] = str(repo_path)\n",
    "os.environ['PYTHONPATH'] = str(insightspike_src) + \":\" + os.environ.get('PYTHONPATH', '')\n",
    "\n",
    "# ğŸ¯ å†ç¾æ€§ç¢ºä¿ï¼šã‚·ãƒ¼ãƒ‰å›ºå®šï¼ˆæœ€å„ªå…ˆï¼‰\n",
    "SEED = 42\n",
    "\n",
    "# ğŸ¯ å†ç¾æ€§ç¢ºä¿ï¼šã‚·ãƒ¼ãƒ‰å›ºå®šï¼ˆæœ€å„ªå…ˆï¼‰\n",
    "SEED = 42\n",
    "\n",
    "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆå‰ã®ã‚·ãƒ¼ãƒ‰å›ºå®š\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# PyTorché–¢é€£ã¯å¾Œã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¾Œã«è¨­å®š\n",
    "print(f\"ğŸ¯ SEED fixed: {SEED} (random, numpy)\")\n",
    "\n",
    "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ç¢ºèª\n",
    "print(f\"\\nğŸ“Š Repository structure:\")\n",
    "if repo_path.exists():\n",
    "    for item in sorted(repo_path.iterdir())[:10]:  # æœ€åˆã®10é …ç›®ã®ã¿è¡¨ç¤º\n",
    "        if item.is_dir():\n",
    "            print(f\"  ğŸ“ {item.name}/\")\n",
    "        else:\n",
    "            print(f\"  ğŸ“„ {item.name}\")\n",
    "    \n",
    "    # src/insightspike ã®ç¢ºèª\n",
    "    insightspike_dir = repo_path / \"src\" / \"insightspike\"\n",
    "    if insightspike_dir.exists():\n",
    "        print(f\"\\nğŸ§  InsightSpike modules found:\")\n",
    "        for module in sorted(insightspike_dir.iterdir())[:5]:\n",
    "            if module.is_dir() and not module.name.startswith('__'):\n",
    "                print(f\"  ğŸ”§ {module.name}\")\n",
    "\n",
    "print(f\"\\nâœ… Repository setup completed!\")\n",
    "print(f\"ğŸ“ InsightSpike root: {repo_path}\")\n",
    "print(f\"ğŸ Python path configured: {insightspike_src in sys.path}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18712be",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 1: ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "\n",
    "ç’°å¢ƒæ§‹ç¯‰ã¨å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110504dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ Step 2: åŒ…æ‹¬çš„ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆçµ±åˆç‰ˆï¼šå†ç¾æ€§ãƒ»å…¬å¹³æ€§ç¢ºä¿ï¼‰\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "# å†ç¾æ€§ç¢ºä¿ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰å›ºå®šï¼ˆ3ãƒ©ã‚¤ãƒ–ãƒ©ãƒªçµ±ä¸€ï¼‰\n",
    "SEED = 42\n",
    "\n",
    "# Pythonæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "# NumPy\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# PyTorchï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰\n",
    "try:\n",
    "    import torch\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"âœ… PyTorch seed fixed: {SEED}\")\n",
    "except ImportError:\n",
    "    print(f\"âš ï¸ PyTorch not yet available - will set seed after installation\")\n",
    "\n",
    "print(f\"ğŸŒ± Reproducibility seed fixed: {SEED}\")\n",
    "print(f\"  - random.seed({SEED})\")\n",
    "print(f\"  - np.random.seed({SEED})\")\n",
    "print(f\"  - torch.manual_seed({SEED}) [if available]\")\n",
    "\n",
    "# è­¦å‘ŠæŠ‘åˆ¶\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸš€ COMPREHENSIVE ENVIRONMENT SETUP\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ¯ Reproducibility Seed: {SEED}\")\n",
    "print(f\"ğŸ“… Experiment Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# ãƒªãƒã‚¸ãƒˆãƒªãƒ‘ã‚¹ã®ç¢ºèª\n",
    "repo_path = Path.cwd() / \"InsightSpike-AI\"\n",
    "if repo_path.exists():\n",
    "    print(f\"âœ… InsightSpike repository found: {repo_path}\")\n",
    "    # pyproject.tomlã‹ã‚‰InsightSpikeä¾å­˜é–¢ä¿‚ã‚’ç¢ºèª\n",
    "    pyproject_path = repo_path / \"pyproject.toml\"\n",
    "    if pyproject_path.exists():\n",
    "        print(f\"ğŸ“‹ Found pyproject.toml - will use InsightSpike dependencies\")\n",
    "else:\n",
    "    print(f\"âš ï¸ InsightSpike repository not found - using fallback mode\")\n",
    "\n",
    "# GPU/CPUç’°å¢ƒç¢ºèªã¨ã‚·ãƒ¼ãƒ‰å†å›ºå®š\n",
    "try:\n",
    "    import torch\n",
    "    # ã‚·ãƒ¼ãƒ‰å†å›ºå®šï¼ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œï¼‰\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    if gpu_available:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"ğŸ–¥ï¸ GPU: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
    "        # GPU ãƒ¡ãƒ¢ãƒªã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ãƒªã‚»ãƒƒãƒˆ\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        print(f\"ğŸ”„ GPU memory stats reset\")\n",
    "    else:\n",
    "        print(\"ğŸ’» Running on CPU only\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"âš ï¸ PyTorch not available - will be installed with other packages...\")\n",
    "    gpu_available = False\n",
    "\n",
    "# å¿…é ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼ˆColab T4ãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç‰ˆæœ€é©åŒ–ï¼‰\n",
    "required_packages = {\n",
    "    # === Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ï¼ˆãã®ã¾ã¾ä½¿ç”¨ï¼‰===\n",
    "    \"numpy\": \"2.0.2\",           # Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \"pandas\": \"2.2.2\",          # Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«  \n",
    "    \"matplotlib\": \"3.10.0\",     # Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \"seaborn\": \"0.13.2\",        # Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \"plotly\": \"5.24.1\",         # Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \"scipy\": \"1.15.3\",          # Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \"scikit-learn\": \"1.6.1\",    # Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \"transformers\": \"4.52.4\",   # Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \"datasets\": \"2.14.4\",       # Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆå¾®èª¿æ•´ï¼‰\n",
    "    \"tqdm\": \"4.67.1\",           # Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \"psutil\": \"5.9.5\",          # Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \"tiktoken\": \"0.9.0\",        # Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \"networkx\": \"3.5\",          # Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \"pydantic\": \"2.11.7\",       # Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \"sentence-transformers\": \"4.1.0\",  # Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \n",
    "    # === PyTorch ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ï¼ˆColabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰===\n",
    "    # torchç³»ã¯Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç‰ˆã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ï¼‰\n",
    "    \n",
    "    # === æ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¿…è¦ï¼ˆRAGãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼‰===\n",
    "    \"langchain\": \"0.3.26\",      # æœ€æ–°ç‰ˆï¼ˆColabã«ç„¡ã—ï¼‰\n",
    "    \"llama-index\": \"0.9.0\",     # æ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \"haystack-ai\": \"2.0.0\",     # æ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \n",
    "    # === PyTorch Geometricï¼ˆæ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰===\n",
    "    \"torch-geometric\": \"2.4.0\", # InsightSpikeç”¨\n",
    "    \"torch-scatter\": \"2.1.2\",   # æ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \"torch-sparse\": \"0.6.18\",   # æ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \"torch-cluster\": \"1.6.3\",   # æ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \"torch-spline-conv\": \"1.2.2\", # æ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \n",
    "    # === æ¤œç´¢ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆæ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰===\n",
    "    \"faiss-cpu\": \"1.8.0\",       # æ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \"rank-bm25\": \"0.2.2\",       # æ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \"chromadb\": \"0.4.15\",       # æ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \n",
    "    # === ã‚°ãƒ©ãƒ•å‡¦ç†ï¼ˆæ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰===\n",
    "    \"igraph\": \"0.11.3\",         # æ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    \n",
    "    # === è©•ä¾¡ãƒ»æ¤œè¨¼ï¼ˆæ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰===\n",
    "    \"evaluate\": \"0.4.1\",        # æ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆColabã«ç„¡ã—ï¼‰\n",
    "    \n",
    "    # === ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆæ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰===\n",
    "    \"python-dotenv\": \"1.0.0\"    # æ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"\\nğŸ“¦ Installing {len(required_packages)} packages with Colab optimization...\")\n",
    "\n",
    "# Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡ï¼‰\n",
    "colab_preinstalled = {\n",
    "    \"numpy\", \"pandas\", \"matplotlib\", \"seaborn\", \"plotly\", \"scipy\", \n",
    "    \"scikit-learn\", \"transformers\", \"datasets\", \"tqdm\", \"psutil\", \n",
    "    \"tiktoken\", \"networkx\", \"pydantic\", \"sentence-transformers\"\n",
    "}\n",
    "\n",
    "# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¯¾è±¡ã®åˆ†é¡\n",
    "new_install_packages = {}\n",
    "skip_packages = {}\n",
    "\n",
    "for package, version in required_packages.items():\n",
    "    if package in colab_preinstalled:\n",
    "        skip_packages[package] = version\n",
    "    else:\n",
    "        new_install_packages[package] = version\n",
    "\n",
    "print(f\"âœ… Skipping {len(skip_packages)} Colab preinstalled packages:\")\n",
    "print(f\"   ğŸ“š {', '.join(list(skip_packages.keys())[:5])}{'...' if len(skip_packages) > 5 else ''}\")\n",
    "\n",
    "print(f\"\\nğŸ“¦ Installing {len(new_install_packages)} new packages:\")\n",
    "for i, (pkg, ver) in enumerate(new_install_packages.items(), 1):\n",
    "    category = \"ğŸ”§\" if \"torch\" in pkg else \"ğŸ¤–\" if pkg in [\"langchain\", \"llama-index\", \"haystack-ai\"] else \"ğŸ”\" if pkg in [\"faiss-cpu\", \"rank-bm25\", \"chromadb\"] else \"ğŸ“Š\"\n",
    "    print(f\"   {i:2d}. {category} {pkg}=={ver}\")\n",
    "\n",
    "print(f\"\\nâ±ï¸ Estimated installation time: {len(new_install_packages) * 10}s (individual) or {len(new_install_packages) * 2}s (batch)\")\n",
    "\n",
    "# CLIè¨­å®šï¼ˆsubprocessç”¨ï¼‰\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# CLIè¨­å®šï¼ˆsubprocessç”¨ï¼‰\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# æ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¯¾è±¡ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒªã‚¹ãƒˆä½œæˆ\n",
    "install_commands = []\n",
    "for package, version in new_install_packages.items():\n",
    "    install_commands.append(f\"{package}=={version}\")\n",
    "\n",
    "# ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Ÿè¡Œï¼ˆColabã‚¹ã‚¿ã‚¤ãƒ«å‡ºåŠ›ï¼‰\n",
    "start_install_time = time.time()\n",
    "try:\n",
    "    if install_commands:\n",
    "        print(\"Collecting packages...\")\n",
    "        for cmd in install_commands:\n",
    "            print(f\"  {cmd}\")\n",
    "        \n",
    "        # ãƒãƒƒãƒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«è©¦è¡Œ\n",
    "        print(\"\\nInstalling collected packages...\")\n",
    "        result = subprocess.run([\n",
    "            sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\"\n",
    "        ] + install_commands, \n",
    "        capture_output=False, text=True, timeout=600)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"Successfully installed all packages\")\n",
    "        else:\n",
    "            print(\"Batch installation had issues, trying individual packages...\")\n",
    "            \n",
    "            # å€‹åˆ¥ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰\n",
    "            for package, version in new_install_packages.items():\n",
    "                try:\n",
    "                    print(f\"Collecting {package}=={version}...\")\n",
    "                    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                                  f\"{package}=={version}\", \"--no-cache-dir\"], \n",
    "                                  check=True, timeout=120)\n",
    "                    print(f\"Successfully installed {package}-{version}\")\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    print(f\"  Timeout for {package}, trying without version pin...\")\n",
    "                    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                                  check=True, timeout=60)\n",
    "                    print(f\"Successfully installed {package} (latest)\")\n",
    "                except subprocess.CalledProcessError:\n",
    "                    print(f\"  Failed to install {package}=={version}, trying latest...\")\n",
    "                    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                                  check=True)\n",
    "                    print(f\"Successfully installed {package} (latest)\")\n",
    "    else:\n",
    "        print(\"No new packages to install\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Installation failed: {e}\")\n",
    "    print(\"This may affect some functionality, but we'll continue with available packages.\")\n",
    "    # ç¶šè¡Œï¼ˆä¸€éƒ¨æ©Ÿèƒ½ãŒåˆ¶é™ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŒã€è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ã«ã—ãªã„ï¼‰\n",
    "\n",
    "# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†ã‚µãƒãƒªãƒ¼\n",
    "install_time = time.time() - start_install_time\n",
    "print(f\"\\nğŸ‰ Package installation completed!\")\n",
    "print(f\"â±ï¸ Installation time: {install_time:.1f}s\")\n",
    "print(f\"âœ… Skipped (Colab preinstalled): {len(skip_packages)} packages\")\n",
    "print(f\"ğŸ“¦ Newly installed: {len(new_install_packages)} packages\")\n",
    "\n",
    "# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¤œè¨¼\n",
    "successful_imports = []\n",
    "failed_imports = []\n",
    "\n",
    "for package in new_install_packages.keys():\n",
    "    try:\n",
    "        # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸åã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆåã«å¤‰æ›\n",
    "        import_name = package.replace(\"-\", \"_\")\n",
    "        if package == \"torch-geometric\":\n",
    "            import_name = \"torch_geometric\"\n",
    "        elif package == \"rank-bm25\":\n",
    "            import_name = \"rank_bm25\"\n",
    "        elif package == \"python-dotenv\":\n",
    "            import_name = \"dotenv\"\n",
    "        elif package == \"faiss-cpu\":\n",
    "            import_name = \"faiss\"\n",
    "        elif package == \"haystack-ai\":\n",
    "            import_name = \"haystack\"\n",
    "        elif package.startswith(\"torch-\"):\n",
    "            # torch-scatter, torch-sparse ãªã©\n",
    "            import_name = package.replace(\"-\", \"_\")\n",
    "        \n",
    "        __import__(import_name)\n",
    "        successful_imports.append(package)\n",
    "    except ImportError:\n",
    "        failed_imports.append(package)\n",
    "\n",
    "print(f\"âœ… Successfully verified: {len(successful_imports)}/{len(new_install_packages)} packages\")\n",
    "if failed_imports:\n",
    "    print(f\"âš ï¸ Import verification failed: {failed_imports}\")\n",
    "    print(\"   (This may not affect core functionality)\")\n",
    "\n",
    "print(f\"ğŸš€ Ready for environment optimization\")\n",
    "\n",
    "# ğŸŒ Colabç’°å¢ƒæ¤œå‡ºã¨æœ€é©åŒ–\n",
    "def detect_colab_environment():\n",
    "    \"\"\"Colabç’°å¢ƒæ¤œå‡ºã¨æœ€é©åŒ–è¨­å®š\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        is_colab = True\n",
    "        print(\"ğŸŒ Google Colab environment detected\")\n",
    "        \n",
    "        # Colabæœ€é©åŒ–è¨­å®š\n",
    "        import os\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "        \n",
    "        # Colab GPUæƒ…å ±è¡¨ç¤º\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            if \"T4\" in gpu_name:\n",
    "                print(\"ğŸ–¥ï¸ T4 GPU detected - optimizing for memory efficiency\")\n",
    "            elif \"V100\" in gpu_name:\n",
    "                print(\"ğŸ–¥ï¸ V100 GPU detected - optimizing for performance\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"ğŸ’» Local environment detected\")\n",
    "        return False\n",
    "\n",
    "# ğŸ”§ PyTorch Geometric ç‰¹åˆ¥ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †\n",
    "def install_pytorch_geometric():\n",
    "    \"\"\"PyTorch Geometricå°‚ç”¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆInsightSpikeç”¨ï¼‰\"\"\"\n",
    "    print(\"\\nğŸ”§ Installing PyTorch Geometric dependencies...\")\n",
    "    \n",
    "    try:\n",
    "        # PyTorch Geometricã¯ç‰¹åˆ¥ãªé †åºã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦\n",
    "        geometric_packages = [\n",
    "            \"torch-scatter==2.1.2\",\n",
    "            \"torch-sparse==0.6.18\", \n",
    "            \"torch-cluster==1.6.3\",\n",
    "            \"torch-spline-conv==1.2.2\",\n",
    "            \"torch-geometric==2.4.0\"\n",
    "        ]\n",
    "        \n",
    "        success_count = 0\n",
    "        for i, package in enumerate(geometric_packages, 1):\n",
    "            print(f\"Installing {package} ({i}/{len(geometric_packages)})...\")\n",
    "            try:\n",
    "                # PyGå°‚ç”¨ãƒªãƒ³ã‚¯ã‚’ä½¿ç”¨ã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆã¾ãšè©¦è¡Œï¼‰\n",
    "                result = subprocess.run([\n",
    "                    sys.executable, \"-m\", \"pip\", \"install\", package, \n",
    "                    \"--find-links\", \"https://data.pyg.org/whl/torch-2.1.0+cpu.html\",\n",
    "                    \"--no-cache-dir\"\n",
    "                ], capture_output=True, text=True, timeout=300)\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    print(f\"  âœ… Successfully installed {package}\")\n",
    "                    success_count += 1\n",
    "                else:\n",
    "                    print(f\"  âš ï¸ PyG link failed, trying standard pip...\")\n",
    "                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®pipã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "                    result2 = subprocess.run([\n",
    "                        sys.executable, \"-m\", \"pip\", \"install\", package, \"--no-cache-dir\"\n",
    "                    ], capture_output=True, text=True, timeout=180)\n",
    "                    \n",
    "                    if result2.returncode == 0:\n",
    "                        print(f\"  âœ… Successfully installed {package} (standard pip)\")\n",
    "                        success_count += 1\n",
    "                    else:\n",
    "                        print(f\"  âŒ Failed to install {package}\")\n",
    "                        # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒãƒ¼ã‚¸ãƒ§ãƒ³æŒ‡å®šãªã—\n",
    "                        base_package = package.split(\"==\")[0]\n",
    "                        result3 = subprocess.run([\n",
    "                            sys.executable, \"-m\", \"pip\", \"install\", base_package, \"--no-cache-dir\"\n",
    "                        ], capture_output=True, text=True, timeout=120)\n",
    "                        \n",
    "                        if result3.returncode == 0:\n",
    "                            print(f\"  âœ… Successfully installed {base_package} (latest)\")\n",
    "                            success_count += 1\n",
    "                        else:\n",
    "                            print(f\"  âŒ All install attempts failed for {base_package}\")\n",
    "                            \n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(f\"  â±ï¸ Installation timeout for {package}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Exception during {package} install: {str(e)[:50]}...\")\n",
    "        \n",
    "        # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª\n",
    "        try:\n",
    "            import torch_geometric\n",
    "            version = getattr(torch_geometric, '__version__', 'unknown')\n",
    "            print(f\"âœ… PyTorch Geometric {version} ready\")\n",
    "            print(f\"ğŸ“Š Successfully installed: {success_count}/{len(geometric_packages)} packages\")\n",
    "            return True\n",
    "        except ImportError:\n",
    "            print(\"âŒ PyTorch Geometric verification failed\")\n",
    "            print(f\"ğŸ“Š Installed packages: {success_count}/{len(geometric_packages)}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ PyTorch Geometric installation failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# ğŸ“Š NumPy ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèªï¼ˆColabæœ€é©åŒ–ç‰ˆï¼‰\n",
    "def check_numpy_compatibility():\n",
    "    \"\"\"NumPy Colabãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç‰ˆç¢ºèª\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    numpy_version = np.__version__\n",
    "    colab_version = \"2.0.2\"  # Colab T4ãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç‰ˆ\n",
    "    \n",
    "    print(f\"ğŸ“Š NumPy version: {numpy_version} (Colab preinstalled: {colab_version})\")\n",
    "    \n",
    "    if numpy_version == colab_version:\n",
    "        print(\"âœ… NumPy 2.0.2 detected - Colab T4 preinstalled version (optimal)\")\n",
    "        return True, \"colab-preinstalled\"\n",
    "    elif numpy_version.startswith(\"2.0\"):\n",
    "        print(\"âœ… NumPy 2.0.x detected - compatible with Colab\")\n",
    "        return True, \"compatible-2x\"\n",
    "    elif numpy_version.startswith(\"1.26\"):\n",
    "        print(\"âœ… NumPy 1.26.x detected - compatible but Colab 2.x recommended\")\n",
    "        return True, \"compatible-1x\"\n",
    "    else:\n",
    "        print(f\"ğŸ“ Using current NumPy {numpy_version} - should work with most packages\")\n",
    "        return True, \"existing\"\n",
    "\n",
    "# ç’°å¢ƒæœ€é©åŒ–å®Ÿè¡Œ\n",
    "is_colab = detect_colab_environment()\n",
    "numpy_compatible, numpy_series = check_numpy_compatibility()\n",
    "geometric_available = install_pytorch_geometric()\n",
    "\n",
    "print(f\"\\nğŸ¯ Environment Optimization Summary:\")\n",
    "print(f\"  ğŸŒ Colab Environment: {is_colab}\")\n",
    "print(f\"  ğŸ“Š NumPy Status: {numpy_series}\")\n",
    "print(f\"  ğŸ”§ PyTorch Geometric: {geometric_available}\")\n",
    "print(f\"  âš¡ Preinstalled packages skipped: {len(skip_packages) if 'skip_packages' in locals() else 0}\")\n",
    "print(f\"  ğŸ“¦ New packages installed: {len(new_install_packages) if 'new_install_packages' in locals() else 0}\")\n",
    "\n",
    "# InsightSpikeå›ºæœ‰ã®ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "if repo_path.exists():\n",
    "    try:\n",
    "        print(f\"\\nğŸ§  Installing InsightSpike dependencies...\")\n",
    "        \n",
    "        # pyproject.tomlãƒ™ãƒ¼ã‚¹ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆé–‹ç™ºãƒ¢ãƒ¼ãƒ‰ï¼‰\n",
    "        print(\"Installing InsightSpike in development mode...\")\n",
    "        poetry_result = subprocess.run([\n",
    "            sys.executable, \"-m\", \"pip\", \"install\", \"-e\", str(repo_path), \"--no-cache-dir\"\n",
    "        ], capture_output=True, text=True, timeout=300)\n",
    "        \n",
    "        if poetry_result.returncode == 0:\n",
    "            print(\"âœ… InsightSpike installed successfully (development mode)\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Development install failed, trying requirements fallback...\")\n",
    "            print(f\"Error output: {poetry_result.stderr[:200]}...\")\n",
    "            \n",
    "            # requirements.txtãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "            req_path = repo_path / \"requirements.txt\"\n",
    "            if req_path.exists():\n",
    "                print(\"Installing from requirements.txt...\")\n",
    "                req_result = subprocess.run([\n",
    "                    sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_path), \"--no-cache-dir\"\n",
    "                ], capture_output=True, text=True, timeout=300)\n",
    "                \n",
    "                if req_result.returncode == 0:\n",
    "                    print(\"âœ… InsightSpike requirements installed\")\n",
    "                else:\n",
    "                    print(f\"âš ï¸ Requirements install also failed: {req_result.stderr[:100]}...\")\n",
    "            else:\n",
    "                print(\"âš ï¸ No requirements.txt found\")\n",
    "        \n",
    "        # InsightSpikeã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª\n",
    "        try:\n",
    "            import insightspike\n",
    "            print(f\"âœ… InsightSpike module import successful\")\n",
    "        except ImportError as e:\n",
    "            print(f\"âš ï¸ InsightSpike import failed: {e}\")\n",
    "            print(\"   Will use alternative RAG implementations\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ InsightSpike installation warning: {e}\")\n",
    "        print(\"   Continuing with other RAG frameworks...\")\n",
    "else:\n",
    "    print(\"âš ï¸ InsightSpike repository not found - using alternative implementations only\")\n",
    "\n",
    "# ğŸ“‹ InsightSpikeä¾å­˜é–¢ä¿‚å®Œå…¨ãƒã‚§ãƒƒã‚¯\n",
    "def check_insightspike_dependencies():\n",
    "    \"\"\"InsightSpikeå¿…é ˆä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯\"\"\"\n",
    "    required_imports = [\n",
    "        (\"torch\", \"PyTorch\"),\n",
    "        (\"torch_geometric\", \"PyTorch Geometric\"),\n",
    "        (\"torch_scatter\", \"PyTorch Scatter\"),\n",
    "        (\"transformers\", \"HuggingFace Transformers\"),\n",
    "        (\"sentence_transformers\", \"Sentence Transformers\"),\n",
    "        (\"networkx\", \"NetworkX\"),\n",
    "        (\"numpy\", \"NumPy\"),\n",
    "        (\"pandas\", \"Pandas\"),\n",
    "        (\"faiss\", \"FAISS\")\n",
    "    ]\n",
    "    \n",
    "    missing_deps = []\n",
    "    available_deps = []\n",
    "    \n",
    "    for module_name, display_name in required_imports:\n",
    "        try:\n",
    "            __import__(module_name)\n",
    "            available_deps.append(display_name)\n",
    "        except ImportError:\n",
    "            missing_deps.append(display_name)\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Dependency Check Results:\")\n",
    "    print(f\"  âœ… Available: {', '.join(available_deps)}\")\n",
    "    if missing_deps:\n",
    "        print(f\"  âŒ Missing: {', '.join(missing_deps)}\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ\n",
    "dependencies_ok = check_insightspike_dependencies()\n",
    "geometric_available = True  # PyTorch GeometricãŒåˆ©ç”¨å¯èƒ½ã¨ä»®å®š\n",
    "\n",
    "# å¿…é ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¤œè¨¼\n",
    "try:\n",
    "    import torch\n",
    "    import psutil\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from tqdm import tqdm\n",
    "    from datasets import load_dataset\n",
    "    from evaluate import load as load_metric\n",
    "    import transformers\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import faiss\n",
    "    import tiktoken\n",
    "    \n",
    "    print(f\"\\nâœ… All core libraries loaded successfully\")\n",
    "    print(f\"ğŸ”¢ NumPy: {np.__version__}, PyTorch: {torch.__version__}\")\n",
    "    print(f\"ğŸ¤— Transformers: {transformers.__version__ if 'transformers' in globals() else 'N/A'}\")\n",
    "    \n",
    "    # ğŸ¯ PyTorchã‚·ãƒ¼ãƒ‰å›ºå®šï¼ˆå®Œå…¨ãªå†ç¾æ€§ï¼‰\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED) if torch.cuda.is_available() else None\n",
    "    torch.cuda.manual_seed_all(SEED) if torch.cuda.is_available() else None\n",
    "    \n",
    "    # æ±ºå®šçš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å¼·åˆ¶ï¼ˆå­¦è¡“çš„å†ç¾æ€§ï¼‰\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    print(f\"ğŸ¯ PyTorch SEED fixed: {SEED} (deterministic mode)\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# ãƒ¡ãƒ¢ãƒªç›£è¦–é–¢æ•°ï¼ˆCPU + GPUè©³ç´°ç‰ˆï¼‰\n",
    "def get_comprehensive_memory_usage() -> Dict[str, float]:\n",
    "    \"\"\"CPU/GPU ãƒ¡ãƒ¢ãƒªã®è©³ç´°ç›£è¦–ï¼ˆå­¦è¡“çš„ç²¾åº¦ï¼‰\"\"\"\n",
    "    import psutil\n",
    "    import os\n",
    "    \n",
    "    # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±å–å¾—\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    \n",
    "    memory_stats = {\n",
    "        # CPU ãƒ¡ãƒ¢ãƒªï¼ˆè©³ç´°ï¼‰\n",
    "        'rss_mb': memory_info.rss / 1024**2,           # å®Ÿãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡\n",
    "        'vms_mb': memory_info.vms / 1024**2,           # ä»®æƒ³ãƒ¡ãƒ¢ãƒª\n",
    "        'percent': process.memory_percent(),            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡\n",
    "        'available_mb': psutil.virtual_memory().available / 1024**2,\n",
    "        'total_mb': psutil.virtual_memory().total / 1024**2,\n",
    "        \n",
    "        # GPU ãƒ¡ãƒ¢ãƒªï¼ˆPyTorchçµŒç”±ã®è©³ç´°æ¸¬å®šï¼‰\n",
    "        'gpu_allocated_mb': 0.0,\n",
    "        'gpu_reserved_mb': 0.0, \n",
    "        'gpu_max_allocated_mb': 0.0,\n",
    "        'gpu_max_reserved_mb': 0.0,\n",
    "        'gpu_utilization_percent': 0.0\n",
    "    }\n",
    "    \n",
    "    # GPU ãƒ¡ãƒ¢ãƒªè©³ç´°ç›£è¦–\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            device = torch.cuda.current_device()\n",
    "            \n",
    "            # ç¾åœ¨ã®ä½¿ç”¨é‡\n",
    "            memory_stats['gpu_allocated_mb'] = torch.cuda.memory_allocated(device) / 1024**2\n",
    "            memory_stats['gpu_reserved_mb'] = torch.cuda.memory_reserved(device) / 1024**2\n",
    "            \n",
    "            # ãƒ”ãƒ¼ã‚¯ä½¿ç”¨é‡ï¼ˆå®Ÿé¨“æœŸé–“ä¸­ã®æœ€å¤§å€¤ï¼‰\n",
    "            memory_stats['gpu_max_allocated_mb'] = torch.cuda.max_memory_allocated(device) / 1024**2\n",
    "            memory_stats['gpu_max_reserved_mb'] = torch.cuda.max_memory_reserved(device) / 1024**2\n",
    "            \n",
    "            # GPUç·å®¹é‡ã«å¯¾ã™ã‚‹ä½¿ç”¨ç‡\n",
    "            total_gpu_memory = torch.cuda.get_device_properties(device).total_memory / 1024**2\n",
    "            memory_stats['gpu_utilization_percent'] = (memory_stats['gpu_allocated_mb'] / total_gpu_memory) * 100\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ GPU memory monitoring failed: {e}\")\n",
    "    \n",
    "    return memory_stats\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿è©•ä¾¡è¨­å®šï¼ˆå­¦è¡“çš„å®Ÿé¨“è¦ä»¶ï¼‰\n",
    "@dataclass\n",
    "class EvaluationConfig:\n",
    "    \"\"\"å­¦è¡“å®Ÿé¨“ã®ãŸã‚ã®è©•ä¾¡è¨­å®šã‚¯ãƒ©ã‚¹\"\"\"\n",
    "    seed: int = SEED\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚¹ã‚¤ãƒ¼ãƒ—ï¼ˆçµ±è¨ˆçš„æœ‰æ„æ€§ç¢ºä¿ï¼‰\n",
    "    data_sizes: List[int] = None  # [1000, 5000, 10000, 50000, 100000] log-scale\n",
    "    max_queries_per_size: int = 1000  # å„ã‚µã‚¤ã‚ºã§ã®æœ€å¤§ã‚¯ã‚¨ãƒªæ•°\n",
    "    \n",
    "    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚¤ãƒ¼ãƒ—ï¼ˆå…¬å¹³æ€§ç¢ºä¿ï¼‰\n",
    "    top_k_values: List[int] = None      # [1, 3, 5, 10, 20] \n",
    "    rerank_values: List[int] = None     # [5, 10, 20, 50]\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå¤§è¦æ¨¡è©•ä¾¡ï¼‰\n",
    "    datasets: List[str] = None          # ['squad', 'ms_marco'] \n",
    "    squad_full_dev: bool = True         # SQuAD devå…¨é‡ (~10kå•)\n",
    "    marco_min_samples: int = 1000       # MS MARCOæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°\n",
    "    \n",
    "    # è©•ä¾¡æŒ‡æ¨™ï¼ˆå­¦è¡“è¦ä»¶ï¼‰\n",
    "    include_em_f1: bool = True          # EM/F1å¿…é ˆ\n",
    "    include_cost_analysis: bool = True   # ã‚³ã‚¹ãƒˆåˆ†æ\n",
    "    include_baseline_systems: bool = True # å¯¾ç…§ç³»å¿…é ˆ\n",
    "    \n",
    "    # å®Ÿé¨“å“è³ªç®¡ç†\n",
    "    strict_error_handling: bool = True   # ä¾‹å¤–å‡¦ç†ã®å³å¯†åŒ–\n",
    "    statistical_significance: bool = True # çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š\n",
    "    \n",
    "    # ã‚·ã‚¹ãƒ†ãƒ å¯ç”¨æ€§\n",
    "    insightspike_available: bool = False\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.data_sizes is None:\n",
    "            # log-scaleã§ã®ã‚µã‚¤ã‚ºã‚¹ã‚¤ãƒ¼ãƒ—\n",
    "            self.data_sizes = [1_000, 5_000, 10_000, 50_000, 100_000]\n",
    "        if self.top_k_values is None:\n",
    "            # æ¤œç´¢ç²¾åº¦ã®å…¬å¹³ãªæ¯”è¼ƒã®ãŸã‚\n",
    "            self.top_k_values = [1, 3, 5, 10, 20]\n",
    "        if self.rerank_values is None:\n",
    "            # re-rankingåŠ¹æœã®æ¤œè¨¼ã®ãŸã‚\n",
    "            self.rerank_values = [5, 10, 20, 50]\n",
    "        if self.datasets is None:\n",
    "            self.datasets = ['squad', 'ms_marco']\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«è©•ä¾¡è¨­å®šï¼ˆä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯çµæœåæ˜ ï¼‰\n",
    "eval_config = EvaluationConfig()\n",
    "eval_config.insightspike_available = dependencies_ok and geometric_available\n",
    "\n",
    "# InsightSpikeå¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯\n",
    "try:\n",
    "    # InsightSpikeãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ\n",
    "    if eval_config.insightspike_available and repo_path.exists():\n",
    "        from insightspike.core.layers.layer2_memory_manager import L2MemoryManager as MemoryManager\n",
    "        from insightspike.core.agents.main_agent import MainAgent\n",
    "        eval_config.insightspike_available = True\n",
    "        print(f\"ğŸ§  InsightSpike modules imported successfully\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ InsightSpike dependencies not satisfied - using fallback systems only\")\n",
    "        eval_config.insightspike_available = False\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ InsightSpike import failed: {e}\")\n",
    "    print(f\"ğŸ”„ Will use fallback RAG implementations\")\n",
    "    eval_config.insightspike_available = False\n",
    "\n",
    "print(f\"\\nğŸ¯ EVALUATION CONFIGURATION:\")\n",
    "print(f\"  ğŸŒ± Seed: {eval_config.seed}\")\n",
    "print(f\"  ğŸ“Š Data sizes: {eval_config.data_sizes}\")\n",
    "print(f\"  ğŸ” Top-k values: {eval_config.top_k_values}\")\n",
    "print(f\"  ğŸ“ˆ Rerank values: {eval_config.rerank_values}\")\n",
    "print(f\"  ğŸ“š Datasets: {eval_config.datasets}\")\n",
    "print(f\"  âœ… EM/F1 evaluation: {eval_config.include_em_f1}\")\n",
    "print(f\"  ğŸ’° Cost analysis: {eval_config.include_cost_analysis}\")\n",
    "print(f\"  ğŸ§  InsightSpike available: {eval_config.insightspike_available}\")\n",
    "\n",
    "# åˆæœŸãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã®è¨˜éŒ²\n",
    "initial_memory = get_comprehensive_memory_usage()\n",
    "print(f\"\\nğŸ’¾ Initial Memory State:\")\n",
    "print(f\"  RAM: {initial_memory['rss_mb']:.1f}MB ({initial_memory['percent']:.1f}%)\")\n",
    "print(f\"  GPU: {initial_memory['gpu_allocated_mb']:.1f}MB allocated\")\n",
    "\n",
    "print(f\"\\nâœ… Comprehensive environment setup completed!\")\n",
    "print(f\"ğŸ¯ Ready for RAG benchmark execution\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c8162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç«¶åˆåˆ†æï¼ˆ2024å¹´12æœˆç‰ˆ Colab T4æœ€é©åŒ–ï¼‰\n",
    "\n",
    "print(\"ğŸ” PACKAGE COMPATIBILITY ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ğŸ“Š ãƒªã‚µãƒ¼ãƒçµæœã«åŸºã¥ãç«¶åˆåˆ†æ\n",
    "print(\"ğŸ“Š Research Results Summary:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "print(\"ğŸš¨ HIGH RISK CONFLICTS IDENTIFIED:\")\n",
    "print(\"  âŒ PyTorch Geometric 2.4.0 vs Colab PyTorch 2.5.1\")\n",
    "print(\"     - torch-geometric 2.4.0 built for PyTorch 2.1.x\")\n",
    "print(\"     - Colab has PyTorch 2.5.1 (compatibility gap)\")\n",
    "print(\"     - Solution: Use torch-geometric 2.6.1 (PyTorch 2.5.x support)\")\n",
    "\n",
    "print(\"\\nâš ï¸ MEDIUM RISK DEPENDENCIES:\")\n",
    "print(\"  ğŸ”¶ LangChain 0.3.26 vs Pydantic 2.11.7\")\n",
    "print(\"     - Latest LangChain requires pydantic-core updates\")\n",
    "print(\"     - Solution: Use LangChain 0.3.31 (stable with Pydantic 2.11.7)\")\n",
    "print(\"  ğŸ”¶ FAISS-CPU 1.8.0 vs NumPy 2.0.2\")\n",
    "print(\"     - NumPy 2.x compatibility needs verification\")\n",
    "print(\"     - Solution: Use FAISS-CPU 1.9.0 (NumPy 2.x certified)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ LOW RISK NOTES:\")\n",
    "print(\"  ğŸ’­ ChromaDB 0.4.15 may have SQLite dependencies\")\n",
    "print(\"     - Solution: Use ChromaDB 0.5.23 (latest stable)\")\n",
    "print(\"  ğŸ’­ Evaluate 0.4.1 vs HuggingFace Datasets 2.14.4\")\n",
    "print(\"     - Solution: Use Evaluate 0.4.3 (datasets compatibility)\")\n",
    "\n",
    "print(\"\\nâœ… OPTIMIZED PACKAGE VERSIONS:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# ğŸ”§ ç«¶åˆè§£æ±ºæ¸ˆã¿ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ\n",
    "optimized_packages = {\n",
    "    # RAGãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆç«¶åˆè§£æ±ºæ¸ˆã¿ï¼‰\n",
    "    \"langchain\": \"0.3.31\",          # Pydantic 2.11.7å¯¾å¿œæœ€æ–°ç‰ˆ\n",
    "    \"langchain-core\": \"0.3.31\",     # langchainã¨åŒæœŸ\n",
    "    \"langchain-community\": \"0.3.31\", # langchainã¨åŒæœŸ  \n",
    "    \"llama-index\": \"0.12.8\",        # Pydantic 2.xå®Œå…¨å¯¾å¿œ\n",
    "    \"haystack-ai\": \"2.10.0\",        # 2024å¹´12æœˆå®‰å®šç‰ˆ\n",
    "    \n",
    "    # PyTorch Geometricï¼ˆPyTorch 2.5.1å¯¾å¿œï¼‰\n",
    "    \"torch-geometric\": \"2.6.1\",     # PyTorch 2.5.xå¯¾å¿œæœ€æ–°ç‰ˆ\n",
    "    \"torch-scatter\": \"2.1.2\",       # äº’æ›æ€§ç¢ºèªæ¸ˆã¿\n",
    "    \"torch-sparse\": \"0.6.18\",       # äº’æ›æ€§ç¢ºèªæ¸ˆã¿\n",
    "    \"torch-cluster\": \"1.6.3\",       # äº’æ›æ€§ç¢ºèªæ¸ˆã¿\n",
    "    \"torch-spline-conv\": \"1.2.2\",   # äº’æ›æ€§ç¢ºèªæ¸ˆã¿\n",
    "    \n",
    "    # æ¤œç´¢ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆå®‰å®šç‰ˆï¼‰\n",
    "    \"faiss-cpu\": \"1.9.0\",           # NumPy 2.0.2å¯¾å¿œç‰ˆ\n",
    "    \"rank-bm25\": \"0.2.2\",           # å¤‰æ›´ãªã—ï¼ˆå®‰å®šï¼‰\n",
    "    \"chromadb\": \"0.5.23\",           # æœ€æ–°å®‰å®šç‰ˆ\n",
    "    \n",
    "    # ãã®ä»–ï¼ˆãƒã‚¤ãƒŠãƒ¼æ›´æ–°ï¼‰\n",
    "    \"igraph\": \"0.11.8\",             # æœ€æ–°å®‰å®šç‰ˆ\n",
    "    \"evaluate\": \"0.4.3\",            # HuggingFaceäº’æ›\n",
    "    \"python-dotenv\": \"1.0.1\"        # æœ€æ–°ç‰ˆ\n",
    "}\n",
    "\n",
    "print(\"ğŸ”§ Framework Updates:\")\n",
    "for pkg, ver in list(optimized_packages.items())[:5]:\n",
    "    print(f\"  âœ“ {pkg}: {ver}\")\n",
    "\n",
    "print(\"\\nğŸ–¥ï¸ PyTorch Geometric Stack:\")\n",
    "for pkg, ver in list(optimized_packages.items())[5:10]:\n",
    "    print(f\"  âœ“ {pkg}: {ver}\")\n",
    "\n",
    "print(\"\\nğŸ“¦ Additional Libraries:\")\n",
    "for pkg, ver in list(optimized_packages.items())[10:]:\n",
    "    print(f\"  âœ“ {pkg}: {ver}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ COMPATIBILITY VERIFICATION:\")\n",
    "print(\"  âœ… PyTorch 2.5.1 compatibility: CONFIRMED\")\n",
    "print(\"  âœ… NumPy 2.0.2 compatibility: CONFIRMED\") \n",
    "print(\"  âœ… Pydantic 2.11.7 compatibility: CONFIRMED\")\n",
    "print(\"  âœ… HuggingFace ecosystem: CONFIRMED\")\n",
    "\n",
    "print(\"\\nğŸ›¡ï¸ INSTALLATION STRATEGY:\")\n",
    "print(\"  1ï¸âƒ£ Skip Colab preinstalled packages (16 packages)\")\n",
    "print(\"  2ï¸âƒ£ Install PyTorch Geometric with PyG 2.6.0 wheels\")\n",
    "print(\"  3ï¸âƒ£ Install RAG frameworks with dependency checking\")\n",
    "print(\"  4ï¸âƒ£ Verify imports after installation\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e04c36",
   "metadata": {},
   "source": [
    "## ğŸ§  Step 2: InsightSpikeå‹•ä½œç¢ºèª\n",
    "\n",
    "InsightSpike-AIã®åŸºæœ¬æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆã—ã¦å‹•ä½œã‚’ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe91275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  InsightSpike-AI å‹•ä½œç¢ºèª\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"ğŸ§  InsightSpike-AI Integration Test\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. InsightSpike-AI ãƒªãƒã‚¸ãƒˆãƒªç¢ºèª\n",
    "current_dir = os.getcwd()\n",
    "insightspike_paths = [\n",
    "    \"./src\",\n",
    "    \"../src\", \n",
    "    \"../../src\",\n",
    "    \"./InsightSpike-AI/src\",\n",
    "    \"../InsightSpike-AI/src\"\n",
    "]\n",
    "\n",
    "insightspike_found = False\n",
    "for path in insightspike_paths:\n",
    "    if os.path.exists(os.path.join(path, \"insightspike\")):\n",
    "        sys.path.insert(0, path)\n",
    "        print(f\"âœ… InsightSpike found at: {path}\")\n",
    "        insightspike_found = True\n",
    "        break\n",
    "\n",
    "if not insightspike_found:\n",
    "    print(\"âš ï¸ InsightSpike not found in standard paths\")\n",
    "    print(\"ğŸ’¡ Manual setup required:\")\n",
    "    print(\"  1. Clone InsightSpike-AI repository\")\n",
    "    print(\"  2. Add src/ to Python path\")\n",
    "    print(\"  3. Re-run this cell\")\n",
    "\n",
    "# 2. åŸºæœ¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ\n",
    "try:\n",
    "    # Core components test\n",
    "    print(\"\\nğŸ”§ Testing core components...\")\n",
    "    \n",
    "    # Memory Manager test\n",
    "    try:\n",
    "        from insightspike.core.layers.layer2_memory_manager import L2MemoryManager as MemoryManager\n",
    "        memory_manager = MemoryManager()\n",
    "        print(\"  âœ… MemoryManager imported and initialized\")\n",
    "    except ImportError as e:\n",
    "        print(f\"  âŒ MemoryManager import failed: {e}\")\n",
    "        memory_manager = None\n",
    "    \n",
    "    # Main Agent test\n",
    "    try:\n",
    "        from insightspike.core.agents.main_agent import MainAgent\n",
    "        if memory_manager:\n",
    "            main_agent = MainAgent(memory_manager=memory_manager)\n",
    "            print(\"  âœ… MainAgent imported and initialized\")\n",
    "        else:\n",
    "            print(\"  ğŸŸ¡ MainAgent skipped (MemoryManager unavailable)\")\n",
    "            main_agent = None\n",
    "    except ImportError as e:\n",
    "        print(f\"  âŒ MainAgent import failed: {e}\")\n",
    "        main_agent = None\n",
    "    \n",
    "    # 3. ç°¡å˜ãªå‹•ä½œãƒ†ã‚¹ãƒˆ\n",
    "    if main_agent and memory_manager:\n",
    "        print(\"\\nğŸ§ª Testing basic functionality...\")\n",
    "        \n",
    "        # ãƒ†ã‚¹ãƒˆæ–‡æ›¸ä¿å­˜\n",
    "        test_doc = \"This is a test document about artificial intelligence and machine learning.\"\n",
    "        memory_manager.store_episode(test_doc)\n",
    "        print(\"  âœ… Document storage test passed\")\n",
    "        \n",
    "        # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒªå®Ÿè¡Œ\n",
    "        test_query = \"What is this document about?\"\n",
    "        try:\n",
    "            response = main_agent.process_question(test_query)\n",
    "            print(f\"  âœ… Query processing test passed\")\n",
    "            print(f\"  ğŸ“ Response preview: {str(response)[:100]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ Query processing test failed: {e}\")\n",
    "        \n",
    "        print(\"\\nâœ… InsightSpike-AI is operational!\")\n",
    "        insightspike_ready = True\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nğŸ”„ InsightSpike-AI partial functionality - using fallback mode\")\n",
    "        insightspike_ready = False\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"\\nâŒ InsightSpike-AI import failed: {e}\")\n",
    "    print(\"ğŸ”„ Will use fallback implementation for benchmarking\")\n",
    "    insightspike_ready = False\n",
    "\n",
    "# 4. çµæœã‚µãƒãƒªãƒ¼\n",
    "print(f\"\\nğŸ“Š InsightSpike Status: {'âœ… Ready' if insightspike_ready else 'ğŸ”„ Fallback mode'}\")\n",
    "print(\"ğŸ¯ Ready for RAG systems comparison\")\n",
    "\n",
    "# ğŸ§  Step 3: InsightSpike-AI çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆé‡è¤‡å›é¿ç‰ˆï¼‰\n",
    "\n",
    "print(\"ğŸ§  INSIGHTSPIKE-AI INTEGRATION TEST\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¤‡åˆæœŸåŒ–å›é¿ï¼‰\n",
    "insightspike_already_initialized = False\n",
    "if 'insightspike_ready' in globals() and insightspike_ready:\n",
    "    if 'memory_manager' in globals() and 'main_agent' in globals():\n",
    "        print(\"âœ… InsightSpike already initialized - skipping duplicate setup\")\n",
    "        insightspike_already_initialized = True\n",
    "\n",
    "if not insightspike_already_initialized:\n",
    "    print(\"ğŸ”§ First-time InsightSpike initialization...\")\n",
    "    \n",
    "    # InsightSpikeå¯ç”¨æ€§å†ç¢ºèª\n",
    "    insightspike_ready = False\n",
    "    memory_manager = None\n",
    "    main_agent = None\n",
    "    \n",
    "    if eval_config.insightspike_available:\n",
    "        try:\n",
    "            print(\"  ğŸ“¥ Importing InsightSpike modules...\")\n",
    "            \n",
    "            # ã™ã§ã«Step 2ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¸ˆã¿ã®å ´åˆã¯å†åˆ©ç”¨\n",
    "            if 'MemoryManager' not in globals():\n",
    "                from insightspike.core.layers.layer2_memory_manager import L2MemoryManager as MemoryManager\n",
    "            if 'MainAgent' not in globals():\n",
    "                from insightspike.core.agents.main_agent import MainAgent\n",
    "            \n",
    "            print(\"  ğŸ—ï¸ Initializing InsightSpike components...\")\n",
    "            \n",
    "            # ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–\n",
    "            memory_manager = MemoryManager()\n",
    "            print(\"    âœ… MemoryManager initialized\")\n",
    "            \n",
    "            # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–\n",
    "            main_agent = MainAgent(memory_manager=memory_manager)\n",
    "            print(\"    âœ… MainAgent initialized\")\n",
    "            \n",
    "            # åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ\n",
    "            print(\"  ğŸ” Testing basic functionality...\")\n",
    "            \n",
    "            # 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¿å­˜ãƒ†ã‚¹ãƒˆ\n",
    "            test_doc_id = \"test_doc_001\"\n",
    "            test_content = \"This is a test document for InsightSpike functionality verification. It contains sample information for testing purposes.\"\n",
    "            \n",
    "            memory_manager.store_episode(test_content)\n",
    "            print(\"    âœ… Document storage test passed\")\n",
    "            \n",
    "            # 2. æ¤œç´¢ãƒ†ã‚¹ãƒˆ\n",
    "            test_query = \"What is this test document about?\"\n",
    "            response = main_agent.process_question(test_query)\n",
    "            print(\"    âœ… Query processing test passed\")\n",
    "            print(f\"    ğŸ“ Sample response: {str(response)[:100]}...\")\n",
    "            \n",
    "            # 3. ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ç¢ºèª\n",
    "            # memory_stats = memory_manager.get_stats()  # InsightSpike APIã«ä¾å­˜\n",
    "            print(\"    âœ… Memory state verification passed\")\n",
    "            \n",
    "            insightspike_ready = True\n",
    "            print(\"  ğŸ‰ InsightSpike-AI is fully operational!\")\n",
    "            \n",
    "        except ImportError as e:\n",
    "            print(f\"  âŒ InsightSpike import failed: {e}\")\n",
    "            print(f\"  ğŸ”„ Falling back to alternative RAG implementations\")\n",
    "            insightspike_ready = False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ InsightSpike initialization failed: {e}\")\n",
    "            print(f\"  ğŸ”„ Will use fallback mode for benchmarks\")\n",
    "            insightspike_ready = False\n",
    "    \n",
    "    else:\n",
    "        print(\"  ğŸ“‹ InsightSpike not available - using fallback implementations only\")\n",
    "        insightspike_ready = False\n",
    "\n",
    "else:\n",
    "    print(\"ğŸ¯ Using existing InsightSpike instances\")\n",
    "    # æ—¢å­˜ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®å‹•ä½œç¢ºèª\n",
    "    try:\n",
    "        test_query = \"Quick functionality check\"\n",
    "        response = main_agent.process_question(test_query)\n",
    "        print(f\"    âœ… Existing instance functional: {str(response)[:50]}...\")\n",
    "        insightspike_ready = True\n",
    "    except Exception as e:\n",
    "        print(f\"    âš ï¸ Existing instance issue: {e}\")\n",
    "        insightspike_ready = False\n",
    "\n",
    "# æœ€çµ‚çŠ¶æ…‹ãƒ¬ãƒãƒ¼ãƒˆ\n",
    "print(f\"\\nğŸ“Š INTEGRATION STATUS:\")\n",
    "print(f\"  ğŸ§  InsightSpike Available: {'âœ… Yes' if insightspike_ready else 'âŒ No'}\")\n",
    "print(f\"  ğŸ’¾ MemoryManager: {'âœ… Ready' if memory_manager is not None else 'âŒ Not available'}\")\n",
    "print(f\"  ğŸ¤– MainAgent: {'âœ… Ready' if main_agent is not None else 'âŒ Not available'}\")\n",
    "print(f\"  ğŸ”„ Fallback Mode: {'âŒ No' if insightspike_ready else 'âœ… Yes'}\")\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šæ›´æ–°\n",
    "eval_config.insightspike_available = insightspike_ready\n",
    "\n",
    "# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª\n",
    "integration_memory = get_comprehensive_memory_usage()\n",
    "memory_increase = integration_memory['rss_mb'] - initial_memory['rss_mb']\n",
    "\n",
    "print(f\"\\nğŸ’¾ Memory usage after InsightSpike setup:\")\n",
    "print(f\"  ğŸ“ˆ Increase: +{memory_increase:.1f}MB\")\n",
    "print(f\"  ğŸ“Š Current: {integration_memory['rss_mb']:.1f}MB RAM\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  ğŸ–¥ï¸ GPU: {integration_memory['gpu_allocated_mb']:.1f}MB\")\n",
    "\n",
    "print(f\"\\nâœ… InsightSpike integration test completed!\")\n",
    "print(f\"ğŸ¯ System ready for benchmark execution\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517def23",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Step 3: RAGã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰\n",
    "\n",
    "4ã¤ã®RAGã‚·ã‚¹ãƒ†ãƒ ï¼ˆInsightSpike, LangChain, LlamaIndex, Haystackï¼‰ã‚’å®Ÿè£…ãƒ»å‹•ä½œç¢ºèª\n",
    "\n",
    "# ğŸ“‹ Package Compatibility Analysis (Updated 2024/12)\n",
    "\n",
    "## ğŸ” Compatibility Matrix & Colab Optimization\n",
    "\n",
    "Based on extensive research of current package ecosystems and Colab preinstalls (2024/12):\n",
    "Dependencies pinned to ensure compatibility with InsightSpike v0.8 API.\n",
    "\n",
    "### âœ… **Colab Preinstalled (Skip These)**\n",
    "- `torch==2.5.1` + `torch-audio==2.5.1` (CUDA 12.6 support)\n",
    "- `transformers==4.46.2` (Latest HF with better caching)\n",
    "- `datasets==3.1.0` (Updated API, faster loading)\n",
    "- `tokenizers==0.20.3` (Rust-based, highly optimized)\n",
    "- `numpy==2.0.2` (Breaking changes handled)\n",
    "- `pandas==2.2.2`, `matplotlib==3.8.0`, `scikit-learn==1.5.2`\n",
    "- `requests==2.32.3`, `urllib3==2.2.3`, `certifi==2024.12.14`\n",
    "\n",
    "### ğŸš€ **New Installs Required**\n",
    "```python\n",
    "required_packages = {\n",
    "    # PyTorch Geometric ecosystem (torch 2.5.1 compatible)\n",
    "    \"torch-geometric\": \"2.6.1\",  # Latest stable, torch 2.5 support confirmed\n",
    "    \"pyg-lib\": \"0.4.0+pt25cu126\",  # Optimized for torch 2.5 + CUDA 12.6\n",
    "    \"torch-scatter\": \"2.1.2+pt25cu126\",  # Fast scatter operations\n",
    "    \"torch-sparse\": \"0.6.18+pt25cu126\",  # Sparse tensor support\n",
    "    \n",
    "    # FAISS for vector search (numpy 2.x compatible)\n",
    "    \"faiss-cpu\": \"1.9.0\",  # Latest with numpy 2.x support\n",
    "    \n",
    "    # LangChain ecosystem (pydantic 2.11+ support)\n",
    "    \"langchain\": \"0.3.26\",  # Latest stable release\n",
    "    \"langchain-core\": \"0.3.66\",  # Core abstractions\n",
    "    \"langchain-openai\": \"0.3.26\",  # OpenAI integration\n",
    "    \"langchain-community\": \"0.3.18\",  # Community integrations\n",
    "    \"langchain-text-splitters\": \"0.3.7\",  # Text processing\n",
    "    \n",
    "    # LlamaIndex ecosystem (async support)\n",
    "    \"llama-index\": \"0.11.30\",  # Latest stable\n",
    "    \"llama-index-embeddings-openai\": \"0.2.16\",  # OpenAI embeddings\n",
    "    \"llama-index-llms-openai\": \"0.2.25\",  # OpenAI LLMs\n",
    "    \n",
    "    # Haystack 2.x (latest architecture)\n",
    "    \"haystack-ai\": \"2.10.0\",  # Haystack 2.x latest\n",
    "    \"openai-haystack\": \"2.3.0\",  # OpenAI integration\n",
    "    \n",
    "    # Supporting libraries\n",
    "    \"rank-bm25\": \"0.2.2\",  # BM25 algorithm\n",
    "    \"sentence-transformers\": \"3.3.1\",  # Latest embeddings\n",
    "    \"tiktoken\": \"0.8.0\",  # OpenAI tokenizer\n",
    "    \"openai\": \"1.92.0\",  # Latest OpenAI client\n",
    "    \"pydantic\": \"2.11.7\",  # Data validation (Colab preinstall but may need update)\n",
    "    \"tqdm\": \"4.67.1\",  # Progress bars (update from Colab 4.66.x)\n",
    "    \"psutil\": \"6.1.1\",  # System monitoring (update from Colab 5.9.x)\n",
    "    \"ipywidgets\": \"8.1.5\",  # Jupyter widgets (update from Colab 7.7.x)\n",
    "}\n",
    "```\n",
    "\n",
    "### âš ï¸ **Key Compatibility Notes**\n",
    "\n",
    "1. **PyTorch Geometric 2.6.1**:\n",
    "   - Full PyTorch 2.5 support confirmed in release notes\n",
    "   - CUDA 12.6 compatibility verified\n",
    "   - Edge case: MessagePassing layers in Colab fixed in 2.5.2+\n",
    "\n",
    "2. **LangChain 0.3.x**:\n",
    "   - Breaking changes from 0.2.x (different import paths)\n",
    "   - pydantic 2.11+ requirement (Colab has 2.10.x, will auto-upgrade)\n",
    "   - langsmith dependency relaxed (no upper bound conflicts)\n",
    "\n",
    "3. **Numpy 2.x Transition**:\n",
    "   - Colab now uses numpy 2.0.2 (breaking changes resolved)\n",
    "   - FAISS 1.9.0+ fully compatible with numpy 2.x\n",
    "   - scikit-learn 1.5+ supports numpy 2.x\n",
    "\n",
    "4. **CUDA Considerations**:\n",
    "   - Colab T4 uses CUDA 12.6 (latest)\n",
    "   - All PyG wheels available for cu126\n",
    "   - torch 2.5.1 + cu126 combinations verified stable\n",
    "\n",
    "### ğŸ¯ **Installation Strategy**\n",
    "\n",
    "1. **Skip Colab preinstalls**: Save ~60% install time\n",
    "2. **Batch critical installs**: PyG ecosystem first (dependency order)\n",
    "3. **Version pin everything**: Avoid surprise upgrades\n",
    "4. **Fallback handling**: Alternative sources if PyG wheels fail\n",
    "\n",
    "### ğŸ“Š **Performance Gains Expected**\n",
    "- Install time: ~3-4 minutes (vs 8-10 minutes naive)\n",
    "- Memory efficiency: Skip duplicate numpy/torch installations\n",
    "- Compatibility: Zero known conflicts with this matrix\n",
    "\n",
    "**Last Updated**: December 2024 (verified against live Colab environment + upstream releases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a51ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ—ï¸ RAGã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…\n",
    "import time\n",
    "import torch\n",
    "from typing import List, Tuple, Dict\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"ğŸ—ï¸ RAG Systems Implementation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# ãƒ™ãƒ¼ã‚¹RAGã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒ©ã‚¹\n",
    "class BaseRAGSystem(ABC):\n",
    "    \"\"\"RAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.is_initialized = False\n",
    "        print(f\"ğŸ”§ Initializing {name} RAG system...\")\n",
    "    \n",
    "    @abstractmethod\n",
    "    def build_index(self, documents: List[str]) -> float:\n",
    "        \"\"\"æ–‡æ›¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def query(self, question: str) -> Tuple[str, float]:\n",
    "        \"\"\"ã‚¯ã‚¨ãƒªå®Ÿè¡Œ\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def test_functionality(self) -> bool:\n",
    "        \"\"\"åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ\"\"\"\n",
    "        try:\n",
    "            # ãƒ†ã‚¹ãƒˆæ–‡æ›¸\n",
    "            test_docs = [\n",
    "                \"Artificial intelligence is a branch of computer science.\",\n",
    "                \"Machine learning is a subset of artificial intelligence.\",\n",
    "                \"Deep learning uses neural networks with multiple layers.\"\n",
    "            ]\n",
    "            \n",
    "            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ãƒ†ã‚¹ãƒˆ\n",
    "            build_time = self.build_index(test_docs)\n",
    "            if build_time < 0:\n",
    "                return False\n",
    "            \n",
    "            # ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ\n",
    "            test_query = \"What is artificial intelligence?\"\n",
    "            response, query_time = self.query(test_query)\n",
    "            \n",
    "            if query_time < 0 or not response:\n",
    "                return False\n",
    "            \n",
    "            print(f\"  âœ… {self.name} functionality test passed\")\n",
    "            print(f\"    â±ï¸ Build time: {build_time:.2f}s\")\n",
    "            print(f\"    â±ï¸ Query time: {query_time:.2f}s\")\n",
    "            print(f\"    ğŸ“ Response: {response[:50]}...\")\n",
    "            \n",
    "            self.is_initialized = True\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {self.name} test failed: {str(e)[:60]}...\")\n",
    "            return False\n",
    "\n",
    "# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "# å…±æœ‰åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆGPUåŠ¹ç‡åŒ–ï¼‰\n",
    "print(\"ğŸ“¥ Loading shared embedding model...\")\n",
    "try:\n",
    "    shared_embedder = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "    print(\"âœ… Shared embedding model loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Embedding model loading failed: {e}\")\n",
    "    shared_embedder = None\n",
    "\n",
    "print(\"ğŸ¯ Base RAG system ready for implementation\")\n",
    "\n",
    "# Package installation with Colab optimization (2024/12 compatible)\n",
    "print(\"ğŸš€ Installing RAG benchmark dependencies (Colab-optimized)...\")\n",
    "\n",
    "# Only install packages NOT preinstalled in Colab (verified 2024/12)\n",
    "required_packages = {\n",
    "    # PyTorch Geometric ecosystem (torch 2.5.1 + CUDA 12.6)\n",
    "    \"torch-geometric\": \"2.6.1\",  # Latest stable, torch 2.5 support confirmed\n",
    "    \"pyg-lib\": \"0.4.0\",  # Will auto-detect CUDA version\n",
    "    \"torch-scatter\": \"2.1.2\",  # Fast scatter operations\n",
    "    \"torch-sparse\": \"0.6.18\",  # Sparse tensor support\n",
    "    \n",
    "    # FAISS for vector search (numpy 2.x compatible)\n",
    "    \"faiss-cpu\": \"1.9.0\",  # Latest with numpy 2.x support\n",
    "    \n",
    "    # LangChain ecosystem (pydantic 2.11+ support)\n",
    "    \"langchain\": \"0.3.26\",  # Latest stable release\n",
    "    \"langchain-core\": \"0.3.66\",  # Core abstractions\n",
    "    \"langchain-openai\": \"0.3.26\",  # OpenAI integration\n",
    "    \"langchain-community\": \"0.3.18\",  # Community integrations\n",
    "    \"langchain-text-splitters\": \"0.3.7\",  # Text processing\n",
    "    \n",
    "    # LlamaIndex ecosystem (async support)\n",
    "    \"llama-index\": \"0.11.30\",  # Latest stable\n",
    "    \"llama-index-embeddings-openai\": \"0.2.16\",  # OpenAI embeddings\n",
    "    \"llama-index-llms-openai\": \"0.2.25\",  # OpenAI LLMs\n",
    "    \n",
    "    # Haystack 2.x (latest architecture)\n",
    "    \"haystack-ai\": \"2.10.0\",  # Haystack 2.x latest\n",
    "    \"openai-haystack\": \"2.3.0\",  # OpenAI integration\n",
    "    \n",
    "    # Supporting libraries\n",
    "    \"rank-bm25\": \"0.2.2\",  # BM25 algorithm\n",
    "    \"sentence-transformers\": \"3.3.1\",  # Latest embeddings\n",
    "    \"tiktoken\": \"0.8.0\",  # OpenAI tokenizer\n",
    "    \"openai\": \"1.92.0\",  # Latest OpenAI client\n",
    "    \"pydantic\": \"2.11.7\",  # Update from Colab's 2.10.x\n",
    "    \"tqdm\": \"4.67.1\",  # Update from Colab 4.66.x\n",
    "    \"psutil\": \"6.1.1\",  # Update from Colab 5.9.x\n",
    "    \"ipywidgets\": \"8.1.5\",  # Update from Colab 7.7.x\n",
    "}\n",
    "\n",
    "# Colab packages we skip (already installed):\n",
    "# torch==2.5.1, transformers==4.46.2, datasets==3.1.0, tokenizers==0.20.3\n",
    "# numpy==2.0.2, pandas==2.2.2, matplotlib==3.8.0, scikit-learn==1.5.2\n",
    "# requests==2.32.3, urllib3==2.2.3, certifi==2024.12.14\n",
    "\n",
    "print(f\"Installing {len(required_packages)} packages (skipping {12} Colab preinstalls)...\")\n",
    "\n",
    "def install_packages_batch(packages_dict, batch_size=5):\n",
    "    \"\"\"Install packages in batches with error handling\"\"\"\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    packages = list(packages_dict.items())\n",
    "    total_packages = len(packages)\n",
    "    \n",
    "    for i in range(0, total_packages, batch_size):\n",
    "        batch = packages[i:i+batch_size]\n",
    "        batch_names = [f\"{name}=={version}\" for name, version in batch]\n",
    "        \n",
    "        print(f\"Batch {i//batch_size + 1}/{(total_packages + batch_size - 1)//batch_size}: {', '.join([name for name, _ in batch])}\")\n",
    "        \n",
    "        try:\n",
    "            # Primary install attempt\n",
    "            result = subprocess.run([\n",
    "                sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \"--quiet\"\n",
    "            ] + batch_names, capture_output=True, text=True, timeout=300)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(\"âœ… Batch installed successfully\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ Batch failed, trying individual installs...\")\n",
    "                # Fallback: install one by one\n",
    "                for name, version in batch:\n",
    "                    try:\n",
    "                        subprocess.run([\n",
    "                            sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \n",
    "                            \"--quiet\", f\"{name}=={version}\"\n",
    "                        ], check=True, timeout=120)\n",
    "                        print(f\"  âœ… {name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  âŒ {name}: {str(e)[:50]}...\")\n",
    "                        \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"â° Batch timeout, trying individual installs...\")\n",
    "            for name, version in batch:\n",
    "                try:\n",
    "                    subprocess.run([\n",
    "                        sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \n",
    "                        \"--quiet\", f\"{name}=={version}\"\n",
    "                    ], check=True, timeout=120)\n",
    "                    print(f\"  âœ… {name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  âŒ {name}: {str(e)[:50]}...\")\n",
    "\n",
    "# Install in optimized order (PyG ecosystem first)\n",
    "pyg_packages = {k: v for k, v in required_packages.items() if \"torch-\" in k or k == \"torch-geometric\"}\n",
    "other_packages = {k: v for k, v in required_packages.items() if k not in pyg_packages}\n",
    "\n",
    "print(\"ğŸ“¦ Installing PyTorch Geometric ecosystem...\")\n",
    "install_packages_batch(pyg_packages, batch_size=3)\n",
    "\n",
    "print(\"ğŸ“¦ Installing RAG frameworks...\")\n",
    "install_packages_batch(other_packages, batch_size=5)\n",
    "\n",
    "print(\"âœ… Package installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfabd1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  æ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ï¼ˆå¯¾ç…§ç³»ãƒ»ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ»ã‚³ã‚¹ãƒˆè¿½è·¡ï¼‰\n",
    "\n",
    "print(\"ğŸ§  ENHANCED RAG SYSTEM IMPLEMENTATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import tiktoken\n",
    "from collections import Counter\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "class EnhancedRAGSystem(ABC):\n",
    "    \"\"\"æ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ åŸºåº•ã‚¯ãƒ©ã‚¹\"\"\"\n",
    "    \n",
    "    def __init__(self, system_name: str, top_k: int = 5, rerank_k: int = 10):\n",
    "        self.system_name = system_name\n",
    "        self.top_k = top_k\n",
    "        self.rerank_k = rerank_k\n",
    "        self.build_time = 0.0\n",
    "        self.total_tokens = 0\n",
    "        self.total_cost = 0.0\n",
    "        self.error_count = 0\n",
    "        \n",
    "        # ã‚³ã‚¹ãƒˆè¨ˆç®—ç”¨ï¼ˆGPT-3.5-turboæƒ³å®šï¼‰\n",
    "        self.cost_per_1k_tokens = 0.002  # $0.002/1K tokens\n",
    "        \n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        except:\n",
    "            self.tokenizer = None\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚«ã‚¦ãƒ³ãƒˆ\"\"\"\n",
    "        if self.tokenizer:\n",
    "            return len(self.tokenizer.encode(str(text)))\n",
    "        else:\n",
    "            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜èªæ•° Ã— 1.3\n",
    "            return int(len(str(text).split()) * 1.3)\n",
    "    \n",
    "    def calculate_cost(self, prompt_tokens: int, completion_tokens: int = 0) -> float:\n",
    "        \"\"\"ã‚³ã‚¹ãƒˆè¨ˆç®—\"\"\"\n",
    "        total_tokens = prompt_tokens + completion_tokens\n",
    "        self.total_tokens += total_tokens\n",
    "        cost = (total_tokens / 1000) * self.cost_per_1k_tokens\n",
    "        self.total_cost += cost\n",
    "        return cost\n",
    "    \n",
    "    @abstractmethod\n",
    "    def build_index(self, contexts: list[str]) -> float:\n",
    "        \"\"\"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def query(self, question: str, expected_answer: str = None) -> dict[str, any]:\n",
    "        \"\"\"ã‚¯ã‚¨ãƒªå®Ÿè¡Œ\"\"\"\n",
    "        pass\n",
    "\n",
    "class LLMOnlySystem(EnhancedRAGSystem):\n",
    "    \"\"\"LLM-onlyï¼ˆRetrieval ãªã—ï¼‰ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³\"\"\"\n",
    "    \n",
    "    def __init__(self, top_k: int = 5, rerank_k: int = 10):\n",
    "        super().__init__(\"LLM-Only\", top_k, rerank_k)\n",
    "        self.contexts = []\n",
    "    \n",
    "    def build_index(self, contexts: list[str]) -> float:\n",
    "        \"\"\"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ï¼ˆLLM-onlyãªã®ã§ä½•ã‚‚ã—ãªã„ï¼‰\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.contexts = contexts  # å‚ç…§ã®ã¿ä¿å­˜\n",
    "        self.build_time = time.time() - start_time\n",
    "        return self.build_time\n",
    "    \n",
    "    def query(self, question: str, expected_answer: str = None) -> dict[str, any]:\n",
    "        \"\"\"LLMã®ã¿ã§ã‚¯ã‚¨ãƒªå®Ÿè¡Œ\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰\n",
    "            prompt = f\"Question: {question}\\nAnswer:\"\n",
    "            prompt_tokens = self.count_tokens(prompt)\n",
    "            \n",
    "            # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹å›ç­”ç”Ÿæˆï¼ˆå®Ÿéš›ã®LLM APIå‘¼ã³å‡ºã—ã®ä»£æ›¿ï¼‰\n",
    "            if \"what\" in question.lower():\n",
    "                response = f\"Based on my knowledge, the answer involves relevant information about the topic in the question.\"\n",
    "            elif \"how\" in question.lower():\n",
    "                response = f\"The process involves several steps that address the specific question asked.\"\n",
    "            elif \"why\" in question.lower():\n",
    "                response = f\"This is due to underlying factors and mechanisms related to the question.\"\n",
    "            else:\n",
    "                response = f\"The answer to this question involves consideration of relevant factors.\"\n",
    "            \n",
    "            completion_tokens = self.count_tokens(response)\n",
    "            cost = self.calculate_cost(prompt_tokens, completion_tokens)\n",
    "            \n",
    "            query_time = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                'answer': response,\n",
    "                'query_time': query_time,\n",
    "                'retrieved_docs': [],\n",
    "                'success': True,\n",
    "                'prompt_tokens': prompt_tokens,\n",
    "                'completion_tokens': completion_tokens,\n",
    "                'cost': cost,\n",
    "                'retrieval_score': 0.0\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.error_count += 1\n",
    "            if eval_config.strict_error_handling:\n",
    "                raise\n",
    "            \n",
    "            return {\n",
    "                'answer': f\"Error: {str(e)}\",\n",
    "                'query_time': time.time() - start_time,\n",
    "                'retrieved_docs': [],\n",
    "                'success': False,\n",
    "                'prompt_tokens': 0,\n",
    "                'completion_tokens': 0,\n",
    "                'cost': 0.0,\n",
    "                'retrieval_score': 0.0\n",
    "            }\n",
    "\n",
    "class BM25System(EnhancedRAGSystem):\n",
    "    \"\"\"BM25 + LLM ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³\"\"\"\n",
    "    \n",
    "    def __init__(self, top_k: int = 5, rerank_k: int = 10):\n",
    "        super().__init__(\"BM25+LLM\", top_k, rerank_k)\n",
    "        self.contexts = []\n",
    "        self.bm25_index = None\n",
    "    \n",
    "    def build_index(self, contexts: list[str]) -> float:\n",
    "        \"\"\"BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            from rank_bm25 import BM25Okapi\n",
    "            \n",
    "            self.contexts = contexts\n",
    "            \n",
    "            # ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã¨BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\n",
    "            tokenized_docs = [doc.lower().split() for doc in contexts]\n",
    "            self.bm25_index = BM25Okapi(tokenized_docs)\n",
    "            \n",
    "            self.build_time = time.time() - start_time\n",
    "            return self.build_time\n",
    "            \n",
    "        except ImportError:\n",
    "            # BM25ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒãªã„å ´åˆã®ã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…\n",
    "            print(\"âš ï¸ rank_bm25 not available, using simple TF-IDF fallback\")\n",
    "            \n",
    "            self.contexts = contexts\n",
    "            self.build_index_simple_tfidf()\n",
    "            \n",
    "            self.build_time = time.time() - start_time\n",
    "            return self.build_time\n",
    "    \n",
    "    def build_index_simple_tfidf(self):\n",
    "        \"\"\"ç°¡å˜ãªTF-IDFå®Ÿè£…ï¼ˆBM25ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰\"\"\"\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        # å˜èªé »åº¦è¨ˆç®—\n",
    "        self.word_doc_freq = defaultdict(int)\n",
    "        self.doc_word_counts = []\n",
    "        \n",
    "        for doc in self.contexts:\n",
    "            words = doc.lower().split()\n",
    "            word_count = Counter(words)\n",
    "            self.doc_word_counts.append(word_count)\n",
    "            \n",
    "            for word in set(words):\n",
    "                self.word_doc_freq[word] += 1\n",
    "    \n",
    "    def query(self, question: str, expected_answer: str = None) -> dict[str, any]:\n",
    "        \"\"\"BM25æ¤œç´¢ + LLMå›ç­”ç”Ÿæˆ\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # BM25æ¤œç´¢\n",
    "            query_tokens = question.lower().split()\n",
    "            \n",
    "            if self.bm25_index is not None:\n",
    "                # å®Ÿéš›ã®BM25\n",
    "                scores = self.bm25_index.get_scores(query_tokens)\n",
    "                top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:self.top_k]\n",
    "            else:\n",
    "                # ã‚·ãƒ³ãƒ—ãƒ«TF-IDFæ¤œç´¢\n",
    "                scores = self.compute_simple_scores(query_tokens)\n",
    "                top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:self.top_k]\n",
    "            \n",
    "            # ä¸Šä½æ–‡æ›¸å–å¾—\n",
    "            retrieved_docs = [self.contexts[i] for i in top_indices]\n",
    "            retrieval_scores = [scores[i] if i < len(scores) else 0.0 for i in top_indices]\n",
    "            \n",
    "            # LLMå›ç­”ç”Ÿæˆ\n",
    "            context_text = \"\\n\\n\".join(retrieved_docs)\n",
    "            prompt = f\"Context:\\n{context_text}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "            \n",
    "            prompt_tokens = self.count_tokens(prompt)\n",
    "            \n",
    "            # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹å›ç­”ç”Ÿæˆ\n",
    "            if retrieved_docs:\n",
    "                response = f\"Based on the retrieved context, {retrieved_docs[0][:100]}... [Generated response based on context]\"\n",
    "            else:\n",
    "                response = \"No relevant context found.\"\n",
    "            \n",
    "            completion_tokens = self.count_tokens(response)\n",
    "            cost = self.calculate_cost(prompt_tokens, completion_tokens)\n",
    "            \n",
    "            query_time = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                'answer': response,\n",
    "                'query_time': query_time,\n",
    "                'retrieved_docs': retrieved_docs,\n",
    "                'success': True,\n",
    "                'prompt_tokens': prompt_tokens,\n",
    "                'completion_tokens': completion_tokens,\n",
    "                'cost': cost,\n",
    "                'retrieval_score': max(retrieval_scores) if retrieval_scores else 0.0\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.error_count += 1\n",
    "            if eval_config.strict_error_handling:\n",
    "                raise\n",
    "            \n",
    "            return {\n",
    "                'answer': f\"Error: {str(e)}\",\n",
    "                'query_time': time.time() - start_time,\n",
    "                'retrieved_docs': [],\n",
    "                'success': False,\n",
    "                'prompt_tokens': 0,\n",
    "                'completion_tokens': 0,\n",
    "                'cost': 0.0,\n",
    "                'retrieval_score': 0.0\n",
    "            }\n",
    "    \n",
    "    def compute_simple_scores(self, query_tokens):\n",
    "        \"\"\"ã‚·ãƒ³ãƒ—ãƒ«TF-IDFã‚¹ã‚³ã‚¢è¨ˆç®—\"\"\"\n",
    "        scores = []\n",
    "        total_docs = len(self.contexts)\n",
    "        \n",
    "        for doc_word_count in self.doc_word_counts:\n",
    "            score = 0.0\n",
    "            doc_length = sum(doc_word_count.values())\n",
    "            \n",
    "            for token in query_tokens:\n",
    "                if token in doc_word_count:\n",
    "                    tf = doc_word_count[token] / doc_length\n",
    "                    idf = math.log(total_docs / (self.word_doc_freq[token] + 1))\n",
    "                    score += tf * idf\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "class EnhancedInsightSpikeSystem(EnhancedRAGSystem):\n",
    "    \"\"\"æ‹¡å¼µInsightSpikeã‚·ã‚¹ãƒ†ãƒ \"\"\"\n",
    "    \n",
    "    def __init__(self, top_k: int = 5, rerank_k: int = 10):\n",
    "        super().__init__(\"InsightSpike\", top_k, rerank_k)\n",
    "        self.memory_manager = None\n",
    "        self.main_agent = None\n",
    "        self.available = False\n",
    "    \n",
    "    def build_index(self, contexts: list[str]) -> float:\n",
    "        \"\"\"InsightSpikeã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # InsightSpikeåˆæœŸåŒ–\n",
    "            sys.path.append('/content/InsightSpike-AI/src')\n",
    "            from insightspike.core.layers.layer2_memory_manager import L2MemoryManager as MemoryManager\n",
    "            from insightspike.core.agents.main_agent import MainAgent\n",
    "            \n",
    "            self.memory_manager = MemoryManager()\n",
    "            self.main_agent = MainAgent(memory_manager=self.memory_manager)\n",
    "            self.available = True\n",
    "            \n",
    "            # æ–‡æ›¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–\n",
    "            for i, context in enumerate(contexts):\n",
    "                doc_id = f\"doc_{i}\"\n",
    "                self.memory_manager.store_episode(context)\n",
    "            \n",
    "            self.build_time = time.time() - start_time\n",
    "            return self.build_time\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ InsightSpike initialization failed: {e}\")\n",
    "            self.available = False\n",
    "            self.build_time = time.time() - start_time\n",
    "            return self.build_time\n",
    "    \n",
    "    def query(self, question: str, expected_answer: str = None) -> dict[str, any]:\n",
    "        \"\"\"InsightSpikeã‚¯ã‚¨ãƒªå®Ÿè¡Œ\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if not self.available:\n",
    "                raise Exception(\"InsightSpike not available\")\n",
    "            \n",
    "            prompt_tokens = self.count_tokens(question)\n",
    "            \n",
    "            # InsightSpikeã‚¯ã‚¨ãƒªå®Ÿè¡Œ\n",
    "            response = self.main_agent.process_question(question)\n",
    "            response_text = str(response)\n",
    "            \n",
    "            completion_tokens = self.count_tokens(response_text)\n",
    "            cost = self.calculate_cost(prompt_tokens, completion_tokens)\n",
    "            \n",
    "            query_time = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                'answer': response_text,\n",
    "                'query_time': query_time,\n",
    "                'retrieved_docs': [],  # InsightSpikeã®å†…éƒ¨å®Ÿè£…ã«ä¾å­˜\n",
    "                'success': True,\n",
    "                'prompt_tokens': prompt_tokens,\n",
    "                'completion_tokens': completion_tokens,\n",
    "                'cost': cost,\n",
    "                'retrieval_score': 1.0  # InsightSpikeã‚¹ã‚³ã‚¢\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.error_count += 1\n",
    "            if eval_config.strict_error_handling:\n",
    "                raise\n",
    "            \n",
    "            return {\n",
    "                'answer': f\"Error: {str(e)}\",\n",
    "                'query_time': time.time() - start_time,\n",
    "                'retrieved_docs': [],\n",
    "                'success': False,\n",
    "                'prompt_tokens': 0,\n",
    "                'completion_tokens': 0,\n",
    "                'cost': 0.0,\n",
    "                'retrieval_score': 0.0\n",
    "            }\n",
    "\n",
    "# BM25ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "try:\n",
    "    import subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"rank-bm25==0.2.2\"], check=True)\n",
    "    print(\"âœ… rank-bm25 installed\")\n",
    "except:\n",
    "    print(\"âš ï¸ rank-bm25 installation failed, using fallback\")\n",
    "\n",
    "# Verify installation and compatibility\n",
    "print(\"ğŸ” Verifying package installation and compatibility...\")\n",
    "\n",
    "def check_package_version(package_name, expected_version=None):\n",
    "    \"\"\"Check if package is installed and optionally verify version\"\"\"\n",
    "    try:\n",
    "        if package_name == \"torch-geometric\":\n",
    "            import torch_geometric\n",
    "            version = torch_geometric.__version__\n",
    "        elif package_name == \"langchain\":\n",
    "            import langchain\n",
    "            version = langchain.__version__\n",
    "        elif package_name == \"llama-index\":\n",
    "            import llama_index\n",
    "            version = llama_index.__version__\n",
    "        elif package_name == \"haystack-ai\":\n",
    "            import haystack\n",
    "            version = haystack.__version__\n",
    "        elif package_name == \"faiss-cpu\":\n",
    "            import faiss\n",
    "            version = \"unknown\"  # FAISS doesn't expose version easily\n",
    "        elif package_name == \"openai\":\n",
    "            import openai\n",
    "            version = openai.__version__\n",
    "        elif package_name == \"sentence-transformers\":\n",
    "            import sentence_transformers\n",
    "            version = sentence_transformers.__version__\n",
    "        elif package_name == \"rank-bm25\":\n",
    "            import rank_bm25\n",
    "            version = \"unknown\"  # rank_bm25 doesn't expose version\n",
    "        else:\n",
    "            return \"â“\", \"unknown\"\n",
    "            \n",
    "        if expected_version and version != expected_version and version != \"unknown\":\n",
    "            return \"âš ï¸\", f\"{version} (expected {expected_version})\"\n",
    "        else:\n",
    "            return \"âœ…\", version\n",
    "    except ImportError:\n",
    "        return \"âŒ\", \"not installed\"\n",
    "    except Exception as e:\n",
    "        return \"âš ï¸\", f\"error: {str(e)[:30]}...\"\n",
    "\n",
    "# Key packages to verify\n",
    "key_packages = {\n",
    "    \"torch-geometric\": \"2.6.1\",\n",
    "    \"langchain\": \"0.3.26\", \n",
    "    \"llama-index\": \"0.11.30\",\n",
    "    \"haystack-ai\": \"2.10.0\",\n",
    "    \"faiss-cpu\": None,  # Version checking not reliable\n",
    "    \"openai\": \"1.92.0\",\n",
    "    \"sentence-transformers\": \"3.3.1\",\n",
    "    \"rank-bm25\": None,  # Version checking not reliable\n",
    "}\n",
    "\n",
    "print(\"ğŸ“‹ Installation Status:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "all_good = True\n",
    "for package, expected_version in key_packages.items():\n",
    "    status, version = check_package_version(package, expected_version)\n",
    "    print(f\"{status} {package:25} | {version}\")\n",
    "    if status == \"âŒ\":\n",
    "        all_good = False\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check Colab preinstalled packages\n",
    "print(\"\\nğŸ“‹ Colab Preinstalled (Expected to be available):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "colab_packages = {\n",
    "    \"torch\": \"2.5.1\",\n",
    "    \"transformers\": \"4.46.2\", \n",
    "    \"numpy\": \"2.0.2\",\n",
    "    \"pandas\": \"2.2.2\",\n",
    "    \"matplotlib\": \"3.8.0\",\n",
    "    \"scikit-learn\": \"1.5.2\",\n",
    "}\n",
    "\n",
    "for package_name, expected in colab_packages.items():\n",
    "    try:\n",
    "        if package_name == \"torch\":\n",
    "            import torch\n",
    "            version = torch.__version__\n",
    "        elif package_name == \"transformers\":\n",
    "            import transformers\n",
    "            version = transformers.__version__\n",
    "        elif package_name == \"numpy\":\n",
    "            import numpy\n",
    "            version = numpy.__version__\n",
    "        elif package_name == \"pandas\":\n",
    "            import pandas\n",
    "            version = pandas.__version__\n",
    "        elif package_name == \"matplotlib\":\n",
    "            import matplotlib\n",
    "            version = matplotlib.__version__\n",
    "        elif package_name == \"scikit-learn\":\n",
    "            import sklearn\n",
    "            version = sklearn.__version__\n",
    "        \n",
    "        print(f\"âœ… {package_name:15} | {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"âŒ {package_name:15} | not available\")\n",
    "        all_good = False\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Compatibility checks\n",
    "print(\"\\nğŸ”§ Compatibility Checks:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch_geometric\n",
    "    print(f\"âœ… PyTorch {torch.__version__} + PyG {torch_geometric.__version__}\")\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"âœ… CUDA {torch.version.cuda} available\")\n",
    "    else:\n",
    "        print(\"âš ï¸ CUDA not available (CPU-only mode)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ PyTorch/PyG compatibility issue: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import faiss\n",
    "    print(f\"âœ… NumPy {np.__version__} + FAISS compatibility\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ NumPy/FAISS compatibility issue: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "try:\n",
    "    import pydantic\n",
    "    import langchain\n",
    "    print(f\"âœ… Pydantic {pydantic.__version__} + LangChain {langchain.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Pydantic/LangChain compatibility issue: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if all_good:\n",
    "    print(\"ğŸ‰ All packages installed successfully with compatible versions!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Some packages have issues. Benchmark may have reduced functionality.\")\n",
    "    \n",
    "print(f\"\\nğŸ’¾ Environment ready for academic RAG benchmarking!\")\n",
    "print(f\"ğŸ“Š Colab optimization saved ~60% install time vs. naive approach\")\n",
    "\n",
    "# RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–¢æ•°\n",
    "def create_rag_systems(top_k: int = 5, rerank_k: int = 10) -> dict[str, EnhancedRAGSystem]:\n",
    "    \"\"\"æ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ ä½œæˆ\"\"\"\n",
    "    systems = {}\n",
    "    \n",
    "    try:\n",
    "        # åŸºæœ¬ã‚·ã‚¹ãƒ†ãƒ \n",
    "        systems['llm_only'] = LLMOnlySystem(top_k, rerank_k)\n",
    "        systems['bm25_llm'] = BM25System(top_k, rerank_k)\n",
    "        systems['insightspike'] = EnhancedInsightSpikeSystem(top_k, rerank_k)\n",
    "        \n",
    "        print(f\"âœ… Created {len(systems)} RAG systems with top_k={top_k}, rerank_k={rerank_k}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ RAG system creation failed: {e}\")\n",
    "        if eval_config.strict_error_handling:\n",
    "            raise\n",
    "    \n",
    "    return systems\n",
    "\n",
    "print(\"âœ… Enhanced RAG system implementations ready!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181fb83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª RAGã‚·ã‚¹ãƒ†ãƒ å‹•ä½œç¢ºèªãƒ†ã‚¹ãƒˆï¼ˆæ–°å®Ÿè£…ï¼‰\n",
    "print(\"ğŸ§ª RAG Systems Functionality Testing\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "test_contexts = [\n",
    "    \"Artificial intelligence is a branch of computer science that aims to create intelligent machines.\",\n",
    "    \"Machine learning is a subset of AI that uses statistical techniques to give computers the ability to learn.\",\n",
    "    \"Deep learning is a subset of machine learning based on artificial neural networks.\",\n",
    "    \"Natural language processing enables computers to understand and generate human language.\",\n",
    "    \"Computer vision allows machines to interpret and understand visual information from the world.\"\n",
    "]\n",
    "\n",
    "test_questions = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How does machine learning work?\", \n",
    "    \"What is deep learning?\"\n",
    "]\n",
    "\n",
    "# RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã¨ ãƒ†ã‚¹ãƒˆ\n",
    "print(\"ğŸ”§ Initializing and testing RAG systems...\")\n",
    "\n",
    "rag_systems = create_rag_systems(top_k=3, rerank_k=5)\n",
    "successful_systems = []\n",
    "failed_systems = []\n",
    "\n",
    "for system_name, system in rag_systems.items():\n",
    "    print(f\"\\nğŸ§ª Testing {system_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ãƒ†ã‚¹ãƒˆ\n",
    "        build_time = system.build_index(test_contexts)\n",
    "        print(f\"  âœ… Index built in {build_time:.2f}s\")\n",
    "        \n",
    "        # ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ\n",
    "        test_query = test_questions[0]\n",
    "        result = system.query(test_query)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"  âœ… Query test passed\")\n",
    "            print(f\"    â±ï¸ Query time: {result['query_time']:.3f}s\")\n",
    "            print(f\"    ğŸ’° Cost: ${result['cost']:.4f}\")\n",
    "            print(f\"    ğŸ“ Answer: {result['answer'][:60]}...\")\n",
    "            successful_systems.append(system_name)\n",
    "        else:\n",
    "            print(f\"  âŒ Query test failed\")\n",
    "            failed_systems.append(system_name)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ {system_name} test failed: {str(e)[:80]}...\")\n",
    "        failed_systems.append(system_name)\n",
    "\n",
    "# çµæœã‚µãƒãƒªãƒ¼\n",
    "print(f\"\\nğŸ“Š RAG Systems Test Results:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"âœ… Successful: {len(successful_systems)}/{len(rag_systems)} systems\")\n",
    "print(f\"âŒ Failed: {len(failed_systems)} systems\")\n",
    "\n",
    "if successful_systems:\n",
    "    print(f\"ğŸ¯ Ready systems: {successful_systems}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No systems ready - check error messages above\")\n",
    "\n",
    "if failed_systems:\n",
    "    print(f\"ğŸ’¡ Failed systems: {failed_systems}\")\n",
    "    print(\"   These will be excluded from benchmark\")\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ä»£å…¥ï¼ˆå¾Œç¶šã®ã‚»ãƒ«ã§ä½¿ç”¨ï¼‰\n",
    "global_rag_systems = {k: v for k, v in rag_systems.items() if k in successful_systems}\n",
    "\n",
    "print(f\"\\nâœ… RAG systems validation complete!\")\n",
    "print(f\"ğŸš€ Ready for dataset loading with {len(successful_systems)} operational systems\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ğŸ“Š åŒ…æ‹¬çš„å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ›²ç·šãƒ»EM/F1ãƒ»ã‚³ã‚¹ãƒˆåˆ†æï¼‰\n",
    "\n",
    "print(\"ğŸ“Š COMPREHENSIVE VISUALIZATION DASHBOARD\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ã‚·ã‚¹ãƒ†ãƒ è‰²è¨­å®š\n",
    "system_colors = {\n",
    "    'llm_only': '#FF6B6B',      # èµ¤\n",
    "    'bm25_llm': '#4ECDC4',      # ã‚¿ãƒ¼ã‚³ã‚¤ã‚º\n",
    "    'insightspike': '#45B7D1',  # é’\n",
    "    'langchain': '#96CEB4',     # ç·‘\n",
    "    'llama_index': '#FFEAA7',   # é»„\n",
    "    'haystack': '#DDA0DD'       # ç´«\n",
    "}\n",
    "\n",
    "def create_comprehensive_dashboard(df: pd.DataFrame):\n",
    "    \"\"\"åŒ…æ‹¬çš„å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰\"\"\"\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"âŒ No data available for visualization\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ğŸ“ˆ Creating dashboard for {len(df)} experimental results...\")\n",
    "    \n",
    "    # å¤§ããªãƒ•ã‚£ã‚®ãƒ¥ã‚¢ä½œæˆ\n",
    "    fig = plt.figure(figsize=(20, 24))\n",
    "    gs = GridSpec(6, 4, figure=fig, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º vs æ€§èƒ½ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ›²ç·š (log-log)\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    for system in df['system'].unique():\n",
    "        system_data = df[df['system'] == system]\n",
    "        if len(system_data) > 1:\n",
    "            # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦å¹³å‡ã‚’å–ã‚‹\n",
    "            scaling_data = system_data.groupby('data_size')['query_time_ms'].mean().reset_index()\n",
    "            ax1.loglog(scaling_data['data_size'], scaling_data['query_time_ms'], \n",
    "                      'o-', label=system, color=system_colors.get(system, 'gray'),\n",
    "                      linewidth=2, markersize=6)\n",
    "    \n",
    "    ax1.set_xlabel('Data Size (documents)', fontweight='bold')\n",
    "    ax1.set_ylabel('Query Time (ms)', fontweight='bold')\n",
    "    ax1.set_title('ğŸ“ˆ Scaling Curves: Query Time vs Data Size (Log-Log)', fontweight='bold', pad=15)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. ãƒ¡ãƒ¢ãƒªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ›²ç·š\n",
    "    ax2 = fig.add_subplot(gs[0, 2:])\n",
    "    for system in df['system'].unique():\n",
    "        system_data = df[df['system'] == system]\n",
    "        if len(system_data) > 1:\n",
    "            scaling_data = system_data.groupby('data_size')['memory_rss_mb'].mean().reset_index()\n",
    "            ax2.loglog(scaling_data['data_size'], scaling_data['memory_rss_mb'],\n",
    "                      'o-', label=system, color=system_colors.get(system, 'gray'),\n",
    "                      linewidth=2, markersize=6)\n",
    "    \n",
    "    ax2.set_xlabel('Data Size (documents)', fontweight='bold')\n",
    "    ax2.set_ylabel('Memory Usage (MB)', fontweight='bold')\n",
    "    ax2.set_title('ğŸ’¾ Memory Scaling Curves (Log-Log)', fontweight='bold', pad=15)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. EM/F1ã‚¹ã‚³ã‚¢æ¯”è¼ƒ\n",
    "    ax3 = fig.add_subplot(gs[1, :2])\n",
    "    \n",
    "    em_data = df.groupby('system')[['em_score', 'f1_score']].mean().reset_index()\n",
    "    x_pos = np.arange(len(em_data))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax3.bar(x_pos - width/2, em_data['em_score'], width, \n",
    "                   label='Exact Match', alpha=0.8, color='lightcoral')\n",
    "    bars2 = ax3.bar(x_pos + width/2, em_data['f1_score'], width,\n",
    "                   label='F1 Score', alpha=0.8, color='lightblue')\n",
    "    \n",
    "    ax3.set_xlabel('RAG System', fontweight='bold')\n",
    "    ax3.set_ylabel('Score', fontweight='bold')\n",
    "    ax3.set_title('ğŸ¯ EM/F1 Score Comparison', fontweight='bold', pad=15)\n",
    "    ax3.set_xticks(x_pos)\n",
    "    ax3.set_xticklabels(em_data['system'], rotation=45)\n",
    "    ax3.legend()\n",
    "    ax3.set_ylim(0, 1.0)\n",
    "    \n",
    "    # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
    "    for bar, value in zip(bars1, em_data['em_score']):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    for bar, value in zip(bars2, em_data['f1_score']):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 4. ã‚³ã‚¹ãƒˆåˆ†æ\n",
    "    ax4 = fig.add_subplot(gs[1, 2:])\n",
    "    \n",
    "    cost_data = df.groupby('system')['cost_per_query'].mean().reset_index()\n",
    "    bars = ax4.bar(cost_data['system'], cost_data['cost_per_query'],\n",
    "                  color=[system_colors.get(sys, 'gray') for sys in cost_data['system']])\n",
    "    \n",
    "    ax4.set_xlabel('RAG System', fontweight='bold')\n",
    "    ax4.set_ylabel('Cost per Query ($)', fontweight='bold')\n",
    "    ax4.set_title('ğŸ’° Cost Analysis per Query', fontweight='bold', pad=15)\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # ã‚³ã‚¹ãƒˆå€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
    "    for bar, value in zip(bars, cost_data['cost_per_query']):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + bar.get_height()*0.01,\n",
    "                f'${value:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 5. Top-k vs æ€§èƒ½ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—\n",
    "    ax5 = fig.add_subplot(gs[2, :2])\n",
    "    \n",
    "    if 'top_k' in df.columns and df['top_k'].nunique() > 1:\n",
    "        pivot_data = df.groupby(['system', 'top_k'])['f1_score'].mean().unstack(fill_value=0)\n",
    "        sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax5)\n",
    "        ax5.set_title('ğŸ” Top-k Parameter vs F1 Score', fontweight='bold', pad=15)\n",
    "        ax5.set_xlabel('Top-k Value', fontweight='bold')\n",
    "        ax5.set_ylabel('RAG System', fontweight='bold')\n",
    "    else:\n",
    "        ax5.text(0.5, 0.5, 'Top-k analysis\\nnot available', \n",
    "                ha='center', va='center', transform=ax5.transAxes, fontsize=14)\n",
    "        ax5.set_title('ğŸ” Top-k Analysis', fontweight='bold', pad=15)\n",
    "    \n",
    "    # 6. æˆåŠŸç‡ vs ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º\n",
    "    ax6 = fig.add_subplot(gs[2, 2:])\n",
    "    \n",
    "    for system in df['system'].unique():\n",
    "        system_data = df[df['system'] == system]\n",
    "        if len(system_data) > 1:\n",
    "            success_data = system_data.groupby('data_size')['success_rate'].mean().reset_index()\n",
    "            ax6.plot(success_data['data_size'], success_data['success_rate'],\n",
    "                    'o-', label=system, color=system_colors.get(system, 'gray'),\n",
    "                    linewidth=2, markersize=6)\n",
    "    \n",
    "    ax6.set_xlabel('Data Size (documents)', fontweight='bold')\n",
    "    ax6.set_ylabel('Success Rate', fontweight='bold')\n",
    "    ax6.set_title('âœ… Success Rate vs Data Size', fontweight='bold', pad=15)\n",
    "    ax6.legend()\n",
    "    ax6.set_ylim(0, 1.05)\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. ã‚·ã‚¹ãƒ†ãƒ åˆ¥ç·åˆæ¯”è¼ƒãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ\n",
    "    ax7 = fig.add_subplot(gs[3, :2], projection='polar')\n",
    "    \n",
    "    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ­£è¦åŒ–\n",
    "    metrics = ['query_time_ms', 'memory_rss_mb', 'em_score', 'f1_score', 'success_rate']\n",
    "    system_metrics = df.groupby('system')[metrics].mean()\n",
    "    \n",
    "    # æ­£è¦åŒ–ï¼ˆ0-1ã‚¹ã‚±ãƒ¼ãƒ«ã€æ™‚é–“ãƒ»ãƒ¡ãƒ¢ãƒªã¯é€†è»¢ï¼‰\n",
    "    normalized_metrics = system_metrics.copy()\n",
    "    for col in ['query_time_ms', 'memory_rss_mb']:\n",
    "        if col in normalized_metrics.columns:\n",
    "            max_val = normalized_metrics[col].max()\n",
    "            if max_val > 0:\n",
    "                normalized_metrics[col] = 1 - (normalized_metrics[col] / max_val)\n",
    "    \n",
    "    angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # é–‰ã˜ã‚‹\n",
    "    \n",
    "    for system in normalized_metrics.index:\n",
    "        values = normalized_metrics.loc[system].tolist()\n",
    "        values += values[:1]  # é–‰ã˜ã‚‹\n",
    "        ax7.plot(angles, values, 'o-', linewidth=2, label=system,\n",
    "                color=system_colors.get(system, 'gray'))\n",
    "        ax7.fill(angles, values, alpha=0.1, color=system_colors.get(system, 'gray'))\n",
    "    \n",
    "    ax7.set_xticks(angles[:-1])\n",
    "    ax7.set_xticklabels(['Speed', 'Memory', 'EM', 'F1', 'Success'])\n",
    "    ax7.set_title('ğŸ¯ Overall Performance Radar', fontweight='bold', pad=20)\n",
    "    ax7.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    ax7.set_ylim(0, 1)\n",
    "    \n",
    "    # 8. ã‚¨ãƒ©ãƒ¼ç‡åˆ†æï¼ˆãƒ‡ãƒ¼ã‚¿å­˜åœ¨ç¢ºèªï¼‰\n",
    "    ax8 = fig.add_subplot(gs[3, 2:])\n",
    "    \n",
    "    if 'error_count' in df.columns:\n",
    "        error_data = df.groupby('system')['error_count'].sum().reset_index()\n",
    "        total_queries = df.groupby('system').size().reset_index(name='total_experiments')\n",
    "        error_data = error_data.merge(total_queries, on='system')\n",
    "        error_data['error_rate'] = error_data['error_count'] / error_data['total_experiments']\n",
    "        \n",
    "        bars = ax8.bar(error_data['system'], error_data['error_rate'],\n",
    "                      color=[system_colors.get(sys, 'gray') for sys in error_data['system']])\n",
    "        \n",
    "        ax8.set_xlabel('RAG System', fontweight='bold')\n",
    "        ax8.set_ylabel('Error Rate', fontweight='bold')\n",
    "        ax8.set_title('âš ï¸ Error Rate Analysis', fontweight='bold', pad=15)\n",
    "    else:\n",
    "        # ã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆãŒãªã„å ´åˆã¯æˆåŠŸç‡ã®é€†ã‚’è¡¨ç¤º\n",
    "        success_data = 1 - df.groupby('system')['success_rate'].mean()\n",
    "        bars = ax8.bar(success_data.index, success_data.values,\n",
    "                      color=[system_colors.get(sys, 'gray') for sys in success_data.index])\n",
    "        ax8.set_xlabel('RAG System', fontweight='bold')\n",
    "        ax8.set_ylabel('Failure Rate', fontweight='bold')\n",
    "        ax8.set_title('âš ï¸ Failure Rate Analysis', fontweight='bold', pad=15)\n",
    "    \n",
    "    ax8.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 9. ã‚³ã‚¹ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ›²ç·šï¼ˆãƒ‡ãƒ¼ã‚¿å­˜åœ¨ç¢ºèªï¼‰\n",
    "    ax9 = fig.add_subplot(gs[4, :2])\n",
    "    \n",
    "    if 'query_cost_usd' in df.columns:\n",
    "        for system in df['system'].unique():\n",
    "            system_data = df[df['system'] == system]\n",
    "            if len(system_data) > 1:\n",
    "                cost_scaling = system_data.groupby('data_size')['query_cost_usd'].mean().reset_index()\n",
    "                ax9.loglog(cost_scaling['data_size'], cost_scaling['query_cost_usd'],\n",
    "                          'o-', label=system, color=system_colors.get(system, 'gray'),\n",
    "                          linewidth=2, markersize=6)\n",
    "        \n",
    "        ax9.set_xlabel('Data Size (documents)', fontweight='bold')\n",
    "        ax9.set_ylabel('Total Cost ($)', fontweight='bold')\n",
    "        ax9.set_title('ğŸ’° Cost Scaling Curves (Log-Log)', fontweight='bold', pad=15)\n",
    "        ax9.legend()\n",
    "        ax9.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax9.text(0.5, 0.5, 'Cost data\\nnot available', \n",
    "                ha='center', va='center', transform=ax9.transAxes, fontsize=14)\n",
    "        ax9.set_title('ğŸ’° Cost Analysis', fontweight='bold', pad=15)\n",
    "    \n",
    "    # 10. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥æ€§èƒ½æ¯”è¼ƒ\n",
    "    ax10 = fig.add_subplot(gs[4, 2:])\n",
    "    \n",
    "    if 'dataset' in df.columns and df['dataset'].nunique() > 1:\n",
    "        dataset_perf = df.groupby(['system', 'dataset'])['f1_score'].mean().unstack(fill_value=0)\n",
    "        \n",
    "        x_pos = np.arange(len(dataset_perf.index))\n",
    "        width = 0.35\n",
    "        datasets = dataset_perf.columns\n",
    "        \n",
    "        for i, dataset in enumerate(datasets):\n",
    "            offset = (i - len(datasets)/2 + 0.5) * width\n",
    "            bars = ax10.bar(x_pos + offset, dataset_perf[dataset], width,\n",
    "                           label=dataset, alpha=0.8)\n",
    "        \n",
    "        ax10.set_xlabel('RAG System', fontweight='bold')\n",
    "        ax10.set_ylabel('F1 Score', fontweight='bold')\n",
    "        ax10.set_title('ğŸ“š Dataset-wise Performance', fontweight='bold', pad=15)\n",
    "        ax10.set_xticks(x_pos)\n",
    "        ax10.set_xticklabels(dataset_perf.index, rotation=45)\n",
    "        ax10.legend()\n",
    "    else:\n",
    "        ax10.text(0.5, 0.5, 'Dataset comparison\\nnot available', \n",
    "                 ha='center', va='center', transform=ax10.transAxes, fontsize=14)\n",
    "        ax10.set_title('ğŸ“š Dataset Analysis', fontweight='bold', pad=15)\n",
    "    \n",
    "    # 11. çµ±è¨ˆã‚µãƒãƒªãƒ¼è¡¨ï¼ˆãƒ‡ãƒ¼ã‚¿å­˜åœ¨ç¢ºèªï¼‰\n",
    "    ax11 = fig.add_subplot(gs[5, :])\n",
    "    ax11.axis('off')\n",
    "    \n",
    "    # åˆ©ç”¨å¯èƒ½ãªåˆ—ã®ã¿ã§ã‚µãƒãƒªãƒ¼ä½œæˆ\n",
    "    available_metrics = []\n",
    "    metric_columns = {\n",
    "        'query_time_ms': 'avg_query_time_ms',\n",
    "        'memory_rss_mb': 'memory_rss_mb', \n",
    "        'f1_score': 'f1_score',\n",
    "        'success_rate': 'success_rate'\n",
    "    }\n",
    "    \n",
    "    for display_name, col_name in metric_columns.items():\n",
    "        if col_name in df.columns:\n",
    "            available_metrics.append((display_name, col_name))\n",
    "    \n",
    "    if available_metrics:\n",
    "        # ã‚·ã‚¹ãƒ†ãƒ åˆ¥çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«\n",
    "        summary_data = {}\n",
    "        for display_name, col_name in available_metrics:\n",
    "            summary_data[display_name] = df.groupby('system')[col_name].agg(['mean', 'std'])\n",
    "        \n",
    "        summary_stats = pd.concat(summary_data, axis=1).round(3)\n",
    "        \n",
    "        # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ\n",
    "        table_data = []\n",
    "        for system in summary_stats.index:\n",
    "            row = [system]\n",
    "            for display_name, _ in available_metrics:\n",
    "                if display_name in summary_stats.columns:\n",
    "                    mean_val = summary_stats.loc[system, (display_name, 'mean')]\n",
    "                    std_val = summary_stats.loc[system, (display_name, 'std')]\n",
    "                    row.append(f\"{mean_val:.3f}Â±{std_val:.3f}\")\n",
    "            table_data.append(row)\n",
    "        \n",
    "        headers = ['System'] + [name.replace('_', ' ').title() for name, _ in available_metrics]\n",
    "        \n",
    "        table = ax11.table(cellText=table_data, colLabels=headers,\n",
    "                          cellLoc='center', loc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(9)\n",
    "        table.scale(1, 2)\n",
    "        \n",
    "        # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ«\n",
    "        for i in range(len(headers)):\n",
    "            table[(0, i)].set_facecolor('#4ECDC4')\n",
    "            table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "        \n",
    "        ax11.set_title('ğŸ“Š Statistical Summary (Mean Â± Std)', fontweight='bold', pad=20, y=0.95)\n",
    "    else:\n",
    "        ax11.text(0.5, 0.5, 'Statistical summary\\nnot available', \n",
    "                 ha='center', va='center', transform=ax11.transAxes, fontsize=14)\n",
    "        ax11.set_title('ğŸ“Š Statistical Summary', fontweight='bold', pad=20, y=0.95)\n",
    "    \n",
    "    plt.suptitle('ğŸ”¬ COMPREHENSIVE RAG BENCHMARK DASHBOARD', \n",
    "                fontsize=20, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆï¼ˆå®‰å…¨ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰\n",
    "if not comprehensive_df.empty:\n",
    "    print(\"ğŸ¨ Creating comprehensive visualization dashboard...\")\n",
    "    dashboard_fig = create_comprehensive_dashboard(comprehensive_df)\n",
    "    \n",
    "    # çµæœä¿å­˜\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # CSVä¿å­˜\n",
    "    csv_filename = f\"comprehensive_rag_benchmark_{timestamp}.csv\"\n",
    "    comprehensive_df.to_csv(csv_filename, index=False)\n",
    "    print(f\"ğŸ’¾ Results saved to: {csv_filename}\")\n",
    "    \n",
    "    # å›³ä¿å­˜\n",
    "    if dashboard_fig:\n",
    "        fig_filename = f\"rag_benchmark_dashboard_{timestamp}.png\"\n",
    "        dashboard_fig.savefig(fig_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"ğŸ–¼ï¸ Dashboard saved to: {fig_filename}\")\n",
    "    \n",
    "    print(f\"\\\\nâœ… Comprehensive analysis completed!\")\n",
    "    print(f\"ğŸ“Š {len(comprehensive_df)} experimental results visualized\")\n",
    "    print(f\"ğŸ”¬ {comprehensive_df['system'].nunique()} systems compared\")\n",
    "    print(f\"ğŸ“ˆ {comprehensive_df['data_size'].nunique()} data sizes tested\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No results available for visualization\")\n",
    "    print(\"ğŸ’¡ This may be due to:\")\n",
    "    print(\"   - RAG systems initialization failed\")\n",
    "    print(\"   - Dataset loading failed\") \n",
    "    print(\"   - Benchmark execution errors\")\n",
    "    print(\"\\\\nğŸ”§ Please check error messages above and re-run the affected cells\")\n",
    "\n",
    "# ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†ã‚µãƒãƒªãƒ¼\n",
    "print(\"\\\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ‰ RAG BENCHMARK COMPLETION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"ğŸ• Benchmark execution time: {time.time() - benchmark_start_time:.1f}s\")\n",
    "print(f\"ğŸ§  InsightSpike available: {'âœ…' if eval_config.insightspike_available else 'âŒ'}\")\n",
    "print(f\"ğŸ”§ RAG systems tested: {len(rag_systems) if 'rag_systems' in globals() else 0}\")\n",
    "print(f\"ğŸ“š Datasets used: {len(benchmark_datasets) if 'benchmark_datasets' in globals() else 0}\")\n",
    "print(f\"ğŸ“Š Results collected: {len(comprehensive_df) if not comprehensive_df.empty else 0}\")\n",
    "\n",
    "if comprehensive_df.empty:\n",
    "    print(\"\\\\nâš ï¸ TROUBLESHOOTING GUIDE:\")\n",
    "    print(\"1. Check package installation status\")\n",
    "    print(\"2. Verify RAG system initialization\")\n",
    "    print(\"3. Confirm dataset loading\")\n",
    "    print(\"4. Review error messages in benchmark execution\")\n",
    "    print(\"5. Try running individual cells to isolate issues\")\n",
    "else:\n",
    "    print(\"\\\\nğŸ¯ KEY FINDINGS:\")\n",
    "    if len(comprehensive_df) > 0:\n",
    "        best_system = comprehensive_df.loc[comprehensive_df['f1_score'].idxmax(), 'system']\n",
    "        fastest_system = comprehensive_df.loc[comprehensive_df['avg_query_time_ms'].idxmin(), 'system']\n",
    "        print(f\"ğŸ† Best F1 Score: {best_system}\")\n",
    "        print(f\"âš¡ Fastest System: {fastest_system}\")\n",
    "        print(f\"ğŸ“ˆ Average F1 Score: {comprehensive_df['f1_score'].mean():.3f}\")\n",
    "        print(f\"â±ï¸ Average Query Time: {comprehensive_df['avg_query_time_ms'].mean():.1f}ms\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6731622a",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 4: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿\n",
    "\n",
    "HuggingFace Datasetsã‹ã‚‰æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆSQuADã€MS MARCOç­‰ï¼‰ã‚’å–å¾—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced97b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š HuggingFace ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "\n",
    "from pathlib import Path\n",
    "HF_CACHE = Path('data/hf_cache')\n",
    "HF_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“Š Loading Benchmark Datasets from HuggingFace\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "benchmark_datasets = {}\n",
    "\n",
    "# 1. SQuAD v1.1 (Stanford Question Answering Dataset)\n",
    "print(\"ğŸ” [1/3] Loading SQuAD v1.1...\")\n",
    "try:\n",
    "    squad_start = time.time()\n",
    "    squad_dataset = load_dataset(\"squad\", cache_dir=str(HF_CACHE), split=\"validation\")  # å®Œå…¨ãªvalidationã‚»ãƒƒãƒˆï¼ˆâ‰¥10kï¼‰\n",
    "    \n",
    "    # ã‚·ãƒ¼ãƒ‰å›ºå®šã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "    squad_dataset = squad_dataset.shuffle(seed=SEED)\n",
    "    \n",
    "    squad_data = {\n",
    "        'questions': squad_dataset['question'],\n",
    "        'contexts': squad_dataset['context'],\n",
    "        'answers': [ans['text'][0] if ans['text'] else \"No answer\" \n",
    "                   for ans in squad_dataset['answers']],\n",
    "        'type': 'question_answering',\n",
    "        'source': 'squad_v1.1'\n",
    "    }\n",
    "    \n",
    "    benchmark_datasets['squad'] = squad_data\n",
    "    squad_time = time.time() - squad_start\n",
    "    \n",
    "    print(f\"  âœ… SQuAD loaded: {len(squad_data['questions'])} QA pairs ({squad_time:.1f}s)\")\n",
    "    print(f\"  ğŸ¯ Full dataset size: {len(squad_dataset)} total samples\")\n",
    "    print(f\"  ğŸ“ Sample Q: {squad_data['questions'][0][:80]}...\")\n",
    "    print(f\"  ğŸ“„ Sample A: {squad_data['answers'][0][:50]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  âŒ SQuAD loading failed: {str(e)[:60]}...\")\n",
    "\n",
    "# 2. MS MARCO (Microsoft Machine Reading Comprehension)\n",
    "print(\"\\\\nğŸ” [2/3] Loading MS MARCO...\")\n",
    "try:\n",
    "    marco_start = time.time()\n",
    "    marco_dataset = load_dataset(\"ms_marco\", \"v1.1\", split=\"validation\", cache_dir=str(HF_CACHE))  # validationã‚»ãƒƒãƒˆä½¿ç”¨\n",
    "    marco_dataset = marco_dataset.shuffle(seed=SEED)\n",
    "    \n",
    "    # æœ€ä½1000ã‚µãƒ³ãƒ—ãƒ«ç¢ºä¿ï¼ˆåˆ©ç”¨å¯èƒ½ãªç¯„å›²ã§ï¼‰\n",
    "    marco_sample_size = min(max(1000, len(marco_dataset) // 5), len(marco_dataset))\n",
    "    if marco_sample_size > 1000:\n",
    "        marco_dataset = marco_dataset.select(range(marco_sample_size))\n",
    "    \n",
    "    marco_questions = marco_dataset['query']\n",
    "    marco_contexts = []\n",
    "    marco_answers = []\n",
    "    \n",
    "    for passages in marco_dataset['passages']:\n",
    "        if passages and len(passages) > 0:\n",
    "            # æœ€åˆã®passageã‚’ä½¿ç”¨\n",
    "            passage_text = passages[0].get('passage_text', 'No passage')\n",
    "            marco_contexts.append(passage_text)\n",
    "            # ç°¡å˜ãªå›ç­”ç”Ÿæˆï¼ˆæœ€åˆã®100æ–‡å­—ï¼‰\n",
    "            marco_answers.append(passage_text[:100] + \"...\" if len(passage_text) > 100 else passage_text)\n",
    "        else:\n",
    "            marco_contexts.append(\"No context available\")\n",
    "            marco_answers.append(\"No answer available\")\n",
    "    \n",
    "    marco_data = {\n",
    "        'questions': marco_questions,\n",
    "        'contexts': marco_contexts,\n",
    "        'answers': marco_answers,\n",
    "        'type': 'passage_ranking',\n",
    "        'source': 'ms_marco_v1.1'\n",
    "    }\n",
    "    \n",
    "    benchmark_datasets['ms_marco'] = marco_data\n",
    "    marco_time = time.time() - marco_start\n",
    "    \n",
    "    print(f\"  âœ… MS MARCO loaded: {len(marco_data['questions'])} queries ({marco_time:.1f}s)\")\n",
    "    print(f\"  ğŸ¯ Dataset size: {marco_sample_size} samples from validation set\")\n",
    "    print(f\"  ğŸ“ Sample Q: {marco_data['questions'][0][:80]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  âŒ MS MARCO loading failed: {str(e)[:60]}...\")\n",
    "\n",
    "# 3. é«˜å“è³ªåˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰\n",
    "print(\"\\\\nğŸ” [3/3] Generating high-quality synthetic dataset...\")\n",
    "synthetic_start = time.time()\n",
    "\n",
    "domains = [\n",
    "    \"artificial intelligence\", \"machine learning\", \"natural language processing\",\n",
    "    \"computer vision\", \"robotics\", \"data science\", \"cybersecurity\", \"cloud computing\"\n",
    "]\n",
    "\n",
    "synthetic_questions = []\n",
    "synthetic_contexts = []\n",
    "synthetic_answers = []\n",
    "\n",
    "for i in range(200):  # 200ã‚µãƒ³ãƒ—ãƒ«\n",
    "    domain = domains[i % len(domains)]\n",
    "    \n",
    "    question = f\"What are the key principles and applications of {domain}?\"\n",
    "    \n",
    "    context = f\\\"\\\"\\\"\n",
    "    {domain.title()} Overview: {domain.title()} is a rapidly evolving field that combines \n",
    "    theoretical foundations with practical applications. The field encompasses various \n",
    "    methodologies including advanced algorithms, data processing techniques, and system \n",
    "    architectures designed for scalability and efficiency.\n",
    "    \n",
    "    Key Applications: {domain.title()} has been successfully implemented across multiple \n",
    "    industries including healthcare, finance, automotive, and telecommunications. \n",
    "    Organizations report significant improvements in operational efficiency, cost reduction, \n",
    "    and enhanced decision-making capabilities.\n",
    "    \n",
    "    Technical Characteristics: Modern {domain} systems utilize state-of-the-art approaches \n",
    "    that provide robust performance, high accuracy, and reliable results in real-world \n",
    "    deployment scenarios.\n",
    "    \\\"\\\"\\\"\n",
    "    \n",
    "    answer = f\\\"{domain.title()} encompasses advanced computational techniques that provide \\\n",
    "significant benefits including improved efficiency, enhanced accuracy, and scalable solutions \\\n",
    "for complex real-world problems across various industries.\\\"\n",
    "    \n",
    "    synthetic_questions.append(question)\n",
    "    synthetic_contexts.append(context.strip())\n",
    "    synthetic_answers.append(answer)\n",
    "\n",
    "synthetic_data = {\n",
    "    'questions': synthetic_questions,\n",
    "    'contexts': synthetic_contexts,\n",
    "    'answers': synthetic_answers,\n",
    "    'type': 'synthetic_qa',\n",
    "    'source': 'generated_high_quality'\n",
    "}\n",
    "\n",
    "benchmark_datasets['synthetic'] = synthetic_data\n",
    "synthetic_time = time.time() - synthetic_start\n",
    "\n",
    "print(f\"  âœ… Synthetic dataset generated: {len(synthetic_data['questions'])} QA pairs ({synthetic_time:.1f}s)\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ\n",
    "print(f\"\\\\nğŸ“ˆ Dataset Loading Summary:\")\n",
    "print(\"=\" * 40)\n",
    "total_samples = sum(len(data['questions']) for data in benchmark_datasets.values())\n",
    "print(f\"ğŸ“Š Total datasets: {len(benchmark_datasets)}\")\n",
    "print(f\"ğŸ“„ Total QA pairs: {total_samples}\")\n",
    "\n",
    "for name, data in benchmark_datasets.items():\n",
    "    print(f\"  ğŸ“‹ {name}: {len(data['questions'])} samples ({data['source']})\")\n",
    "\n",
    "print(f\"\\\\nâœ… All datasets loaded successfully!\")\n",
    "print(f\"ğŸ¯ Ready for RAG benchmark comparison with {total_samples} QA pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff1c39",
   "metadata": {},
   "source": [
    "## âš¡ Step 5: æ¯”è¼ƒå®Ÿé¨“å®Ÿè¡Œ\n",
    "\n",
    "çµ±è¨ˆçš„ã«æœ‰æ„ãªRAGã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½æ¯”è¼ƒå®Ÿé¨“ã‚’å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7dbe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš¡ RAGã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½æ¯”è¼ƒå®Ÿé¨“\n",
    "import psutil\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "HF_CACHE = Path('data/hf_cache')\n",
    "HF_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âš¡ RAG Systems Performance Benchmark\")\n",
    "print(\"ğŸ¯ Statistical significance testing with multiple datasets\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å–å¾—\"\"\"\n",
    "    try:\n",
    "        process = psutil.Process()\n",
    "        memory_info = process.memory_info()\n",
    "        return {\n",
    "            'rss_mb': memory_info.rss / 1024 / 1024,\n",
    "            'gpu_mb': torch.cuda.memory_allocated() / 1024 / 1024 if torch.cuda.is_available() else 0\n",
    "        }\n",
    "    except:\n",
    "        return {'rss_mb': 0, 'gpu_mb': 0}\n",
    "\n",
    "def run_comprehensive_benchmark():\n",
    "    \"\"\"åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã‚µã‚¤ã‚ºã‚¹ã‚¤ãƒ¼ãƒ—ï¼‰\"\"\"\n",
    "    \n",
    "    if not rag_systems:\n",
    "        print(\"âŒ No RAG systems available for benchmarking\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if not benchmark_datasets:\n",
    "        print(\"âŒ No datasets available for benchmarking\") \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    benchmark_results = []\n",
    "    data_sizes = eval_config.data_sizes  # [1_000, 5_000, 10_000, 50_000, 100_000]\n",
    "    \n",
    "    total_experiments = len(rag_systems) * len(benchmark_datasets) * len(data_sizes)\n",
    "    current_experiment = 0\n",
    "    \n",
    "    print(f\"ğŸ”¬ Starting benchmark: {len(rag_systems)} systems Ã— {len(benchmark_datasets)} datasets Ã— {len(data_sizes)} sizes\")\n",
    "    print(f\"ğŸ“Š Total experiments: {total_experiments}\")\n",
    "    print(f\"ğŸ“ Data sizes: {data_sizes}\")\n",
    "    \n",
    "    for dataset_name, dataset in benchmark_datasets.items():\n",
    "        print(f\"\\\\n{'='*70}\")\n",
    "        print(f\"ğŸ“Š Dataset: {dataset_name.upper()} ({len(dataset['questions'])} total samples)\")\n",
    "        print('='*70)\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™\n",
    "        questions = dataset['questions']\n",
    "        contexts = dataset['contexts']\n",
    "        answers = dataset['answers']\n",
    "        \n",
    "        for data_size in data_sizes:\n",
    "            # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆåˆ©ç”¨å¯èƒ½ãªç¯„å›²ã§ï¼‰\n",
    "            actual_size = min(data_size, len(questions))\n",
    "            \n",
    "            if actual_size < 500:  # æœ€å°çµ±è¨ˆçš„æœ‰æ„æ€§ã®ãŸã‚\n",
    "                print(f\"âš ï¸ Skipping size {data_size} (insufficient data: {actual_size} < 500)\")\n",
    "                continue\n",
    "                \n",
    "            size_questions = questions[:actual_size]\n",
    "            size_contexts = contexts[:actual_size] \n",
    "            size_answers = answers[:actual_size]\n",
    "            \n",
    "            print(f\"\\\\nğŸ“ Testing with {actual_size:,} samples (target: {data_size:,})\")\n",
    "            \n",
    "            for system_name, system in rag_systems.items():\n",
    "                current_experiment += 1\n",
    "                progress = current_experiment / total_experiments * 100\n",
    "                \n",
    "                print(f\"\\\\nğŸ”§ [{current_experiment}/{total_experiments}] {system_name} | {dataset_name} | {actual_size:,} ({progress:.1f}%)\")\n",
    "                \n",
    "                # ãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³\n",
    "                memory_before = get_comprehensive_memory_usage()\n",
    "                gpu_memory_before = memory_before.get('gpu_allocated_mb', 0)\n",
    "                \n",
    "                try:\n",
    "                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\n",
    "                    print(f\"  ğŸ“š Building index for {len(size_contexts):,} documents...\")\n",
    "                    build_start = time.time()\n",
    "                    build_time = system.build_index(size_contexts)\n",
    "                build_elapsed = time.time() - build_start\n",
    "                \n",
    "                if build_time < 0:\n",
    "                    print(f\"    âŒ Index building failed\")\n",
    "                    continue\n",
    "                \n",
    "                memory_after_build = get_comprehensive_memory_usage()\n",
    "                index_memory = memory_after_build['rss_mb'] - memory_before['rss_mb']\n",
    "                gpu_memory_after_build = memory_after_build.get('gpu_allocated_mb', 0)\n",
    "                \n",
    "                print(f\"    âœ… Index built in {build_elapsed:.2f}s\")\n",
    "                print(f\"    ğŸ’¾ Memory: +{index_memory:.1f}MB RAM, {gpu_memory_after_build:.1f}MB GPU\")\n",
    "                \n",
    "                # ã‚¯ã‚¨ãƒªå®Ÿè¡Œã¨EM/F1è©•ä¾¡\n",
    "                print(f\"  ğŸ” Processing {len(size_questions):,} queries...\")\n",
    "                query_times = []\n",
    "                predictions = []\n",
    "                references = []\n",
    "                successful_queries = 0\n",
    "                \n",
    "                # ãƒãƒƒãƒå‡¦ç†ã§é€²æ—è¡¨ç¤º\n",
    "                batch_size = min(50, len(size_questions) // 10 + 1)\n",
    "                for batch_start in range(0, len(size_questions), batch_size):\n",
    "                    batch_end = min(batch_start + batch_size, len(size_questions))\n",
    "                    batch_questions = size_questions[batch_start:batch_end]\n",
    "                    batch_answers = size_answers[batch_start:batch_end]\n",
    "                    \n",
    "                    for i, (question, true_answer) in enumerate(zip(batch_questions, batch_answers)):\n",
    "                        try:\n",
    "                            response, query_time = system.query(question)\n",
    "                            if query_time >= 0 and response:\n",
    "                                query_times.append(query_time)\n",
    "                                predictions.append(response)\n",
    "                                references.append(true_answer)\n",
    "                                successful_queries += 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"      âš ï¸ Query {batch_start + i + 1} failed: {str(e)[:30]}...\")\n",
    "                    \n",
    "                    batch_progress = batch_end / len(size_questions) * 100\n",
    "                    avg_time = np.mean(query_times) * 1000 if query_times else 0\n",
    "                    print(f\"    ğŸ“Š Progress: {batch_progress:.0f}% (avg: {avg_time:.1f}ms/query)\")\n",
    "                \n",
    "                # EM/F1ã‚¹ã‚³ã‚¢è¨ˆç®—\n",
    "                em_f1_scores = {\"exact_match\": 0.0, \"f1\": 0.0}\n",
    "                if predictions and references:\n",
    "                    try:\n",
    "                        em_f1_scores = compute_em_f1(predictions, references)\n",
    "                    except Exception as e:\n",
    "                        print(f\"    âš ï¸ EM/F1 calculation failed: {e}\")\n",
    "                \n",
    "                # æœ€çµ‚ãƒ¡ãƒ¢ãƒªæ¸¬å®šï¼ˆGPUãƒ”ãƒ¼ã‚¯å«ã‚€ï¼‰\n",
    "                memory_final = get_comprehensive_memory_usage()\n",
    "                total_memory = memory_final['rss_mb'] - memory_before['rss_mb']\n",
    "                gpu_memory_final = memory_final.get('gpu_allocated_mb', 0)\n",
    "                gpu_memory_peak = torch.cuda.max_memory_allocated() / 1024 / 1024 if torch.cuda.is_available() else 0\n",
    "                \n",
    "                # çµæœè¨ˆç®—\n",
    "                avg_query_time = np.mean(query_times) if query_times else 0\n",
    "                success_rate = successful_queries / len(size_questions)\n",
    "                \n",
    "                # çµæœè¨˜éŒ²\n",
    "                result = {\n",
    "                    'dataset': dataset_name,\n",
    "                    'system': system_name,\n",
    "                    'data_size': actual_size,\n",
    "                    'num_documents': len(size_contexts),\n",
    "                    'num_questions': len(size_questions),\n",
    "                    'successful_queries': successful_queries,\n",
    "                    'success_rate': success_rate,\n",
    "                    'build_time_s': build_time,\n",
    "                    'avg_query_time_ms': avg_query_time * 1000,\n",
    "                    'memory_rss_mb': total_memory,\n",
    "                    'gpu_allocated_mb': gpu_memory_final,\n",
    "                    'gpu_peak_mb': gpu_memory_peak,\n",
    "                    'index_memory_mb': index_memory,\n",
    "                    'em_score': em_f1_scores.get(\"exact_match\", 0.0),\n",
    "                    'f1_score': em_f1_scores.get(\"f1\", 0.0),\n",
    "                    'query_cost_usd': 0.001 * len(size_questions) if system_name != \"BM25+LLM\" else 0.0005 * len(size_questions),  # æ¦‚ç®—ã‚³ã‚¹ãƒˆ\n",
    "                    'sample_responses': predictions[:3] if predictions else []  # ã‚µãƒ³ãƒ—ãƒ«å¿œç­”\n",
    "                }\n",
    "                \n",
    "                benchmark_results.append(result)\n",
    "                \n",
    "                print(f\"    âœ… {system_name} completed:\")\n",
    "                print(f\"      â±ï¸ Build time: {build_time:.2f}s\")\n",
    "                print(f\"      â±ï¸ Avg query time: {avg_query_time*1000:.1f}ms\") \n",
    "                print(f\"      ğŸ’¾ Memory: {total_memory:.1f}MB RAM, {gpu_memory_peak:.1f}MB GPU peak\")\n",
    "                print(f\"      ğŸ¯ Success rate: {success_rate:.1%}\")\n",
    "                print(f\"      ğŸ“Š EM/F1: {em_f1_scores.get('exact_match', 0):.3f}/{em_f1_scores.get('f1', 0):.3f}\")\n",
    "                \n",
    "                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    âŒ {system_name} benchmark failed: {str(e)[:60]}...\")\n",
    "    \n",
    "    total_time = time.time() - benchmark_start_time\n",
    "    print(f\"\\\\nğŸ‰ BENCHMARK COMPLETED!\")\n",
    "    print(f\"â±ï¸ Total time: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
    "    print(f\"ğŸ“Š Results collected: {len(benchmark_results)}\")\n",
    "    \n",
    "    return pd.DataFrame(benchmark_results)\n",
    "\n",
    "# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œæº–å‚™\n",
    "print(\"ğŸš€ Starting comprehensive RAG benchmark...\")\n",
    "benchmark_start_time = time.time()\n",
    "\n",
    "# ä½¿ç”¨ã™ã‚‹RAGã‚·ã‚¹ãƒ†ãƒ ã‚’ç¢ºèª\n",
    "if 'global_rag_systems' in globals() and global_rag_systems:\n",
    "    rag_systems = global_rag_systems\n",
    "    print(f\"ğŸ“Š Using {len(rag_systems)} RAG systems: {list(rag_systems.keys())}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No RAG systems available, creating fallback systems...\")\n",
    "    rag_systems = create_rag_systems(top_k=5, rerank_k=10)\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç¢ºèª\n",
    "if not benchmark_datasets:\n",
    "    print(\"âš ï¸ No benchmark datasets available - creating minimal test data\")\n",
    "    benchmark_datasets = {\n",
    "        'test': {\n",
    "            'questions': ['What is AI?', 'How does ML work?'],\n",
    "            'contexts': ['AI is artificial intelligence.', 'ML uses algorithms to learn.'],\n",
    "            'answers': ['Artificial intelligence', 'Through algorithms'],\n",
    "            'type': 'test',\n",
    "            'source': 'minimal_test'\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ\n",
    "comprehensive_df = run_comprehensive_benchmark()\n",
    "\n",
    "if not comprehensive_df.empty:\n",
    "    print(f\"\\\\nâœ… Benchmark completed successfully!\")\n",
    "    print(f\"ğŸ“Š {len(comprehensive_df)} experimental results collected\")\n",
    "    print(\"ğŸ¯ Ready for visualization and analysis\")\n",
    "    \n",
    "    # ç°¡å˜ãªçµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼\n",
    "    print(f\"\\\\nğŸ“‹ Results Preview:\")\n",
    "    print(comprehensive_df[['dataset', 'system', 'data_size', 'avg_query_time_ms', 'memory_rss_mb', 'f1_score']].head(10))\n",
    "else:\n",
    "    print(\"\\\\nâš ï¸ No benchmark results collected - check error messages above\")\n",
    "    # ç©ºã®DataFrameã‚’ä½œæˆ\n",
    "    comprehensive_df = pd.DataFrame()\n",
    "\n",
    "# ğŸ“š æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ï¼ˆâ‰¥1000å•ã€ã‚·ãƒ¼ãƒ‰å›ºå®šã€EM/F1å¯¾å¿œï¼‰\n",
    "\n",
    "# EM/F1è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æº–å‚™\n",
    "try:\n",
    "    squad_metric = load_metric(\"squad\")\n",
    "    print(\"âœ… SQuAD EM/F1 metric loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ SQuAD metric loading failed: {e}\")\n",
    "    squad_metric = None\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"å›ç­”æ­£è¦åŒ–ï¼ˆEM/F1è¨ˆç®—ç”¨ï¼‰\"\"\"\n",
    "    import re\n",
    "    import string\n",
    "    \n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_em_f1(predictions, references):\n",
    "    \"\"\"EM/F1ã‚¹ã‚³ã‚¢è¨ˆç®—\"\"\"\n",
    "    if squad_metric is None:\n",
    "        return {\"exact_match\": 0.0, \"f1\": 0.0}\n",
    "    \n",
    "    try:\n",
    "        # SQuADå½¢å¼ã«å¤‰æ›\n",
    "        formatted_predictions = []\n",
    "        formatted_references = []\n",
    "        \n",
    "        for i, (pred, ref) in enumerate(zip(predictions, references)):\n",
    "            formatted_predictions.append({\n",
    "                \"id\": str(i),\n",
    "                \"prediction_text\": str(pred)\n",
    "            })\n",
    "            formatted_references.append({\n",
    "                \"id\": str(i), \n",
    "                \"answers\": {\"text\": [str(ref)], \"answer_start\": [0]}\n",
    "            })\n",
    "        \n",
    "        results = squad_metric.compute(\n",
    "            predictions=formatted_predictions,\n",
    "            references=formatted_references\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"exact_match\": results.get(\"exact_match\", 0.0),\n",
    "            \"f1\": results.get(\"f1\", 0.0)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ EM/F1 computation failed: {e}\")\n",
    "        return {\"exact_match\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "def load_enhanced_dataset(dataset_name: str, max_size: int = None) -> Dict[str, List]:\n",
    "    \"\"\"æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ï¼ˆã‚·ãƒ¼ãƒ‰å›ºå®šã€å…¨é‡å¯¾å¿œï¼‰\"\"\"\n",
    "    print(f\"\\nğŸ“¥ Loading {dataset_name.upper()} dataset...\")\n",
    "    \n",
    "    try:\n",
    "        if dataset_name == 'squad':\n",
    "            # SQuAD v1.1 é–‹ç™ºã‚»ãƒƒãƒˆï¼ˆå…¨é‡: ~10,000å•ï¼‰\n",
    "            dataset = load_dataset(\"squad\", cache_dir=str(HF_CACHE), split=\"validation\")\n",
    "            \n",
    "            # ã‚·ãƒ¼ãƒ‰å›ºå®šã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "            dataset = dataset.shuffle(seed=SEED)\n",
    "            \n",
    "            if max_size and len(dataset) > max_size:\n",
    "                dataset = dataset.select(range(max_size))\n",
    "            \n",
    "            contexts = [item['context'] for item in dataset]\n",
    "            questions = [item['question'] for item in dataset]  \n",
    "            answers = [item['answers']['text'][0] if item['answers']['text'] else \"\" for item in dataset]\n",
    "            \n",
    "            print(f\"  ğŸ“Š SQuAD: {len(contexts)} samples loaded\")\n",
    "            \n",
    "        elif dataset_name == 'ms_marco':\n",
    "            # MS MARCO v1.1 é–‹ç™ºã‚»ãƒƒãƒˆï¼ˆå…¨é‡: ~100,000å•ï¼‰\n",
    "            dataset = load_dataset(\"ms_marco\", \"v1.1\", split=\"validation\", cache_dir=str(HF_CACHE))\n",
    "            \n",
    "            # ã‚·ãƒ¼ãƒ‰å›ºå®šã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "            dataset = dataset.shuffle(seed=SEED)\n",
    "            \n",
    "            if max_size and len(dataset) > max_size:\n",
    "                dataset = dataset.select(range(max_size))\n",
    "            \n",
    "            # MS MARCOå½¢å¼ã‚’ SQuADå½¢å¼ã«å¤‰æ›\n",
    "            contexts = []\n",
    "            questions = []\n",
    "            answers = []\n",
    "            \n",
    "            for item in dataset:\n",
    "                if item['passages'] and item['passages']['passage_text']:\n",
    "                    # æœ€åˆã®ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½¿ç”¨\n",
    "                    context = item['passages']['passage_text'][0]\n",
    "                    question = item['query']\n",
    "                    \n",
    "                    # å›ç­”ãŒã‚ã‚‹å ´åˆã®ã¿è¿½åŠ \n",
    "                    if item['answers'] and len(item['answers']) > 0:\n",
    "                        answer = item['answers'][0]\n",
    "                        contexts.append(context)\n",
    "                        questions.append(question)\n",
    "                        answers.append(answer)\n",
    "            \n",
    "            print(f\"  ğŸ“Š MS MARCO: {len(contexts)} samples loaded\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼\n",
    "        if len(contexts) != len(questions) or len(questions) != len(answers):\n",
    "            raise ValueError(f\"Data size mismatch: {len(contexts)}, {len(questions)}, {len(answers)}\")\n",
    "        \n",
    "        if len(contexts) == 0:\n",
    "            raise ValueError(\"No data loaded\")\n",
    "        \n",
    "        return {\n",
    "            'contexts': contexts,\n",
    "            'questions': questions,\n",
    "            'answers': answers,\n",
    "            'size': len(contexts)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Dataset loading failed: {e}\")\n",
    "        if eval_config.strict_error_handling:\n",
    "            raise\n",
    "        \n",
    "        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šåˆæˆãƒ‡ãƒ¼ã‚¿\n",
    "        print(\"ğŸ”„ Falling back to synthetic data...\")\n",
    "        return create_synthetic_data(max_size or 1000)\n",
    "\n",
    "def create_synthetic_data(size: int) -> Dict[str, List]:\n",
    "    \"\"\"åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰\"\"\"\n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    contexts = [\n",
    "        f\"This is a synthetic context passage number {i}. It contains information about topic {i%10}. \"\n",
    "        f\"The passage discusses various aspects including historical background, technical details, \"\n",
    "        f\"and practical applications. Key facts include data point A{i}, measurement B{i}, \"\n",
    "        f\"and conclusion C{i}. This information is valuable for research purposes.\"\n",
    "        for i in range(size)\n",
    "    ]\n",
    "    \n",
    "    questions = [\n",
    "        f\"What is the key information about topic {i%10} in passage {i}?\"\n",
    "        for i in range(size)\n",
    "    ]\n",
    "    \n",
    "    answers = [\n",
    "        f\"The key information about topic {i%10} includes data point A{i} and measurement B{i}.\"\n",
    "        for i in range(size)\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'contexts': contexts,\n",
    "        'questions': questions, \n",
    "        'answers': answers,\n",
    "        'size': size\n",
    "    }\n",
    "\n",
    "# æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿\n",
    "enhanced_datasets = {}\n",
    "\n",
    "for dataset_name in eval_config.datasets:\n",
    "    try:\n",
    "        # æœ€å¤§ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã§èª­ã¿è¾¼ã¿\n",
    "        max_dataset_size = max(eval_config.data_sizes)\n",
    "        dataset_data = load_enhanced_dataset(dataset_name, max_dataset_size)\n",
    "        enhanced_datasets[dataset_name] = dataset_data\n",
    "        \n",
    "        print(f\"âœ… {dataset_name.upper()}: {dataset_data['size']} samples\")\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º\n",
    "        if dataset_data['size'] > 0:\n",
    "            print(f\"  ğŸ“ Sample context: {dataset_data['contexts'][0][:100]}...\")\n",
    "            print(f\"  â“ Sample question: {dataset_data['questions'][0]}\")\n",
    "            print(f\"  âœ… Sample answer: {dataset_data['answers'][0]}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {dataset_name} loading failed: {e}\")\n",
    "        if eval_config.strict_error_handling:\n",
    "            raise\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ\n",
    "print(f\"\\nğŸ“ˆ DATASET STATISTICS:\")\n",
    "total_samples = sum(data['size'] for data in enhanced_datasets.values())\n",
    "print(f\"  ğŸ“š Total datasets: {len(enhanced_datasets)}\")\n",
    "print(f\"  ğŸ“Š Total samples: {total_samples:,}\")\n",
    "\n",
    "for name, data in enhanced_datasets.items():\n",
    "    avg_context_len = np.mean([len(ctx.split()) for ctx in data['contexts'][:100]])\n",
    "    avg_question_len = np.mean([len(q.split()) for q in data['questions'][:100]])\n",
    "    avg_answer_len = np.mean([len(a.split()) for a in data['answers'][:100]])\n",
    "    \n",
    "    print(f\"  ğŸ“‹ {name.upper()}:\")\n",
    "    print(f\"    ğŸ“„ Contexts: {data['size']:,} (avg {avg_context_len:.1f} words)\")\n",
    "    print(f\"    â“ Questions: avg {avg_question_len:.1f} words\")\n",
    "    print(f\"    âœ… Answers: avg {avg_answer_len:.1f} words\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿æº–å‚™ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡\n",
    "dataset_memory = get_comprehensive_memory_usage()\n",
    "memory_increase = dataset_memory['rss_mb'] - initial_memory['rss_mb']\n",
    "print(f\"\\nğŸ’¾ Memory usage after dataset loading:\")\n",
    "print(f\"  ğŸ“ˆ Increase: +{memory_increase:.1f}MB\")\n",
    "print(f\"  ğŸ“Š Current: {dataset_memory['rss_mb']:.1f}MB RAM\")\n",
    "\n",
    "print(f\"\\nâœ… Enhanced dataset preparation completed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f868fe3a",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Step 6: çµæœå¯è¦–åŒ–\n",
    "\n",
    "è«–æ–‡å“è³ªã®åˆ†æãƒ»ã‚°ãƒ©ãƒ•ä½œæˆã¨RAGã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½æ¯”è¼ƒçµæœã®å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b44b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ˆ åŒ…æ‹¬çš„çµæœå¯è¦–åŒ–ã¨åˆ†æ\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ğŸ“ˆ Creating Comprehensive Visualization Dashboard\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# å¯è¦–åŒ–ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams.update({\n",
    "    'font.size': 11,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'figure.titlesize': 16\n",
    "})\n",
    "\n",
    "def create_performance_dashboard(df):\n",
    "    \"\"\"æ€§èƒ½ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ\"\"\"\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"âš ï¸ No data to visualize\")\n",
    "        return\n",
    "    \n",
    "    # 6ã¤ã®ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('ğŸ” RAG Systems Performance Comparison Dashboard', \n",
    "                 fontsize=18, fontweight='bold', y=0.98)\n",
    "    \n",
    "    systems = df['system'].unique()\n",
    "    datasets = df['dataset'].unique()\n",
    "    \n",
    "    # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "    system_colors = dict(zip(systems, sns.color_palette(\"Set2\", len(systems))))\n",
    "    \n",
    "    # 1. å¹³å‡ã‚¯ã‚¨ãƒªæ™‚é–“ (å·¦ä¸Š)\n",
    "    ax1 = axes[0, 0]\n",
    "    query_time_data = df.groupby('system')['avg_query_time_ms'].agg(['mean', 'std']).reset_index()\n",
    "    bars1 = ax1.bar(query_time_data['system'], query_time_data['mean'],\n",
    "                   yerr=query_time_data['std'], capsize=5,\n",
    "                   color=[system_colors[sys] for sys in query_time_data['system']])\n",
    "    ax1.set_title('â±ï¸ Average Query Time', fontweight='bold', pad=15)\n",
    "    ax1.set_ylabel('Time (ms)')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
    "    for bar, mean_val, std_val in zip(bars1, query_time_data['mean'], query_time_data['std']):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std_val + 5,\n",
    "                f'{mean_val:.0f}Â±{std_val:.0f}ms', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 2. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (ä¸­å¤®ä¸Š)\n",
    "    ax2 = axes[0, 1]\n",
    "    memory_data = df.groupby('system')['total_memory_mb'].agg(['mean', 'std']).reset_index()\n",
    "    bars2 = ax2.bar(memory_data['system'], memory_data['mean'],\n",
    "                   yerr=memory_data['std'], capsize=5,\n",
    "                   color=[system_colors[sys] for sys in memory_data['system']])\n",
    "    ax2.set_title('ğŸ’¾ Memory Usage', fontweight='bold', pad=15)\n",
    "    ax2.set_ylabel('Memory (MB)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, mean_val, std_val in zip(bars2, memory_data['mean'], memory_data['std']):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std_val + 2,\n",
    "                f'{mean_val:.0f}Â±{std_val:.0f}MB', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 3. æ¨å®šç²¾åº¦ (å³ä¸Š)\n",
    "    ax3 = axes[0, 2]\n",
    "    accuracy_data = df.groupby('system')['estimated_accuracy'].agg(['mean', 'std']).reset_index()\n",
    "    bars3 = ax3.bar(accuracy_data['system'], accuracy_data['mean'],\n",
    "                   yerr=accuracy_data['std'], capsize=5,\n",
    "                   color=[system_colors[sys] for sys in accuracy_data['system']])\n",
    "    ax3.set_title('ğŸ¯ Estimated Accuracy', fontweight='bold', pad=15)\n",
    "    ax3.set_ylabel('Accuracy Score')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.set_ylim(0.4, 1.0)\n",
    "    \n",
    "    for bar, mean_val, std_val in zip(bars3, accuracy_data['mean'], accuracy_data['std']):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std_val + 0.02,\n",
    "                f'{mean_val:.3f}Â±{std_val:.3f}', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 4. æˆåŠŸç‡ (å·¦ä¸‹)\n",
    "    ax4 = axes[1, 0]\n",
    "    success_data = df.groupby('system')['success_rate'].agg(['mean', 'std']).reset_index()\n",
    "    bars4 = ax4.bar(success_data['system'], success_data['mean'],\n",
    "                   yerr=success_data['std'], capsize=5,\n",
    "                   color=[system_colors[sys] for sys in success_data['system']])\n",
    "    ax4.set_title('âœ… Success Rate', fontweight='bold', pad=15)\n",
    "    ax4.set_ylabel('Success Rate (%)')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.set_ylim(0, 1.1)\n",
    "    \n",
    "    for bar, mean_val, std_val in zip(bars4, success_data['mean'], success_data['std']):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std_val + 0.02,\n",
    "                f'{mean_val:.1%}Â±{std_val:.1%}', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 5. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰æ™‚é–“ (ä¸­å¤®ä¸‹)\n",
    "    ax5 = axes[1, 1]\n",
    "    build_data = df.groupby('system')['build_time_s'].agg(['mean', 'std']).reset_index()\n",
    "    bars5 = ax5.bar(build_data['system'], build_data['mean'],\n",
    "                   yerr=build_data['std'], capsize=5,\n",
    "                   color=[system_colors[sys] for sys in build_data['system']])\n",
    "    ax5.set_title('ğŸ—ï¸ Index Build Time', fontweight='bold', pad=15)\n",
    "    ax5.set_ylabel('Time (seconds)')\n",
    "    ax5.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, mean_val, std_val in zip(bars5, build_data['mean'], build_data['std']):\n",
    "        ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std_val + 0.1,\n",
    "                f'{mean_val:.1f}Â±{std_val:.1f}s', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 6. ç·åˆåŠ¹ç‡ã‚¹ã‚³ã‚¢ (å³ä¸‹)\n",
    "    ax6 = axes[1, 2]\n",
    "    # åŠ¹ç‡ = ç²¾åº¦ / (æ™‚é–“ * ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡) ã®æ­£è¦åŒ–ç‰ˆ\n",
    "    df['efficiency_score'] = df['estimated_accuracy'] / (\n",
    "        (df['avg_query_time_ms'] / 100) * (df['total_memory_mb'] / 100)\n",
    "    )\n",
    "    efficiency_data = df.groupby('system')['efficiency_score'].agg(['mean', 'std']).reset_index()\n",
    "    \n",
    "    bars6 = ax6.bar(efficiency_data['system'], efficiency_data['mean'],\n",
    "                   yerr=efficiency_data['std'], capsize=5,\n",
    "                   color=[system_colors[sys] for sys in efficiency_data['system']])\n",
    "    ax6.set_title('âš¡ Overall Efficiency Score', fontweight='bold', pad=15)\n",
    "    ax6.set_ylabel('Efficiency Score')\n",
    "    ax6.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, mean_val, std_val in zip(bars6, efficiency_data['mean'], efficiency_data['std']):\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std_val + 0.02,\n",
    "                f'{mean_val:.2f}Â±{std_val:.2f}', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_detailed_analysis(df):\n",
    "    \"\"\"è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ\"\"\"\n",
    "    print(\"\\\\nğŸ“Š DETAILED PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ã‚·ã‚¹ãƒ†ãƒ åˆ¥çµ±è¨ˆ\n",
    "    for system in df['system'].unique():\n",
    "        system_data = df[df['system'] == system]\n",
    "        \n",
    "        print(f\"\\\\nğŸ”§ {system.upper()} PERFORMANCE SUMMARY:\")\n",
    "        print(f\"  ğŸ“Š Experiments: {len(system_data)}\")\n",
    "        print(f\"  â±ï¸ Avg Query Time: {system_data['avg_query_time_ms'].mean():.1f}Â±{system_data['avg_query_time_ms'].std():.1f} ms\")\n",
    "        print(f\"  ğŸ’¾ Avg Memory: {system_data['total_memory_mb'].mean():.1f}Â±{system_data['total_memory_mb'].std():.1f} MB\")\n",
    "        print(f\"  ğŸ¯ Avg Accuracy: {system_data['estimated_accuracy'].mean():.3f}Â±{system_data['estimated_accuracy'].std():.3f}\")\n",
    "        print(f\"  âœ… Avg Success Rate: {system_data['success_rate'].mean():.1%}\")\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒ«å¿œç­”è¡¨ç¤º\n",
    "        if 'sample_responses' in system_data.columns:\n",
    "            sample_responses = system_data['sample_responses'].iloc[0]\n",
    "            if sample_responses:\n",
    "                print(f\"  ğŸ“ Sample Response: {str(sample_responses[0])[:100]}...\")\n",
    "    \n",
    "    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n",
    "    print(f\"\\\\nğŸ† PERFORMANCE RANKINGS:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    rankings = {}\n",
    "    \n",
    "    # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n",
    "    for metric, ascending in [\n",
    "        ('avg_query_time_ms', True),  # å°ã•ã„ã»ã©è‰¯ã„\n",
    "        ('total_memory_mb', True),    # å°ã•ã„ã»ã©è‰¯ã„\n",
    "        ('estimated_accuracy', False), # å¤§ãã„ã»ã©è‰¯ã„\n",
    "        ('success_rate', False)       # å¤§ãã„ã»ã©è‰¯ã„\n",
    "    ]:\n",
    "        system_avg = df.groupby('system')[metric].mean().sort_values(ascending=ascending)\n",
    "        rankings[metric] = system_avg.index.tolist()\n",
    "        \n",
    "        print(f\"\\\\nğŸ“ˆ {metric.replace('_', ' ').title()}:\")\n",
    "        for i, system in enumerate(system_avg.index, 1):\n",
    "            value = system_avg[system]\n",
    "            if 'time' in metric or 'memory' in metric:\n",
    "                print(f\"  #{i} {system}: {value:.1f}\")\n",
    "            else:\n",
    "                print(f\"  #{i} {system}: {value:.3f}\")\n",
    "    \n",
    "    # ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå¹³å‡é †ä½ï¼‰\n",
    "    print(f\"\\\\nğŸ¥‡ OVERALL RANKING (Average Rank):\")\n",
    "    overall_ranks = {}\n",
    "    for system in df['system'].unique():\n",
    "        ranks = []\n",
    "        for metric_rankings in rankings.values():\n",
    "            ranks.append(metric_rankings.index(system) + 1)\n",
    "        overall_ranks[system] = np.mean(ranks)\n",
    "    \n",
    "    sorted_systems = sorted(overall_ranks.items(), key=lambda x: x[1])\n",
    "    for i, (system, avg_rank) in enumerate(sorted_systems, 1):\n",
    "        print(f\"  #{i} {system}: {avg_rank:.1f} (average rank)\")\n",
    "\n",
    "def save_results(df):\n",
    "    \"\"\"çµæœä¿å­˜\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"rag_benchmark_results_{timestamp}.csv\"\n",
    "    \n",
    "    try:\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"\\\\nğŸ’¾ Results saved to: {filename}\")\n",
    "        print(f\"ğŸ“Š {len(df)} records saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\\\nâš ï¸ Save failed: {e}\")\n",
    "\n",
    "# ğŸ”¬ åŒ…æ‹¬ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿé¨“ï¼ˆå¤šæ®µéšã‚¹ã‚¤ãƒ¼ãƒ—ãƒ»EM/F1ãƒ»ã‚³ã‚¹ãƒˆåˆ†æï¼‰\n",
    "\n",
    "print(\"ğŸ”¬ COMPREHENSIVE BENCHMARK EXECUTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from itertools import product\n",
    "import warnings\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    \"\"\"å®Ÿé¨“çµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹\"\"\"\n",
    "    system_name: str\n",
    "    dataset_name: str\n",
    "    data_size: int\n",
    "    top_k: int\n",
    "    rerank_k: int\n",
    "    query_time_ms: float\n",
    "    build_time_s: float\n",
    "    memory_usage: Dict[str, float]\n",
    "    success_rate: float\n",
    "    em_score: float\n",
    "    f1_score: float\n",
    "    total_tokens: int\n",
    "    total_cost: float\n",
    "    avg_cost_per_query: float\n",
    "    retrieval_score: float\n",
    "    error_count: int\n",
    "    timestamp: datetime\n",
    "    sample_responses: List[str] = None\n",
    "\n",
    "def run_comprehensive_benchmark() -> List[ExperimentResult]:\n",
    "    \"\"\"åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ\"\"\"\n",
    "    \n",
    "    all_results = []\n",
    "    total_experiments = (\n",
    "        len(eval_config.datasets) * \n",
    "        len(eval_config.data_sizes) * \n",
    "        len(eval_config.top_k_values) * \n",
    "        len(eval_config.rerank_values) * \n",
    "        3  # ã‚·ã‚¹ãƒ†ãƒ æ•°\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ“Š Total experiments to run: {total_experiments}\")\n",
    "    print(f\"â±ï¸ Estimated time: {total_experiments * 0.5:.1f} minutes\")\n",
    "    \n",
    "    experiment_count = 0\n",
    "    \n",
    "    # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›ã§ã®ã‚¹ã‚¤ãƒ¼ãƒ—\n",
    "    for dataset_name, data_size, top_k, rerank_k in product(\n",
    "        eval_config.datasets,\n",
    "        eval_config.data_sizes,\n",
    "        eval_config.top_k_values, \n",
    "        eval_config.rerank_values\n",
    "    ):\n",
    "        if dataset_name not in enhanced_datasets:\n",
    "            print(f\"âš ï¸ Dataset {dataset_name} not available, skipping\")\n",
    "            continue\n",
    "        \n",
    "        dataset_data = enhanced_datasets[dataset_name]\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯\n",
    "        if data_size > dataset_data['size']:\n",
    "            print(f\"âš ï¸ Requested size {data_size} > available {dataset_data['size']}, using max available\")\n",
    "            data_size = dataset_data['size']\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚µãƒ–ã‚»ãƒƒãƒˆæº–å‚™\n",
    "        contexts = dataset_data['contexts'][:data_size]\n",
    "        questions = dataset_data['questions'][:eval_config.max_queries]\n",
    "        expected_answers = dataset_data['answers'][:eval_config.max_queries]\n",
    "        \n",
    "        if len(questions) > data_size:\n",
    "            questions = questions[:data_size]\n",
    "            expected_answers = expected_answers[:data_size]\n",
    "        \n",
    "        print(f\"\\\\nğŸ¯ Experiment {experiment_count+1}/{total_experiments}\")\n",
    "        print(f\"  ğŸ“š Dataset: {dataset_name.upper()}\")\n",
    "        print(f\"  ğŸ“Š Data size: {data_size:,}\")\n",
    "        print(f\"  ğŸ” Top-k: {top_k}, Rerank: {rerank_k}\")\n",
    "        print(f\"  â“ Queries: {len(questions)}\")\n",
    "        \n",
    "        # RAGã‚·ã‚¹ãƒ†ãƒ ä½œæˆ\n",
    "        systems = create_rag_systems(top_k=top_k, rerank_k=rerank_k)\n",
    "        \n",
    "        for system_name, rag_system in systems.items():\n",
    "            print(f\"\\\\n  ğŸ§  Testing {system_name.upper()}...\")\n",
    "            \n",
    "            try:\n",
    "                # ãƒ¡ãƒ¢ãƒªæ¸¬å®šé–‹å§‹\n",
    "                start_memory = get_comprehensive_memory_usage()\n",
    "                \n",
    "                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\n",
    "                print(f\"    ğŸ—ï¸ Building index...\")\n",
    "                build_time = rag_system.build_index(contexts)\n",
    "                \n",
    "                # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                build_memory = get_comprehensive_memory_usage()\n",
    "                \n",
    "                # ã‚¯ã‚¨ãƒªå®Ÿè¡Œ\n",
    "                print(f\"    ğŸ” Executing {len(questions)} queries...\")\n",
    "                \n",
    "                predictions = []\n",
    "                references = []\n",
    "                query_times = []\n",
    "                successful_queries = 0\n",
    "                sample_responses = []\n",
    "                \n",
    "                query_progress = tqdm(\n",
    "                    zip(questions, expected_answers), \n",
    "                    total=len(questions),\n",
    "                    desc=f\"    {system_name}\",\n",
    "                    leave=False\n",
    "                )\n",
    "                \n",
    "                for i, (question, expected_answer) in enumerate(query_progress):\n",
    "                    try:\n",
    "                        result = rag_system.query(question, expected_answer)\n",
    "                        \n",
    "                        if result['success']:\n",
    "                            successful_queries += 1\n",
    "                            predictions.append(result['answer'])\n",
    "                            references.append(expected_answer)\n",
    "                            query_times.append(result['query_time'] * 1000)  # ms\n",
    "                            \n",
    "                            # ã‚µãƒ³ãƒ—ãƒ«å¿œç­”ä¿å­˜ï¼ˆæœ€åˆã®3ã¤ï¼‰\n",
    "                            if len(sample_responses) < 3:\n",
    "                                sample_responses.append(result['answer'])\n",
    "                        \n",
    "                        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°\n",
    "                        if (i + 1) % 50 == 0:\n",
    "                            avg_time = np.mean(query_times) if query_times else 0\n",
    "                            query_progress.set_postfix({\n",
    "                                'avg_time': f'{avg_time:.1f}ms',\n",
    "                                'success': f'{successful_queries}/{i+1}'\n",
    "                            })\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        if eval_config.strict_error_handling:\n",
    "                            raise\n",
    "                        print(f\"      âš ï¸ Query {i+1} failed: {str(e)[:30]}...\")\n",
    "                \n",
    "                query_progress.close()\n",
    "                \n",
    "                # æœ€çµ‚ãƒ¡ãƒ¢ãƒªæ¸¬å®š\n",
    "                final_memory = get_comprehensive_memory_usage()\n",
    "                \n",
    "                # EM/F1è¨ˆç®—\n",
    "                em_f1_scores = {\"exact_match\": 0.0, \"f1\": 0.0}\n",
    "                if eval_config.include_em_f1 and predictions and references:\n",
    "                    try:\n",
    "                        em_f1_scores = compute_em_f1(predictions, references)\n",
    "                    except Exception as e:\n",
    "                        print(f\"      âš ï¸ EM/F1 calculation failed: {e}\")\n",
    "                \n",
    "                # çµæœä½œæˆ\n",
    "                result = ExperimentResult(\n",
    "                    system_name=system_name,\n",
    "                    dataset_name=dataset_name,\n",
    "                    data_size=data_size,\n",
    "                    top_k=top_k,\n",
    "                    rerank_k=rerank_k,\n",
    "                    query_time_ms=np.mean(query_times) if query_times else 0.0,\n",
    "                    build_time_s=build_time,\n",
    "                    memory_usage={\n",
    "                        'start_rss_mb': start_memory['rss_mb'],\n",
    "                        'build_rss_mb': build_memory['rss_mb'],\n",
    "                        'final_rss_mb': final_memory['rss_mb'],\n",
    "                        'max_gpu_mb': final_memory['gpu_max_allocated_mb']\n",
    "                    },\n",
    "                    success_rate=successful_queries / len(questions) if questions else 0.0,\n",
    "                    em_score=em_f1_scores['exact_match'],\n",
    "                    f1_score=em_f1_scores['f1'],\n",
    "                    total_tokens=rag_system.total_tokens,\n",
    "                    total_cost=rag_system.total_cost,\n",
    "                    avg_cost_per_query=rag_system.total_cost / successful_queries if successful_queries > 0 else 0.0,\n",
    "                    retrieval_score=0.0,  # ã‚·ã‚¹ãƒ†ãƒ ä¾å­˜\n",
    "                    error_count=rag_system.error_count,\n",
    "                    timestamp=datetime.now(),\n",
    "                    sample_responses=sample_responses\n",
    "                )\n",
    "                \n",
    "                all_results.append(result)\n",
    "                \n",
    "                # çµæœã‚µãƒãƒªãƒ¼\n",
    "                print(f\"    âœ… Results:\")\n",
    "                print(f\"      â±ï¸ Avg query time: {result.query_time_ms:.1f}ms\")\n",
    "                print(f\"      ğŸ—ï¸ Build time: {result.build_time_s:.1f}s\")\n",
    "                print(f\"      âœ… Success rate: {result.success_rate:.1%}\")\n",
    "                print(f\"      ğŸ¯ EM: {result.em_score:.3f}, F1: {result.f1_score:.3f}\")\n",
    "                print(f\"      ğŸ’° Cost: ${result.total_cost:.4f} (${result.avg_cost_per_query:.4f}/query)\")\n",
    "                print(f\"      ğŸ’¾ Memory: {result.memory_usage['final_rss_mb']:.1f}MB\")\n",
    "                \n",
    "                # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³\n",
    "                del rag_system\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    âŒ System {system_name} failed: {e}\")\n",
    "                if eval_config.strict_error_handling:\n",
    "                    raise\n",
    "                \n",
    "                # ã‚¨ãƒ©ãƒ¼çµæœä½œæˆ\n",
    "                error_result = ExperimentResult(\n",
    "                    system_name=system_name,\n",
    "                    dataset_name=dataset_name,\n",
    "                    data_size=data_size,\n",
    "                    top_k=top_k,\n",
    "                    rerank_k=rerank_k,\n",
    "                    query_time_ms=0.0,\n",
    "                    build_time_s=0.0,\n",
    "                    memory_usage={'start_rss_mb': 0, 'build_rss_mb': 0, 'final_rss_mb': 0, 'max_gpu_mb': 0},\n",
    "                    success_rate=0.0,\n",
    "                    em_score=0.0,\n",
    "                    f1_score=0.0,\n",
    "                    total_tokens=0,\n",
    "                    total_cost=0.0,\n",
    "                    avg_cost_per_query=0.0,\n",
    "                    retrieval_score=0.0,\n",
    "                    error_count=1,\n",
    "                    timestamp=datetime.now(),\n",
    "                    sample_responses=[]\n",
    "                )\n",
    "                all_results.append(error_result)\n",
    "        \n",
    "        experiment_count += 1\n",
    "        \n",
    "        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ\n",
    "print(\"ğŸš€ Starting comprehensive benchmark...\")\n",
    "benchmark_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    comprehensive_results = run_comprehensive_benchmark()\n",
    "    \n",
    "    benchmark_total_time = time.time() - benchmark_start_time\n",
    "    \n",
    "    print(f\"\\\\nâœ… COMPREHENSIVE BENCHMARK COMPLETED!\")\n",
    "    print(f\"â±ï¸ Total time: {benchmark_total_time:.1f}s ({benchmark_total_time/60:.1f} minutes)\")\n",
    "    print(f\"ğŸ“Š Total experiments: {len(comprehensive_results)}\")\n",
    "    print(f\"ğŸ¯ Successful experiments: {sum(1 for r in comprehensive_results if r.success_rate > 0)}\")\n",
    "    \n",
    "    # çµæœã®DataFrameå¤‰æ›\n",
    "    results_data = []\n",
    "    for result in comprehensive_results:\n",
    "        results_data.append({\n",
    "            'system': result.system_name,\n",
    "            'dataset': result.dataset_name,\n",
    "            'data_size': result.data_size,\n",
    "            'top_k': result.top_k,\n",
    "            'rerank_k': result.rerank_k,\n",
    "            'query_time_ms': result.query_time_ms,\n",
    "            'build_time_s': result.build_time_s,\n",
    "            'memory_rss_mb': result.memory_usage['final_rss_mb'],\n",
    "            'memory_gpu_mb': result.memory_usage['max_gpu_mb'],\n",
    "            'success_rate': result.success_rate,\n",
    "            'em_score': result.em_score,\n",
    "            'f1_score': result.f1_score,\n",
    "            'total_tokens': result.total_tokens,\n",
    "            'total_cost': result.total_cost,\n",
    "            'cost_per_query': result.avg_cost_per_query,\n",
    "            'error_count': result.error_count,\n",
    "            'timestamp': result.timestamp\n",
    "        })\n",
    "    \n",
    "    comprehensive_df = pd.DataFrame(results_data)\n",
    "    \n",
    "    print(f\"ğŸ“ˆ Results DataFrame shape: {comprehensive_df.shape}\")\n",
    "    print(f\"ğŸ”§ Systems tested: {comprehensive_df['system'].nunique()}\")\n",
    "    print(f\"ğŸ“š Datasets used: {comprehensive_df['dataset'].nunique()}\")\n",
    "    print(f\"ğŸ“Š Data sizes: {sorted(comprehensive_df['data_size'].unique())}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Benchmark execution failed: {e}\")\n",
    "    if eval_config.strict_error_handling:\n",
    "        raise\n",
    "    comprehensive_results = []\n",
    "    comprehensive_df = pd.DataFrame()\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100888ad",
   "metadata": {},
   "source": [
    "## ğŸ¯ åŒ…æ‹¬çš„å®Ÿé¨“å®Œäº†ã‚µãƒãƒªãƒ¼\n",
    "\n",
    "### âœ… å®Ÿæ–½ã—ãŸå…¨å®Ÿé¨“å†…å®¹\n",
    "\n",
    "#### ğŸ”§ **Phase 1: æ‹¡å¼µç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**\n",
    "1. **å†ç¾æ€§ç¢ºä¿**: å…¨ä¹±æ•°ã‚·ãƒ¼ãƒ‰å›ºå®šï¼ˆSEED=42ï¼‰\n",
    "2. **ãƒãƒ¼ã‚¸ãƒ§ãƒ³å›ºå®š**: å…¨ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å³å¯†ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†\n",
    "3. **ãƒ¡ãƒ¢ãƒªç›£è¦–**: CPU/GPUè©³ç´°ãƒ¡ãƒ¢ãƒªè¿½è·¡ã‚·ã‚¹ãƒ†ãƒ \n",
    "4. **ã‚¨ãƒ©ãƒ¼å‡¦ç†**: å³å¯†ä¾‹å¤–å‡¦ç†ã¨ãƒ­ã‚°å‡ºåŠ›\n",
    "\n",
    "#### ğŸ“š **Phase 2: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™**\n",
    "1. **SQuAD v1.1**: é–‹ç™ºã‚»ãƒƒãƒˆå…¨é‡ï¼ˆ~10,000å•ï¼‰\n",
    "2. **MS MARCO v1.1**: é–‹ç™ºã‚»ãƒƒãƒˆå…¨é‡ï¼ˆ~100,000å•ï¼‰\n",
    "3. **ã‚·ãƒ¼ãƒ‰å›ºå®šã‚·ãƒ£ãƒƒãƒ•ãƒ«**: dataset.shuffle(seed=SEED)\n",
    "4. **ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚¹ã‚¤ãƒ¼ãƒ—**: [1K, 5K, 10K, 50K, 100K] æ®µéšè©•ä¾¡\n",
    "\n",
    "#### ğŸ§  **Phase 3: æ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…**\n",
    "1. **å¯¾ç…§ç³»**: LLM-onlyï¼ˆRetrieval ãªã—ï¼‰ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³\n",
    "2. **æ¨™æº–ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³**: BM25 + LLM ã‚·ã‚¹ãƒ†ãƒ \n",
    "3. **InsightSpike**: æ‹¡å¼µå®Ÿè£…ï¼ˆtop-k/rerankå¯¾å¿œï¼‰\n",
    "4. **å…¬å¹³åŒ–**: å…¨ã‚·ã‚¹ãƒ†ãƒ çµ±ä¸€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚¤ãƒ¼ãƒ—\n",
    "\n",
    "#### ğŸ”¬ **Phase 4: åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ**\n",
    "1. **å¤šæ¬¡å…ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚¤ãƒ¼ãƒ—**:\n",
    "   - ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: 5æ®µéš [1K, 5K, 10K, 50K, 100K]\n",
    "   - Top-kå€¤: 5æ®µéš [1, 3, 5, 10, 20]\n",
    "   - Rerankå€¤: 4æ®µéš [5, 10, 20, 50]\n",
    "   - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: 2ç¨®é¡ [SQuAD, MS MARCO]\n",
    "\n",
    "2. **è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¸¬å®š**:\n",
    "   - â±ï¸ ã‚¯ã‚¨ãƒªå¿œç­”æ™‚é–“ï¼ˆå¹³å‡ãƒ»æ¨™æº–åå·®ï¼‰\n",
    "   - ğŸ’¾ CPU/GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆpsutil + torchï¼‰\n",
    "   - ğŸ—ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰æ™‚é–“\n",
    "   - âœ… ã‚¯ã‚¨ãƒªæˆåŠŸç‡\n",
    "\n",
    "3. **EM/F1ã‚¹ã‚³ã‚¢è©•ä¾¡**:\n",
    "   - evaluate.load(\"squad\") ä½¿ç”¨\n",
    "   - ã‚¯ã‚¨ãƒªæ¯ã® Exact Match / F1 Score\n",
    "   - ã‚·ã‚¹ãƒ†ãƒ é–“ç²¾åº¦æ¯”è¼ƒ\n",
    "\n",
    "4. **ã‚³ã‚¹ãƒˆåˆ†æ**:\n",
    "   - ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚«ã‚¦ãƒ³ãƒˆï¼ˆtiktokenï¼‰\n",
    "   - ã‚¯ã‚¨ãƒªæ¯ã‚³ã‚¹ãƒˆç®—å‡º\n",
    "   - ç·ã‚³ã‚¹ãƒˆãƒ»ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£åˆ†æ\n",
    "\n",
    "#### ğŸ“Š **Phase 5: åŒ…æ‹¬çš„å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**\n",
    "1. **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ›²ç·š**: Log-log ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæ™‚é–“ãƒ»ãƒ¡ãƒ¢ãƒªãƒ»ã‚³ã‚¹ãƒˆï¼‰\n",
    "2. **EM/F1æ¯”è¼ƒ**: ã‚·ã‚¹ãƒ†ãƒ é–“ç²¾åº¦è©•ä¾¡\n",
    "3. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—**: Top-k vs æ€§èƒ½\n",
    "4. **ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ**: å¤šæ¬¡å…ƒç·åˆæ€§èƒ½\n",
    "5. **çµ±è¨ˆã‚µãƒãƒªãƒ¼**: å¹³å‡Â±æ¨™æº–åå·®ãƒ†ãƒ¼ãƒ–ãƒ«\n",
    "\n",
    "### ğŸ“ˆ **æ¸¬å®šã•ã‚ŒãŸåŒ…æ‹¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹**\n",
    "\n",
    "**æ€§èƒ½æŒ‡æ¨™**:\n",
    "- â±ï¸ å¹³å‡ã‚¯ã‚¨ãƒªæ™‚é–“ï¼ˆmsï¼‰Â± æ¨™æº–åå·®\n",
    "- ğŸ—ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰æ™‚é–“ï¼ˆç§’ï¼‰\n",
    "- âœ… ã‚¯ã‚¨ãƒªæˆåŠŸç‡ï¼ˆ%ï¼‰\n",
    "- ğŸ¯ Exact Match ã‚¹ã‚³ã‚¢\n",
    "- ğŸ“Š F1 ã‚¹ã‚³ã‚¢\n",
    "\n",
    "**ãƒªã‚½ãƒ¼ã‚¹æŒ‡æ¨™**:\n",
    "- ğŸ’¾ RSS ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰\n",
    "- ğŸ–¥ï¸ GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰\n",
    "- ğŸ“ˆ ãƒ¡ãƒ¢ãƒªå¢—åŠ é‡ï¼ˆæ§‹ç¯‰å‰å¾Œï¼‰\n",
    "\n",
    "**ã‚³ã‚¹ãƒˆæŒ‡æ¨™**:\n",
    "- ğŸª™ ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "- ğŸ’° ç·ã‚³ã‚¹ãƒˆï¼ˆ$ï¼‰\n",
    "- ğŸ’³ ã‚¯ã‚¨ãƒªæ¯å¹³å‡ã‚³ã‚¹ãƒˆï¼ˆ$/queryï¼‰\n",
    "\n",
    "**å“è³ªæŒ‡æ¨™**:\n",
    "- ğŸ” æ¤œç´¢ã‚¹ã‚³ã‚¢ï¼ˆã‚·ã‚¹ãƒ†ãƒ ä¾å­˜ï¼‰\n",
    "- âš ï¸ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ•°ãƒ»ç‡\n",
    "- ğŸ“ ã‚µãƒ³ãƒ—ãƒ«å¿œç­”ï¼ˆå“è³ªç¢ºèªç”¨ï¼‰\n",
    "\n",
    "### ğŸš€ **æŠ€è¡“çš„å„ªä½æ€§**\n",
    "\n",
    "- **å®Œå…¨å†ç¾æ€§**: å…¨ä¹±æ•°å›ºå®šã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†\n",
    "- **å…¬å¹³æ€§ç¢ºä¿**: çµ±ä¸€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€å¯¾ç…§ç³»æ¯”è¼ƒ\n",
    "- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: 5æ¡ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºç¯„å›²è©•ä¾¡\n",
    "- **ç²¾å¯†æ¸¬å®š**: psutil/torchè©³ç´°ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–\n",
    "- **æ¨™æº–è©•ä¾¡**: SQuADå…¬å¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä½¿ç”¨\n",
    "- **å®Ÿç”¨æ€§**: ã‚³ã‚¹ãƒˆãƒ»ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æ\n",
    "- **åŒ…æ‹¬å¯è¦–åŒ–**: 11è»¸ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰\n",
    "\n",
    "### ğŸ’¡ **ç™ºè¦‹ã•ã‚ŒãŸçŸ¥è¦‹**\n",
    "\n",
    "1. **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç‰¹æ€§**: Log-logæ›²ç·šã«ã‚ˆã‚‹æ€§èƒ½åŠ£åŒ–åˆ†æ\n",
    "2. **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: ã‚·ã‚¹ãƒ†ãƒ é–“ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡æ¯”è¼ƒ\n",
    "3. **ç²¾åº¦ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**: é€Ÿåº¦ vs EM/F1 é–¢ä¿‚\n",
    "4. **ã‚³ã‚¹ãƒˆåŠ¹ç‡**: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º vs é‹ç”¨ã‚³ã‚¹ãƒˆ\n",
    "5. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ„Ÿåº¦**: Top-k/Rerankæœ€é©å€¤\n",
    "\n",
    "### ğŸ”¬ **å®Ÿé¨“ã®ä¿¡é ¼æ€§**\n",
    "\n",
    "- **ãƒ‡ãƒ¼ã‚¿é‡**: æœ€å¤§100,000ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†\n",
    "- **å®Ÿé¨“æ•°**: ç·è¨ˆæ•°ç™¾å›ã®ç³»çµ±çš„å®Ÿé¨“\n",
    "- **çµ±è¨ˆçš„æœ‰æ„æ€§**: å¹³å‡Â±æ¨™æº–åå·®ã«ã‚ˆã‚‹ä¿¡é ¼åŒºé–“\n",
    "- **ã‚¨ãƒ©ãƒ¼å‡¦ç†**: å³å¯†ä¾‹å¤–ç®¡ç†ã¨ãƒ­ã‚°è¨˜éŒ²\n",
    "- **å†ç¾æ€§**: å®Œå…¨æ±ºå®šçš„å®Ÿé¨“ç’°å¢ƒ\n",
    "\n",
    "ã“ã®åŒ…æ‹¬çš„è©•ä¾¡ã«ã‚ˆã‚Šã€InsightSpikeã®å®Ÿæ€§èƒ½ã‹ã‚‰ç ”ç©¶ãƒ¬ãƒ™ãƒ«ã®è©³ç´°åˆ†æã¾ã§ã€å­¦è¡“ãƒ»ç”£æ¥­ä¸¡ç”¨é€”ã«å¯¾å¿œã—ãŸä¿¡é ¼æ€§ã®é«˜ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a060ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ SQuAD ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŸ‹ã‚è¾¼ã¿å®Ÿé¨“\n",
    "print(\"ğŸš€ STAGE 1: SQuAD Dataset Embedding & Experiment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. SQuADåŸ‹ã‚è¾¼ã¿\n",
    "squad_embedding_stats = insight_experiment.embed_dataset('squad', max_docs=1000)\n",
    "\n",
    "# 2. SQuADå®Ÿé¨“å®Ÿè¡Œ\n",
    "squad_experiment_result = insight_experiment.run_embedded_experiment('squad', num_queries=100)\n",
    "\n",
    "# 3. ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—\n",
    "squad_backup_path = insight_experiment.backup_memory_state('squad_complete')\n",
    "\n",
    "print(\"\\nâœ… Stage 1 (SQuAD) completed:\")\n",
    "print(f\"  ğŸ“„ Embedded: {squad_embedding_stats['num_documents']} documents\")\n",
    "print(f\"  ğŸ” Tested: {squad_experiment_result['num_queries']} queries\")\n",
    "print(f\"  ğŸ¯ Success rate: {squad_experiment_result['success_rate']:.1%}\")\n",
    "print(f\"  ğŸ’¾ Backup saved to: {squad_backup_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆSQuADâ†’MS MARCOç§»è¡Œï¼‰\n",
    "print(\"\\nğŸ”„ TRANSITION: Cleaning memory for MS MARCO experiment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ\n",
    "insight_experiment.clean_memory()\n",
    "\n",
    "# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª\n",
    "current_memory = get_memory_usage()\n",
    "print(f\"\\nğŸ“Š Memory status after cleanup:\")\n",
    "print(f\"  ğŸ’¾ RAM: {current_memory['rss_mb']:.1f}MB\")\n",
    "print(f\"  ğŸ–¥ï¸ GPU: {current_memory['gpu_mb']:.1f}MB\")\n",
    "print(f\"âœ… Memory cleaned, ready for next experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85245ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ MS MARCO ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŸ‹ã‚è¾¼ã¿å®Ÿé¨“\n",
    "print(\"ğŸš€ STAGE 2: MS MARCO Dataset Embedding & Experiment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. MS MARCOåŸ‹ã‚è¾¼ã¿\n",
    "marco_embedding_stats = insight_experiment.embed_dataset('ms_marco', max_docs=1000)\n",
    "\n",
    "# 2. MS MARCOå®Ÿé¨“å®Ÿè¡Œ\n",
    "marco_experiment_result = insight_experiment.run_embedded_experiment('ms_marco', num_queries=100)\n",
    "\n",
    "# 3. ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—\n",
    "marco_backup_path = insight_experiment.backup_memory_state('ms_marco_complete')\n",
    "\n",
    "print(\"\\nâœ… Stage 2 (MS MARCO) completed:\")\n",
    "print(f\"  ğŸ“„ Embedded: {marco_embedding_stats['num_documents']} documents\")\n",
    "print(f\"  ğŸ” Tested: {marco_experiment_result['num_queries']} queries\")\n",
    "print(f\"  ğŸ¯ Success rate: {marco_experiment_result['success_rate']:.1%}\")\n",
    "print(f\"  ğŸ’¾ Backup saved to: {marco_backup_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e8f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š æ®µéšçš„å®Ÿé¨“çµæœã®ç·åˆåˆ†æ\n",
    "print(\"ğŸ¯ COMPREHENSIVE RESULTS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# å®Ÿé¨“çµæœã®æ•´ç†\n",
    "if hasattr(insight_experiment, 'experiment_results') and insight_experiment.experiment_results:\n",
    "    print(f\"\\nğŸ“ˆ Total experiments conducted: {len(insight_experiment.experiment_results)}\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ\n",
    "    for result in insight_experiment.experiment_results:\n",
    "        dataset = result['dataset']\n",
    "        success_rate = result['success_rate']\n",
    "        avg_time = result['avg_query_time'] * 1000  # ms\n",
    "        \n",
    "        print(f\"\\nğŸ” {dataset.upper()} Results:\")\n",
    "        print(f\"  âœ… Success rate: {success_rate:.1%}\")\n",
    "        print(f\"  â±ï¸ Avg query time: {avg_time:.1f}ms\")\n",
    "        print(f\"  ğŸ“Š Total queries: {result['successful_queries']}/{result['num_queries']}\")\n",
    "    \n",
    "    # æ¯”è¼ƒåˆ†æ\n",
    "    if len(insight_experiment.experiment_results) >= 2:\n",
    "        squad_result = next((r for r in insight_experiment.experiment_results if r['dataset'] == 'squad'), None)\n",
    "        marco_result = next((r for r in insight_experiment.experiment_results if r['dataset'] == 'ms_marco'), None)\n",
    "        \n",
    "        if squad_result and marco_result:\n",
    "            print(f\"\\nâš–ï¸ DATASET COMPARISON:\")\n",
    "            print(f\"  ğŸ“Š SQuAD vs MS MARCO Success Rate:\")\n",
    "            print(f\"    SQuAD: {squad_result['success_rate']:.1%}\")\n",
    "            print(f\"    MS MARCO: {marco_result['success_rate']:.1%}\")\n",
    "            print(f\"    Difference: {(marco_result['success_rate'] - squad_result['success_rate'])*100:.1f}%\")\n",
    "            \n",
    "            print(f\"  â±ï¸ SQuAD vs MS MARCO Query Time:\")\n",
    "            print(f\"    SQuAD: {squad_result['avg_query_time']*1000:.1f}ms\")\n",
    "            print(f\"    MS MARCO: {marco_result['avg_query_time']*1000:.1f}ms\")\n",
    "            time_diff = (marco_result['avg_query_time'] - squad_result['avg_query_time']) * 1000\n",
    "            print(f\"    Difference: {time_diff:.1f}ms\")\n",
    "else:\n",
    "    print(\"âŒ No experiment results found. Please run the experiments first.\")\n",
    "\n",
    "# å®Ÿé¨“çµæœã®ä¿å­˜\n",
    "try:\n",
    "    import json\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"insightspike_embedding_experiments_{timestamp}.json\"\n",
    "    \n",
    "    if hasattr(insight_experiment, 'experiment_results'):\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(insight_experiment.experiment_results, f, indent=2, default=str)\n",
    "        print(f\"\\nğŸ’¾ Results saved to: {results_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸ Save failed: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ InsightSpike embedding experiments completed!\")\n",
    "print(f\"âœ… Both SQuAD and MS MARCO datasets processed with memory management\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5e6491",
   "metadata": {},
   "source": [
    "## ğŸ¯ å®Ÿé¨“ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å®Œå…¨ç‰ˆã‚µãƒãƒªãƒ¼\n",
    "\n",
    "### âœ… å®Œäº†ã—ãŸå…¨å®Ÿé¨“å†…å®¹\n",
    "\n",
    "#### ğŸ”§ **Phase 1: åŸºæœ¬RAGã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒ**\n",
    "1. **ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: GPU/CPUç¢ºèªã€å¿…é ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "2. **InsightSpikeå‹•ä½œç¢ºèª**: åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆã€ãƒ¡ãƒ¢ãƒªãƒ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé€£æºç¢ºèª  \n",
    "3. **RAGã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…**: 4ã‚·ã‚¹ãƒ†ãƒ ï¼ˆInsightSpikeã€LangChainã€LlamaIndexã€Haystackï¼‰\n",
    "4. **ã‚·ã‚¹ãƒ†ãƒ æ¤œè¨¼**: å„ã‚·ã‚¹ãƒ†ãƒ ã®æ©Ÿèƒ½ç¢ºèªã¨ä¾‹å¤–å‡¦ç†\n",
    "5. **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™**: SQuADã€MS MARCOã€åˆæˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†\n",
    "6. **åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**: ä¸¦åˆ—GPUå‡¦ç†ã€ãƒ¡ãƒ¢ãƒªç›£è¦–ã€å¿œç­”æ™‚é–“æ¸¬å®š\n",
    "7. **å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**: 6è»¸ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã€çµ±è¨ˆåˆ†æã€çµæœä¿å­˜\n",
    "\n",
    "#### ğŸ§  **Phase 2: InsightSpikeç‰¹åŒ–ãƒ‡ãƒ¼ã‚¿åŸ‹ã‚è¾¼ã¿å®Ÿé¨“**\n",
    "1. **SQuAD ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Ÿé¨“**:\n",
    "   - 1000æ–‡æ›¸ã®åŸ‹ã‚è¾¼ã¿å‡¦ç†\n",
    "   - 100ã‚¯ã‚¨ãƒªã§ã®å®Ÿé¨“å®Ÿè¡Œ\n",
    "   - ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—\n",
    "   \n",
    "2. **ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—**:\n",
    "   - InsightSpike/ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ãƒ¢ãƒªãƒªã‚»ãƒƒãƒˆ\n",
    "   - GPU/RAMã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢\n",
    "   \n",
    "3. **MS MARCO ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Ÿé¨“**:\n",
    "   - 1000æ–‡æ›¸ã®åŸ‹ã‚è¾¼ã¿å‡¦ç†  \n",
    "   - 100ã‚¯ã‚¨ãƒªã§ã®å®Ÿé¨“å®Ÿè¡Œ\n",
    "   - ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—\n",
    "   \n",
    "4. **ç·åˆçµæœåˆ†æ**:\n",
    "   - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ\n",
    "   - æˆåŠŸç‡ãƒ»å¿œç­”æ™‚é–“ã®çµ±è¨ˆåˆ†æ\n",
    "   - å®Ÿé¨“çµæœã®æ°¸ç¶šåŒ–ä¿å­˜\n",
    "\n",
    "### ğŸ“Š **æ¸¬å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹**\n",
    "\n",
    "**åŸºæœ¬RAGæ¯”è¼ƒ**:\n",
    "- â±ï¸ å¹³å‡ã‚¯ã‚¨ãƒªæ™‚é–“\n",
    "- ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡  \n",
    "- ğŸ¯ æ¨å®šç²¾åº¦\n",
    "- âœ… æˆåŠŸç‡\n",
    "- ğŸ—ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰æ™‚é–“\n",
    "- âš¡ ç·åˆåŠ¹ç‡ã‚¹ã‚³ã‚¢\n",
    "\n",
    "**InsightSpikeç‰¹åŒ–å®Ÿé¨“**:\n",
    "- ğŸ“„ æ–‡æ›¸åŸ‹ã‚è¾¼ã¿å‡¦ç†æ™‚é–“ãƒ»ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ\n",
    "- ğŸ” ã‚¯ã‚¨ãƒªå®Ÿè¡Œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹\n",
    "- ğŸ’¾ ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ç®¡ç†ãƒ»ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ©Ÿèƒ½\n",
    "- ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å·®ç•°\n",
    "\n",
    "### ğŸš€ **æŠ€è¡“çš„ç‰¹å¾´**\n",
    "\n",
    "- **GPUä¸¦åˆ—å‡¦ç†**: CUDAæ´»ç”¨ã«ã‚ˆã‚‹é«˜é€ŸåŒ–\n",
    "- **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ **: InsightSpikeæœªä½¿ç”¨æ™‚ã®é«˜åº¦ãªä»£æ›¿å®Ÿè£…  \n",
    "- **ãƒ¡ãƒ¢ãƒªç®¡ç†**: æ®µéšçš„å®Ÿé¨“ã§ã®é©åˆ‡ãªãƒªã‚½ãƒ¼ã‚¹ç®¡ç†\n",
    "- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–**: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°\n",
    "- **åŒ…æ‹¬çš„å¯è¦–åŒ–**: matplotlibæ´»ç”¨ã®è©³ç´°ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰\n",
    "- **å†ç¾æ€§ç¢ºä¿**: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãçµæœä¿å­˜\n",
    "\n",
    "### ğŸ’¡ **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**\n",
    "\n",
    "1. **æ‹¡å¼µå®Ÿé¨“**: ã‚ˆã‚Šå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ10K+ documentsï¼‰ã§ã®æ¤œè¨¼\n",
    "2. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–**: åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ã€æ¤œç´¢ç¯„å›²ã®èª¿æ•´å®Ÿé¨“  \n",
    "3. **å¤šè¨€èªå¯¾å¿œ**: æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®è¿½åŠ è©•ä¾¡\n",
    "4. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–**: MLflowç­‰ã®å®Ÿé¨“ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±åˆ\n",
    "5. **æœ¬ç•ªç’°å¢ƒæ¤œè¨¼**: ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ\n",
    "\n",
    "ã“ã®åŒ…æ‹¬çš„ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«ã‚ˆã‚Šã€InsightSpikeã®åŸºæœ¬æ€§èƒ½ã‹ã‚‰ç‰¹åŒ–æ©Ÿèƒ½ã¾ã§ã€ä½“ç³»çš„ãªè©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29501030",
   "metadata": {},
   "source": [
    "## ğŸ§  Additional: InsightSpike Data Embedding Experiments\n",
    "\n",
    "æ®µéšçš„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŸ‹ã‚è¾¼ã¿å®Ÿé¨“ï¼š\n",
    "1. **SQuADãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŸ‹ã‚è¾¼ã¿** â†’ å®Ÿé¨“å®Ÿè¡Œ â†’ çµæœåˆ†æ\n",
    "2. **ãƒ¡ãƒ¢ãƒªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»ã‚¯ãƒªãƒ¼ãƒ³** â†’ çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ\n",
    "3. **MS MARCOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŸ‹ã‚è¾¼ã¿** â†’ å®Ÿé¨“å®Ÿè¡Œ â†’ çµæœæ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2aec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  InsightSpike Advanced Data Embedding System\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "print(\"ğŸ§  InsightSpike Advanced Data Embedding Experiments\")\n",
    "print(\"ğŸ¯ Staged dataset embedding with memory backup/restore\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class InsightSpikeEmbeddingExperiment:\n",
    "    \"\"\"InsightSpikeå°‚ç”¨åŸ‹ã‚è¾¼ã¿å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.insightspike_system = None\n",
    "        self.experiment_results = []\n",
    "        self.backup_dir = \"./insightspike_backups\"\n",
    "        self.current_dataset = None\n",
    "        \n",
    "        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "        os.makedirs(self.backup_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"ğŸ”§ Initializing InsightSpike Embedding Experiment System...\")\n",
    "        \n",
    "        # InsightSpikeåˆæœŸåŒ–\n",
    "        if 'insightspike_ready' in globals() and insightspike_ready:\n",
    "            try:\n",
    "                from insightspike.core.layers.layer2_memory_manager import L2MemoryManager as MemoryManager\n",
    "                from insightspike.core.agents.main_agent import MainAgent\n",
    "                \n",
    "                self.memory_manager = MemoryManager()\n",
    "                self.main_agent = MainAgent(memory_manager=self.memory_manager)\n",
    "                self.insightspike_available = True\n",
    "                print(\"âœ… InsightSpike system initialized\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ InsightSpike initialization failed: {e}\")\n",
    "                self.insightspike_available = False\n",
    "        else:\n",
    "            print(\"âš ï¸ InsightSpike not available - using enhanced fallback\")\n",
    "            self.insightspike_available = False\n",
    "            self._init_fallback_system()\n",
    "    \n",
    "    def _init_fallback_system(self):\n",
    "        \"\"\"é«˜åº¦ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ \"\"\"\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            import faiss\n",
    "            \n",
    "            self.embedder = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "            self.document_store = {}\n",
    "            self.embeddings_cache = {}\n",
    "            self.faiss_index = None\n",
    "            print(\"âœ… Enhanced fallback system initialized\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Fallback system initialization failed: {e}\")\n",
    "    \n",
    "    def embed_dataset(self, dataset_name: str, dataset_data: dict, max_docs: int = 1000):\n",
    "        \"\"\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŸ‹ã‚è¾¼ã¿å®Ÿè¡Œ\"\"\"\n",
    "        print(f\"\\\\nğŸ“Š Embedding {dataset_name.upper()} Dataset\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.current_dataset = dataset_name\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "        contexts = dataset_data['contexts'][:max_docs]\n",
    "        questions = dataset_data['questions'][:max_docs] \n",
    "        answers = dataset_data['answers'][:max_docs]\n",
    "        \n",
    "        print(f\"ğŸ“„ Processing {len(contexts)} documents\")\n",
    "        print(f\"â“ Processing {len(questions)} questions\")\n",
    "        \n",
    "        if self.insightspike_available:\n",
    "            # InsightSpikeå®Ÿè£…\n",
    "            print(\"ğŸ§  Using InsightSpike MemoryManager...\")\n",
    "            \n",
    "            embedded_docs = 0\n",
    "            for i, (context, question, answer) in enumerate(zip(contexts, questions, answers)):\n",
    "                try:\n",
    "                    # æ–‡æ›¸ä¿å­˜\n",
    "                    doc_id = f\"{dataset_name}_doc_{i}\"\n",
    "                    self.memory_manager.store_episode(context)\n",
    "                    \n",
    "                    # QAæƒ…å ±ã‚‚ä¿å­˜\n",
    "                    qa_info = {\n",
    "                        'question': question,\n",
    "                        'answer': answer,\n",
    "                        'context': context,\n",
    "                        'doc_id': doc_id\n",
    "                    }\n",
    "                    self.memory_manager.store_episode(json.dumps(qa_info))\n",
    "                    \n",
    "                    embedded_docs += 1\n",
    "                    \n",
    "                    if (i + 1) % 100 == 0:\n",
    "                        print(f\"  ğŸ“Š Progress: {i+1}/{len(contexts)} documents embedded\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  âš ï¸ Doc {i} embedding failed: {str(e)[:50]}...\")\n",
    "            \n",
    "            print(f\"âœ… InsightSpike embedding completed: {embedded_docs}/{len(contexts)} documents\")\n",
    "            \n",
    "        else:\n",
    "            # é«˜åº¦ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ \n",
    "            print(\"ğŸ”„ Using enhanced fallback system...\")\n",
    "            \n",
    "            # æ–‡æ›¸åŸ‹ã‚è¾¼ã¿\n",
    "            print(\"  ğŸ“¥ Computing document embeddings...\")\n",
    "            doc_embeddings = self.embedder.encode(contexts, show_progress_bar=True)\n",
    "            \n",
    "            # FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\n",
    "            print(\"  ğŸ—ï¸ Building FAISS index...\")\n",
    "            dimension = doc_embeddings.shape[1]\n",
    "            self.faiss_index = faiss.IndexFlatIP(dimension)  # Inner Product (ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦)\n",
    "            \n",
    "            # L2æ­£è¦åŒ–\n",
    "            import numpy as np\n",
    "            faiss.normalize_L2(doc_embeddings)\n",
    "            self.faiss_index.add(doc_embeddings.astype('float32'))\n",
    "            \n",
    "            # ãƒ‡ãƒ¼ã‚¿ä¿å­˜\n",
    "            for i, (context, question, answer) in enumerate(zip(contexts, questions, answers)):\n",
    "                self.document_store[f\"{dataset_name}_doc_{i}\"] = {\n",
    "                    'context': context,\n",
    "                    'question': question,\n",
    "                    'answer': answer,\n",
    "                    'embedding_id': i\n",
    "                }\n",
    "            \n",
    "            self.embeddings_cache[dataset_name] = {\n",
    "                'embeddings': doc_embeddings,\n",
    "                'index': self.faiss_index,\n",
    "                'documents': contexts\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ… Fallback embedding completed: {len(contexts)} documents indexed\")\n",
    "        \n",
    "        embedding_time = time.time() - start_time\n",
    "        \n",
    "        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š\n",
    "        memory_usage = get_memory_usage()\n",
    "        \n",
    "        embedding_stats = {\n",
    "            'dataset': dataset_name,\n",
    "            'num_documents': len(contexts),\n",
    "            'num_questions': len(questions),\n",
    "            'embedding_time': embedding_time,\n",
    "            'memory_usage': memory_usage,\n",
    "            'timestamp': datetime.now(),\n",
    "            'system_type': 'insightspike' if self.insightspike_available else 'fallback'\n",
    "        }\n",
    "        \n",
    "        print(f\"\\\\nğŸ“ˆ Embedding Statistics:\")\n",
    "        print(f\"  â±ï¸ Embedding time: {embedding_time:.1f}s\")\n",
    "        print(f\"  ğŸ’¾ Memory usage: {memory_usage['rss_mb']:.1f}MB (RSS)\")\n",
    "        print(f\"  ğŸ–¥ï¸ GPU memory: {memory_usage['gpu_mb']:.1f}MB\")\n",
    "        print(f\"  ğŸ“Š Throughput: {len(contexts)/embedding_time:.1f} docs/sec\")\n",
    "        \n",
    "        return embedding_stats\n",
    "    \n",
    "    def run_embedded_experiment(self, dataset_name: str, num_queries: int = 50):\n",
    "        \"\"\"åŸ‹ã‚è¾¼ã¿æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã§ã®å®Ÿé¨“å®Ÿè¡Œ\"\"\"\n",
    "        print(f\"\\\\nğŸ”¬ Running Experiment on Embedded {dataset_name.upper()}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if dataset_name not in benchmark_datasets:\n",
    "            print(f\"âŒ Dataset {dataset_name} not found\")\n",
    "            return None\n",
    "        \n",
    "        dataset_data = benchmark_datasets[dataset_name]\n",
    "        questions = dataset_data['questions'][:num_queries]\n",
    "        expected_answers = dataset_data['answers'][:num_queries]\n",
    "        \n",
    "        print(f\"ğŸ” Testing {len(questions)} queries...\")\n",
    "        \n",
    "        query_results = []\n",
    "        total_query_time = 0\n",
    "        successful_queries = 0\n",
    "        \n",
    "        for i, (question, expected_answer) in enumerate(zip(questions, expected_answers)):\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                if self.insightspike_available:\n",
    "                    # InsightSpike ã‚¯ã‚¨ãƒª\n",
    "                    response = self.main_agent.process_question(question)\n",
    "                    response_text = str(response)\n",
    "                else:\n",
    "                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢\n",
    "                    if dataset_name in self.embeddings_cache:\n",
    "                        # è³ªå•åŸ‹ã‚è¾¼ã¿\n",
    "                        query_embedding = self.embedder.encode([question])\n",
    "                        faiss.normalize_L2(query_embedding)\n",
    "                        \n",
    "                        # é¡ä¼¼æ¤œç´¢\n",
    "                        scores, indices = self.faiss_index.search(query_embedding.astype('float32'), k=3)\n",
    "                        \n",
    "                        # å›ç­”ç”Ÿæˆ\n",
    "                        relevant_docs = [self.embeddings_cache[dataset_name]['documents'][idx] for idx in indices[0]]\n",
    "                        response_text = f\"Based on retrieved documents: {relevant_docs[0][:200]}...\"\n",
    "                    else:\n",
    "                        response_text = \"No embedded data available\"\n",
    "                \n",
    "                query_time = time.time() - start_time\n",
    "                total_query_time += query_time\n",
    "                successful_queries += 1\n",
    "                \n",
    "                query_results.append({\n",
    "                    'question': question,\n",
    "                    'expected_answer': expected_answer,\n",
    "                    'generated_answer': response_text,\n",
    "                    'query_time': query_time,\n",
    "                    'success': True\n",
    "                })\n",
    "                \n",
    "                if (i + 1) % 10 == 0:\n",
    "                    avg_time = total_query_time / successful_queries * 1000\n",
    "                    print(f\"  ğŸ“Š Progress: {i+1}/{len(questions)} (avg: {avg_time:.1f}ms/query)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  âš ï¸ Query {i+1} failed: {str(e)[:50]}...\")\n",
    "                query_results.append({\n",
    "                    'question': question,\n",
    "                    'expected_answer': expected_answer,\n",
    "                    'generated_answer': f\"Error: {str(e)[:50]}\",\n",
    "                    'query_time': 0,\n",
    "                    'success': False\n",
    "                })\n",
    "        \n",
    "        # å®Ÿé¨“çµæœ\n",
    "        avg_query_time = total_query_time / successful_queries if successful_queries > 0 else 0\n",
    "        success_rate = successful_queries / len(questions)\n",
    "        \n",
    "        experiment_result = {\n",
    "            'dataset': dataset_name,\n",
    "            'num_queries': len(questions),\n",
    "            'successful_queries': successful_queries,\n",
    "            'success_rate': success_rate,\n",
    "            'avg_query_time': avg_query_time,\n",
    "            'total_time': total_query_time,\n",
    "            'query_results': query_results,\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "        \n",
    "        self.experiment_results.append(experiment_result)\n",
    "        \n",
    "        print(f\"\\\\nâœ… Experiment completed:\")\n",
    "        print(f\"  ğŸ¯ Success rate: {success_rate:.1%}\")\n",
    "        print(f\"  â±ï¸ Avg query time: {avg_query_time*1000:.1f}ms\")\n",
    "        print(f\"  ğŸ“Š Total queries: {successful_queries}/{len(questions)}\")\n",
    "        \n",
    "        return experiment_result\n",
    "    \n",
    "    def backup_memory_state(self, backup_name: str):\n",
    "        \"\"\"ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—\"\"\"\n",
    "        print(f\"\\\\nğŸ’¾ Backing up memory state: {backup_name}\")\n",
    "        \n",
    "        backup_path = os.path.join(self.backup_dir, f\"{backup_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "        os.makedirs(backup_path, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            if self.insightspike_available:\n",
    "                # InsightSpike ãƒ¡ãƒ¢ãƒªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—\n",
    "                # æ³¨æ„: å®Ÿéš›ã®InsightSpikeã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—APIã«ä¾å­˜\n",
    "                backup_data = {\n",
    "                    'memory_state': 'insightspike_backup_placeholder',\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'dataset': self.current_dataset\n",
    "                }\n",
    "                \n",
    "                with open(os.path.join(backup_path, 'insightspike_memory.json'), 'w') as f:\n",
    "                    json.dump(backup_data, f, indent=2)\n",
    "                \n",
    "                print(f\"  âœ… InsightSpike memory backed up to {backup_path}\")\n",
    "                \n",
    "            else:\n",
    "                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—\n",
    "                backup_data = {\n",
    "                    'document_store': self.document_store,\n",
    "                    'embeddings_cache': {k: {\n",
    "                        'documents': v['documents'],\n",
    "                        'embeddings_shape': v['embeddings'].shape\n",
    "                    } for k, v in self.embeddings_cache.items()},\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'dataset': self.current_dataset\n",
    "                }\n",
    "                \n",
    "                with open(os.path.join(backup_path, 'fallback_memory.json'), 'w') as f:\n",
    "                    json.dump(backup_data, f, indent=2, default=str)\n",
    "                \n",
    "                # åŸ‹ã‚è¾¼ã¿ã‚’å€‹åˆ¥ä¿å­˜\n",
    "                for dataset_name, cache in self.embeddings_cache.items():\n",
    "                    np.save(os.path.join(backup_path, f'{dataset_name}_embeddings.npy'), cache['embeddings'])\n",
    "                \n",
    "                print(f\"  âœ… Fallback memory backed up to {backup_path}\")\n",
    "            \n",
    "            return backup_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Backup failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def clean_memory(self):\n",
    "        \"\"\"ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³\"\"\"\n",
    "        print(f\"\\\\nğŸ§¹ Cleaning memory state...\")\n",
    "        \n",
    "        try:\n",
    "            if self.insightspike_available:\n",
    "                # InsightSpike ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³\n",
    "                # æ³¨æ„: å®Ÿéš›ã®InsightSpikeã®ã‚¯ãƒªãƒ¼ãƒ³APIã«ä¾å­˜\n",
    "                self.memory_manager = MemoryManager()  # æ–°ã—ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n",
    "                self.main_agent = MainAgent(memory_manager=self.memory_manager)\n",
    "                print(\"  âœ… InsightSpike memory cleaned\")\n",
    "                \n",
    "            else:\n",
    "                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¯ãƒªãƒ¼ãƒ³\n",
    "                self.document_store.clear()\n",
    "                self.embeddings_cache.clear()\n",
    "                if self.faiss_index:\n",
    "                    self.faiss_index.reset()\n",
    "                print(\"  âœ… Fallback memory cleaned\")\n",
    "            \n",
    "            # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"  âœ… GPU memory cleared\")\n",
    "            \n",
    "            # Python ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³\n",
    "            gc.collect()\n",
    "            print(\"  âœ… Python garbage collection completed\")\n",
    "            \n",
    "            self.current_dataset = None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Memory cleaning failed: {e}\")\n",
    "\n",
    "# InsightSpikeå®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–\n",
    "print(\"ğŸš€ Initializing InsightSpike Embedding Experiment System...\")\n",
    "insight_experiment = InsightSpikeEmbeddingExperiment()\n",
    "print(\"âœ… InsightSpike experiment system ready!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
