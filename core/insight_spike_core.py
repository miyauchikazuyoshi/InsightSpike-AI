"""
InsightSpike-AI Core Technology Implementation

This module implements the insight detection capabilities of InsightSpike-AI,
including patent-pending technologies (JP Application 2025-082988, JP Application 2025-082989).

âš ï¸ IMPLEMENTATION STATUS âš ï¸
Current core implementation is transitioning from proof-of-concept to medium-term stage
ç¾åœ¨ã®ã‚³ã‚¢å®Ÿè£…ã¯æ¦‚å¿µå®Ÿè¨¼æ®µéšã‹ã‚‰ä¸­æœŸæ®µéšã¸ã®ç§»è¡ŒæœŸã«ã‚ã‚Šã¾ã™

ğŸ”¬ GENUINE IMPLEMENTATIONS (True Implementations):
- Î”GED/Î”IG calculation algorithms: Mathematically grounded insight detection
- AdaptiveLearning: Brain science-based learning rate adjustment mechanism
- BrainInspiredArchitecture: 4-layer processing based on neuroscience principles

ğŸ“‹ ENHANCEMENT OPPORTUNITIES (Improvement Areas):
- More sophisticated state representation models
- Dynamic graph structure optimization
- Extended real-world environment validation

Key Features:
1. Î”GED (Global Exploration Difficulty) calculation
2. Î”IG (Information Gain) calculation  
3. Real-time insight detection
4. Adaptive learning mechanism
5. Brain science-based architecture

Author: Kazuyoshi Miyauchi
Date: 2025-06-04
Patent: JP Application 2025-082988, JP Application 2025-082989
"""

import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict, deque
import time

@dataclass
class InsightMoment:
    """
    æ´å¯Ÿç¬é–“ã‚’è¨˜éŒ²ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
    
    InsightSpike-AIã®æ ¸å¿ƒæŠ€è¡“ã¨ã—ã¦ã€å­¦ç¿’ä¸­ã®æˆ¦ç•¥çš„çªç ´ç‚¹ã‚’
    æ•°å­¦çš„ã«å®šé‡åŒ–ã—è¨˜éŒ²ã—ã¾ã™ã€‚
    """
    episode: int            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç•ªå·
    step: int              # ã‚¹ãƒ†ãƒƒãƒ—ç•ªå·
    dged_value: float      # Î” Global Exploration Difficulty
    dig_value: float       # Î” Information Gain
    state: Tuple[int, int] # æ´å¯Ÿç™ºç”Ÿæ™‚ã®çŠ¶æ…‹
    action: str            # å®Ÿè¡Œã•ã‚ŒãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    description: str       # æ´å¯Ÿã®èª¬æ˜

class InsightDetector:
    """
    InsightSpike-AI æ´å¯Ÿæ¤œå‡ºã‚¨ãƒ³ã‚¸ãƒ³
    
    ç‰¹è¨±æŠ€è¡“JPç‰¹é¡˜2025-082988ã€Œäººå·¥çŸ¥èƒ½ã«ãŠã‘ã‚‹æ´å¯Ÿæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ã€ã®
    ã‚³ã‚¢å®Ÿè£…ã‚¯ãƒ©ã‚¹ã€‚Î”GED/Î”IGæŒ‡æ¨™ã‚’ç”¨ã„ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ´å¯Ÿã‚’æ¤œå‡ºã€‚
    """
    
    def __init__(self, dged_threshold: float = -0.3, dig_threshold: float = 1.0):
        """
        æ´å¯Ÿæ¤œå‡ºå™¨ã‚’åˆæœŸåŒ–
        
        Args:
            dged_threshold: Î”GEDé–¾å€¤ï¼ˆæ¢ç´¢åŠ¹ç‡å¤‰åŒ–ã®æ¤œå‡ºæ„Ÿåº¦ï¼‰
            dig_threshold: Î”IGé–¾å€¤ï¼ˆæƒ…å ±ã‚²ã‚¤ãƒ³ã®æ¤œå‡ºæ„Ÿåº¦ï¼‰
        """
        self.dged_threshold = dged_threshold
        self.dig_threshold = dig_threshold
        
        # æ´å¯Ÿæ¤œå‡ºç”¨ãƒ‡ãƒ¼ã‚¿è“„ç©
        self.exploration_history = []
        self.reward_history = []
        self.state_visit_count = defaultdict(int)
        self.insight_moments = []
        
    def calculate_dged(self, state: Tuple[int, int], action: int) -> float:
        """
        Î” Global Exploration Difficulty (Î”GED) è¨ˆç®—
        
        æ¢ç´¢åŠ¹ç‡ã®æ§‹é€ çš„å¤‰åŒ–ã‚’å®šé‡åŒ–ã™ã‚‹ç‰¹è¨±æŠ€è¡“ã€‚
        è² ã®å€¤ã¯æ¢ç´¢å›°é›£åº¦ã®å¢—åŠ ï¼ˆæˆ¦ç•¥è»¢æ›ç‚¹ï¼‰ã‚’ç¤ºå”†ã€‚
        
        æ•°å¼:
        Î”GED = æœ€æ–°æ¢ç´¢åŠ¹ç‡ - ç¾åœ¨æ¢ç´¢åŠ¹ç‡
        æ¢ç´¢åŠ¹ç‡ = ãƒ¦ãƒ‹ãƒ¼ã‚¯çŠ¶æ…‹æ•° / ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°
        
        Args:
            state: ç¾åœ¨ã®çŠ¶æ…‹
            action: å®Ÿè¡Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            
        Returns:
            Î”GEDå€¤ï¼ˆ-1.0 ~ 1.0ã®ç¯„å›²ï¼‰
        """
        if len(self.exploration_history) < 5:
            return 0.0
            
        # ç¾åœ¨ã®æ¢ç´¢åŠ¹ç‡
        unique_states = len(set(self.exploration_history))
        total_steps = len(self.exploration_history)
        current_efficiency = unique_states / total_steps if total_steps > 0 else 0
        
        # æœ€è¿‘ã®æ¢ç´¢åŠ¹ç‡ï¼ˆç›´è¿‘10ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
        recent_history = self.exploration_history[-10:]
        if len(recent_history) > 3:
            recent_unique = len(set(recent_history))
            recent_efficiency = recent_unique / len(recent_history)
        else:
            recent_efficiency = current_efficiency
            
        # Î”GED = åŠ¹ç‡å¤‰åŒ–
        dged = recent_efficiency - current_efficiency
        return np.clip(dged, -1.0, 1.0)
    
    def calculate_dig(self, state: Tuple[int, int], reward: float) -> float:
        """
        Î” Information Gain (Î”IG) è¨ˆç®—
        
        çŠ¶æ…‹ã®æ–°è¦æ€§ã¨å ±é…¬ã«åŸºã¥ãæƒ…å ±ç²å¾—é‡ã‚’å®šé‡åŒ–ã™ã‚‹ç‰¹è¨±æŠ€è¡“ã€‚
        é«˜ã„å€¤ã¯é‡è¦ãªå­¦ç¿’æ©Ÿä¼šã‚’ç¤ºå”†ã€‚
        
        æ•°å¼:
        Î”IG = åŸºæœ¬ã‚²ã‚¤ãƒ³ Ã— å ±é…¬ä¿‚æ•° Ã— ãƒˆãƒ¬ãƒ³ãƒ‰èª¿æ•´
        
        åŸºæœ¬ã‚²ã‚¤ãƒ³ = f(è¨ªå•å›æ•°)  # æ–°è¦çŠ¶æ…‹ã»ã©é«˜ã„
        å ±é…¬ä¿‚æ•° = g(å ±é…¬å€¤)     # é«˜å ±é…¬ã»ã©é«˜ã„
        ãƒˆãƒ¬ãƒ³ãƒ‰èª¿æ•´ = h(å ±é…¬å±¥æ­´) # æ”¹å–„å‚¾å‘ã§å¢—åŠ 
        
        Args:
            state: ç¾åœ¨ã®çŠ¶æ…‹
            reward: ç²å¾—å ±é…¬
            
        Returns:
            Î”IGå€¤ï¼ˆ0.0ä»¥ä¸Šï¼‰
        """
        visit_count = self.state_visit_count[state]
        
        # åŸºæœ¬æƒ…å ±ã‚²ã‚¤ãƒ³ï¼ˆæ–°è¦æ€§ãƒ™ãƒ¼ã‚¹ï¼‰
        if visit_count == 0:
            base_gain = 3.0      # æ–°è¦çŠ¶æ…‹
        elif visit_count == 1:
            base_gain = 1.5      # 2å›ç›®è¨ªå•
        elif visit_count < 5:
            base_gain = 0.5      # å°‘æ•°è¨ªå•
        else:
            base_gain = 0.1      # é »ç¹è¨ªå•
            
        # å ±é…¬ä¿‚æ•°
        if reward > 50:          # ã‚´ãƒ¼ãƒ«é”æˆç´š
            reward_multiplier = 2.0
        elif reward > 0:         # ãƒã‚¸ãƒ†ã‚£ãƒ–å ±é…¬
            reward_multiplier = 1.5
        elif reward > -0.5:      # è»½å¾®ãƒšãƒŠãƒ«ãƒ†ã‚£
            reward_multiplier = 1.0
        else:                    # é‡å¤§ãƒšãƒŠãƒ«ãƒ†ã‚£
            reward_multiplier = 0.3
            
        dig = base_gain * reward_multiplier
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰èª¿æ•´ï¼ˆæœ€è¿‘ã®å ±é…¬æ”¹å–„ï¼‰
        if len(self.reward_history) > 5:
            recent_avg = np.mean(self.reward_history[-5:])
            if reward > recent_avg + 1.0:  # é¡•è‘—ãªæ”¹å–„
                dig *= 1.5
                
        return max(0.0, dig)
    
    def detect_insight(self, state: Tuple[int, int], action: int, reward: float,
                      episode: int, step: int) -> Optional[InsightMoment]:
        """
        ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡º
        
        Î”GED/Î”IGæŒ‡æ¨™ã‚’çµ±åˆã—ã¦æˆ¦ç•¥çš„æ´å¯Ÿç¬é–“ã‚’æ¤œå‡ºã™ã‚‹
        ç‰¹è¨±æŠ€è¡“JPç‰¹é¡˜2025-082988ã®æ ¸å¿ƒã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚
        
        æ´å¯Ÿæ¡ä»¶:
        1. Primary: Î”GED < threshold AND Î”IG > threshold
        2. Secondary: Major reward (>50)
        3. Tertiary: High information gain (>2.0) + new state
        
        Args:
            state: ç¾åœ¨çŠ¶æ…‹
            action: å®Ÿè¡Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³  
            reward: ç²å¾—å ±é…¬
            episode: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç•ªå·
            step: ã‚¹ãƒ†ãƒƒãƒ—ç•ªå·
            
        Returns:
            InsightMoment or None
        """
        dged = self.calculate_dged(state, action)
        dig = self.calculate_dig(state, reward)
        
        insight_detected = False
        description = ""
        
        # ä¸»è¦æ´å¯Ÿæ¡ä»¶: æ¢ç´¢åŠ¹ç‡ä½ä¸‹ + é«˜æƒ…å ±ã‚²ã‚¤ãƒ³
        if dged < self.dged_threshold and dig > self.dig_threshold:
            insight_detected = True
            description = f"Strategic Insight: Exploration efficiency change={dged:.3f}, Info gain={dig:.3f}"
        
        # å‰¯æ¬¡æ´å¯Ÿæ¡ä»¶: ã‚´ãƒ¼ãƒ«ç™ºè¦‹
        elif reward > 50:
            insight_detected = True
            description = f"Goal Discovery Insight: Major reward={reward:.1f}, Info gain={dig:.3f}"
        
        # ç¬¬ä¸‰æ´å¯Ÿæ¡ä»¶: ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
        elif dig > 2.0 and self.state_visit_count[state] == 0:
            insight_detected = True
            description = f"Exploration Insight: New valuable area discovered, Info gain={dig:.3f}"
        
        if insight_detected:
            insight = InsightMoment(
                episode=episode,
                step=step,
                dged_value=dged,
                dig_value=dig,
                state=state,
                action=['â†‘', 'â†’', 'â†“', 'â†'][action],
                description=description
            )
            self.insight_moments.append(insight)
            return insight
            
        return None
    
    def update_history(self, state: Tuple[int, int], reward: float):
        """å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°"""
        self.exploration_history.append(state)
        self.reward_history.append(reward)
        self.state_visit_count[state] += 1

class AdaptiveLearning:
    """
    é©å¿œçš„å­¦ç¿’æ©Ÿæ§‹
    
    ç‰¹è¨±æŠ€è¡“JPç‰¹é¡˜2025-082989ã€Œè„³ç§‘å­¦ãƒ™ãƒ¼ã‚¹é©å¿œå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ã®
    å®Ÿè£…ã€‚æ´å¯Ÿæ¤œå‡ºã«åŸºã¥ãå­¦ç¿’ç‡ã¨Îµ-greedyæˆ¦ç•¥ã®å‹•çš„èª¿æ•´ã€‚
    """
    
    def __init__(self, base_lr: float = 0.15, base_epsilon: float = 0.4):
        """
        é©å¿œå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        
        Args:
            base_lr: åŸºæœ¬å­¦ç¿’ç‡
            base_epsilon: åŸºæœ¬æ¢ç´¢ç‡
        """
        self.base_lr = base_lr
        self.base_epsilon = base_epsilon
        self.insight_bonus = 0.0
        self.steps_since_insight = 0
        
    def get_learning_rate(self, recent_insights: int) -> float:
        """
        æ´å¯Ÿãƒ™ãƒ¼ã‚¹å­¦ç¿’ç‡è¨ˆç®—
        
        æ´å¯Ÿæ¤œå‡ºå¾Œã¯å­¦ç¿’ç‡ã‚’ä¸€æ™‚çš„ã«å¢—åŠ ã•ã›ã€
        é‡è¦ãªç™ºè¦‹ã‚’è¿…é€Ÿã«å­¦ç¿’ã«åæ˜ ã€‚
        
        Returns:
            èª¿æ•´æ¸ˆã¿å­¦ç¿’ç‡
        """
        if self.steps_since_insight < 10:  # æ´å¯Ÿå¾Œ10ã‚¹ãƒ†ãƒƒãƒ—
            return self.base_lr * 1.5
        return self.base_lr
    
    def get_epsilon(self, total_insights: int) -> float:
        """
        æ´å¯Ÿãƒ™ãƒ¼ã‚¹æ¢ç´¢ç‡è¨ˆç®—
        
        æ´å¯Ÿè“„ç©ã«ã‚ˆã‚Šæ¢ç´¢ç‡ã‚’å‹•çš„ã«æ¸›å°‘ã€‚
        å­¦ç¿’ãŒé€²ã‚€ã«ã¤ã‚Œã¦æˆ¦ç•¥çš„è¡Œå‹•ã‚’é‡è¦–ã€‚
        
        Returns:
            èª¿æ•´æ¸ˆã¿æ¢ç´¢ç‡
        """
        adaptive_epsilon = max(0.05, self.base_epsilon - self.insight_bonus)
        return adaptive_epsilon
    
    def update_after_insight(self):
        """æ´å¯Ÿæ¤œå‡ºå¾Œã®æ›´æ–°"""
        self.insight_bonus += 0.02
        self.steps_since_insight = 0
    
    def step(self):
        """ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®æ›´æ–°"""
        self.steps_since_insight += 1

class BrainInspiredArchitecture:
    """
    è„³ç§‘å­¦ãƒ™ãƒ¼ã‚¹4å±¤ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
    
    äººé–“ã®è„³æ§‹é€ ã‚’æ¨¡å€£ã—ãŸæƒ…å ±å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã€‚
    å„å±¤ãŒç‰¹å®šã®èªçŸ¥æ©Ÿèƒ½ã‚’æ‹…å½“ã—ã€çµ±åˆçš„ãªæ„æ€æ±ºå®šã‚’å®Ÿç¾ã€‚
    """
    
    def __init__(self):
        """ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆæœŸåŒ–"""
        # å°è„³å±¤: åŸºæœ¬è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.cerebellum = {"motor_patterns": defaultdict(float)}
        
        # LC+æµ·é¦¬å±¤: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ãƒ»æƒ…å ±çµ±åˆ
        self.lc_hippocampus = {
            "episodic_memory": [],
            "working_memory": deque(maxlen=10)
        }
        
        # å‰é ­å‰é‡å±¤: æˆ¦ç•¥çš„æ„æ€æ±ºå®š
        self.prefrontal_cortex = {
            "strategies": defaultdict(float),
            "goal_tracking": {}
        }
        
        # è¨€èªé‡å±¤: æ´å¯Ÿã®è¨€èªåŒ–
        self.language_areas = {
            "insight_descriptions": [],
            "explanation_templates": {}
        }
    
    def process_insight(self, insight: InsightMoment) -> Dict:
        """
        æ´å¯Ÿã®çµ±åˆå‡¦ç†
        
        4å±¤ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§æ´å¯Ÿã‚’å¤šè§’çš„ã«å‡¦ç†ã—ã€
        åŒ…æ‹¬çš„ãªç†è§£ã¨å¿œç”¨ã‚’å®Ÿç¾ã€‚
        
        Args:
            insight: æ¤œå‡ºã•ã‚ŒãŸæ´å¯Ÿ
            
        Returns:
            å‡¦ç†çµæœè¾æ›¸
        """
        # å°è„³å±¤: ãƒ¢ãƒ¼ã‚¿ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³æ›´æ–°
        motor_pattern = f"{insight.state}_{insight.action}"
        self.cerebellum["motor_patterns"][motor_pattern] += insight.dig_value
        
        # LC+æµ·é¦¬å±¤: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ä¿å­˜
        episode_record = {
            "insight": insight,
            "timestamp": time.time(),
            "context": f"Episode {insight.episode}"
        }
        self.lc_hippocampus["episodic_memory"].append(episode_record)
        
        # å‰é ­å‰é‡å±¤: æˆ¦ç•¥æ›´æ–°
        strategy_key = f"state_type_{insight.state[0]//3}_{insight.state[1]//3}"
        self.prefrontal_cortex["strategies"][strategy_key] += 1
        
        # è¨€èªé‡å±¤: èª¬æ˜ç”Ÿæˆ
        explanation = self._generate_explanation(insight)
        self.language_areas["insight_descriptions"].append(explanation)
        
        return {
            "motor_activation": self.cerebellum["motor_patterns"][motor_pattern],
            "memory_strength": len(self.lc_hippocampus["episodic_memory"]),
            "strategy_confidence": self.prefrontal_cortex["strategies"][strategy_key],
            "explanation": explanation
        }
    
    def _generate_explanation(self, insight: InsightMoment) -> str:
        """æ´å¯Ÿã®è¨€èªçš„èª¬æ˜ç”Ÿæˆ"""
        templates = {
            "strategic": "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰{episode}ã®ã‚¹ãƒ†ãƒƒãƒ—{step}ã§æˆ¦ç•¥çš„çªç ´ã‚’æ¤œå‡ºã€‚æ¢ç´¢åŠ¹ç‡å¤‰åŒ–{dged:.3f}ã€æƒ…å ±ã‚²ã‚¤ãƒ³{dig:.3f}ã«ã‚ˆã‚Šæ–°ãŸãªå­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç™ºè¦‹ã€‚",
            "goal": "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰{episode}ã§ã‚´ãƒ¼ãƒ«ç™ºè¦‹æ´å¯Ÿã‚’æ¤œå‡ºã€‚é«˜å ±é…¬{reward}ç²å¾—ã«ã‚ˆã‚Šé‡è¦ãªæˆ¦ç•¥çš„çŸ¥è­˜ã‚’ç²å¾—ã€‚",
            "exploration": "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰{episode}ã§æ¢ç´¢æ´å¯Ÿã‚’æ¤œå‡ºã€‚æ–°è¦æœ‰ç”¨é ˜åŸŸã®ç™ºè¦‹ã«ã‚ˆã‚Šæ¢ç´¢æˆ¦ç•¥ã‚’æ›´æ–°ã€‚"
        }
        
        if "Strategic" in insight.description:
            return templates["strategic"].format(
                episode=insight.episode, step=insight.step,
                dged=insight.dged_value, dig=insight.dig_value
            )
        elif "Goal" in insight.description:
            return templates["goal"].format(
                episode=insight.episode, reward=insight.dig_value*50
            )
        else:
            return templates["exploration"].format(episode=insight.episode)

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰
if __name__ == "__main__":
    print("InsightSpike-AI Core Technology Test")
    print("=" * 50)
    
    # æ´å¯Ÿæ¤œå‡ºå™¨åˆæœŸåŒ–
    detector = InsightDetector()
    learner = AdaptiveLearning()
    brain = BrainInspiredArchitecture()
    
    # ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
    state = (5, 3)
    action = 1
    reward = 100.0
    episode = 10
    step = 150
    
    # å±¥æ­´æ›´æ–°
    detector.update_history(state, reward)
    
    # æ´å¯Ÿæ¤œå‡º
    insight = detector.detect_insight(state, action, reward, episode, step)
    
    if insight:
        print(f"ğŸ§  æ´å¯Ÿæ¤œå‡ºæˆåŠŸ!")
        print(f"Î”GED: {insight.dged_value:.3f}")
        print(f"Î”IG: {insight.dig_value:.3f}")
        print(f"èª¬æ˜: {insight.description}")
        
        # è„³å‡¦ç†
        brain_response = brain.process_insight(insight)
        print(f"è„³å‡¦ç†çµæœ: {brain_response}")
        
        # é©å¿œå­¦ç¿’æ›´æ–°
        learner.update_after_insight()
        print(f"èª¿æ•´å¾Œå­¦ç¿’ç‡: {learner.get_learning_rate(1):.3f}")
        print(f"èª¿æ•´å¾Œæ¢ç´¢ç‡: {learner.get_epsilon(1):.3f}")
    else:
        print("æ´å¯Ÿæœªæ¤œå‡º")
    
    print("\nğŸ‰ InsightSpike-AI ã‚³ã‚¢æŠ€è¡“ãƒ†ã‚¹ãƒˆå®Œäº†!")
