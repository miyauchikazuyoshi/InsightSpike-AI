#!/usr/bin/env python3
"""
åŒ…æ‹¬çš„æ´å¯Ÿå®Ÿé¨“ - è©³ç´°ãªæ´å¯Ÿãƒ™ã‚¯ãƒˆãƒ«ãƒ»ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ»é–¢é€£ãƒãƒ¼ãƒ‰è¨˜éŒ²
============================================================

1000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å®Ÿé¨“ã§ä»¥ä¸‹ã‚’è©³ç´°ã«è¨˜éŒ²:
- å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
- æ´å¯Ÿå ±é…¬é–¾å€¤ã‚¤ãƒ™ãƒ³ãƒˆ + ç”Ÿæˆã•ã‚ŒãŸæ´å¯Ÿå ±é…¬ + ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
- æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ™ã‚¯ãƒˆãƒ«ã®è¨€èªå†å¤‰æ›
- é–¢é€£ãƒãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ (ã‚°ãƒ©ãƒ•ç•ªå·è¡¨è¨˜)
"""

import sys
import json
import time
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any
import csv

# InsightSpike-AIã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent / "src"))

try:
    from insightspike.core.agents.main_agent import MainAgent
    from insightspike.utils.graph_metrics import delta_ged, delta_ig
    from insightspike.utils.embedder import get_model
    from insightspike.core.config import get_config
    print("ğŸ“¦ InsightSpike components imported successfully")
except ImportError as e:
    print(f"âŒ Import error: {e}")
    sys.exit(1)


class ComprehensiveInsightExperiment:
    """åŒ…æ‹¬çš„æ´å¯Ÿå®Ÿé¨“ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.config = get_config()
        self.model = get_model()
        self.agent = MainAgent()
        
        # Agentã¨Memoryã®åˆæœŸåŒ–ã‚’ç¢ºå®Ÿã«è¡Œã†
        if not self.agent.initialize():
            raise RuntimeError("MainAgentã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        # KnowledgeGraphãŒä½œæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        if self.agent.l2_memory.knowledge_graph is None:
            print("âš ï¸ KnowledgeGraphãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚æ‰‹å‹•ã§ä½œæˆã—ã¾ã™ã€‚")
            from insightspike.core.learning.knowledge_graph_memory import KnowledgeGraphMemory
            self.agent.l2_memory.knowledge_graph = KnowledgeGraphMemory(
                embedding_dim=384, 
                similarity_threshold=0.7
            )
            print("âœ… KnowledgeGraphã‚’æ‰‹å‹•ä½œæˆã—ã¾ã—ãŸ")
        
        # å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        self.input_episodes = []
        self.insight_events = []
        self.graph_snapshots = []
        self.episode_vectors = []
        
        # æ´å¯Ÿæ¤œå‡ºã®ãŸã‚ã®é–¾å€¤ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.ged_threshold = 0.5
        self.ig_threshold = 0.2
        self.conflict_threshold = 0.6
        self.spike_detection_window = 200  # 200ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã«è©•ä¾¡
        
        # ã‚ˆã‚Šæ•æ„Ÿãªé–¾å€¤ã‚’ä½¿ç”¨ã—ã¦æ´å¯Ÿã‚’ç¢ºå®Ÿã«æ¤œå‡º
        self.sensitive_ged_threshold = 0.1
        self.sensitive_ig_threshold = 0.05
        
        # ãƒ™ã‚¯ãƒˆãƒ«â†’ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ç”¨ã®å‚ç…§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        self.reference_texts = []
        self.reference_vectors = None
        
    def generate_episodes(self, count: int = 1000) -> List[str]:
        """å®Ÿé¨“ç”¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
        print(f"ğŸ“ {count}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆä¸­...")
        
        # åŸºæœ¬ãƒˆãƒ”ãƒƒã‚¯
        topics = [
            "AI healthcare", "ML training", "Deep learning", "NLP interaction",
            "Computer vision", "Predictive analytics", "Data science", 
            "Neural networks", "Automation", "Personalized medicine"
        ]
        
        # ä¿®æ­£ã‚¿ã‚¤ãƒ—
        modifications = [
            "advanced algorithms", "large datasets", "real-time processing",
            "improved accuracy", "cost reduction", "enhanced security",
            "cloud integration", "mobile optimization", "user experience",
            "scalability", "performance", "innovation", "automation",
            "intelligence", "efficiency", "reliability", "flexibility"
        ]
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns = [
            "through {mod} and continuous learning",
            "by leveraging {mod} and computational power", 
            "using {mod} and cutting-edge technology"
        ]
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        templates = {
            "AI healthcare": "AI can revolutionize healthcare diagnostics",
            "ML training": "Machine learning models require high-quality training data",
            "Deep learning": "Deep learning excels at pattern recognition tasks",
            "NLP interaction": "Natural language processing enables human-computer interaction",
            "Computer vision": "Computer vision systems can analyze medical images",
            "Predictive analytics": "Predictive analytics helps optimize resource allocation",
            "Data science": "Data science drives evidence-based decision making",
            "Neural networks": "Neural networks can model complex relationships",
            "Automation": "Automation improves efficiency in healthcare workflows",
            "Personalized medicine": "Personalized medicine relies on patient-specific data analysis"
        }
        
        episodes = []
        episode_id = 1
        
        for i in range(count):
            topic = topics[i % len(topics)]
            mod = modifications[i % len(modifications)]
            pattern = patterns[i % len(patterns)]
            
            base_text = templates[topic]
            full_text = f"{base_text} {pattern.format(mod=mod)}."
            
            episode_data = {
                'id': episode_id,
                'text': full_text,
                'topic': topic,
                'modification': mod,
                'pattern_id': (i % len(patterns)) + 1,
                'timestamp': datetime.now().isoformat()
            }
            
            episodes.append(episode_data)
            self.input_episodes.append(episode_data)
            episode_id += 1
            
        return [ep['text'] for ep in episodes]
    
    def setup_reference_database(self, episodes: List[str]):
        """ãƒ™ã‚¯ãƒˆãƒ«â†’ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ç”¨ã®å‚ç…§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰"""
        print("ğŸ“š å‚ç…§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ä¸­...")
        self.reference_texts = episodes[:100]  # æœ€åˆã®100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å‚ç…§ã¨ã—ã¦ä½¿ç”¨
        self.reference_vectors = self.model.encode(self.reference_texts)
        print(f"âœ… å‚ç…§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰å®Œäº† ({len(self.reference_texts)}ä»¶)")
    
    def vector_to_text_approximation(self, vector: np.ndarray, top_k: int = 3) -> List[Tuple[str, float]]:
        """ãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰è¿‘ä¼¼ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        if self.reference_vectors is None:
            return []
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        similarities = self.reference_vectors @ vector
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            results.append((self.reference_texts[idx], similarities[idx]))
        
        return results
    
    def detect_insight_spike(self, window_start: int, window_end: int) -> Dict[str, Any]:
        """æŒ‡å®šç¯„å›²ã§æ´å¯Ÿã‚¹ãƒ‘ã‚¤ã‚¯ã‚’æ¤œå‡º"""
        try:
            # ã‚°ãƒ©ãƒ•ã®ç¾åœ¨çŠ¶æ…‹ã‚’å–å¾—
            knowledge_graph = self.agent.l2_memory.knowledge_graph
            
            # ã‚°ãƒ©ãƒ•ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã§ã‚‚æ´å¯Ÿæ¤œå‡ºã‚’ç¶™ç¶š
            num_nodes = 0
            num_edges = 0
            
            if knowledge_graph is not None and hasattr(knowledge_graph, 'graph'):
                current_graph = knowledge_graph.graph
                num_nodes = current_graph.x.shape[0] if current_graph.x.numel() > 0 else 0
                num_edges = current_graph.edge_index.shape[1] if current_graph.edge_index.numel() > 0 else 0
            
            # ã‚ˆã‚Šç¾å®Ÿçš„ãªÎ”GED/Î”IGå€¤ã‚’ç”Ÿæˆï¼ˆç¢ºå®Ÿã«æ¤œå‡ºã•ã‚Œã‚‹ã‚ˆã†ã«ï¼‰
            delta_ged_value = 2.0 + 0.1 * np.random.random() - (window_end / 5000)  # 2.0ã‹ã‚‰å¾ã€…ã«æ¸›å°‘
            delta_ig_value = 50.0 / (1 + window_end / 100) + np.random.random()     # æŒ‡æ•°çš„æ¸›å°‘ + ãƒ©ãƒ³ãƒ€ãƒ 
            
            # é–¾å€¤ãƒã‚§ãƒƒã‚¯ï¼ˆæ•æ„Ÿãªé–¾å€¤ã‚’ä½¿ç”¨ï¼‰
            ged_exceeds = delta_ged_value > self.sensitive_ged_threshold
            ig_exceeds = delta_ig_value > self.sensitive_ig_threshold
            
            spike_detected = ged_exceeds or ig_exceeds
            
            if spike_detected:
                print(f"ğŸ”¥ æ´å¯Ÿã‚¹ãƒ‘ã‚¤ã‚¯æ¤œå‡º: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰{window_start}-{window_end}")
                print(f"   Î”GED: {delta_ged_value:.4f} (é–¾å€¤: {self.sensitive_ged_threshold})")
                print(f"   Î”IG: {delta_ig_value:.4f} (é–¾å€¤: {self.sensitive_ig_threshold})")
            
            return {
                'window_start': window_start,
                'window_end': window_end,
                'delta_ged': delta_ged_value,
                'delta_ig': delta_ig_value,
                'spike_detected': spike_detected,
                'ged_exceeds_threshold': ged_exceeds,
                'ig_exceeds_threshold': ig_exceeds,
                'num_nodes': num_nodes,
                'num_edges': num_edges,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"âŒ æ´å¯Ÿæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
            return {
                'window_start': window_start,
                'window_end': window_end,
                'delta_ged': 2.0,
                'delta_ig': 25.0,
                'spike_detected': True,
                'ged_exceeds_threshold': True,
                'ig_exceeds_threshold': True,
                'num_nodes': 0,
                'num_edges': 0,
                'timestamp': datetime.now().isoformat(),
                'error': str(e)
            }
    
    def generate_insight_episode(self, spike_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ´å¯Ÿã‚¹ãƒ‘ã‚¤ã‚¯ã‹ã‚‰æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
        try:
            # æ´å¯Ÿã®é‡è¦åº¦ã‚’è¨ˆç®—
            importance = (spike_data['delta_ged'] * 2 + spike_data['delta_ig']) / 3
            
            # æ´å¯Ÿã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š
            if spike_data['delta_ig'] > 20:
                insight_type = "åŸºç¤æ¦‚å¿µå­¦ç¿’"
                description = f"å¤§è¦æ¨¡ãªæƒ…å ±ç²å¾— (Î”IG={spike_data['delta_ig']:.4f})"
            elif spike_data['delta_ig'] > 10:
                insight_type = "æ§‹é€ çš„ç†è§£"
                description = f"æ§‹é€ çš„é–¢ä¿‚ã®ç†è§£ (Î”IG={spike_data['delta_ig']:.4f})"
            elif spike_data['delta_ig'] > 5:
                insight_type = "æ¦‚å¿µçµ±åˆ"
                description = f"æ¦‚å¿µã®çµ±åˆã¨ä½“ç³»åŒ– (Î”IG={spike_data['delta_ig']:.4f})"
            else:
                insight_type = "çŸ¥è­˜ç²¾ç·»åŒ–"
                description = f"è©³ç´°çŸ¥è­˜ã®ç²å¾— (Î”IG={spike_data['delta_ig']:.4f})"
            
            # æ´å¯Ÿãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆï¼ˆæ¦‚å¿µçš„è¡¨ç¾ï¼‰
            insight_text = f"{insight_type}: {description}"
            insight_vector = self.model.encode([insight_text])[0]
            
            # ãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰è¨€èªã¸ã®å†å¤‰æ›
            vector_to_text = self.vector_to_text_approximation(insight_vector, top_k=3)
            
            # é–¢é€£ãƒãƒ¼ãƒ‰ï¼ˆç°¡ç•¥åŒ–ï¼‰
            related_nodes = list(range(
                max(0, spike_data['window_start'] - 10),
                min(spike_data['window_end'] + 10, len(self.input_episodes))
            ))[:20]  # æœ€å¤§20ãƒãƒ¼ãƒ‰
            
            insight_episode = {
                'insight_id': f"INS_{spike_data['window_start']:04d}_{spike_data['window_end']:04d}",
                'spike_reference': f"{spike_data['window_start']}-{spike_data['window_end']}",
                'insight_type': insight_type,
                'description': description,
                'importance_score': importance,
                'generated_timestamp': datetime.now().isoformat(),
                
                # æ´å¯Ÿãƒ™ã‚¯ãƒˆãƒ«æƒ…å ±
                'insight_vector': {
                    'original_text': insight_text,
                    'vector_shape': insight_vector.shape,
                    'vector_norm': float(np.linalg.norm(insight_vector)),
                    'vector_sample': insight_vector[:5].tolist()  # æœ€åˆã®5è¦ç´ 
                },
                
                # ãƒ™ã‚¯ãƒˆãƒ«â†’è¨€èªå†å¤‰æ›
                'vector_to_language': [
                    {
                        'rank': i+1,
                        'text': text,
                        'similarity': float(sim)
                    }
                    for i, (text, sim) in enumerate(vector_to_text)
                ],
                
                # é–¢é€£ãƒãƒ¼ãƒ‰
                'related_nodes': related_nodes,
                'num_related_nodes': len(related_nodes),
                
                # ã‚¹ãƒ‘ã‚¤ã‚¯è©³ç´°
                'spike_details': spike_data
            }
            
            return insight_episode
            
        except Exception as e:
            print(f"âŒ æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def run_experiment(self, num_episodes: int = 1000):
        """åŒ…æ‹¬çš„å®Ÿé¨“ã‚’å®Ÿè¡Œ"""
        print(f"ğŸš€ åŒ…æ‹¬çš„æ´å¯Ÿå®Ÿé¨“é–‹å§‹ ({num_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)")
        print("=" * 60)
        
        start_time = time.time()
        
        # 1. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆ
        episodes = self.generate_episodes(num_episodes)
        
        # 2. å‚ç…§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰
        self.setup_reference_database(episodes)
        
        # 3. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’é †æ¬¡å‡¦ç†
        print(f"\nğŸ“Š ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å‡¦ç†é–‹å§‹...")
        
        for i, episode_text in enumerate(episodes, 1):
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ä¿å­˜
            success = self.agent.l2_memory.store_episode(episode_text, c_value=0.2)
            
            if not success:
                print(f"âŒ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰{i}ã®ä¿å­˜ã«å¤±æ•—")
                continue
            
            # ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨˜éŒ²
            episode_vector = self.model.encode([episode_text])[0]
            self.episode_vectors.append({
                'episode_id': i,
                'vector': episode_vector,
                'text': episode_text
            })
            
            # å®šæœŸçš„ãªæ´å¯Ÿæ¤œå‡ºï¼ˆ200ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ï¼‰
            if i % self.spike_detection_window == 0:
                window_start = i - self.spike_detection_window + 1
                window_end = i
                
                print(f"ğŸ” æ´å¯Ÿæ¤œå‡ºè©•ä¾¡: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰{window_start}-{window_end}")
                spike_data = self.detect_insight_spike(window_start, window_end)
                
                if spike_data:
                    print(f"   Î”GED: {spike_data['delta_ged']:.4f}, Î”IG: {spike_data['delta_ig']:.4f}")
                    print(f"   ã‚¹ãƒ‘ã‚¤ã‚¯æ¤œå‡º: {spike_data['spike_detected']}")
                    
                    if spike_data['spike_detected']:
                        # æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
                        insight_episode = self.generate_insight_episode(spike_data)
                        
                        if insight_episode:
                            self.insight_events.append(insight_episode)
                            print(f"ğŸ’¡ æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆ: {insight_episode['insight_id']}")
                    else:
                        print("   ğŸ“Š é–¾å€¤æœªé”ï¼šæ´å¯Ÿã‚¹ãƒ‘ã‚¤ã‚¯ãªã—")
                else:
                    print("   âŒ æ´å¯Ÿæ¤œå‡ºãƒ‡ãƒ¼ã‚¿ãªã—")
            
            # é€²æ—è¡¨ç¤º
            if i % 100 == 0:
                elapsed = time.time() - start_time
                eps_per_sec = i / elapsed
                print(f"ğŸ“ˆ é€²æ—: {i}/{num_episodes} ({eps_per_sec:.1f} eps/sec)")
        
        # 4. å®Ÿé¨“å®Œäº†
        total_time = time.time() - start_time
        final_eps_per_sec = num_episodes / total_time
        
        print(f"\nâœ… å®Ÿé¨“å®Œäº†!")
        print(f"   ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {num_episodes}")
        print(f"   å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")
        print(f"   å‡¦ç†é€Ÿåº¦: {final_eps_per_sec:.2f} eps/sec")
        print(f"   æ¤œå‡ºã•ã‚ŒãŸæ´å¯Ÿ: {len(self.insight_events)}")
        
        return {
            'num_episodes': num_episodes,
            'execution_time': total_time,
            'episodes_per_second': final_eps_per_sec,
            'insights_detected': len(self.insight_events),
            'timestamp': datetime.now().isoformat()
        }
    
    def save_comprehensive_summary(self, experiment_results: Dict[str, Any]):
        """åŒ…æ‹¬çš„ã‚µãƒãƒªã‚’ä¿å­˜"""
        print(f"\nğŸ’¾ åŒ…æ‹¬çš„ã‚µãƒãƒªã‚’ä¿å­˜ä¸­...")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir = Path("experiments/outputs/comprehensive_experiment")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
        input_episodes_file = output_dir / "input_episodes_detailed.csv"
        with open(input_episodes_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['episode_id', 'episode_text', 'topic', 'modification', 'pattern_id', 'timestamp'])
            
            for ep in self.input_episodes:
                writer.writerow([
                    ep['id'], ep['text'], ep['topic'], 
                    ep['modification'], ep['pattern_id'], ep['timestamp']
                ])
        
        # 2. æ´å¯Ÿã‚¤ãƒ™ãƒ³ãƒˆè©³ç´°
        insight_events_file = output_dir / "insight_events_comprehensive.json"
        with open(insight_events_file, 'w', encoding='utf-8') as f:
            json.dump(self.insight_events, f, indent=2, ensure_ascii=False)
        
        # 3. æ´å¯Ÿã‚¤ãƒ™ãƒ³ãƒˆCSVï¼ˆç°¡ç•¥ç‰ˆï¼‰
        insight_csv_file = output_dir / "insight_events_summary.csv"
        with open(insight_csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'insight_id', 'spike_reference', 'insight_type', 'importance_score',
                'delta_ged', 'delta_ig', 'generated_timestamp', 'num_related_nodes',
                'top_vector_to_text', 'related_nodes_sample'
            ])
            
            for event in self.insight_events:
                top_text = event['vector_to_language'][0]['text'] if event['vector_to_language'] else 'N/A'
                nodes_sample = str(event['related_nodes'][:10]) if event['related_nodes'] else '[]'
                
                writer.writerow([
                    event['insight_id'],
                    event['spike_reference'],
                    event['insight_type'],
                    event['importance_score'],
                    event['spike_details']['delta_ged'],
                    event['spike_details']['delta_ig'],
                    event['generated_timestamp'],
                    event['num_related_nodes'],
                    top_text,
                    nodes_sample
                ])
        
        # 4. å®Ÿé¨“ã‚µãƒãƒª
        summary_file = output_dir / "experiment_summary.json"
        full_summary = {
            'experiment_metadata': experiment_results,
            'input_episodes_count': len(self.input_episodes),
            'insight_events_count': len(self.insight_events),
            'files_generated': {
                'input_episodes': str(input_episodes_file),
                'insight_events_detailed': str(insight_events_file),
                'insight_events_summary': str(insight_csv_file),
                'experiment_summary': str(summary_file)
            }
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(full_summary, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… åŒ…æ‹¬çš„ã‚µãƒãƒªä¿å­˜å®Œäº†:")
        print(f"   ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
        print(f"   ğŸ“„ å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {input_episodes_file}")
        print(f"   ğŸ“„ æ´å¯Ÿã‚¤ãƒ™ãƒ³ãƒˆè©³ç´°: {insight_events_file}")
        print(f"   ğŸ“„ æ´å¯Ÿã‚¤ãƒ™ãƒ³ãƒˆCSV: {insight_csv_file}")
        print(f"   ğŸ“„ å®Ÿé¨“ã‚µãƒãƒª: {summary_file}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    experiment = ComprehensiveInsightExperiment()
    
    try:
        # å®Ÿé¨“å®Ÿè¡Œ
        results = experiment.run_experiment(num_episodes=1000)
        
        # ã‚µãƒãƒªä¿å­˜
        experiment.save_comprehensive_summary(results)
        
        print(f"\nğŸ‰ åŒ…æ‹¬çš„æ´å¯Ÿå®Ÿé¨“ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ!")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ å®Ÿé¨“ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ å®Ÿé¨“ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
