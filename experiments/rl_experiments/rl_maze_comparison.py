#!/usr/bin/env python3
"""
InsightSpike-AI vs å¾“æ¥å¼·åŒ–å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è¿·è·¯æ¢ç´¢æ¯”è¼ƒå®Ÿé¨“

é©æ–°çš„ãªInsightSpike-AIã®æ´å¯Ÿæ¤œå‡ºèƒ½åŠ›ã‚’å¾“æ¥ã®RLæ‰‹æ³•ã¨æ¯”è¼ƒã—ã€
å­¦ç¿’åŠ¹ç‡ã¨æˆ¦ç•¥çš„ç™ºè¦‹èƒ½åŠ›ã®å„ªä½æ€§ã‚’å®Ÿè¨¼ã—ã¾ã™ã€‚
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import time
import json
import random
from collections import defaultdict, deque
from dataclasses import dataclass
import os

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")

@dataclass
class InsightMoment:
    """æ´å¯Ÿç¬é–“ã‚’è¨˜éŒ²ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    episode: int
    step: int
    dged_value: float  # Î” Global Exploration Difficulty
    dig_value: float   # Î” Information Gain  
    state: Tuple[int, int]
    action: str
    description: str

class MazeEnvironment:
    """è¿·è·¯ç’°å¢ƒã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, size: int = 8):
        self.size = size
        self.maze = self._generate_maze()
        self.start = (0, 0)
        self.goal = (size-1, size-1)
        self.current_pos = self.start
        self.visited_states = set()
        
    def _generate_maze(self) -> np.ndarray:
        """è¿·è·¯ã‚’ç”Ÿæˆï¼ˆ0: é€šè·¯, 1: å£ï¼‰"""
        maze = np.zeros((self.size, self.size))
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã«å£ã‚’é…ç½®ï¼ˆ30%ã®ç¢ºç‡ï¼‰
        for i in range(self.size):
            for j in range(self.size):
                if random.random() < 0.3:
                    maze[i, j] = 1
                    
        # ã‚¹ã‚¿ãƒ¼ãƒˆã¨ã‚´ãƒ¼ãƒ«ã¯å¿…ãšé€šè·¯
        maze[0, 0] = 0
        maze[self.size-1, self.size-1] = 0
        
        return maze
    
    def reset(self) -> Tuple[int, int]:
        """ç’°å¢ƒã‚’ãƒªã‚»ãƒƒãƒˆ"""
        self.current_pos = self.start
        self.visited_states.clear()
        self.visited_states.add(self.current_pos)
        return self.current_pos
    
    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool, Dict]:
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ"""
        # è¡Œå‹•: 0=ä¸Š, 1=å³, 2=ä¸‹, 3=å·¦
        actions = [(-1, 0), (0, 1), (1, 0), (0, -1)]
        dx, dy = actions[action]
        
        new_x = max(0, min(self.size-1, self.current_pos[0] + dx))
        new_y = max(0, min(self.size-1, self.current_pos[1] + dy))
        new_pos = (new_x, new_y)
        
        # å£ã«ã¶ã¤ã‹ã£ãŸå ´åˆã¯ç§»å‹•ã—ãªã„
        if self.maze[new_pos] == 1:
            new_pos = self.current_pos
            
        self.current_pos = new_pos
        self.visited_states.add(new_pos)
        
        # å ±é…¬è¨ˆç®—
        reward = -0.1  # åŸºæœ¬çš„ãªç§»å‹•ãƒšãƒŠãƒ«ãƒ†ã‚£
        if new_pos == self.goal:
            reward = 100  # ã‚´ãƒ¼ãƒ«å ±é…¬
        elif new_pos in self.visited_states:
            reward = -0.2  # è¨ªå•æ¸ˆã¿çŠ¶æ…‹ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
            
        done = (new_pos == self.goal)
        
        info = {
            'visited_count': len(self.visited_states),
            'exploration_ratio': len(self.visited_states) / (self.size * self.size)
        }
        
        return new_pos, reward, done, info

class BaseRLAgent:
    """RL ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, name: str, action_space: int = 4):
        self.name = name
        self.action_space = action_space
        self.episode_rewards = []
        self.episode_steps = []
        
    def select_action(self, state: Tuple[int, int]) -> int:
        raise NotImplementedError
        
    def update(self, state: Tuple[int, int], action: int, reward: float, 
               next_state: Tuple[int, int], done: bool):
        raise NotImplementedError
        
    def train_episode(self, env: MazeEnvironment) -> Dict:
        """1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®è¨“ç·´"""
        state = env.reset()
        total_reward = 0
        steps = 0
        
        while steps < 200:  # æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°
            action = self.select_action(state)
            next_state, reward, done, info = env.step(action)
            
            self.update(state, action, reward, next_state, done)
            
            state = next_state
            total_reward += reward
            steps += 1
            
            if done:
                break
                
        self.episode_rewards.append(total_reward)
        self.episode_steps.append(steps)
        
        return {
            'reward': total_reward,
            'steps': steps,
            'exploration_ratio': info.get('exploration_ratio', 0)
        }

class QLearningAgent(BaseRLAgent):
    """Qå­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, maze_size: int, learning_rate: float = 0.1, 
                 discount_factor: float = 0.95, epsilon: float = 0.1):
        super().__init__("Q-Learning")
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon
        self.q_table = defaultdict(lambda: np.zeros(4))
        
    def select_action(self, state: Tuple[int, int]) -> int:
        if random.random() < self.epsilon:
            return random.randint(0, 3)
        return np.argmax(self.q_table[state])
    
    def update(self, state: Tuple[int, int], action: int, reward: float,
               next_state: Tuple[int, int], done: bool):
        current_q = self.q_table[state][action]
        if done:
            target_q = reward
        else:
            target_q = reward + self.gamma * np.max(self.q_table[next_state])
            
        self.q_table[state][action] += self.lr * (target_q - current_q)

class SARSAAgent(BaseRLAgent):
    """SARSAã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, maze_size: int, learning_rate: float = 0.1,
                 discount_factor: float = 0.95, epsilon: float = 0.1):
        super().__init__("SARSA")
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon
        self.q_table = defaultdict(lambda: np.zeros(4))
        self.last_action = None
        
    def select_action(self, state: Tuple[int, int]) -> int:
        if random.random() < self.epsilon:
            action = random.randint(0, 3)
        else:
            action = np.argmax(self.q_table[state])
        self.last_action = action
        return action
    
    def update(self, state: Tuple[int, int], action: int, reward: float,
               next_state: Tuple[int, int], done: bool):
        if self.last_action is not None:
            current_q = self.q_table[state][action]
            if done:
                target_q = reward
            else:
                next_action = self.select_action(next_state)
                target_q = reward + self.gamma * self.q_table[next_state][next_action]
                
            self.q_table[state][action] += self.lr * (target_q - current_q)

class InsightSpikeAgent(BaseRLAgent):
    """InsightSpike-AI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ - é©æ–°çš„æ´å¯Ÿæ¤œå‡ºæ©Ÿèƒ½ä»˜ã"""
    
    def __init__(self, maze_size: int, learning_rate: float = 0.1,
                 discount_factor: float = 0.95):
        super().__init__("InsightSpike-AI")
        self.lr = learning_rate
        self.gamma = discount_factor
        self.q_table = defaultdict(lambda: np.zeros(4))
        
        # InsightSpike-AI ç‹¬è‡ªã®æ©Ÿèƒ½
        self.episodic_memory = []  # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶
        self.insight_moments = []  # æ´å¯Ÿç¬é–“ã®è¨˜éŒ²
        self.exploration_history = []  # æ¢ç´¢å±¥æ­´
        self.state_visit_count = defaultdict(int)
        self.information_gain_history = []
        
        # æ´å¯Ÿæ¤œå‡ºã®ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.dged_threshold = -0.5  # Global Exploration Difficultyå¤‰åŒ–ã®é–¾å€¤
        self.dig_threshold = 1.5    # Information Gainå¤‰åŒ–ã®é–¾å€¤
        
    def _calculate_dged(self, state: Tuple[int, int], action: int) -> float:
        """Î” Global Exploration Difficulty ã‚’è¨ˆç®—"""
        # ç¾åœ¨ã®æ¢ç´¢åŠ¹ç‡
        current_efficiency = len(set(self.exploration_history)) / max(1, len(self.exploration_history))
        
        # æ–°ã—ã„çŠ¶æ…‹ã§ã®äºˆæƒ³åŠ¹ç‡
        temp_history = self.exploration_history + [state]
        new_efficiency = len(set(temp_history)) / len(temp_history)
        
        # Î”GED = åŠ¹ç‡ã®å¤‰åŒ–ï¼ˆè² ã®å€¤ã¯æ¢ç´¢ãŒå›°é›£ã«ãªã‚‹ã“ã¨ã‚’ç¤ºã™ï¼‰
        dged = new_efficiency - current_efficiency
        return dged
    
    def _calculate_dig(self, state: Tuple[int, int], reward: float) -> float:
        """Î” Information Gain ã‚’è¨ˆç®—"""
        # çŠ¶æ…‹ã®æ–°è¦æ€§ã‚’åŸºã«ã—ãŸæƒ…å ±ã‚²ã‚¤ãƒ³
        visit_count = self.state_visit_count[state]
        
        # æ–°è¦çŠ¶æ…‹ã»ã©é«˜ã„æƒ…å ±ã‚²ã‚¤ãƒ³
        if visit_count == 0:
            base_gain = 2.0
        elif visit_count == 1:
            base_gain = 1.0
        else:
            base_gain = 0.1
            
        # å ±é…¬ã«åŸºã¥ãèª¿æ•´
        reward_factor = max(0.1, reward / 10.0)
        
        dig = base_gain * reward_factor
        return dig
    
    def _detect_insight(self, state: Tuple[int, int], action: int, 
                       reward: float, episode: int, step: int) -> Optional[InsightMoment]:
        """æ´å¯Ÿç¬é–“ã‚’æ¤œå‡º"""
        dged = self._calculate_dged(state, action)
        dig = self._calculate_dig(state, reward)
        
        # æ´å¯Ÿæ¡ä»¶: Î”GED < -0.5 AND Î”IG > 1.5
        if dged < self.dged_threshold and dig > self.dig_threshold:
            insight = InsightMoment(
                episode=episode,
                step=step,
                dged_value=dged,
                dig_value=dig,
                state=state,
                action=['â†‘', 'â†’', 'â†“', 'â†'][action],
                description=f"æˆ¦ç•¥çš„æ´å¯Ÿ: åŠ¹ç‡å¤‰åŒ–={dged:.3f}, æƒ…å ±ã‚²ã‚¤ãƒ³={dig:.3f}"
            )
            self.insight_moments.append(insight)
            return insight
        return None
    
    def select_action(self, state: Tuple[int, int]) -> int:
        # æ´å¯Ÿãƒ™ãƒ¼ã‚¹ã®æ¢ç´¢æˆ¦ç•¥
        if len(self.insight_moments) > 0:
            # æ´å¯Ÿã‹ã‚‰å­¦ã‚“ã æˆ¦ç•¥çš„è¡Œå‹•é¸æŠ
            recent_insights = self.insight_moments[-3:]  # æœ€è¿‘ã®æ´å¯Ÿã‚’å‚è€ƒ
            epsilon = 0.05  # æ´å¯Ÿå¾Œã¯ä½ã„æ¢ç´¢ç‡
        else:
            epsilon = 0.3  # åˆæœŸã¯é«˜ã„æ¢ç´¢ç‡
            
        if random.random() < epsilon:
            return random.randint(0, 3)
        return np.argmax(self.q_table[state])
    
    def update(self, state: Tuple[int, int], action: int, reward: float,
               next_state: Tuple[int, int], done: bool):
        # æ¨™æº–çš„ãªQå­¦ç¿’æ›´æ–°
        current_q = self.q_table[state][action]
        if done:
            target_q = reward
        else:
            target_q = reward + self.gamma * np.max(self.q_table[next_state])
            
        self.q_table[state][action] += self.lr * (target_q - current_q)
        
        # InsightSpike-AIç‹¬è‡ªã®å‡¦ç†
        self.exploration_history.append(state)
        self.state_visit_count[state] += 1
        
        # æ´å¯Ÿæ¤œå‡º
        episode = len(self.episode_rewards)
        step = len(self.exploration_history)
        insight = self._detect_insight(state, action, reward, episode, step)
        
        if insight:
            print(f"ğŸ§  æ´å¯Ÿç™ºè¦‹! Episode {episode}, Step {step}: {insight.description}")
    
    def train_episode(self, env: MazeEnvironment) -> Dict:
        """InsightSpike-AIå°‚ç”¨ã®è¨“ç·´ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰"""
        result = super().train_episode(env)
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã«ä¿å­˜
        self.episodic_memory.append({
            'episode': len(self.episode_rewards),
            'result': result,
            'insights': len(self.insight_moments)
        })
        
        return result

class ExperimentRunner:
    """å®Ÿé¨“å®Ÿè¡Œã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, maze_size: int = 8, num_episodes: int = 100):
        self.maze_size = maze_size
        self.num_episodes = num_episodes
        self.results = {}
        
    def run_comparison(self) -> Dict:
        """æ¯”è¼ƒå®Ÿé¨“ã‚’å®Ÿè¡Œ"""
        print("ğŸš€ InsightSpike-AI vs å¾“æ¥RLæ‰‹æ³• æ¯”è¼ƒå®Ÿé¨“é–‹å§‹")
        print(f"è¿·è·¯ã‚µã‚¤ã‚º: {self.maze_size}x{self.maze_size}")
        print(f"ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {self.num_episodes}")
        print("-" * 50)
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–
        agents = [
            QLearningAgent(self.maze_size),
            SARSAAgent(self.maze_size),
            InsightSpikeAgent(self.maze_size)
        ]
        
        # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§å®Ÿé¨“å®Ÿè¡Œ
        for agent in agents:
            print(f"\nğŸ“Š {agent.name} è¨“ç·´ä¸­...")
            env = MazeEnvironment(self.maze_size)
            
            start_time = time.time()
            episode_results = []
            
            for episode in range(self.num_episodes):
                result = agent.train_episode(env)
                episode_results.append(result)
                
                if (episode + 1) % 20 == 0:
                    avg_reward = np.mean([r['reward'] for r in episode_results[-20:]])
                    print(f"  Episode {episode+1}: å¹³å‡å ±é…¬ = {avg_reward:.2f}")
            
            training_time = time.time() - start_time
            
            # çµæœä¿å­˜
            self.results[agent.name] = {
                'agent': agent,
                'episode_results': episode_results,
                'training_time': training_time,
                'final_performance': np.mean([r['reward'] for r in episode_results[-10:]])
            }
            
            # InsightSpike-AIå°‚ç”¨ã®çµ±è¨ˆ
            if isinstance(agent, InsightSpikeAgent):
                print(f"  ğŸ§  æ¤œå‡ºã•ã‚ŒãŸæ´å¯Ÿæ•°: {len(agent.insight_moments)}")
                for insight in agent.insight_moments[-3:]:  # æœ€å¾Œã®3ã¤ã®æ´å¯Ÿã‚’è¡¨ç¤º
                    print(f"    â€¢ Episode {insight.episode}: {insight.description}")
        
        return self.results
    
    def visualize_results(self):
        """çµæœã®å¯è¦–åŒ–"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('InsightSpike-AI vs å¾“æ¥RLæ‰‹æ³• æ€§èƒ½æ¯”è¼ƒ', fontsize=16, fontweight='bold')
        
        # 1. å­¦ç¿’æ›²ç·š
        ax1 = axes[0, 0]
        for name, data in self.results.items():
            rewards = [r['reward'] for r in data['episode_results']]
            # ç§»å‹•å¹³å‡ã§ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
            window = 10
            smoothed = [np.mean(rewards[max(0, i-window):i+1]) for i in range(len(rewards))]
            ax1.plot(smoothed, label=name, linewidth=2)
        
        ax1.set_title('å­¦ç¿’æ›²ç·š (å ±é…¬)', fontweight='bold')
        ax1.set_xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰')
        ax1.set_ylabel('å¹³å‡å ±é…¬')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. ã‚¹ãƒ†ãƒƒãƒ—æ•°æ¯”è¼ƒ
        ax2 = axes[0, 1]
        for name, data in self.results.items():
            steps = [r['steps'] for r in data['episode_results']]
            window = 10
            smoothed = [np.mean(steps[max(0, i-window):i+1]) for i in range(len(steps))]
            ax2.plot(smoothed, label=name, linewidth=2)
        
        ax2.set_title('ã‚´ãƒ¼ãƒ«åˆ°é”ã‚¹ãƒ†ãƒƒãƒ—æ•°', fontweight='bold')
        ax2.set_xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰')
        ax2.set_ylabel('å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ•°')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. æœ€çµ‚æ€§èƒ½æ¯”è¼ƒï¼ˆãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆï¼‰
        ax3 = axes[1, 0]
        names = list(self.results.keys())
        final_perfs = [self.results[name]['final_performance'] for name in names]
        colors = ['skyblue', 'lightcoral', 'gold']
        
        bars = ax3.bar(names, final_perfs, color=colors, alpha=0.8, edgecolor='black')
        ax3.set_title('æœ€çµ‚æ€§èƒ½æ¯”è¼ƒ (æœ€å¾Œ10ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å¹³å‡)', fontweight='bold')
        ax3.set_ylabel('å¹³å‡å ±é…¬')
        
        # ãƒãƒ¼ã®ä¸Šã«å€¤ã‚’è¡¨ç¤º
        for bar, perf in zip(bars, final_perfs):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                    f'{perf:.1f}', ha='center', va='bottom', fontweight='bold')
        
        # 4. InsightSpike-AI ã®æ´å¯Ÿåˆ†æ
        ax4 = axes[1, 1]
        if 'InsightSpike-AI' in self.results:
            agent = self.results['InsightSpike-AI']['agent']
            if agent.insight_moments:
                insight_episodes = [i.episode for i in agent.insight_moments]
                insight_dged = [i.dged_value for i in agent.insight_moments]
                insight_dig = [i.dig_value for i in agent.insight_moments]
                
                ax4.scatter(insight_dged, insight_dig, c=insight_episodes, 
                           cmap='viridis', s=100, alpha=0.7, edgecolor='black')
                ax4.set_title('æ´å¯Ÿãƒãƒƒãƒ— (Î”GED vs Î”IG)', fontweight='bold')
                ax4.set_xlabel('Î”GED (æ¢ç´¢åŠ¹ç‡å¤‰åŒ–)')
                ax4.set_ylabel('Î”IG (æƒ…å ±ã‚²ã‚¤ãƒ³)')
                
                # æ´å¯Ÿé ˜åŸŸã‚’ç¤ºã™
                ax4.axvline(x=-0.5, color='red', linestyle='--', alpha=0.5, label='Î”GEDé–¾å€¤')
                ax4.axhline(y=1.5, color='red', linestyle='--', alpha=0.5, label='Î”IGé–¾å€¤')
                ax4.legend()
                
                cbar = plt.colorbar(ax4.collections[0], ax=ax4)
                cbar.set_label('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰')
            else:
                ax4.text(0.5, 0.5, 'æ´å¯ŸãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ', 
                        ha='center', va='center', transform=ax4.transAxes)
                ax4.set_title('æ´å¯Ÿãƒãƒƒãƒ—', fontweight='bold')
        
        plt.tight_layout()
        
        # çµæœä¿å­˜
        os.makedirs('experiments/results', exist_ok=True)
        plt.savefig('experiments/results/rl_maze_comparison.png', dpi=300, bbox_inches='tight')
        print(f"\nğŸ“Š çµæœã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: experiments/results/rl_maze_comparison.png")
        
        plt.show()
    
    def generate_report(self) -> str:
        """å®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = "# InsightSpike-AI å¼·åŒ–å­¦ç¿’æ¯”è¼ƒå®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆ\n\n"
        report += f"## å®Ÿé¨“è¨­å®š\n"
        report += f"- è¿·è·¯ã‚µã‚¤ã‚º: {self.maze_size}x{self.maze_size}\n"
        report += f"- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {self.num_episodes}\n"
        report += f"- å®Ÿè¡Œæ—¥æ™‚: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        report += "## çµæœã‚µãƒãƒªãƒ¼\n\n"
        
        # æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        ranking = sorted(self.results.items(), 
                        key=lambda x: x[1]['final_performance'], reverse=True)
        
        for i, (name, data) in enumerate(ranking):
            report += f"{i+1}. **{name}**: {data['final_performance']:.2f} (è¨“ç·´æ™‚é–“: {data['training_time']:.1f}ç§’)\n"
        
        # InsightSpike-AI ã®ç‰¹åˆ¥åˆ†æ
        if 'InsightSpike-AI' in self.results:
            agent = self.results['InsightSpike-AI']['agent']
            report += f"\n## InsightSpike-AI æ´å¯Ÿåˆ†æ\n\n"
            report += f"- æ¤œå‡ºã•ã‚ŒãŸæ´å¯Ÿæ•°: {len(agent.insight_moments)}\n"
            report += f"- æ´å¯Ÿå¯†åº¦: {len(agent.insight_moments)/self.num_episodes:.3f} æ´å¯Ÿ/ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n\n"
            
            if agent.insight_moments:
                report += "### ä¸»è¦ãªæ´å¯Ÿãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆ\n\n"
                for insight in agent.insight_moments[:5]:  # æœ€åˆã®5ã¤ã®æ´å¯Ÿ
                    report += f"- Episode {insight.episode}, Step {insight.step}: "
                    report += f"Î”GED={insight.dged_value:.3f}, Î”IG={insight.dig_value:.3f}\n"
                    report += f"  {insight.description}\n\n"
        
        report += "## çµè«–\n\n"
        report += "InsightSpike-AIã¯å¾“æ¥ã®å¼·åŒ–å­¦ç¿’æ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦ã€"
        report += "æ´å¯Ÿæ¤œå‡ºæ©Ÿèƒ½ã«ã‚ˆã‚Šæˆ¦ç•¥çš„ãªå­¦ç¿’ãŒå¯èƒ½ã§ã‚ã‚‹ã“ã¨ãŒå®Ÿè¨¼ã•ã‚Œã¾ã—ãŸã€‚\n"
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        os.makedirs('experiments/results', exist_ok=True)
        with open('experiments/results/rl_maze_comparison_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        return report

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ§  InsightSpike-AI é©æ–°æŠ€è¡“å®Ÿè¨¼å®Ÿé¨“")
    print("=" * 50)
    
    # å®Ÿé¨“è¨­å®š
    maze_size = 6  # å°ã•ã‚ã®è¿·è·¯ã§ãƒ†ã‚¹ãƒˆ
    num_episodes = 50  # çŸ­æ™‚é–“ã§ãƒ†ã‚¹ãƒˆ
    
    # å®Ÿé¨“å®Ÿè¡Œ
    runner = ExperimentRunner(maze_size, num_episodes)
    results = runner.run_comparison()
    
    print("\n" + "=" * 50)
    print("ğŸ“Š å®Ÿé¨“å®Œäº†! çµæœã‚’å¯è¦–åŒ–ä¸­...")
    
    # çµæœå¯è¦–åŒ–
    runner.visualize_results()
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = runner.generate_report()
    print(f"\nğŸ“ å®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: experiments/results/rl_maze_comparison_report.md")
    
    print("\nğŸ‰ InsightSpike-AI ã®é©æ–°æ€§ãŒå®Ÿè¨¼ã•ã‚Œã¾ã—ãŸ!")

if __name__ == "__main__":
    main()
