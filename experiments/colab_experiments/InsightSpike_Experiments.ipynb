{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InsightSpike-AI Large-Scale Experiments on Google Colab\n",
    "\n",
    "This notebook provides a comprehensive environment for running InsightSpike experiments at scale.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Sunwood-ai-labs/InsightSpike-AI/blob/main/experiments/colab_experiments/InsightSpike_Experiments.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's set up the Google Colab environment with all necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone InsightSpike repository\n",
    "!git clone https://github.com/Sunwood-ai-labs/InsightSpike-AI.git\n",
    "%cd InsightSpike-AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run setup script\n",
    "!python experiments/colab_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create results directory in Drive\n",
    "import os\n",
    "results_dir = '/content/drive/MyDrive/InsightSpike_Results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "print(f\"üìÅ Results will be saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Weights & Biases Setup (Optional)\n",
    "\n",
    "Set up experiment tracking with W&B for better visualization and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Weights & Biases (optional but recommended)\n",
    "use_wandb = False  # Set to True to enable W&B tracking\n",
    "\n",
    "if use_wandb:\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    print(\"‚úÖ W&B initialized for experiment tracking\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Running without W&B tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Experiment Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add InsightSpike to path\n",
    "import sys\n",
    "sys.path.append('/content/InsightSpike-AI/src')\n",
    "sys.path.append('/content/InsightSpike-AI')\n",
    "\n",
    "# Import experiment modules\n",
    "from experiments.colab_experiments.insight_benchmarks import (\n",
    "    InsightBenchmarkSuite, run_rat_benchmark, run_all_benchmarks\n",
    ")\n",
    "from experiments.colab_experiments.scalability_testing import (\n",
    "    ScalabilityTestSuite, test_memory_scaling, test_all_scalability\n",
    ")\n",
    "from experiments.colab_experiments.comparative_analysis import (\n",
    "    ComparativeAnalysisSuite, compare_quality, compare_all_systems\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All experiment modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Tests\n",
    "\n",
    "Let's run some quick tests to ensure everything is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick RAT test (5 problems)\n",
    "print(\"üß™ Running quick RAT test...\")\n",
    "quick_results = run_rat_benchmark(n_problems=5, use_wandb=use_wandb)\n",
    "\n",
    "print(f\"\\nüìä Quick Results:\")\n",
    "print(f\"Accuracy: {quick_results['metrics']['accuracy']:.1f}%\")\n",
    "print(f\"Spike Rate: {quick_results['metrics']['spike_rate']:.1f}%\")\n",
    "print(f\"Avg Processing Time: {quick_results['metrics']['avg_processing_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick memory scaling test\n",
    "print(\"üíæ Running quick memory scaling test...\")\n",
    "memory_results = test_memory_scaling(episode_counts=[100, 500, 1000])\n",
    "\n",
    "print(f\"\\nüìä Memory Scaling Results:\")\n",
    "for measurement in memory_results['measurements']:\n",
    "    print(f\"{measurement['n_episodes']} episodes: {measurement['avg_retrieval_time']*1000:.1f}ms avg retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Insight Task Benchmarks\n",
    "\n",
    "Run comprehensive benchmarks on various insight discovery tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize benchmark suite\n",
    "benchmark_suite = InsightBenchmarkSuite(use_wandb=use_wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RAT benchmark (100 problems)\n",
    "print(\"üß† Running full RAT benchmark (100 problems)...\")\n",
    "rat_results = benchmark_suite.run_rat_benchmark(n_problems=100)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nüìä RAT Benchmark Results:\")\n",
    "print(f\"Accuracy: {rat_results['metrics']['accuracy']:.1f}%\")\n",
    "print(f\"Spike Rate: {rat_results['metrics']['spike_rate']:.1f}%\")\n",
    "print(f\"Spike-Accuracy Correlation: {rat_results['metrics']['spike_accuracy_correlation']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RAT results\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create results DataFrame\n",
    "df = pd.DataFrame(rat_results['problems'])\n",
    "\n",
    "# Plot accuracy by spike detection\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Accuracy distribution\n",
    "ax = axes[0]\n",
    "spike_accuracy = df[df['spike_detected']]['correct'].mean() * 100\n",
    "no_spike_accuracy = df[~df['spike_detected']]['correct'].mean() * 100\n",
    "\n",
    "ax.bar(['With Spike', 'Without Spike'], [spike_accuracy, no_spike_accuracy], \n",
    "       color=['green', 'orange'], alpha=0.7)\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Accuracy by Spike Detection')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Processing time distribution\n",
    "ax = axes[1]\n",
    "ax.hist(df['processing_time'], bins=30, color='blue', alpha=0.7)\n",
    "ax.set_xlabel('Processing Time (s)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Processing Time Distribution')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run other benchmarks\n",
    "print(\"üî¨ Running scientific discovery benchmark...\")\n",
    "sci_results = benchmark_suite.run_scientific_discovery_benchmark()\n",
    "\n",
    "print(f\"\\nüìä Scientific Discovery Results:\")\n",
    "print(f\"Discovery Rate: {sci_results['metrics']['discovery_rate']:.1f}%\")\n",
    "print(f\"Novel Insight Rate: {sci_results['metrics']['novel_insight_rate']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive benchmark suite\n",
    "print(\"üèÉ Running comprehensive benchmark suite (this may take a while)...\")\n",
    "comprehensive_results = benchmark_suite.run_comprehensive_benchmark()\n",
    "\n",
    "print(\"\\n‚úÖ Comprehensive benchmarks complete!\")\n",
    "for benchmark_name, results in comprehensive_results['benchmarks'].items():\n",
    "    if 'metrics' in results:\n",
    "        print(f\"\\n{benchmark_name}:\")\n",
    "        for metric, value in results['metrics'].items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"  {metric}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scalability Testing\n",
    "\n",
    "Test InsightSpike's performance at different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scalability test suite\n",
    "scalability_suite = ScalabilityTestSuite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test memory scaling\n",
    "print(\"üìà Testing memory system scalability...\")\n",
    "memory_scaling_results = scalability_suite.run_memory_scaling_test(\n",
    "    episode_counts=[100, 500, 1000, 5000, 10000],\n",
    "    query_count=50\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Scaling Analysis:\")\n",
    "print(f\"Complexity: {memory_scaling_results['analysis']['scaling_complexity']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test graph scaling\n",
    "print(\"üï∏Ô∏è Testing graph processing scalability...\")\n",
    "graph_scaling_results = scalability_suite.run_graph_scaling_test(\n",
    "    node_counts=[100, 500, 1000, 2500],\n",
    "    edge_density=0.1\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Graph Complexity:\")\n",
    "print(f\"Estimated: {graph_scaling_results['complexity_analysis']['estimated_complexity']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test concurrent users\n",
    "print(\"üë• Testing concurrent user handling...\")\n",
    "concurrent_results = scalability_suite.run_concurrent_user_test(\n",
    "    user_counts=[1, 5, 10, 20],\n",
    "    queries_per_user=5\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Scalability Metrics:\")\n",
    "metrics = concurrent_results['scalability_metrics']\n",
    "print(f\"Average Scalability Score: {metrics['avg_scalability_score']:.2f}\")\n",
    "print(f\"Scalability Degradation: {metrics['scalability_degradation']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model sizes\n",
    "print(\"ü§ñ Comparing different model sizes...\")\n",
    "model_comparison = scalability_suite.run_model_size_comparison(\n",
    "    models=[\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"microsoft/phi-2\"],\n",
    "    test_queries=20\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Model Comparison:\")\n",
    "if 'tradeoff_analysis' in model_comparison:\n",
    "    print(f\"Best Efficiency: {model_comparison['tradeoff_analysis']['best_efficiency_model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparative Analysis\n",
    "\n",
    "Compare InsightSpike with baseline RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize comparative analysis suite\n",
    "comparison_suite = ComparativeAnalysisSuite()\n",
    "\n",
    "# Prepare knowledge base\n",
    "knowledge_base = [\n",
    "    \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\",\n",
    "    \"Deep learning uses neural networks with multiple layers to process complex patterns.\",\n",
    "    \"Natural language processing helps computers understand and generate human language.\",\n",
    "    \"Computer vision enables machines to interpret and understand visual information.\",\n",
    "    \"Reinforcement learning allows agents to learn through interaction with an environment.\",\n",
    "    \"Transfer learning leverages pre-trained models for new tasks with limited data.\",\n",
    "    \"Generative AI creates new content like text, images, and music.\",\n",
    "    \"Transformers revolutionized NLP with attention mechanisms.\",\n",
    "    \"Graph neural networks process data with complex relationships.\",\n",
    "    \"Federated learning enables training on distributed data while preserving privacy.\"\n",
    "]\n",
    "\n",
    "comparison_suite.prepare_knowledge_base(knowledge_base)\n",
    "print(\"‚úÖ Knowledge base prepared for both systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run quality comparison\n",
    "test_queries = [\n",
    "    \"What is the relationship between deep learning and machine learning?\",\n",
    "    \"How do transformers work in NLP?\",\n",
    "    \"What are the applications of computer vision?\",\n",
    "    \"Explain the concept of transfer learning\",\n",
    "    \"What is the difference between supervised and reinforcement learning?\"\n",
    "]\n",
    "\n",
    "print(\"üìä Running quality comparison...\")\n",
    "quality_results = comparison_suite.run_quality_comparison(\n",
    "    test_queries=test_queries * 4,  # Run 20 queries\n",
    "    evaluate_quality=True\n",
    ")\n",
    "\n",
    "metrics = quality_results['comparative_metrics']\n",
    "print(f\"\\nüìà Quality Improvements:\")\n",
    "print(f\"Overall Quality: {metrics['quality_improvement']:.1f}%\")\n",
    "print(f\"Relevance: {metrics['relevance_improvement']:.1f}%\")\n",
    "print(f\"Spike Rate: {metrics['spike_rate']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run performance comparison\n",
    "print(\"‚ö° Running performance comparison...\")\n",
    "performance_results = comparison_suite.run_performance_comparison(\n",
    "    workloads=[10, 25, 50]\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Performance Results:\")\n",
    "for measurement in performance_results['measurements']:\n",
    "    print(f\"\\n{measurement['n_queries']} queries:\")\n",
    "    print(f\"  InsightSpike: {measurement['insightspike']['avg_response_time']:.3f}s\")\n",
    "    print(f\"  Baseline RAG: {measurement['baseline']['avg_response_time']:.3f}s\")\n",
    "    print(f\"  Speedup: {measurement['speedup']:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run insight discovery comparison\n",
    "creative_queries = [\n",
    "    \"What unexpected connections exist between quantum computing and biology?\",\n",
    "    \"How might AI transform education in ways we haven't considered?\",\n",
    "    \"What are the hidden relationships between climate change and technology?\",\n",
    "    \"Propose novel applications of blockchain beyond cryptocurrency\",\n",
    "    \"What insights can we draw from comparing human and artificial intelligence?\"\n",
    "]\n",
    "\n",
    "print(\"üí° Running insight discovery comparison...\")\n",
    "insight_results = comparison_suite.run_insight_discovery_comparison(\n",
    "    creative_queries=creative_queries * 2\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Insight Discovery Results:\")\n",
    "print(f\"InsightSpike Discovery Rate: {insight_results['metrics']['insightspike_discovery_rate']*100:.1f}%\")\n",
    "print(f\"Baseline Discovery Rate: {insight_results['metrics']['baseline_discovery_rate']*100:.1f}%\")\n",
    "print(f\"Average Insight Advantage: {insight_results['metrics']['avg_insight_advantage']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Analysis\n",
    "\n",
    "Run a complete comparison across multiple domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive comparison (this will take significant time)\n",
    "print(\"üèÉ Running comprehensive comparison across domains...\")\n",
    "print(\"This will test: general, scientific, creative, and analytical domains\")\n",
    "print(\"Expected time: 15-30 minutes\\n\")\n",
    "\n",
    "comprehensive_comparison = comparison_suite.run_comprehensive_comparison(\n",
    "    n_queries=40,  # 10 queries per domain\n",
    "    domains=['general', 'scientific', 'creative', 'analytical']\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Comprehensive comparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Summary & Export\n",
    "\n",
    "Summarize all results and save to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "summary = {\n",
    "    'experiment_date': datetime.now().isoformat(),\n",
    "    'environment': 'Google Colab',\n",
    "    'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None',\n",
    "    'results_summary': {\n",
    "        'rat_benchmark': {\n",
    "            'accuracy': rat_results['metrics']['accuracy'],\n",
    "            'spike_rate': rat_results['metrics']['spike_rate']\n",
    "        } if 'rat_results' in locals() else {},\n",
    "        'scalability': {\n",
    "            'memory_complexity': memory_scaling_results['analysis']['scaling_complexity']\n",
    "        } if 'memory_scaling_results' in locals() else {},\n",
    "        'comparison': {\n",
    "            'quality_improvement': quality_results['comparative_metrics']['quality_improvement']\n",
    "        } if 'quality_results' in locals() else {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to Drive\n",
    "summary_path = f\"{results_dir}/experiment_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"üìÑ Summary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all results to Drive\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "local_results = Path('/content/InsightSpike-AI/experiments/colab_experiments/results')\n",
    "if local_results.exists():\n",
    "    drive_results = f\"{results_dir}/detailed_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    shutil.copytree(local_results, drive_results)\n",
    "    print(f\"üìÅ All results copied to: {drive_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resource Monitoring & Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor resource usage\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "print(\"üíª System Resources:\")\n",
    "print(f\"CPU Usage: {psutil.cpu_percent()}%\")\n",
    "print(f\"RAM Usage: {psutil.virtual_memory().percent}%\")\n",
    "print(f\"Available RAM: {psutil.virtual_memory().available / 1e9:.1f} GB\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    for gpu in gpus:\n",
    "        print(f\"\\nGPU {gpu.id}: {gpu.name}\")\n",
    "        print(f\"  Memory Used: {gpu.memoryUsed:.0f} MB / {gpu.memoryTotal:.0f} MB\")\n",
    "        print(f\"  GPU Utilization: {gpu.load*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup GPU memory\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"‚úÖ GPU memory cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Final Summary\n",
    "\n",
    "Congratulations! You've successfully run comprehensive InsightSpike experiments. Here's what we accomplished:\n",
    "\n",
    "1. **Insight Benchmarks**: Tested creative problem-solving capabilities\n",
    "2. **Scalability Tests**: Verified performance with increasing data sizes\n",
    "3. **Comparative Analysis**: Demonstrated advantages over baseline RAG\n",
    "\n",
    "All results have been saved to your Google Drive for further analysis.\n",
    "\n",
    "### Next Steps:\n",
    "- Analyze the detailed results in the generated reports\n",
    "- Fine-tune parameters based on your specific use case\n",
    "- Extend experiments with custom datasets\n",
    "- Share results with the InsightSpike community\n",
    "\n",
    "---\n",
    "\n",
    "*Following CLAUDE.md guidelines: All experiments used genuine processing without mocks or cheats.*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "InsightSpike_Experiments.ipynb",
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}