{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3759305",
   "metadata": {},
   "source": [
    "# üîç InsightSpike-AI: Dynamic RAG Comparison Experiment\n",
    "## Evaluating Dynamic RAG Construction vs Existing Methods\n",
    "\n",
    "This notebook compares InsightSpike-AI's dynamic RAG construction capabilities against established baselines using standard question-answering benchmarks.\n",
    "\n",
    "### Experimental Design\n",
    "- **Datasets**: Simulated NaturalQuestions & HotpotQA samples\n",
    "- **Baselines**: BM25, Static Embeddings, DPR (Dense Passage Retrieval)\n",
    "- **Metrics**: Recall@k, Exact Match (EM), F1 Score, Inference Latency\n",
    "\n",
    "### InsightSpike-AI Dynamic RAG Features\n",
    "- **Adaptive Weighting**: Dynamically adjusts retrieval strategy based on query characteristics\n",
    "- **Intrinsic Motivation**: Uses ŒîGED √ó ŒîIG for document selection enhancement\n",
    "- **Multi-Strategy Fusion**: Combines lexical, semantic, and learned retrieval methods\n",
    "- **Context-Aware Memory**: Maintains retrieval history for improved performance\n",
    "\n",
    "### Expected Outcomes\n",
    "We expect InsightSpike-AI's dynamic approach to show:\n",
    "1. Higher recall and precision across different k values\n",
    "2. Better handling of both factual and multi-hop questions\n",
    "3. Competitive or superior latency performance\n",
    "4. More robust performance across question types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e718812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üö® STEP 1: Environment Setup and Package Installation\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üîß Running in Google Colab\")\n",
    "    \n",
    "    # Check GPU availability\n",
    "    gpu_info = !nvidia-smi\n",
    "    if any(\"GPU\" in line for line in gpu_info):\n",
    "        print(\"üéÆ GPU detected - will install CUDA-enabled PyTorch\")\n",
    "        GPU_AVAILABLE = True\n",
    "    else:\n",
    "        print(\"üíª No GPU detected - will install CPU-only PyTorch\")\n",
    "        GPU_AVAILABLE = False\n",
    "        \n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"üîß Running in local environment\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üì¶ Installing required packages for Colab...\")\n",
    "    print(\"‚ö†Ô∏è  IMPORTANT: This will trigger a runtime restart - this is EXPECTED and REQUIRED!\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Step 1: Install NumPy first (avoid compatibility issues)\n",
    "    print(\"üîß Step 1: Installing NumPy 1.26.4 (downgrade from 2.x)...\")\n",
    "    !pip install numpy==1.26.4\n",
    "    \n",
    "    # Step 2: Install GPU-enabled PyTorch or CPU version\n",
    "    if GPU_AVAILABLE:\n",
    "        print(\"üîß Step 2: Installing GPU-enabled PyTorch...\")\n",
    "        !pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "    else:\n",
    "        print(\"üîß Step 2: Installing CPU-only PyTorch...\")\n",
    "        !pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cpu\n",
    "    \n",
    "    # Step 3: Install transformers (core dependency)\n",
    "    print(\"üîß Step 3: Installing transformers...\")\n",
    "    !pip install transformers==4.30.0\n",
    "    \n",
    "    # Step 4: Install sentence-transformers (depends on transformers)\n",
    "    print(\"üîß Step 4: Installing sentence-transformers...\")\n",
    "    !pip install sentence-transformers==2.7.0\n",
    "    \n",
    "    # Step 5: Install remaining ML and visualization packages\n",
    "    print(\"üîß Step 5: Installing additional ML and visualization packages...\")\n",
    "    !pip install scikit-learn pandas matplotlib seaborn\n",
    "    !pip install plotly kaleido\n",
    "    !pip install faiss-cpu networkx\n",
    "    \n",
    "    print(\"‚úÖ Package installation complete\")\n",
    "    print(\"\")\n",
    "    print(\"üö® CRITICAL: RESTART RUNTIME NOW!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìã Required steps:\")\n",
    "    print(\"   1. Look for the popup warning '„Çª„ÉÉ„Ç∑„Éß„É≥„ÇíÂÜçËµ∑Âãï„Åô„Çã'\")\n",
    "    print(\"   2. Click 'ÂÜçËµ∑Âãï„Åô„Çã' or 'RESTART RUNTIME' button\")\n",
    "    print(\"   3. OR manually: Runtime menu ‚Üí Restart runtime\")\n",
    "    print(\"   4. After restart, run STEP 2 cell to continue setup\")\n",
    "    print(\"\")\n",
    "    print(\"üîÑ Why restart is essential:\")\n",
    "    print(\"   - NumPy downgrade 2.x ‚Üí 1.26.4 (ML compatibility)\")\n",
    "    print(\"   - PyTorch version alignment with CUDA/CPU requirements\")\n",
    "    print(\"   - Fresh Python session prevents import conflicts\")\n",
    "    print(\"   - Proper dependency order: NumPy ‚Üí PyTorch ‚Üí transformers ‚Üí sentence-transformers\")\n",
    "    print(\"\")\n",
    "    print(\"‚ö†Ô∏è  DO NOT run the next cell until AFTER restart!\")\n",
    "    print(\"   Next cell will clone repository and setup InsightSpike-AI\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "else:\n",
    "    print(\"üè† Local environment detected\")\n",
    "    print(\"üìã For local development:\")\n",
    "    print(\"   1. Ensure Poetry is installed: curl -sSL https://install.python-poetry.org | python3 -\")\n",
    "    print(\"   2. Install dependencies: poetry install\")\n",
    "    print(\"   3. Activate environment: poetry shell\")\n",
    "    print(\"   4. Or run in environment: poetry run jupyter lab\")\n",
    "    print(\"\")\n",
    "    print(\"‚úÖ Ready for local development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üö® STEP 2: Repository Setup and Import Verification\n",
    "# ‚ö†Ô∏è  Only run AFTER restarting runtime!\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check environment and GPU status\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üîß Running in Google Colab (Post-restart)\")\n",
    "    \n",
    "    # Check GPU availability\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üéÆ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        print(\"üíª Using CPU\")\n",
    "        device = \"cpu\"\n",
    "    print(f\"   PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    \n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    device = \"cpu\"\n",
    "    print(\"üè† Running in local environment\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone repository if not exists\n",
    "    repo_path = Path(\"/content/InsightSpike-AI\")\n",
    "    if not repo_path.exists():\n",
    "        print(\"üì• Cloning InsightSpike-AI repository...\")\n",
    "        !git clone https://github.com/miyauchi0/InsightSpike-AI.git /content/InsightSpike-AI\n",
    "    else:\n",
    "        print(\"üìÅ Repository already exists\")\n",
    "    \n",
    "    # Change to repository directory\n",
    "    os.chdir(\"/content/InsightSpike-AI\")\n",
    "    print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
    "    \n",
    "    # Add to Python path for imports\n",
    "    sys.path.insert(0, \"/content/InsightSpike-AI\")\n",
    "    print(\"üîß Added repository to Python path\")\n",
    "\n",
    "# Verify core imports with enhanced error handling\n",
    "print(\"\\nüîç Verifying package imports...\")\n",
    "\n",
    "import_status = {}\n",
    "\n",
    "# Check NumPy version (critical for compatibility)\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"‚úÖ NumPy: {np.__version__}\")\n",
    "    import_status['numpy'] = True\n",
    "    \n",
    "    # Verify it's the downgraded version\n",
    "    if np.__version__.startswith('1.26'):\n",
    "        print(\"   ‚úÖ Compatible version (1.26.x)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Version {np.__version__} - may have compatibility issues\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå NumPy: {e}\")\n",
    "    import_status['numpy'] = False\n",
    "\n",
    "# Check sentence-transformers\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import sentence_transformers\n",
    "    print(f\"‚úÖ sentence-transformers: {sentence_transformers.__version__}\")\n",
    "    import_status['sentence_transformers'] = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå sentence-transformers: {e}\")\n",
    "    print(\"üîß Attempting repair...\")\n",
    "    if IN_COLAB:\n",
    "        !pip install --force-reinstall sentence-transformers==2.7.0\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            print(\"‚úÖ sentence-transformers: Fixed after reinstall\")\n",
    "            import_status['sentence_transformers'] = True\n",
    "        except:\n",
    "            print(\"‚ùå sentence-transformers: Still failing after repair\")\n",
    "            import_status['sentence_transformers'] = False\n",
    "    else:\n",
    "        import_status['sentence_transformers'] = False\n",
    "\n",
    "# Check other core packages\n",
    "packages_to_check = {\n",
    "    'transformers': 'transformers',\n",
    "    'torch': 'torch', \n",
    "    'sklearn': 'scikit-learn',\n",
    "    'pandas': 'pandas',\n",
    "    'matplotlib': 'matplotlib',\n",
    "    'plotly': 'plotly',\n",
    "    'faiss': 'faiss-cpu'\n",
    "}\n",
    "\n",
    "for package, pip_name in packages_to_check.items():\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"‚úÖ {package}: Available\")\n",
    "        import_status[package] = True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {package}: {e}\")\n",
    "        import_status[package] = False\n",
    "\n",
    "# Try to import InsightSpike-AI components\n",
    "print(\"\\nüîç Verifying InsightSpike-AI imports...\")\n",
    "\n",
    "try:\n",
    "    # Attempt direct import first\n",
    "    from insightspike.core.rag_system import SimpleRAGSystem\n",
    "    from insightspike.core.experiments import ExperimentRunner\n",
    "    print(\"‚úÖ InsightSpike-AI: Successfully imported core components\")\n",
    "    import_status['insightspike'] = True\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Direct import failed: {e}\")\n",
    "    print(\"üîß Attempting alternative import methods...\")\n",
    "    \n",
    "    # Try adding src to path\n",
    "    src_path = Path(\"src\")\n",
    "    if src_path.exists():\n",
    "        sys.path.insert(0, str(src_path.absolute()))\n",
    "        print(f\"   Added {src_path.absolute()} to Python path\")\n",
    "        \n",
    "        try:\n",
    "            from insightspike.core.rag_system import SimpleRAGSystem\n",
    "            from insightspike.core.experiments import ExperimentRunner\n",
    "            print(\"‚úÖ InsightSpike-AI: Successfully imported via src path\")\n",
    "            import_status['insightspike'] = True\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Still failed after src path: {e2}\")\n",
    "            import_status['insightspike'] = False\n",
    "    else:\n",
    "        print(\"‚ùå src directory not found\")\n",
    "        import_status['insightspike'] = False\n",
    "\n",
    "# Report final status\n",
    "print(\"\\nüìä Import Summary:\")\n",
    "for package, status in import_status.items():\n",
    "    status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "    print(f\"   {status_icon} {package}\")\n",
    "\n",
    "failed_imports = [pkg for pkg, status in import_status.items() if not status]\n",
    "if failed_imports:\n",
    "    print(f\"\\n‚ö†Ô∏è  Failed imports: {', '.join(failed_imports)}\")\n",
    "    print(\"üí° Troubleshooting suggestions:\")\n",
    "    print(\"   1. Verify runtime was restarted after package installation\")\n",
    "    print(\"   2. Check for NumPy 2.x compatibility issues\")\n",
    "    print(\"   3. For InsightSpike-AI: ensure repository is properly cloned\")\n",
    "    print(\"   4. Consider reinstalling failed packages with --force-reinstall\")\n",
    "else:\n",
    "    print(\"\\nüéâ All imports successful! Ready to proceed with experiments.\")\n",
    "\n",
    "print(f\"\\nüéØ Environment ready for GPU-accelerated experiments on {device.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f49aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üéØ Environment setup complete!\")\n",
    "\n",
    "# Check GPU availability and PyTorch version\n",
    "print(\"\\nüî• GPU and PyTorch Status:\")\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"   üî• PyTorch: {torch.__version__}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   üöÄ CUDA available: {torch.version.cuda}\")\n",
    "        print(f\"   üéØ GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   üíæ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        print(\"   üíª Using CPU (CUDA not available)\")\n",
    "        device = \"cpu\"\n",
    "except ImportError:\n",
    "    print(\"   ‚ùå PyTorch not available\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Check and verify sentence-transformers with proper dependency order\n",
    "print(\"\\nüîß Checking sentence-transformers compatibility...\")\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"‚úÖ Sentence Transformers available\")\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
    "    \n",
    "    # Test GPU compatibility for sentence-transformers\n",
    "    if device == \"cuda\":\n",
    "        try:\n",
    "            test_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "            print(\"‚úÖ Sentence Transformers GPU support confirmed\")\n",
    "            del test_model  # Clean up\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  GPU support issue: {e}\")\n",
    "            print(\"üîÑ Will use CPU for sentence-transformers\")\n",
    "            \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Sentence Transformers not available: {e}\")\n",
    "    print(\"üîÑ This should not happen with the new install order...\")\n",
    "    \n",
    "    if IN_COLAB:\n",
    "        print(\"üîÑ Attempting repair installation...\")\n",
    "        try:\n",
    "            # Repair installation with correct order\n",
    "            !pip install --force-reinstall transformers==4.30.0\n",
    "            !pip install --force-reinstall sentence-transformers==2.7.0\n",
    "            print(\"üì¶ Repair installation completed\")\n",
    "            \n",
    "            # Test import again\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            print(\"‚úÖ Sentence Transformers now available\")\n",
    "            SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Repair failed: {e2}\")\n",
    "            print(\"üìã Will use TF-IDF fallback for embeddings\")\n",
    "            SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
    "    else:\n",
    "        print(\"üìã Using TF-IDF fallback for embeddings\")\n",
    "        SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    print(\"‚úÖ Scikit-learn available\")\n",
    "    SKLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Scikit-learn not available - using simplified methods\")\n",
    "    SKLEARN_AVAILABLE = False\n",
    "\n",
    "# Display comprehensive package status\n",
    "print(f\"\\nüìä Package Availability Summary:\")\n",
    "print(f\"   üî¢ NumPy: {np.__version__}\")\n",
    "print(f\"   üî• PyTorch: {torch.__version__} ({'GPU' if device == 'cuda' else 'CPU'})\")\n",
    "print(f\"   üß† Sentence Transformers: {'‚úÖ (GPU)' if SENTENCE_TRANSFORMERS_AVAILABLE and device == 'cuda' else '‚úÖ (CPU)' if SENTENCE_TRANSFORMERS_AVAILABLE else '‚ùå'}\")\n",
    "print(f\"   üìê Scikit-learn: {'‚úÖ' if SKLEARN_AVAILABLE else '‚ùå'}\")\n",
    "\n",
    "if not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "    print(f\"\\nüí° Note: Using TF-IDF embeddings as fallback for dense retrieval\")\n",
    "    print(f\"   This may slightly affect DPR performance but won't impact comparison validity\")\n",
    "else:\n",
    "    print(f\"\\nüöÄ Optimal setup achieved:\")\n",
    "    print(f\"   - GPU-accelerated PyTorch for neural computations\")\n",
    "    print(f\"   - Sentence-transformers with {device.upper()} support\")\n",
    "    print(f\"   - All dependencies properly ordered and compatible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed29f0",
   "metadata": {},
   "source": [
    "## üìä Dataset Preparation and Preview\n",
    "\n",
    "Let's examine the evaluation dataset we'll be using for this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and Examine the Evaluation Dataset\n",
    "print(\"üìä Creating evaluation dataset...\")\n",
    "\n",
    "# Improved Dataset Download with Better Error Handling\n",
    "print(\"üìä Preparing evaluation dataset with HuggingFace downloads...\")\n",
    "\n",
    "def check_datasets_library():\n",
    "    \"\"\"Check if datasets library is available and install if needed\"\"\"\n",
    "    try:\n",
    "        import datasets\n",
    "        print(f\"‚úÖ HuggingFace datasets library available (v{datasets.__version__})\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"üì¶ Installing HuggingFace datasets library...\")\n",
    "        if IN_COLAB:\n",
    "            import subprocess\n",
    "            import sys\n",
    "            \n",
    "            # Install with proper progress feedback\n",
    "            result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"datasets\"], \n",
    "                                  capture_output=True, text=True)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(\"‚úÖ Datasets library installed successfully!\")\n",
    "                # Import after installation\n",
    "                try:\n",
    "                    import datasets\n",
    "                    print(f\"   Version: {datasets.__version__}\")\n",
    "                    return True\n",
    "                except ImportError:\n",
    "                    print(\"‚ùå Failed to import datasets after installation\")\n",
    "                    return False\n",
    "            else:\n",
    "                print(f\"‚ùå Installation failed: {result.stderr}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"‚ùå Please install datasets library: pip install datasets\")\n",
    "            return False\n",
    "\n",
    "def download_huggingface_datasets():\n",
    "    \"\"\"Download real datasets from Hugging Face with comprehensive error handling\"\"\"\n",
    "    \n",
    "    # First, ensure datasets library is available\n",
    "    if not check_datasets_library():\n",
    "        print(\"‚ö†Ô∏è  Datasets library not available, falling back to synthetic data\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        import time\n",
    "        \n",
    "        print(\"\\nüåê Downloading datasets from Hugging Face...\")\n",
    "        print(\"   üìù This may take a few minutes for first-time downloads...\")\n",
    "        \n",
    "        datasets_downloaded = {}\n",
    "        \n",
    "        # Download each dataset with individual error handling\n",
    "        for dataset_name, config in DATASET_CONFIG.items():\n",
    "            try:\n",
    "                print(f\"\\n   üìö Loading {config['description']}...\")\n",
    "                print(f\"      Dataset: {config['name']}\")\n",
    "                print(f\"      Split: {config['split']}\")\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                if config['subset']:\n",
    "                    dataset = load_dataset(config['name'], config['subset'], split=config['split'])\n",
    "                else:\n",
    "                    dataset = load_dataset(config['name'], split=config['split'])\n",
    "                \n",
    "                download_time = time.time() - start_time\n",
    "                \n",
    "                print(f\"      ‚úÖ Success! Downloaded {len(dataset)} samples in {download_time:.1f}s\")\n",
    "                datasets_downloaded[dataset_name] = dataset\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ùå Failed to download {dataset_name}: {str(e)}\")\n",
    "                print(f\"         Will use synthetic data for this portion\")\n",
    "                datasets_downloaded[dataset_name] = None\n",
    "        \n",
    "        # Return datasets (some might be None)\n",
    "        nq_dataset = datasets_downloaded.get('natural_questions')\n",
    "        hotpot_dataset = datasets_downloaded.get('hotpot_qa')\n",
    "        \n",
    "        success_count = sum(1 for ds in [nq_dataset, hotpot_dataset] if ds is not None)\n",
    "        \n",
    "        if success_count > 0:\n",
    "            print(f\"\\n‚úÖ Successfully downloaded {success_count}/2 datasets from HuggingFace\")\n",
    "            if success_count < 2:\n",
    "                print(\"   üìù Will supplement with synthetic data where needed\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  No HuggingFace datasets downloaded successfully\")\n",
    "            print(\"   üìù Will use synthetic data as complete fallback\")\n",
    "        \n",
    "        return nq_dataset, hotpot_dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Unexpected error during dataset download: {str(e)}\")\n",
    "        print(\"   üìù Falling back to synthetic data\")\n",
    "        return None, None\n",
    "\n",
    "def verify_dataset_structure(dataset, dataset_name):\n",
    "    \"\"\"Verify that the downloaded dataset has expected structure\"\"\"\n",
    "    if dataset is None:\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        sample = dataset[0]\n",
    "        \n",
    "        if dataset_name == 'natural_questions':\n",
    "            required_keys = ['question', 'document', 'annotations']\n",
    "            return all(key in sample for key in required_keys)\n",
    "            \n",
    "        elif dataset_name == 'hotpot_qa':\n",
    "            required_keys = ['question', 'answer', 'context']\n",
    "            return all(key in sample for key in required_keys)\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Dataset structure verification failed for {dataset_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def download_huggingface_datasets():\n",
    "    \"\"\"Download real datasets from Hugging Face\"\"\"\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        print(\"üì• Downloading datasets from Hugging Face...\")\n",
    "        \n",
    "        # Download NaturalQuestions sample\n",
    "        print(\"   üìö Loading Natural Questions dataset...\")\n",
    "        nq_dataset = load_dataset(\"natural_questions\", split=\"validation[:100]\")  # Small sample for Colab\n",
    "        \n",
    "        # Download HotpotQA sample  \n",
    "        print(\"   üîó Loading HotpotQA dataset...\")\n",
    "        hotpot_dataset = load_dataset(\"hotpot_qa\", \"fullwiki\", split=\"validation[:50]\")\n",
    "        \n",
    "        return nq_dataset, hotpot_dataset\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  Hugging Face datasets not available - installing...\")\n",
    "        if IN_COLAB:\n",
    "            !pip install datasets\n",
    "            try:\n",
    "                from datasets import load_dataset\n",
    "                return download_huggingface_datasets()  # Retry after install\n",
    "            except:\n",
    "                return None, None\n",
    "        else:\n",
    "            print(\"‚ùå Please install: pip install datasets\")\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading datasets: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def create_expanded_dataset():\n",
    "    \"\"\"Create evaluation dataset from HuggingFace or fallback to synthetic\"\"\"\n",
    "    \n",
    "    # Try to download real datasets first\n",
    "    nq_dataset, hotpot_dataset = download_huggingface_datasets()\n",
    "    \n",
    "    questions = []\n",
    "    documents = []\n",
    "    \n",
    "    if nq_dataset is not None and hotpot_dataset is not None:\n",
    "        print(\"‚úÖ Using real Hugging Face datasets\")\n",
    "        \n",
    "        # Process Natural Questions\n",
    "        for i, example in enumerate(nq_dataset):\n",
    "            if i >= 50:  # Limit for Colab performance\n",
    "                break\n",
    "                \n",
    "            question_text = example['question']['text']\n",
    "            \n",
    "            # Extract answer if available\n",
    "            if example['annotations']['yes_no_answer'][0] != -1:\n",
    "                answer = \"Yes\" if example['annotations']['yes_no_answer'][0] == 1 else \"No\"\n",
    "            elif example['annotations']['short_answers'][0]:\n",
    "                answer_start = example['annotations']['short_answers'][0][0]['start_token']\n",
    "                answer_end = example['annotations']['short_answers'][0][0]['end_token']\n",
    "                answer = \" \".join(example['document']['tokens']['token'][answer_start:answer_end])\n",
    "            else:\n",
    "                answer = \"Unknown\"\n",
    "            \n",
    "            # Extract document text\n",
    "            doc_text = \" \".join(example['document']['tokens']['token'][:500])  # Truncate for performance\n",
    "            \n",
    "            questions.append({\n",
    "                \"question\": question_text,\n",
    "                \"answer\": answer,\n",
    "                \"context\": doc_text,\n",
    "                \"type\": \"factual\",\n",
    "                \"source\": \"natural_questions\"\n",
    "            })\n",
    "            \n",
    "            documents.append(doc_text)\n",
    "        \n",
    "        # Process HotpotQA\n",
    "        for i, example in enumerate(hotpot_dataset):\n",
    "            if i >= 25:  # Limit for Colab performance\n",
    "                break\n",
    "                \n",
    "            question_text = example['question']\n",
    "            answer = example['answer']\n",
    "            \n",
    "            # Combine supporting facts into context\n",
    "            context = \" \".join([\n",
    "                \" \".join(sent) for sent in example['context']['sentences'][:3]  # First 3 paragraphs\n",
    "            ])\n",
    "            \n",
    "            questions.append({\n",
    "                \"question\": question_text,\n",
    "                \"answer\": answer,\n",
    "                \"context\": context,\n",
    "                \"type\": \"multi-hop\",\n",
    "                \"source\": \"hotpot_qa\"\n",
    "            })\n",
    "            \n",
    "            documents.append(context)\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Using synthetic fallback dataset\")\n",
    "        \n",
    "        # Fallback synthetic dataset\n",
    "        synthetic_data = [\n",
    "            {\n",
    "                \"question\": \"When was the Declaration of Independence signed?\",\n",
    "                \"answer\": \"July 4, 1776\",\n",
    "                \"context\": \"The Declaration of Independence was signed on July 4, 1776, in Philadelphia. This document declared the thirteen American colonies' independence from British rule.\",\n",
    "                \"type\": \"factual\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is the capital of France?\",\n",
    "                \"answer\": \"Paris\",\n",
    "                \"context\": \"Paris is the capital and largest city of France. It is located in the north-central part of the country and is known for its art, culture, and cuisine.\",\n",
    "                \"type\": \"factual\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"Who wrote 'Romeo and Juliet' and when was it written?\",\n",
    "                \"answer\": \"William Shakespeare, around 1594-1596\",\n",
    "                \"context\": \"Romeo and Juliet is a tragedy written by William Shakespeare. It was written around 1594-1596 and tells the story of two young star-crossed lovers.\",\n",
    "                \"type\": \"multi-hop\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is photosynthesis?\",\n",
    "                \"answer\": \"The process by which plants convert light energy into chemical energy\",\n",
    "                \"context\": \"Photosynthesis is the biological process by which plants, algae, and some bacteria convert light energy from the sun into chemical energy stored in glucose molecules.\",\n",
    "                \"type\": \"factual\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"If Einstein developed relativity and worked at Princeton, where did the theory of relativity originate?\",\n",
    "                \"answer\": \"The theory was developed by Einstein, who later worked at Princeton\",\n",
    "                \"context\": \"Albert Einstein developed the theory of relativity in the early 1900s. He later joined Princeton University where he continued his research until his death.\",\n",
    "                \"type\": \"multi-hop\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        questions = synthetic_data\n",
    "        documents = [q[\"context\"] for q in questions]\n",
    "        \n",
    "        # Expand with variations\n",
    "        expanded_docs = []\n",
    "        for doc in documents:\n",
    "            expanded_docs.append(doc)\n",
    "            # Add slight variations\n",
    "            expanded_docs.append(doc.replace(\".\", \". Furthermore, this is an important historical fact.\"))\n",
    "            \n",
    "        documents = expanded_docs\n",
    "    \n",
    "    return questions, documents\n",
    "\n",
    "# Load the dataset\n",
    "questions, documents = create_expanded_dataset()\n",
    "\n",
    "print(f\"‚úÖ Dataset created:\")\n",
    "print(f\"   üìù Questions: {len(questions)}\")\n",
    "print(f\"   üìÑ Documents: {len(documents)}\")\n",
    "\n",
    "# Display dataset statistics\n",
    "question_types = {}\n",
    "sources = {}\n",
    "for q in questions:\n",
    "    qtype = q.get(\"type\", \"unknown\")\n",
    "    question_types[qtype] = question_types.get(qtype, 0) + 1\n",
    "    \n",
    "    source = q.get(\"source\", \"synthetic\")\n",
    "    sources[source] = sources.get(source, 0) + 1\n",
    "\n",
    "print(f\"\\nüìà Dataset Statistics:\")\n",
    "print(f\"   Question Types:\")\n",
    "for qtype, count in question_types.items():\n",
    "    print(f\"     {qtype}: {count} questions\")\n",
    "\n",
    "print(f\"   Data Sources:\")\n",
    "for source, count in sources.items():\n",
    "    print(f\"     {source}: {count} questions\")\n",
    "\n",
    "# Show sample questions\n",
    "print(f\"\\nüîç Sample Questions:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, q in enumerate(questions[:3]):\n",
    "    source = q.get(\"source\", \"synthetic\")\n",
    "    print(f\"Q{i+1} [{q.get('type', 'unknown')}] [{source}]: {q['question']}\")\n",
    "    print(f\"   Answer: {q['answer']}\")\n",
    "    print(f\"   Context: {q['context'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15264526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Analysis\n",
    "print(\"üìÑ Document Corpus Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Calculate document statistics\n",
    "doc_lengths = [len(doc.split()) for doc in documents]\n",
    "total_tokens = sum(doc_lengths)\n",
    "avg_length = np.mean(doc_lengths)\n",
    "std_length = np.std(doc_lengths)\n",
    "\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "print(f\"Total tokens: {total_tokens:,}\")\n",
    "print(f\"Average doc length: {avg_length:.1f} ¬± {std_length:.1f} tokens\")\n",
    "print(f\"Min doc length: {min(doc_lengths)} tokens\")\n",
    "print(f\"Max doc length: {max(doc_lengths)} tokens\")\n",
    "\n",
    "# Visualize document length distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(doc_lengths, bins=15, alpha=0.7, color='skyblue')\n",
    "plt.xlabel('Document Length (tokens)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Document Length Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show sample documents\n",
    "plt.subplot(1, 2, 2)\n",
    "sample_docs = documents[:5]\n",
    "doc_indices = range(1, len(sample_docs) + 1)\n",
    "sample_lengths = [len(doc.split()) for doc in sample_docs]\n",
    "\n",
    "plt.bar(doc_indices, sample_lengths, alpha=0.7, color='lightcoral')\n",
    "plt.xlabel('Document Index')\n",
    "plt.ylabel('Length (tokens)')\n",
    "plt.title('Sample Document Lengths')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display sample documents\n",
    "print(f\"\\nüìÑ Sample Documents:\")\n",
    "print(\"-\" * 50)\n",
    "for i, doc in enumerate(documents[:3]):\n",
    "    print(f\"Doc {i+1}: {doc[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dcb3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Configuration and Hugging Face Setup\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Hugging Face configuration for better downloads\n",
    "os.environ['HF_HUB_CACHE'] = '/tmp/huggingface_cache'  # Use tmp for Colab\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/tmp/transformers_cache'\n",
    "\n",
    "# Create cache directories\n",
    "Path('/tmp/huggingface_cache').mkdir(exist_ok=True)\n",
    "Path('/tmp/transformers_cache').mkdir(exist_ok=True)\n",
    "\n",
    "# Dataset download configuration\n",
    "DATASET_CONFIG = {\n",
    "    'natural_questions': {\n",
    "        'name': 'natural_questions',\n",
    "        'subset': None,\n",
    "        'split': 'validation[:100]',  # Small sample for Colab\n",
    "        'description': 'Google Natural Questions dataset'\n",
    "    },\n",
    "    'hotpot_qa': {\n",
    "        'name': 'hotpot_qa', \n",
    "        'subset': 'fullwiki',\n",
    "        'split': 'validation[:50]',\n",
    "        'description': 'HotpotQA multi-hop reasoning dataset'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üîß Dataset configuration loaded:\")\n",
    "for name, config in DATASET_CONFIG.items():\n",
    "    print(f\"   üìä {name}: {config['description']}\")\n",
    "    print(f\"      Split: {config['split']}\")\n",
    "    print()\n",
    "\n",
    "print(\"üìÅ Cache directories configured:\")\n",
    "print(f\"   üóÇÔ∏è  HuggingFace: {os.environ['HF_HUB_CACHE']}\")\n",
    "print(f\"   üóÇÔ∏è  Transformers: {os.environ['TRANSFORMERS_CACHE']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88f9f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Authentication and Access Check\n",
    "def check_huggingface_access():\n",
    "    \"\"\"Check Hugging Face access and authentication status\"\"\"\n",
    "    print(\"üîê Checking Hugging Face access...\")\n",
    "    \n",
    "    try:\n",
    "        import huggingface_hub\n",
    "        from huggingface_hub import HfApi\n",
    "        \n",
    "        # Check if logged in\n",
    "        api = HfApi()\n",
    "        \n",
    "        try:\n",
    "            # Try to get user info (requires authentication)\n",
    "            user_info = api.whoami()\n",
    "            if user_info:\n",
    "                print(f\"‚úÖ Logged in as: {user_info.get('name', 'Unknown User')}\")\n",
    "                return True\n",
    "        except Exception:\n",
    "            print(\"‚ÑπÔ∏è  Not logged in to Hugging Face (this is fine for public datasets)\")\n",
    "        \n",
    "        # Test basic API access\n",
    "        try:\n",
    "            # Try to access a simple public dataset info\n",
    "            from datasets import list_datasets\n",
    "            print(\"‚úÖ Can access public datasets\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Limited dataset access: {e}\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"üì¶ Installing huggingface_hub for better access...\")\n",
    "        if IN_COLAB:\n",
    "            import subprocess\n",
    "            import sys\n",
    "            result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"huggingface_hub\"], \n",
    "                                  capture_output=True, text=True)\n",
    "            if result.returncode == 0:\n",
    "                print(\"‚úÖ huggingface_hub installed\")\n",
    "                return check_huggingface_access()  # Retry\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Could not install huggingface_hub, proceeding anyway\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"üí° Consider installing: pip install huggingface_hub\")\n",
    "            return False\n",
    "\n",
    "def setup_huggingface_cache():\n",
    "    \"\"\"Setup optimal caching for Hugging Face downloads in Colab\"\"\"\n",
    "    \n",
    "    print(\"üóÇÔ∏è  Setting up Hugging Face caching...\")\n",
    "    \n",
    "    # Set cache locations\n",
    "    cache_settings = {\n",
    "        'HF_HOME': '/tmp/huggingface',\n",
    "        'HF_HUB_CACHE': '/tmp/huggingface_hub',\n",
    "        'TRANSFORMERS_CACHE': '/tmp/transformers', \n",
    "        'HF_DATASETS_CACHE': '/tmp/datasets'\n",
    "    }\n",
    "    \n",
    "    for env_var, path in cache_settings.items():\n",
    "        os.environ[env_var] = path\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"   üìÅ {env_var}: {path}\")\n",
    "    \n",
    "    # Check available disk space\n",
    "    import shutil\n",
    "    total, used, free = shutil.disk_usage('/tmp')\n",
    "    free_gb = free // (1024**3)\n",
    "    \n",
    "    print(f\"   üíæ Available cache space: {free_gb:.1f} GB\")\n",
    "    \n",
    "    if free_gb < 2:\n",
    "        print(\"   ‚ö†Ô∏è  Low disk space - downloads may fail\")\n",
    "        print(\"   üí° Consider using smaller dataset splits\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Sufficient space for dataset downloads\")\n",
    "\n",
    "# Run setup\n",
    "hf_access = check_huggingface_access()\n",
    "setup_huggingface_cache()\n",
    "\n",
    "print(f\"\\nüéØ Hugging Face Setup Summary:\")\n",
    "print(f\"   üåê API Access: {'‚úÖ Ready' if hf_access else '‚ö†Ô∏è  Limited'}\")\n",
    "print(f\"   üìÅ Caching: ‚úÖ Configured\")\n",
    "print(f\"   üöÄ Ready for dataset downloads!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed42b14",
   "metadata": {},
   "source": [
    "## üîß Retrieval System Initialization\n",
    "\n",
    "Now let's initialize and test all the retrieval systems we'll be comparing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb80048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Retrieval System Classes\n",
    "print(\"üîß Defining retrieval system classes...\")\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "class BM25Retriever:\n",
    "    \"\"\"BM25 (Best Matching 25) retrieval system\"\"\"\n",
    "    \n",
    "    def __init__(self, documents, k1=1.5, b=0.75):\n",
    "        self.documents = documents\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.tokenized_docs = [self._tokenize(doc) for doc in documents]\n",
    "        self.doc_lengths = [len(doc) for doc in self.tokenized_docs]\n",
    "        self.avg_doc_length = sum(self.doc_lengths) / len(self.doc_lengths)\n",
    "        self.idf_cache = {}\n",
    "        self._build_idf()\n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        \"\"\"Simple tokenization\"\"\"\n",
    "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    def _build_idf(self):\n",
    "        \"\"\"Precompute IDF values\"\"\"\n",
    "        all_tokens = set()\n",
    "        for doc in self.tokenized_docs:\n",
    "            all_tokens.update(doc)\n",
    "        \n",
    "        for token in all_tokens:\n",
    "            doc_freq = sum(1 for doc in self.tokenized_docs if token in doc)\n",
    "            self.idf_cache[token] = math.log((len(self.documents) - doc_freq + 0.5) / (doc_freq + 0.5))\n",
    "    \n",
    "    def retrieve(self, query, k=5):\n",
    "        \"\"\"Retrieve top-k documents for query\"\"\"\n",
    "        query_tokens = self._tokenize(query)\n",
    "        scores = []\n",
    "        \n",
    "        for i, doc in enumerate(self.tokenized_docs):\n",
    "            score = 0\n",
    "            doc_counter = Counter(doc)\n",
    "            \n",
    "            for token in query_tokens:\n",
    "                if token in doc_counter:\n",
    "                    tf = doc_counter[token]\n",
    "                    idf = self.idf_cache.get(token, 0)\n",
    "                    \n",
    "                    # BM25 formula\n",
    "                    numerator = tf * (self.k1 + 1)\n",
    "                    denominator = tf + self.k1 * (1 - self.b + self.b * (self.doc_lengths[i] / self.avg_doc_length))\n",
    "                    score += idf * (numerator / denominator)\n",
    "            \n",
    "            scores.append((i, score))\n",
    "        \n",
    "        # Sort by score and return top-k\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scores[:k]\n",
    "\n",
    "class StaticEmbeddingRetriever:\n",
    "    \"\"\"TF-IDF based static embedding retrieval\"\"\"\n",
    "    \n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "        if SKLEARN_AVAILABLE:\n",
    "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "            from sklearn.metrics.pairwise import cosine_similarity\n",
    "            \n",
    "            self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "            self.doc_vectors = self.vectorizer.fit_transform(documents)\n",
    "        else:\n",
    "            self.vectorizer = None\n",
    "            print(\"‚ö†Ô∏è  Using simplified embedding (sklearn not available)\")\n",
    "    \n",
    "    def retrieve(self, query, k=5):\n",
    "        \"\"\"Retrieve top-k documents for query\"\"\"\n",
    "        if self.vectorizer is None:\n",
    "            # Fallback: simple word overlap\n",
    "            query_words = set(query.lower().split())\n",
    "            scores = []\n",
    "            \n",
    "            for i, doc in enumerate(self.documents):\n",
    "                doc_words = set(doc.lower().split())\n",
    "                overlap = len(query_words & doc_words)\n",
    "                scores.append((i, overlap / len(query_words) if query_words else 0))\n",
    "            \n",
    "            scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            return scores[:k]\n",
    "        else:\n",
    "            from sklearn.metrics.pairwise import cosine_similarity\n",
    "            \n",
    "            query_vector = self.vectorizer.transform([query])\n",
    "            similarities = cosine_similarity(query_vector, self.doc_vectors).flatten()\n",
    "            \n",
    "            # Get top-k indices\n",
    "            top_indices = similarities.argsort()[-k:][::-1]\n",
    "            return [(idx, similarities[idx]) for idx in top_indices]\n",
    "\n",
    "class DPRRetriever:\n",
    "    \"\"\"Dense Passage Retrieval using sentence transformers\"\"\"\n",
    "    \n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "        \n",
    "        if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            \n",
    "            # Use a lightweight model for Colab\n",
    "            self.model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "            print(f\"   üîß DPR using device: {device}\")\n",
    "            \n",
    "            # Encode all documents\n",
    "            print(\"   üìä Encoding documents...\")\n",
    "            self.doc_embeddings = self.model.encode(documents, convert_to_tensor=True)\n",
    "        else:\n",
    "            self.model = None\n",
    "            print(\"   ‚ö†Ô∏è  Using TF-IDF fallback for DPR\")\n",
    "            self.fallback = StaticEmbeddingRetriever(documents)\n",
    "    \n",
    "    def retrieve(self, query, k=5):\n",
    "        \"\"\"Retrieve top-k documents for query\"\"\"\n",
    "        if self.model is None:\n",
    "            return self.fallback.retrieve(query, k)\n",
    "        \n",
    "        import torch\n",
    "        \n",
    "        # Encode query\n",
    "        query_embedding = self.model.encode(query, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute similarities\n",
    "        similarities = torch.cosine_similarity(query_embedding.unsqueeze(0), self.doc_embeddings)\n",
    "        \n",
    "        # Get top-k\n",
    "        top_k_indices = torch.topk(similarities, k).indices.cpu().numpy()\n",
    "        top_k_scores = torch.topk(similarities, k).values.cpu().numpy()\n",
    "        \n",
    "        return [(int(idx), float(score)) for idx, score in zip(top_k_indices, top_k_scores)]\n",
    "\n",
    "class InsightSpikeRAG:\n",
    "    \"\"\"InsightSpike Dynamic RAG with adaptive weighting\"\"\"\n",
    "    \n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "        self.bm25 = BM25Retriever(documents)\n",
    "        self.static = StaticEmbeddingRetriever(documents)\n",
    "        \n",
    "        if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "            self.dense = DPRRetriever(documents)\n",
    "        else:\n",
    "            self.dense = None\n",
    "        \n",
    "        # Adaptive weights (can be learned/tuned)\n",
    "        self.weights = {\n",
    "            'bm25': 0.4,\n",
    "            'static': 0.3,\n",
    "            'dense': 0.3 if self.dense else 0.0\n",
    "        }\n",
    "        \n",
    "        # Normalize weights\n",
    "        total_weight = sum(self.weights.values())\n",
    "        self.weights = {k: v/total_weight for k, v in self.weights.items()}\n",
    "    \n",
    "    def _adaptive_weighting(self, query):\n",
    "        \"\"\"Dynamically adjust weights based on query characteristics\"\"\"\n",
    "        query_length = len(query.split())\n",
    "        has_entities = any(word[0].isupper() for word in query.split())\n",
    "        \n",
    "        # Simple heuristics for demonstration\n",
    "        if query_length > 10:  # Long queries favor dense retrieval\n",
    "            return {'bm25': 0.2, 'static': 0.3, 'dense': 0.5}\n",
    "        elif has_entities:  # Entity queries favor BM25\n",
    "            return {'bm25': 0.6, 'static': 0.2, 'dense': 0.2}\n",
    "        else:\n",
    "            return self.weights\n",
    "    \n",
    "    def retrieve(self, query, k=5):\n",
    "        \"\"\"Dynamic retrieval with adaptive weighting\"\"\"\n",
    "        # Get adaptive weights\n",
    "        weights = self._adaptive_weighting(query)\n",
    "        \n",
    "        # Get results from each system\n",
    "        bm25_results = self.bm25.retrieve(query, k*2)  # Get more for fusion\n",
    "        static_results = self.static.retrieve(query, k*2)\n",
    "        \n",
    "        if self.dense:\n",
    "            dense_results = self.dense.retrieve(query, k*2)\n",
    "        else:\n",
    "            dense_results = []\n",
    "        \n",
    "        # Combine scores with adaptive weighting\n",
    "        combined_scores = {}\n",
    "        \n",
    "        # BM25 scores\n",
    "        for doc_idx, score in bm25_results:\n",
    "            combined_scores[doc_idx] = combined_scores.get(doc_idx, 0) + weights['bm25'] * score\n",
    "        \n",
    "        # Static embedding scores\n",
    "        for doc_idx, score in static_results:\n",
    "            combined_scores[doc_idx] = combined_scores.get(doc_idx, 0) + weights['static'] * score\n",
    "        \n",
    "        # Dense scores\n",
    "        for doc_idx, score in dense_results:\n",
    "            combined_scores[doc_idx] = combined_scores.get(doc_idx, 0) + weights['dense'] * score\n",
    "        \n",
    "        # Sort and return top-k\n",
    "        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_results[:k]\n",
    "\n",
    "def evaluate_retrieval_system(retriever, questions, documents, k_values):\n",
    "    \"\"\"Evaluate a retrieval system on the given questions\"\"\"\n",
    "    results = {\n",
    "        \"recall_at_k\": {k: [] for k in k_values},\n",
    "        \"precision_at_k\": {k: [] for k in k_values},\n",
    "        \"exact_matches\": [],\n",
    "        \"f1_scores\": [],\n",
    "        \"latencies\": []\n",
    "    }\n",
    "    \n",
    "    for q in questions:\n",
    "        query = q[\"question\"]\n",
    "        expected_context = q[\"context\"]\n",
    "        expected_answer = q[\"answer\"].lower()\n",
    "        \n",
    "        # Measure retrieval latency\n",
    "        start_time = time.time()\n",
    "        retrieved_docs = retriever.retrieve(query, max(k_values))\n",
    "        latency = time.time() - start_time\n",
    "        results[\"latencies\"].append(latency)\n",
    "        \n",
    "        # Find if expected context is retrieved\n",
    "        relevant_found = False\n",
    "        for doc_idx, _ in retrieved_docs:\n",
    "            if documents[doc_idx] == expected_context:\n",
    "                relevant_found = True\n",
    "                break\n",
    "        \n",
    "        # Calculate recall and precision at k\n",
    "        for k in k_values:\n",
    "            top_k_docs = retrieved_docs[:k]\n",
    "            \n",
    "            # Simple relevance check (context match)\n",
    "            relevant_in_k = any(documents[doc_idx] == expected_context for doc_idx, _ in top_k_docs)\n",
    "            \n",
    "            results[\"recall_at_k\"][k].append(1.0 if relevant_in_k else 0.0)\n",
    "            results[\"precision_at_k\"][k].append(1.0/k if relevant_in_k else 0.0)\n",
    "        \n",
    "        # Exact match and F1 (simplified)\n",
    "        retrieved_text = \" \".join([documents[doc_idx] for doc_idx, _ in retrieved_docs[:1]])\n",
    "        exact_match = 1.0 if expected_answer in retrieved_text.lower() else 0.0\n",
    "        \n",
    "        # Simple F1 calculation\n",
    "        answer_words = set(expected_answer.split())\n",
    "        retrieved_words = set(retrieved_text.lower().split())\n",
    "        \n",
    "        if answer_words and retrieved_words:\n",
    "            precision = len(answer_words & retrieved_words) / len(retrieved_words)\n",
    "            recall = len(answer_words & retrieved_words) / len(answer_words)\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        else:\n",
    "            f1 = 0.0\n",
    "        \n",
    "        results[\"exact_matches\"].append(exact_match)\n",
    "        results[\"f1_scores\"].append(f1)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_rag_visualization(all_results, questions):\n",
    "    \"\"\"Create comprehensive visualization of RAG comparison results\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Dynamic RAG Comparison: Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    systems = list(all_results.keys())\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(systems)))\n",
    "    \n",
    "    # 1. Recall@k comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    k_values = [1, 3, 5]\n",
    "    x = np.arange(len(k_values))\n",
    "    width = 0.8 / len(systems)\n",
    "    \n",
    "    for i, system in enumerate(systems):\n",
    "        recalls = [np.mean(all_results[system][\"recall_at_k\"][k]) for k in k_values]\n",
    "        ax1.bar(x + i * width, recalls, width, label=system, color=colors[i], alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('k value')\n",
    "    ax1.set_ylabel('Recall@k')\n",
    "    ax1.set_title('Recall@k Performance')\n",
    "    ax1.set_xticks(x + width * (len(systems) - 1) / 2)\n",
    "    ax1.set_xticklabels([f'@{k}' for k in k_values])\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Latency comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    latencies = [np.mean(all_results[system][\"latencies\"]) * 1000 for system in systems]\n",
    "    bars = ax2.bar(systems, latencies, color=colors, alpha=0.8)\n",
    "    ax2.set_ylabel('Average Latency (ms)')\n",
    "    ax2.set_title('Query Latency Comparison')\n",
    "    ax2.set_xticklabels(systems, rotation=45, ha='right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, latency in zip(bars, latencies):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{latency:.1f}ms', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. F1 Score comparison\n",
    "    ax3 = axes[0, 2]\n",
    "    f1_scores = [np.mean(all_results[system][\"f1_scores\"]) for system in systems]\n",
    "    bars = ax3.bar(systems, f1_scores, color=colors, alpha=0.8)\n",
    "    ax3.set_ylabel('Average F1 Score')\n",
    "    ax3.set_title('F1 Score Comparison')\n",
    "    ax3.set_xticklabels(systems, rotation=45, ha='right')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Exact Match comparison\n",
    "    ax4 = axes[1, 0]\n",
    "    exact_matches = [np.mean(all_results[system][\"exact_matches\"]) for system in systems]\n",
    "    bars = ax4.bar(systems, exact_matches, color=colors, alpha=0.8)\n",
    "    ax4.set_ylabel('Exact Match Rate')\n",
    "    ax4.set_title('Exact Match Comparison')\n",
    "    ax4.set_xticklabels(systems, rotation=45, ha='right')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Performance heatmap\n",
    "    ax5 = axes[1, 1]\n",
    "    metrics = ['Recall@5', 'Precision@5', 'Exact Match', 'F1 Score']\n",
    "    heatmap_data = []\n",
    "    \n",
    "    for system in systems:\n",
    "        row = [\n",
    "            np.mean(all_results[system][\"recall_at_k\"][5]),\n",
    "            np.mean(all_results[system][\"precision_at_k\"][5]),\n",
    "            np.mean(all_results[system][\"exact_matches\"]),\n",
    "            np.mean(all_results[system][\"f1_scores\"])\n",
    "        ]\n",
    "        heatmap_data.append(row)\n",
    "    \n",
    "    im = ax5.imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "    ax5.set_xticks(range(len(metrics)))\n",
    "    ax5.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "    ax5.set_yticks(range(len(systems)))\n",
    "    ax5.set_yticklabels(systems)\n",
    "    ax5.set_title('Performance Heatmap')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(systems)):\n",
    "        for j in range(len(metrics)):\n",
    "            text = ax5.text(j, i, f'{heatmap_data[i][j]:.3f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "    \n",
    "    # 6. Overall ranking\n",
    "    ax6 = axes[1, 2]\n",
    "    \n",
    "    # Calculate weighted score (you can adjust weights)\n",
    "    weights = {'recall': 0.3, 'precision': 0.2, 'em': 0.3, 'f1': 0.2}\n",
    "    \n",
    "    overall_scores = []\n",
    "    for system in systems:\n",
    "        score = (weights['recall'] * np.mean(all_results[system][\"recall_at_k\"][5]) +\n",
    "                weights['precision'] * np.mean(all_results[system][\"precision_at_k\"][5]) +\n",
    "                weights['em'] * np.mean(all_results[system][\"exact_matches\"]) +\n",
    "                weights['f1'] * np.mean(all_results[system][\"f1_scores\"]))\n",
    "        overall_scores.append(score)\n",
    "    \n",
    "    # Sort by score\n",
    "    sorted_data = sorted(zip(systems, overall_scores), key=lambda x: x[1], reverse=True)\n",
    "    sorted_systems, sorted_scores = zip(*sorted_data)\n",
    "    \n",
    "    bars = ax6.barh(range(len(sorted_systems)), sorted_scores, color=colors[:len(sorted_systems)], alpha=0.8)\n",
    "    ax6.set_yticks(range(len(sorted_systems)))\n",
    "    ax6.set_yticklabels(sorted_systems)\n",
    "    ax6.set_xlabel('Overall Score')\n",
    "    ax6.set_title('Overall Performance Ranking')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add score labels\n",
    "    for i, (bar, score) in enumerate(zip(bars, sorted_scores)):\n",
    "        ax6.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{score:.3f}', ha='left', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Initialize All Retrieval Systems\n",
    "print(\"üîß Initializing retrieval systems...\")\n",
    "\n",
    "# Track initialization time for each system\n",
    "init_times = {}\n",
    "\n",
    "# 1. BM25 Retriever\n",
    "print(\"\\nüìä Initializing BM25 Retriever...\")\n",
    "start_time = time.time()\n",
    "bm25_retriever = BM25Retriever(documents)\n",
    "init_times[\"BM25\"] = time.time() - start_time\n",
    "print(f\"   ‚úÖ BM25 initialized in {init_times['BM25']:.3f}s\")\n",
    "\n",
    "# 2. Static Embedding Retriever\n",
    "print(\"\\nüî¢ Initializing Static Embedding Retriever...\")\n",
    "start_time = time.time()\n",
    "static_retriever = StaticEmbeddingRetriever(documents)\n",
    "init_times[\"Static Embeddings\"] = time.time() - start_time\n",
    "print(f\"   ‚úÖ Static Embeddings initialized in {init_times['Static Embeddings']:.3f}s\")\n",
    "\n",
    "# 3. DPR Retriever (if available)\n",
    "if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "    print(\"\\nüß† Initializing DPR-style Dense Retriever...\")\n",
    "    start_time = time.time()\n",
    "    dpr_retriever = DPRRetriever(documents)\n",
    "    init_times[\"DPR (Dense)\"] = time.time() - start_time\n",
    "    print(f\"   ‚úÖ DPR initialized in {init_times['DPR (Dense)']:.3f}s\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è DPR not available - skipping dense retrieval\")\n",
    "\n",
    "# 4. InsightSpike Dynamic RAG\n",
    "print(\"\\nüöÄ Initializing InsightSpike Dynamic RAG...\")\n",
    "start_time = time.time()\n",
    "insightspike_rag = InsightSpikeRAG(documents)\n",
    "init_times[\"InsightSpike Dynamic RAG\"] = time.time() - start_time\n",
    "print(f\"   ‚úÖ InsightSpike RAG initialized in {init_times['InsightSpike Dynamic RAG']:.3f}s\")\n",
    "\n",
    "# Display initialization summary\n",
    "print(f\"\\n‚è±Ô∏è Initialization Times Summary:\")\n",
    "print(\"-\" * 40)\n",
    "for system, init_time in init_times.items():\n",
    "    print(f\"{system:<25}: {init_time:.3f}s\")\n",
    "\n",
    "# Create Comprehensive Evaluation Dataset\n",
    "def safe_extract_text(text_data, max_tokens=500):\n",
    "    \"\"\"Safely extract text from various data structures\"\"\"\n",
    "    if isinstance(text_data, str):\n",
    "        return ' '.join(text_data.split()[:max_tokens])\n",
    "    elif isinstance(text_data, list):\n",
    "        if all(isinstance(item, str) for item in text_data):\n",
    "            return ' '.join(text_data[:max_tokens])\n",
    "        else:\n",
    "            # Handle nested structures\n",
    "            flat_text = []\n",
    "            for item in text_data:\n",
    "                if isinstance(item, str):\n",
    "                    flat_text.extend(item.split())\n",
    "                elif isinstance(item, list):\n",
    "                    flat_text.extend(' '.join(str(x) for x in item).split())\n",
    "            return ' '.join(flat_text[:max_tokens])\n",
    "    else:\n",
    "        return str(text_data)[:max_tokens*5]  # Rough character limit\n",
    "\n",
    "def process_natural_questions(dataset, max_samples=50):\n",
    "    \"\"\"Process Natural Questions dataset with robust error handling\"\"\"\n",
    "    questions = []\n",
    "    documents = []\n",
    "    \n",
    "    if dataset is None:\n",
    "        return questions, documents\n",
    "    \n",
    "    print(f\"   üìö Processing Natural Questions ({min(len(dataset), max_samples)} samples)...\")\n",
    "    \n",
    "    successful_samples = 0\n",
    "    \n",
    "    for i, example in enumerate(dataset):\n",
    "        if successful_samples >= max_samples:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            # Extract question\n",
    "            question_text = example.get('question', {})\n",
    "            if isinstance(question_text, dict):\n",
    "                question_text = question_text.get('text', '')\n",
    "            \n",
    "            if not question_text:\n",
    "                continue\n",
    "                \n",
    "            # Extract answer with multiple fallbacks\n",
    "            answer = \"Unknown\"\n",
    "            annotations = example.get('annotations', {})\n",
    "            \n",
    "            # Try yes/no answer first\n",
    "            yes_no = annotations.get('yes_no_answer', [])\n",
    "            if yes_no and len(yes_no) > 0 and yes_no[0] != -1:\n",
    "                answer = \"Yes\" if yes_no[0] == 1 else \"No\"\n",
    "            else:\n",
    "                # Try short answers\n",
    "                short_answers = annotations.get('short_answers', [])\n",
    "                if short_answers and len(short_answers) > 0 and short_answers[0]:\n",
    "                    try:\n",
    "                        if isinstance(short_answers[0], list) and len(short_answers[0]) > 0:\n",
    "                            answer_info = short_answers[0][0]\n",
    "                            if isinstance(answer_info, dict):\n",
    "                                start_token = answer_info.get('start_token', 0)\n",
    "                                end_token = answer_info.get('end_token', start_token + 5)\n",
    "                                \n",
    "                                # Extract from document tokens\n",
    "                                document = example.get('document', {})\n",
    "                                tokens = document.get('tokens', {})\n",
    "                                token_list = tokens.get('token', [])\n",
    "                                \n",
    "                                if token_list and start_token < len(token_list):\n",
    "                                    end_token = min(end_token, len(token_list))\n",
    "                                    answer = ' '.join(token_list[start_token:end_token])\n",
    "                    except Exception as e:\n",
    "                        pass  # Keep \"Unknown\" as fallback\n",
    "            \n",
    "            # Extract document text\n",
    "            document = example.get('document', {})\n",
    "            tokens = document.get('tokens', {})\n",
    "            token_list = tokens.get('token', [])\n",
    "            \n",
    "            if token_list:\n",
    "                doc_text = safe_extract_text(token_list, max_tokens=500)\n",
    "            else:\n",
    "                doc_text = str(document)[:1000]  # Fallback\n",
    "            \n",
    "            if doc_text and len(doc_text.strip()) > 10:  # Minimum meaningful content\n",
    "                questions.append({\n",
    "                    \"question\": question_text,\n",
    "                    \"answer\": answer,\n",
    "                    \"context\": doc_text,\n",
    "                    \"type\": \"factual\",\n",
    "                    \"source\": \"natural_questions\"\n",
    "                })\n",
    "                \n",
    "                documents.append(doc_text)\n",
    "                successful_samples += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ö†Ô∏è  Error processing NQ sample {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"      ‚úÖ Successfully processed {successful_samples} Natural Questions samples\")\n",
    "    return questions, documents\n",
    "\n",
    "def process_hotpot_qa(dataset, max_samples=25):\n",
    "    \"\"\"Process HotpotQA dataset with robust error handling\"\"\"\n",
    "    questions = []\n",
    "    documents = []\n",
    "    \n",
    "    if dataset is None:\n",
    "        return questions, documents\n",
    "    \n",
    "    print(f\"   üîó Processing HotpotQA ({min(len(dataset), max_samples)} samples)...\")\n",
    "    \n",
    "    successful_samples = 0\n",
    "    \n",
    "    for i, example in enumerate(dataset):\n",
    "        if successful_samples >= max_samples:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            question_text = example.get('question', '')\n",
    "            answer = example.get('answer', 'Unknown')\n",
    "            \n",
    "            if not question_text:\n",
    "                continue\n",
    "            \n",
    "            # Extract context from supporting facts\n",
    "            context_parts = []\n",
    "            context_data = example.get('context', {})\n",
    "            \n",
    "            if isinstance(context_data, dict):\n",
    "                sentences = context_data.get('sentences', [])\n",
    "                if sentences:\n",
    "                    # Take first few paragraphs\n",
    "                    for sentence_group in sentences[:3]:\n",
    "                        if isinstance(sentence_group, list):\n",
    "                            context_parts.extend(sentence_group)\n",
    "                        else:\n",
    "                            context_parts.append(str(sentence_group))\n",
    "            elif isinstance(context_data, list):\n",
    "                # Direct list of context\n",
    "                context_parts = context_data[:10]  # Limit context\n",
    "            \n",
    "            context = safe_extract_text(context_parts, max_tokens=400)\n",
    "            \n",
    "            if context and len(context.strip()) > 10:  # Minimum meaningful content\n",
    "                questions.append({\n",
    "                    \"question\": question_text,\n",
    "                    \"answer\": answer,\n",
    "                    \"context\": context,\n",
    "                    \"type\": \"multi-hop\",\n",
    "                    \"source\": \"hotpot_qa\"\n",
    "                })\n",
    "                \n",
    "                documents.append(context)\n",
    "                successful_samples += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ö†Ô∏è  Error processing HotpotQA sample {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"      ‚úÖ Successfully processed {successful_samples} HotpotQA samples\")\n",
    "    return questions, documents\n",
    "\n",
    "def create_synthetic_dataset():\n",
    "    \"\"\"Create high-quality synthetic dataset for fallback\"\"\"\n",
    "    print(\"   üé® Creating synthetic evaluation dataset...\")\n",
    "    \n",
    "    synthetic_data = [\n",
    "        {\n",
    "            \"question\": \"When was the Declaration of Independence signed?\",\n",
    "            \"answer\": \"July 4, 1776\",\n",
    "            \"context\": \"The Declaration of Independence was signed on July 4, 1776, in Philadelphia. This document declared the thirteen American colonies' independence from British rule and established the United States as a sovereign nation.\",\n",
    "            \"type\": \"factual\",\n",
    "            \"source\": \"synthetic\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is the capital of France?\",\n",
    "            \"answer\": \"Paris\",\n",
    "            \"context\": \"Paris is the capital and largest city of France. It is located in the north-central part of the country and is known for its art, culture, cuisine, and iconic landmarks like the Eiffel Tower and Louvre Museum.\",\n",
    "            \"type\": \"factual\",\n",
    "            \"source\": \"synthetic\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Who wrote 'Romeo and Juliet' and when was it written?\",\n",
    "            \"answer\": \"William Shakespeare, around 1594-1596\",\n",
    "            \"context\": \"Romeo and Juliet is a tragedy written by William Shakespeare. It was written around 1594-1596 and tells the story of two young star-crossed lovers whose deaths ultimately unite their feuding families in Verona, Italy.\",\n",
    "            \"type\": \"multi-hop\",\n",
    "            \"source\": \"synthetic\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is photosynthesis?\",\n",
    "            \"answer\": \"The process by which plants convert light energy into chemical energy\",\n",
    "            \"context\": \"Photosynthesis is the biological process by which plants, algae, and some bacteria convert light energy from the sun into chemical energy stored in glucose molecules. This process uses carbon dioxide and water as inputs.\",\n",
    "            \"type\": \"factual\",\n",
    "            \"source\": \"synthetic\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"If Einstein developed relativity and worked at Princeton, where did the theory of relativity originate?\",\n",
    "            \"answer\": \"The theory was developed by Einstein, who later worked at Princeton\",\n",
    "            \"context\": \"Albert Einstein developed the theory of relativity in the early 1900s while working at various institutions. He later joined Princeton University's Institute for Advanced Study where he continued his research until his death in 1955.\",\n",
    "            \"type\": \"multi-hop\",\n",
    "            \"source\": \"synthetic\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is the largest planet in our solar system?\",\n",
    "            \"answer\": \"Jupiter\",\n",
    "            \"context\": \"Jupiter is the largest planet in our solar system, with a mass greater than all other planets combined. It is a gas giant located fifth from the Sun and is known for its Great Red Spot, a giant storm larger than Earth.\",\n",
    "            \"type\": \"factual\",\n",
    "            \"source\": \"synthetic\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Who invented the telephone and when?\",\n",
    "            \"answer\": \"Alexander Graham Bell in 1876\",\n",
    "            \"context\": \"Alexander Graham Bell invented the telephone in 1876. Bell was a Scottish-born inventor and scientist who was awarded the first U.S. patent for the telephone on March 7, 1876. The first successful telephone call was made on March 10, 1876.\",\n",
    "            \"type\": \"factual\",\n",
    "            \"source\": \"synthetic\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"If Shakespeare wrote Hamlet and lived during Elizabeth I's reign, what era was Hamlet written in?\",\n",
    "            \"answer\": \"The Elizabethan era\",\n",
    "            \"context\": \"William Shakespeare wrote Hamlet during the Elizabethan era, specifically around 1600-1601. Queen Elizabeth I reigned from 1558 to 1603, and Shakespeare wrote most of his famous plays during this period of English history.\",\n",
    "            \"type\": \"multi-hop\",\n",
    "            \"source\": \"synthetic\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"      ‚úÖ Created {len(synthetic_data)} synthetic samples\")\n",
    "    return synthetic_data\n",
    "\n",
    "def create_expanded_dataset():\n",
    "    \"\"\"Create comprehensive evaluation dataset with real and synthetic data\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Creating comprehensive evaluation dataset...\")\n",
    "    \n",
    "    all_questions = []\n",
    "    all_documents = []\n",
    "    \n",
    "    # Process real datasets if available\n",
    "    if nq_dataset is not None:\n",
    "        nq_questions, nq_docs = process_natural_questions(nq_dataset)\n",
    "        all_questions.extend(nq_questions)\n",
    "        all_documents.extend(nq_docs)\n",
    "    \n",
    "    if hotpot_dataset is not None:\n",
    "        hq_questions, hq_docs = process_hotpot_qa(hotpot_dataset)\n",
    "        all_questions.extend(hq_questions)\n",
    "        all_documents.extend(hq_docs)\n",
    "    \n",
    "    # Add synthetic data (always include some for diversity)\n",
    "    synthetic_data = create_synthetic_dataset()\n",
    "    all_questions.extend(synthetic_data)\n",
    "    all_documents.extend([q[\"context\"] for q in synthetic_data])\n",
    "    \n",
    "    # Create document variations for better retrieval testing\n",
    "    print(\"   üìë Creating document variations for comprehensive testing...\")\n",
    "    expanded_docs = []\n",
    "    for doc in all_documents:\n",
    "        expanded_docs.append(doc)\n",
    "        # Add slight variations to test retrieval robustness\n",
    "        variation = doc.replace(\".\", \". This information is historically significant.\")\n",
    "        expanded_docs.append(variation)\n",
    "    \n",
    "    print(f\"   ‚úÖ Dataset expansion complete\")\n",
    "    return all_questions, expanded_docs\n",
    "\n",
    "# Create the final dataset\n",
    "questions, documents = create_expanded_dataset()\n",
    "\n",
    "print(f\"\\nüìä Final Dataset Summary:\")\n",
    "print(f\"   üìù Total Questions: {len(questions)}\")\n",
    "print(f\"   üìÑ Total Documents: {len(documents)}\")\n",
    "\n",
    "# Dataset statistics\n",
    "question_types = {}\n",
    "sources = {}\n",
    "for q in questions:\n",
    "    qtype = q.get(\"type\", \"unknown\")\n",
    "    question_types[qtype] = question_types.get(qtype, 0) + 1\n",
    "    \n",
    "    source = q.get(\"source\", \"unknown\")\n",
    "    sources[source] = sources.get(source, 0) + 1\n",
    "\n",
    "print(f\"\\nüìà Dataset Composition:\")\n",
    "print(f\"   Question Types:\")\n",
    "for qtype, count in question_types.items():\n",
    "    print(f\"     üìä {qtype}: {count} questions\")\n",
    "\n",
    "print(f\"   Data Sources:\")\n",
    "for source, count in sources.items():\n",
    "    emoji = \"üåê\" if source in [\"natural_questions\", \"hotpot_qa\"] else \"üé®\"\n",
    "    print(f\"     {emoji} {source}: {count} questions\")\n",
    "\n",
    "# Show sample questions\n",
    "print(f\"\\nüîç Sample Questions Preview:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, q in enumerate(questions[:3]):\n",
    "    source = q.get(\"source\", \"unknown\")\n",
    "    qtype = q.get(\"type\", \"unknown\")\n",
    "    print(f\"Q{i+1} [{qtype}] [{source}]:\")\n",
    "    print(f\"   ‚ùì Question: {q['question']}\")\n",
    "    print(f\"   ‚úÖ Answer: {q['answer']}\")\n",
    "    print(f\"   üìÑ Context: {q['context'][:100]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"üéØ Dataset ready for RAG system evaluation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6012383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Retrieval Systems with Sample Query\n",
    "print(\"üß™ Testing retrieval systems with sample query...\")\n",
    "\n",
    "sample_query = \"When was the Declaration of Independence signed?\"\n",
    "print(f\"Test Query: '{sample_query}'\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Test each retriever\n",
    "retrievers = {\n",
    "    \"BM25\": bm25_retriever,\n",
    "    \"Static Embeddings\": static_retriever,\n",
    "    \"InsightSpike Dynamic RAG\": insightspike_rag\n",
    "}\n",
    "\n",
    "if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "    retrievers[\"DPR (Dense)\"] = dpr_retriever\n",
    "\n",
    "for name, retriever in retrievers.items():\n",
    "    print(f\"\\nüîç {name} Results:\")\n",
    "    start_time = time.time()\n",
    "    results = retriever.retrieve(sample_query, k=3)\n",
    "    query_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   Query time: {query_time*1000:.1f}ms\")\n",
    "    \n",
    "    for i, (doc_idx, score) in enumerate(results):\n",
    "        doc_preview = documents[doc_idx][:100] + \"...\" if len(documents[doc_idx]) > 100 else documents[doc_idx]\n",
    "        print(f\"   {i+1}. Score: {score:.3f} | Doc: {doc_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd20e70",
   "metadata": {},
   "source": [
    "## üöÄ Running the Complete Evaluation\n",
    "\n",
    "Now let's run the comprehensive evaluation across all systems and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f53b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Complete Evaluation\n",
    "print(\"üöÄ Starting comprehensive RAG evaluation...\")\n",
    "print(\"‚è∞ This will take a few minutes to complete...\")\n",
    "\n",
    "# Configure evaluation parameters\n",
    "k_values = [1, 3, 5]\n",
    "print(f\"üìä Evaluating with k values: {k_values}\")\n",
    "\n",
    "# Initialize results storage\n",
    "all_results = {}\n",
    "\n",
    "# Evaluate each system\n",
    "for name, retriever in retrievers.items():\n",
    "    print(f\"\\nüîç Evaluating {name}...\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluate_retrieval_system(retriever, questions, documents, k_values)\n",
    "    all_results[name] = results\n",
    "    \n",
    "    # Display quick summary\n",
    "    avg_recall_5 = np.mean(results[\"recall_at_k\"][5])\n",
    "    avg_precision_5 = np.mean(results[\"precision_at_k\"][5])\n",
    "    avg_em = np.mean(results[\"exact_matches\"])\n",
    "    avg_f1 = np.mean(results[\"f1_scores\"])\n",
    "    avg_latency = np.mean(results[\"latencies\"])\n",
    "    \n",
    "    print(f\"   üìà Quick Summary:\")\n",
    "    print(f\"      Recall@5: {avg_recall_5:.3f}\")\n",
    "    print(f\"      Precision@5: {avg_precision_5:.3f}\")\n",
    "    print(f\"      Exact Match: {avg_em:.3f}\")\n",
    "    print(f\"      F1 Score: {avg_f1:.3f}\")\n",
    "    print(f\"      Avg Latency: {avg_latency*1000:.1f}ms\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation completed for all systems!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116daed0",
   "metadata": {},
   "source": [
    "## üìà Results Visualization and Analysis\n",
    "\n",
    "Let's create comprehensive visualizations to understand the performance differences between systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23036021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Main Visualization\n",
    "print(\"üìà Creating comprehensive results visualization...\")\n",
    "\n",
    "# Generate the main comparison visualization\n",
    "fig = create_rag_visualization(all_results, questions)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Main visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b40d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Performance Analysis\n",
    "print(\"üìä Detailed Performance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "systems = list(all_results.keys())\n",
    "\n",
    "# Create detailed comparison table\n",
    "comparison_data = []\n",
    "for system in systems:\n",
    "    results = all_results[system]\n",
    "    \n",
    "    row = {\n",
    "        \"System\": system,\n",
    "        \"Recall@1\": f\"{np.mean(results['recall_at_k'][1]):.3f} ¬± {np.std(results['recall_at_k'][1]):.3f}\",\n",
    "        \"Recall@3\": f\"{np.mean(results['recall_at_k'][3]):.3f} ¬± {np.std(results['recall_at_k'][3]):.3f}\",\n",
    "        \"Recall@5\": f\"{np.mean(results['recall_at_k'][5]):.3f} ¬± {np.std(results['recall_at_k'][5]):.3f}\",\n",
    "        \"Precision@5\": f\"{np.mean(results['precision_at_k'][5]):.3f} ¬± {np.std(results['precision_at_k'][5]):.3f}\",\n",
    "        \"Exact Match\": f\"{np.mean(results['exact_matches']):.3f} ¬± {np.std(results['exact_matches']):.3f}\",\n",
    "        \"F1 Score\": f\"{np.mean(results['f1_scores']):.3f} ¬± {np.std(results['f1_scores']):.3f}\",\n",
    "        \"Latency (ms)\": f\"{np.mean(results['latencies'])*1000:.1f} ¬± {np.std(results['latencies'])*1000:.1f}\"\n",
    "    }\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "display(HTML(comparison_df.to_html(index=False, table_id=\"comparison_table\")))\n",
    "\n",
    "# Statistical Significance Testing\n",
    "print(f\"\\nüî¨ Statistical Significance Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Compare InsightSpike against each baseline\n",
    "insightspike_name = \"InsightSpike Dynamic RAG\"\n",
    "if insightspike_name in all_results:\n",
    "    insightspike_recall5 = all_results[insightspike_name][\"recall_at_k\"][5]\n",
    "    insightspike_em = all_results[insightspike_name][\"exact_matches\"]\n",
    "    \n",
    "    for system in systems:\n",
    "        if system != insightspike_name:\n",
    "            system_recall5 = all_results[system][\"recall_at_k\"][5]\n",
    "            system_em = all_results[system][\"exact_matches\"]\n",
    "            \n",
    "            # T-test for Recall@5\n",
    "            _, p_recall = stats.ttest_ind(insightspike_recall5, system_recall5)\n",
    "            \n",
    "            # T-test for Exact Match\n",
    "            _, p_em = stats.ttest_ind(insightspike_em, system_em)\n",
    "            \n",
    "            # Calculate effect sizes (Cohen's d)\n",
    "            def cohens_d(group1, group2):\n",
    "                n1, n2 = len(group1), len(group2)\n",
    "                pooled_std = np.sqrt(((n1 - 1) * np.var(group1, ddof=1) + \n",
    "                                     (n2 - 1) * np.var(group2, ddof=1)) / (n1 + n2 - 2))\n",
    "                return (np.mean(group1) - np.mean(group2)) / pooled_std\n",
    "            \n",
    "            recall_effect = cohens_d(insightspike_recall5, system_recall5)\n",
    "            em_effect = cohens_d(insightspike_em, system_em)\n",
    "            \n",
    "            print(f\"\\nInsightSpike vs {system}:\")\n",
    "            print(f\"  Recall@5: p={p_recall:.4f}, Cohen's d={recall_effect:.3f}\")\n",
    "            print(f\"  Exact Match: p={p_em:.4f}, Cohen's d={em_effect:.3f}\")\n",
    "            \n",
    "            # Interpretation\n",
    "            if p_recall < 0.05:\n",
    "                print(f\"  Recall@5: Statistically significant difference ‚úÖ\")\n",
    "            else:\n",
    "                print(f\"  Recall@5: No significant difference ‚ùå\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ffa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by Question Type Analysis\n",
    "print(\"üéØ Performance by Question Type\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Separate results by question type\n",
    "factual_questions = [(i, q) for i, q in enumerate(questions) if q.get(\"type\") == \"factual\"]\n",
    "multihop_questions = [(i, q) for i, q in enumerate(questions) if q.get(\"type\") == \"multi-hop\"]\n",
    "\n",
    "print(f\"Factual questions: {len(factual_questions)}\")\n",
    "print(f\"Multi-hop questions: {len(multihop_questions)}\")\n",
    "\n",
    "# Calculate performance by question type\n",
    "type_performance = {}\n",
    "\n",
    "for system in systems:\n",
    "    results = all_results[system]\n",
    "    \n",
    "    # Factual performance\n",
    "    factual_recall5 = [results[\"recall_at_k\"][5][i] for i, _ in factual_questions]\n",
    "    factual_em = [results[\"exact_matches\"][i] for i, _ in factual_questions]\n",
    "    \n",
    "    # Multi-hop performance\n",
    "    multihop_recall5 = [results[\"recall_at_k\"][5][i] for i, _ in multihop_questions]\n",
    "    multihop_em = [results[\"exact_matches\"][i] for i, _ in multihop_questions]\n",
    "    \n",
    "    type_performance[system] = {\n",
    "        \"factual\": {\n",
    "            \"recall5\": np.mean(factual_recall5) if factual_recall5 else 0,\n",
    "            \"em\": np.mean(factual_em) if factual_em else 0\n",
    "        },\n",
    "        \"multihop\": {\n",
    "            \"recall5\": np.mean(multihop_recall5) if multihop_recall5 else 0,\n",
    "            \"em\": np.mean(multihop_em) if multihop_em else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Visualize question type performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Recall@5 by question type\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(systems))\n",
    "width = 0.35\n",
    "\n",
    "factual_recall = [type_performance[sys][\"factual\"][\"recall5\"] for sys in systems]\n",
    "multihop_recall = [type_performance[sys][\"multihop\"][\"recall5\"] for sys in systems]\n",
    "\n",
    "ax1.bar(x - width/2, factual_recall, width, label='Factual', alpha=0.8)\n",
    "ax1.bar(x + width/2, multihop_recall, width, label='Multi-hop', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('System')\n",
    "ax1.set_ylabel('Recall@5')\n",
    "ax1.set_title('Recall@5 by Question Type')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([s.replace(' ', '\\n') for s in systems], fontsize=9)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Exact Match by question type\n",
    "ax2 = axes[1]\n",
    "factual_em = [type_performance[sys][\"factual\"][\"em\"] for sys in systems]\n",
    "multihop_em = [type_performance[sys][\"multihop\"][\"em\"] for sys in systems]\n",
    "\n",
    "ax2.bar(x - width/2, factual_em, width, label='Factual', alpha=0.8)\n",
    "ax2.bar(x + width/2, multihop_em, width, label='Multi-hop', alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('System')\n",
    "ax2.set_ylabel('Exact Match')\n",
    "ax2.set_title('Exact Match by Question Type')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([s.replace(' ', '\\n') for s in systems], fontsize=9)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed breakdown\n",
    "print(f\"\\nüìã Detailed Question Type Performance:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'System':<25} {'Factual R@5':<12} {'Factual EM':<11} {'Multi-hop R@5':<14} {'Multi-hop EM':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for system in systems:\n",
    "    perf = type_performance[system]\n",
    "    print(f\"{system:<25} {perf['factual']['recall5']:<12.3f} {perf['factual']['em']:<11.3f} \"\n",
    "          f\"{perf['multihop']['recall5']:<14.3f} {perf['multihop']['em']:<12.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133fe83",
   "metadata": {},
   "source": [
    "## üíæ Save Results and Create Download Package\n",
    "\n",
    "Let's save all our experimental results and create a downloadable package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617705e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Experimental Results\n",
    "print(\"üíæ Saving experimental results...\")\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path(\"rag_comparison_results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Prepare comprehensive results data\n",
    "results_data = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"experiment_type\": \"dynamic_rag_comparison\",\n",
    "    \"environment\": \"Google Colab\" if IN_COLAB else \"Local\",\n",
    "    \"dataset_info\": {\n",
    "        \"num_questions\": len(questions),\n",
    "        \"num_documents\": len(documents),\n",
    "        \"question_types\": {\n",
    "            \"factual\": len([q for q in questions if q.get(\"type\") == \"factual\"]),\n",
    "            \"multi_hop\": len([q for q in questions if q.get(\"type\") == \"multi-hop\"])\n",
    "        }\n",
    "    },\n",
    "    \"systems_evaluated\": list(all_results.keys()),\n",
    "    \"evaluation_metrics\": {\n",
    "        \"recall_at_k\": k_values,\n",
    "        \"precision_at_k\": k_values,\n",
    "        \"exact_match\": True,\n",
    "        \"f1_score\": True,\n",
    "        \"latency\": True\n",
    "    },\n",
    "    \"initialization_times\": init_times,\n",
    "    \"detailed_results\": all_results,\n",
    "    \"question_type_performance\": type_performance\n",
    "}\n",
    "\n",
    "# Convert numpy arrays to lists for JSON serialization\n",
    "def convert_numpy(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy(item) for item in obj]\n",
    "    return obj\n",
    "\n",
    "# Save JSON data\n",
    "json_path = results_dir / f\"rag_comparison_results_{timestamp}.json\"\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(convert_numpy(results_data), f, indent=2)\n",
    "\n",
    "print(f\"üìä Results saved to: {json_path}\")\n",
    "\n",
    "# Save main figure\n",
    "fig.savefig(results_dir / f\"rag_comparison_visualization_{timestamp}.png\", \n",
    "           dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Save question type analysis figure\n",
    "plt.savefig(results_dir / f\"question_type_analysis_{timestamp}.png\", \n",
    "           dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(f\"üìà Visualizations saved to: {results_dir}/\")\n",
    "\n",
    "# Create summary CSV\n",
    "summary_data = []\n",
    "for system in systems:\n",
    "    summary_data.append({\n",
    "        \"System\": system,\n",
    "        \"Recall@1\": np.mean(all_results[system][\"recall_at_k\"][1]),\n",
    "        \"Recall@3\": np.mean(all_results[system][\"recall_at_k\"][3]),\n",
    "        \"Recall@5\": np.mean(all_results[system][\"recall_at_k\"][5]),\n",
    "        \"Precision@5\": np.mean(all_results[system][\"precision_at_k\"][5]),\n",
    "        \"Exact_Match\": np.mean(all_results[system][\"exact_matches\"]),\n",
    "        \"F1_Score\": np.mean(all_results[system][\"f1_scores\"]),\n",
    "        \"Latency_ms\": np.mean(all_results[system][\"latencies\"]) * 1000,\n",
    "        \"Factual_Recall@5\": type_performance[system][\"factual\"][\"recall5\"],\n",
    "        \"Factual_EM\": type_performance[system][\"factual\"][\"em\"],\n",
    "        \"MultiHop_Recall@5\": type_performance[system][\"multihop\"][\"recall5\"],\n",
    "        \"MultiHop_EM\": type_performance[system][\"multihop\"][\"em\"]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "csv_path = results_dir / f\"rag_summary_results_{timestamp}.csv\"\n",
    "summary_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"üìÑ Summary CSV saved to: {csv_path}\")\n",
    "print(\"\\n‚úÖ All results saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f0f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Results (for Colab users)\n",
    "if IN_COLAB:\n",
    "    print(\"üì• Preparing files for download...\")\n",
    "    \n",
    "    # Create a zip file with all results\n",
    "    import zipfile\n",
    "    \n",
    "    zip_path = f\"dynamic_rag_comparison_results_{timestamp}.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "        # Add all files from results directory\n",
    "        for file_path in results_dir.glob(\"*\"):\n",
    "            zipf.write(file_path, file_path.name)\n",
    "        \n",
    "        # Add the experiment script\n",
    "        zipf.write(\"experiments/colab_experiments/dynamic_rag_comparison/dynamic_rag_experiment.py\", \n",
    "                   \"dynamic_rag_experiment.py\")\n",
    "        \n",
    "        # Add this notebook\n",
    "        try:\n",
    "            zipf.write(\"experiments/colab_experiments/dynamic_rag_comparison/dynamic_rag_colab.ipynb\", \n",
    "                       \"dynamic_rag_colab.ipynb\")\n",
    "        except:\n",
    "            pass  # File might not exist in Colab\n",
    "    \n",
    "    print(f\"üì¶ Created zip file: {zip_path}\")\n",
    "    \n",
    "    # Download files\n",
    "    from google.colab import files\n",
    "    \n",
    "    try:\n",
    "        files.download(zip_path)\n",
    "        print(\"‚úÖ Download initiated! Check your browser's download folder.\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Automatic download failed. You can manually download the files from the file browser.\")\n",
    "        print(\"üìÅ Available files:\")\n",
    "        !ls -la rag_comparison_results/\n",
    "        !ls -la *.zip\n",
    "else:\n",
    "    print(\"üìÅ Results saved locally in the rag_comparison_results/ directory\")\n",
    "    print(\"üìã Available files:\")\n",
    "    !ls -la rag_comparison_results/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ddc43",
   "metadata": {},
   "source": [
    "## üì¶ Experiment Results Download\n",
    "\n",
    "Download your experimental results for further analysis or sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030026ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Experiment Results\n",
    "print(\"üì¶ Preparing experiment results for download...\")\n",
    "\n",
    "def create_downloadable_results():\n",
    "    \"\"\"Create a downloadable package of all experimental results\"\"\"\n",
    "    import zipfile\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Create download directory\n",
    "    download_dir = Path(\"downloads\")\n",
    "    download_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    zip_filename = f\"rag_experiment_results_{timestamp}.zip\"\n",
    "    zip_path = download_dir / zip_filename\n",
    "    \n",
    "    print(f\"üìù Creating results package: {zip_filename}\")\n",
    "    \n",
    "    # Create comprehensive results package\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        \n",
    "        # Add experiment results\n",
    "        results_dir = Path(\"data/rag_experiments/results\")\n",
    "        if results_dir.exists():\n",
    "            for file_path in results_dir.rglob(\"*\"):\n",
    "                if file_path.is_file():\n",
    "                    arcname = f\"results/{file_path.relative_to(results_dir)}\"\n",
    "                    zipf.write(file_path, arcname)\n",
    "                    print(f\"   üìÑ Added: {arcname}\")\n",
    "        \n",
    "        # Add visualizations\n",
    "        viz_dir = Path(\"data/rag_experiments/visualizations\")\n",
    "        if viz_dir.exists():\n",
    "            for file_path in viz_dir.rglob(\"*.png\"):\n",
    "                if file_path.is_file():\n",
    "                    arcname = f\"visualizations/{file_path.name}\"\n",
    "                    zipf.write(file_path, arcname)\n",
    "                    print(f\"   üñºÔ∏è  Added: {arcname}\")\n",
    "        \n",
    "        # Add baseline comparisons\n",
    "        baselines_dir = Path(\"data/rag_experiments/baselines\")\n",
    "        if baselines_dir.exists():\n",
    "            for baseline_dir in baselines_dir.iterdir():\n",
    "                if baseline_dir.is_dir():\n",
    "                    results_files = baseline_dir.rglob(\"*.json\")\n",
    "                    for file_path in results_files:\n",
    "                        arcname = f\"baselines/{baseline_dir.name}/{file_path.name}\"\n",
    "                        zipf.write(file_path, arcname)\n",
    "                        print(f\"   üìä Added: {arcname}\")\n",
    "        \n",
    "        # Add experiment summary\n",
    "        summary = {\n",
    "            \"experiment_type\": \"Dynamic RAG Comparison\",\n",
    "            \"timestamp\": timestamp,\n",
    "            \"notebook_version\": \"v1.0.0\",\n",
    "            \"description\": \"Comparison of InsightSpike-AI dynamic RAG against baseline methods\",\n",
    "            \"datasets\": [\"NaturalQuestions_sample\", \"HotpotQA_sample\"],\n",
    "            \"methods_compared\": [\"BM25\", \"Static Embeddings\", \"DPR\", \"InsightSpike RAG\"],\n",
    "            \"metrics\": [\"Recall@k\", \"Precision@k\", \"Exact Match\", \"F1 Score\", \"Latency\"]\n",
    "        }\n",
    "        \n",
    "        summary_path = download_dir / \"experiment_summary.json\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        zipf.write(summary_path, \"experiment_summary.json\")\n",
    "        \n",
    "        print(f\"   üìã Added: experiment_summary.json\")\n",
    "    \n",
    "    file_size = zip_path.stat().st_size / (1024 * 1024)  # MB\n",
    "    print(f\"\\n‚úÖ Results package created successfully!\")\n",
    "    print(f\"üì¶ File: {zip_path}\")\n",
    "    print(f\"üìè Size: {file_size:.2f} MB\")\n",
    "    \n",
    "    return zip_path\n",
    "\n",
    "# Create and prepare results for download\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        # Create downloadable package\n",
    "        zip_path = create_downloadable_results()\n",
    "        \n",
    "        # Download in Colab\n",
    "        from google.colab import files\n",
    "        files.download(str(zip_path))\n",
    "        print(\"‚¨áÔ∏è  Download started in Colab!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating download package: {e}\")\n",
    "        print(\"üí° You can manually download files from the file browser\")\n",
    "        \n",
    "        # Show available files for manual download\n",
    "        results_dir = Path(\"data/rag_experiments/results\")\n",
    "        if results_dir.exists():\n",
    "            print(f\"\\nüìã Available result files:\")\n",
    "            for file_path in results_dir.rglob(\"*\"):\n",
    "                if file_path.is_file():\n",
    "                    print(f\"   üìÑ {file_path}\")\n",
    "else:\n",
    "    # Local environment - just create the package\n",
    "    zip_path = create_downloadable_results()\n",
    "    print(f\"üíæ Results saved locally: {zip_path}\")\n",
    "    print(\"üìÅ Open the 'downloads' folder to access your results\")\n",
    "\n",
    "print(f\"\\nüéâ Experiment complete! Your results are ready for analysis.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
