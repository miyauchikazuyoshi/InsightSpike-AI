{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3759305",
   "metadata": {},
   "source": [
    "# üîç InsightSpike-AI: Dynamic RAG Comparison Experiment\n",
    "## Evaluating Dynamic RAG Construction vs Existing Methods\n",
    "\n",
    "This notebook compares InsightSpike-AI's dynamic RAG construction capabilities against established baselines using standard question-answering benchmarks.\n",
    "\n",
    "### Experimental Design\n",
    "- **Datasets**: Simulated NaturalQuestions & HotpotQA samples\n",
    "- **Baselines**: BM25, Static Embeddings, DPR (Dense Passage Retrieval)\n",
    "- **Metrics**: Recall@k, Exact Match (EM), F1 Score, Inference Latency\n",
    "\n",
    "### InsightSpike-AI Dynamic RAG Features\n",
    "- **Adaptive Weighting**: Dynamically adjusts retrieval strategy based on query characteristics\n",
    "- **Intrinsic Motivation**: Uses ŒîGED √ó ŒîIG for document selection enhancement\n",
    "- **Multi-Strategy Fusion**: Combines lexical, semantic, and learned retrieval methods\n",
    "- **Context-Aware Memory**: Maintains retrieval history for improved performance\n",
    "\n",
    "### Expected Outcomes\n",
    "We expect InsightSpike-AI's dynamic approach to show:\n",
    "1. Higher recall and precision across different k values\n",
    "2. Better handling of both factual and multi-hop questions\n",
    "3. Competitive or superior latency performance\n",
    "4. More robust performance across question types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e718812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Environment Setup and Package Installation\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üîß Running in Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"üîß Running in local environment\")\n",
    "\n",
    "# Install required packages for Colab\n",
    "if IN_COLAB:\n",
    "    print(\"üì¶ Installing required packages...\")\n",
    "    # Install compatible versions to avoid meta tensor issues\n",
    "    !pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cpu\n",
    "    !pip install sentence-transformers==2.7.0  # Compatible with PyTorch 2.2.2\n",
    "    !pip install transformers==4.30.0  # Ensure compatibility\n",
    "    !pip install numpy==1.26.4  # Avoid NumPy 2.x compatibility issues\n",
    "    !pip install scikit-learn pandas matplotlib seaborn\n",
    "    !pip install plotly kaleido\n",
    "    !pip install faiss-cpu networkx\n",
    "    print(\"‚úÖ Package installation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Repository and Setup Environment\n",
    "if IN_COLAB:\n",
    "    # Get GitHub token from Colab secrets for private repository\n",
    "    from google.colab import userdata\n",
    "    \n",
    "    try:\n",
    "        github_token = userdata.get('GITHUB_TOKEN')\n",
    "        print(\"‚úÖ GitHub token found in secrets\")\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå GitHub token not found in secrets.\")\n",
    "        print(\"üìù Please add GITHUB_TOKEN to Colab secrets:\")\n",
    "        print(\"   1. Click the key icon (üîë) in the left sidebar\")\n",
    "        print(\"   2. Add new secret: Name='GITHUB_TOKEN', Value='your_github_token'\")\n",
    "        print(\"   3. Get token from: https://github.com/settings/tokens\")\n",
    "        raise e\n",
    "    \n",
    "    # Clone the private repository\n",
    "    print(\"üì• Cloning InsightSpike-AI repository...\")\n",
    "    clone_url = f\"https://{github_token}@github.com/miyauchikazuyoshi/InsightSpike-AI.git\"\n",
    "    !git clone $clone_url\n",
    "    \n",
    "    # Change to project directory\n",
    "    os.chdir('/content/InsightSpike-AI')\n",
    "    sys.path.append('/content/InsightSpike-AI')\n",
    "    print(\"üìÅ Changed to project directory\")\n",
    "else:\n",
    "    # Assume we're already in the project directory\n",
    "    print(\"üìÅ Using local project directory\")\n",
    "\n",
    "# Add experiment module to path\n",
    "sys.path.append('experiments/colab_experiments/dynamic_rag_comparison')\n",
    "\n",
    "# Test import of experiment modules\n",
    "try:\n",
    "    from dynamic_rag_experiment import (\n",
    "        run_dynamic_rag_experiment,\n",
    "        BM25Retriever,\n",
    "        StaticEmbeddingRetriever,\n",
    "        DPRRetriever,\n",
    "        InsightSpikeRAG,\n",
    "        create_expanded_dataset,\n",
    "        evaluate_retrieval_system,\n",
    "        create_rag_visualization\n",
    "    )\n",
    "    print(\"‚úÖ Successfully imported experiment modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"üìã Available files:\")\n",
    "    !ls -la experiments/colab_experiments/dynamic_rag_comparison/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f49aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üéØ Environment setup complete!\")\n",
    "\n",
    "# Check available packages\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"‚úÖ Sentence Transformers available\")\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Sentence Transformers not available - using fallback methods\")\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    print(\"‚úÖ Scikit-learn available\")\n",
    "    SKLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Scikit-learn not available - using simplified methods\")\n",
    "    SKLEARN_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed29f0",
   "metadata": {},
   "source": [
    "## üìä Dataset Preparation and Preview\n",
    "\n",
    "Let's examine the evaluation dataset we'll be using for this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and Examine the Evaluation Dataset\n",
    "print(\"üìä Creating evaluation dataset...\")\n",
    "\n",
    "# Load the expanded dataset\n",
    "questions, documents = create_expanded_dataset()\n",
    "\n",
    "print(f\"‚úÖ Dataset created:\")\n",
    "print(f\"   üìù Questions: {len(questions)}\")\n",
    "print(f\"   üìÑ Documents: {len(documents)}\")\n",
    "\n",
    "# Display dataset statistics\n",
    "question_types = {}\n",
    "for q in questions:\n",
    "    qtype = q.get(\"type\", \"unknown\")\n",
    "    question_types[qtype] = question_types.get(qtype, 0) + 1\n",
    "\n",
    "print(f\"\\nüìà Question Type Distribution:\")\n",
    "for qtype, count in question_types.items():\n",
    "    print(f\"   {qtype}: {count} questions\")\n",
    "\n",
    "# Show sample questions\n",
    "print(f\"\\nüîç Sample Questions:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, q in enumerate(questions[:3]):\n",
    "    print(f\"Q{i+1} [{q.get('type', 'unknown')}]: {q['question']}\")\n",
    "    print(f\"   Answer: {q['answer']}\")\n",
    "    print(f\"   Context: {q['context'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15264526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Analysis\n",
    "print(\"üìÑ Document Corpus Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Calculate document statistics\n",
    "doc_lengths = [len(doc.split()) for doc in documents]\n",
    "total_tokens = sum(doc_lengths)\n",
    "avg_length = np.mean(doc_lengths)\n",
    "std_length = np.std(doc_lengths)\n",
    "\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "print(f\"Total tokens: {total_tokens:,}\")\n",
    "print(f\"Average doc length: {avg_length:.1f} ¬± {std_length:.1f} tokens\")\n",
    "print(f\"Min doc length: {min(doc_lengths)} tokens\")\n",
    "print(f\"Max doc length: {max(doc_lengths)} tokens\")\n",
    "\n",
    "# Visualize document length distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(doc_lengths, bins=15, alpha=0.7, color='skyblue')\n",
    "plt.xlabel('Document Length (tokens)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Document Length Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show sample documents\n",
    "plt.subplot(1, 2, 2)\n",
    "sample_docs = documents[:5]\n",
    "doc_indices = range(1, len(sample_docs) + 1)\n",
    "sample_lengths = [len(doc.split()) for doc in sample_docs]\n",
    "\n",
    "plt.bar(doc_indices, sample_lengths, alpha=0.7, color='lightcoral')\n",
    "plt.xlabel('Document Index')\n",
    "plt.ylabel('Length (tokens)')\n",
    "plt.title('Sample Document Lengths')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display sample documents\n",
    "print(f\"\\nüìÑ Sample Documents:\")\n",
    "print(\"-\" * 50)\n",
    "for i, doc in enumerate(documents[:3]):\n",
    "    print(f\"Doc {i+1}: {doc[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed42b14",
   "metadata": {},
   "source": [
    "## üîß Retrieval System Initialization\n",
    "\n",
    "Now let's initialize and test all the retrieval systems we'll be comparing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb80048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize All Retrieval Systems\n",
    "print(\"üîß Initializing retrieval systems...\")\n",
    "\n",
    "# Track initialization time for each system\n",
    "init_times = {}\n",
    "\n",
    "# 1. BM25 Retriever\n",
    "print(\"\\nüìä Initializing BM25 Retriever...\")\n",
    "start_time = time.time()\n",
    "bm25_retriever = BM25Retriever(documents)\n",
    "init_times[\"BM25\"] = time.time() - start_time\n",
    "print(f\"   ‚úÖ BM25 initialized in {init_times['BM25']:.3f}s\")\n",
    "\n",
    "# 2. Static Embedding Retriever\n",
    "print(\"\\nüî¢ Initializing Static Embedding Retriever...\")\n",
    "start_time = time.time()\n",
    "static_retriever = StaticEmbeddingRetriever(documents)\n",
    "init_times[\"Static Embeddings\"] = time.time() - start_time\n",
    "print(f\"   ‚úÖ Static Embeddings initialized in {init_times['Static Embeddings']:.3f}s\")\n",
    "\n",
    "# 3. DPR Retriever (if available)\n",
    "if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "    print(\"\\nüß† Initializing DPR-style Dense Retriever...\")\n",
    "    start_time = time.time()\n",
    "    dpr_retriever = DPRRetriever(documents)\n",
    "    init_times[\"DPR (Dense)\"] = time.time() - start_time\n",
    "    print(f\"   ‚úÖ DPR initialized in {init_times['DPR (Dense)']:.3f}s\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è DPR not available - skipping dense retrieval\")\n",
    "\n",
    "# 4. InsightSpike Dynamic RAG\n",
    "print(\"\\nüöÄ Initializing InsightSpike Dynamic RAG...\")\n",
    "start_time = time.time()\n",
    "insightspike_rag = InsightSpikeRAG(documents)\n",
    "init_times[\"InsightSpike Dynamic RAG\"] = time.time() - start_time\n",
    "print(f\"   ‚úÖ InsightSpike RAG initialized in {init_times['InsightSpike Dynamic RAG']:.3f}s\")\n",
    "\n",
    "# Display initialization summary\n",
    "print(f\"\\n‚è±Ô∏è Initialization Times Summary:\")\n",
    "print(\"-\" * 40)\n",
    "for system, init_time in init_times.items():\n",
    "    print(f\"{system:<25}: {init_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6012383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Retrieval Systems with Sample Query\n",
    "print(\"üß™ Testing retrieval systems with sample query...\")\n",
    "\n",
    "sample_query = \"When was the Declaration of Independence signed?\"\n",
    "print(f\"Test Query: '{sample_query}'\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Test each retriever\n",
    "retrievers = {\n",
    "    \"BM25\": bm25_retriever,\n",
    "    \"Static Embeddings\": static_retriever,\n",
    "    \"InsightSpike Dynamic RAG\": insightspike_rag\n",
    "}\n",
    "\n",
    "if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "    retrievers[\"DPR (Dense)\"] = dpr_retriever\n",
    "\n",
    "for name, retriever in retrievers.items():\n",
    "    print(f\"\\nüîç {name} Results:\")\n",
    "    start_time = time.time()\n",
    "    results = retriever.retrieve(sample_query, k=3)\n",
    "    query_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   Query time: {query_time*1000:.1f}ms\")\n",
    "    \n",
    "    for i, (doc_idx, score) in enumerate(results):\n",
    "        doc_preview = documents[doc_idx][:100] + \"...\" if len(documents[doc_idx]) > 100 else documents[doc_idx]\n",
    "        print(f\"   {i+1}. Score: {score:.3f} | Doc: {doc_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd20e70",
   "metadata": {},
   "source": [
    "## üöÄ Running the Complete Evaluation\n",
    "\n",
    "Now let's run the comprehensive evaluation across all systems and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f53b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Complete Evaluation\n",
    "print(\"üöÄ Starting comprehensive RAG evaluation...\")\n",
    "print(\"‚è∞ This will take a few minutes to complete...\")\n",
    "\n",
    "# Configure evaluation parameters\n",
    "k_values = [1, 3, 5]\n",
    "print(f\"üìä Evaluating with k values: {k_values}\")\n",
    "\n",
    "# Initialize results storage\n",
    "all_results = {}\n",
    "\n",
    "# Evaluate each system\n",
    "for name, retriever in retrievers.items():\n",
    "    print(f\"\\nüîç Evaluating {name}...\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluate_retrieval_system(retriever, questions, documents, k_values)\n",
    "    all_results[name] = results\n",
    "    \n",
    "    # Display quick summary\n",
    "    avg_recall_5 = np.mean(results[\"recall_at_k\"][5])\n",
    "    avg_precision_5 = np.mean(results[\"precision_at_k\"][5])\n",
    "    avg_em = np.mean(results[\"exact_matches\"])\n",
    "    avg_f1 = np.mean(results[\"f1_scores\"])\n",
    "    avg_latency = np.mean(results[\"latencies\"])\n",
    "    \n",
    "    print(f\"   üìà Quick Summary:\")\n",
    "    print(f\"      Recall@5: {avg_recall_5:.3f}\")\n",
    "    print(f\"      Precision@5: {avg_precision_5:.3f}\")\n",
    "    print(f\"      Exact Match: {avg_em:.3f}\")\n",
    "    print(f\"      F1 Score: {avg_f1:.3f}\")\n",
    "    print(f\"      Avg Latency: {avg_latency*1000:.1f}ms\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation completed for all systems!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116daed0",
   "metadata": {},
   "source": [
    "## üìà Results Visualization and Analysis\n",
    "\n",
    "Let's create comprehensive visualizations to understand the performance differences between systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23036021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Main Visualization\n",
    "print(\"üìà Creating comprehensive results visualization...\")\n",
    "\n",
    "# Generate the main comparison visualization\n",
    "fig = create_rag_visualization(all_results, questions)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Main visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b40d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Performance Analysis\n",
    "print(\"üìä Detailed Performance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "systems = list(all_results.keys())\n",
    "\n",
    "# Create detailed comparison table\n",
    "comparison_data = []\n",
    "for system in systems:\n",
    "    results = all_results[system]\n",
    "    \n",
    "    row = {\n",
    "        \"System\": system,\n",
    "        \"Recall@1\": f\"{np.mean(results['recall_at_k'][1]):.3f} ¬± {np.std(results['recall_at_k'][1]):.3f}\",\n",
    "        \"Recall@3\": f\"{np.mean(results['recall_at_k'][3]):.3f} ¬± {np.std(results['recall_at_k'][3]):.3f}\",\n",
    "        \"Recall@5\": f\"{np.mean(results['recall_at_k'][5]):.3f} ¬± {np.std(results['recall_at_k'][5]):.3f}\",\n",
    "        \"Precision@5\": f\"{np.mean(results['precision_at_k'][5]):.3f} ¬± {np.std(results['precision_at_k'][5]):.3f}\",\n",
    "        \"Exact Match\": f\"{np.mean(results['exact_matches']):.3f} ¬± {np.std(results['exact_matches']):.3f}\",\n",
    "        \"F1 Score\": f\"{np.mean(results['f1_scores']):.3f} ¬± {np.std(results['f1_scores']):.3f}\",\n",
    "        \"Latency (ms)\": f\"{np.mean(results['latencies'])*1000:.1f} ¬± {np.std(results['latencies'])*1000:.1f}\"\n",
    "    }\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "display(HTML(comparison_df.to_html(index=False, table_id=\"comparison_table\")))\n",
    "\n",
    "# Statistical Significance Testing\n",
    "print(f\"\\nüî¨ Statistical Significance Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Compare InsightSpike against each baseline\n",
    "insightspike_name = \"InsightSpike Dynamic RAG\"\n",
    "if insightspike_name in all_results:\n",
    "    insightspike_recall5 = all_results[insightspike_name][\"recall_at_k\"][5]\n",
    "    insightspike_em = all_results[insightspike_name][\"exact_matches\"]\n",
    "    \n",
    "    for system in systems:\n",
    "        if system != insightspike_name:\n",
    "            system_recall5 = all_results[system][\"recall_at_k\"][5]\n",
    "            system_em = all_results[system][\"exact_matches\"]\n",
    "            \n",
    "            # T-test for Recall@5\n",
    "            _, p_recall = stats.ttest_ind(insightspike_recall5, system_recall5)\n",
    "            \n",
    "            # T-test for Exact Match\n",
    "            _, p_em = stats.ttest_ind(insightspike_em, system_em)\n",
    "            \n",
    "            # Calculate effect sizes (Cohen's d)\n",
    "            def cohens_d(group1, group2):\n",
    "                n1, n2 = len(group1), len(group2)\n",
    "                pooled_std = np.sqrt(((n1 - 1) * np.var(group1, ddof=1) + \n",
    "                                     (n2 - 1) * np.var(group2, ddof=1)) / (n1 + n2 - 2))\n",
    "                return (np.mean(group1) - np.mean(group2)) / pooled_std\n",
    "            \n",
    "            recall_effect = cohens_d(insightspike_recall5, system_recall5)\n",
    "            em_effect = cohens_d(insightspike_em, system_em)\n",
    "            \n",
    "            print(f\"\\nInsightSpike vs {system}:\")\n",
    "            print(f\"  Recall@5: p={p_recall:.4f}, Cohen's d={recall_effect:.3f}\")\n",
    "            print(f\"  Exact Match: p={p_em:.4f}, Cohen's d={em_effect:.3f}\")\n",
    "            \n",
    "            # Interpretation\n",
    "            if p_recall < 0.05:\n",
    "                print(f\"  Recall@5: Statistically significant difference ‚úÖ\")\n",
    "            else:\n",
    "                print(f\"  Recall@5: No significant difference ‚ùå\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ffa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by Question Type Analysis\n",
    "print(\"üéØ Performance by Question Type\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Separate results by question type\n",
    "factual_questions = [(i, q) for i, q in enumerate(questions) if q.get(\"type\") == \"factual\"]\n",
    "multihop_questions = [(i, q) for i, q in enumerate(questions) if q.get(\"type\") == \"multi-hop\"]\n",
    "\n",
    "print(f\"Factual questions: {len(factual_questions)}\")\n",
    "print(f\"Multi-hop questions: {len(multihop_questions)}\")\n",
    "\n",
    "# Calculate performance by question type\n",
    "type_performance = {}\n",
    "\n",
    "for system in systems:\n",
    "    results = all_results[system]\n",
    "    \n",
    "    # Factual performance\n",
    "    factual_recall5 = [results[\"recall_at_k\"][5][i] for i, _ in factual_questions]\n",
    "    factual_em = [results[\"exact_matches\"][i] for i, _ in factual_questions]\n",
    "    \n",
    "    # Multi-hop performance\n",
    "    multihop_recall5 = [results[\"recall_at_k\"][5][i] for i, _ in multihop_questions]\n",
    "    multihop_em = [results[\"exact_matches\"][i] for i, _ in multihop_questions]\n",
    "    \n",
    "    type_performance[system] = {\n",
    "        \"factual\": {\n",
    "            \"recall5\": np.mean(factual_recall5) if factual_recall5 else 0,\n",
    "            \"em\": np.mean(factual_em) if factual_em else 0\n",
    "        },\n",
    "        \"multihop\": {\n",
    "            \"recall5\": np.mean(multihop_recall5) if multihop_recall5 else 0,\n",
    "            \"em\": np.mean(multihop_em) if multihop_em else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Visualize question type performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Recall@5 by question type\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(systems))\n",
    "width = 0.35\n",
    "\n",
    "factual_recall = [type_performance[sys][\"factual\"][\"recall5\"] for sys in systems]\n",
    "multihop_recall = [type_performance[sys][\"multihop\"][\"recall5\"] for sys in systems]\n",
    "\n",
    "ax1.bar(x - width/2, factual_recall, width, label='Factual', alpha=0.8)\n",
    "ax1.bar(x + width/2, multihop_recall, width, label='Multi-hop', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('System')\n",
    "ax1.set_ylabel('Recall@5')\n",
    "ax1.set_title('Recall@5 by Question Type')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([s.replace(' ', '\\n') for s in systems], fontsize=9)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Exact Match by question type\n",
    "ax2 = axes[1]\n",
    "factual_em = [type_performance[sys][\"factual\"][\"em\"] for sys in systems]\n",
    "multihop_em = [type_performance[sys][\"multihop\"][\"em\"] for sys in systems]\n",
    "\n",
    "ax2.bar(x - width/2, factual_em, width, label='Factual', alpha=0.8)\n",
    "ax2.bar(x + width/2, multihop_em, width, label='Multi-hop', alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('System')\n",
    "ax2.set_ylabel('Exact Match')\n",
    "ax2.set_title('Exact Match by Question Type')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([s.replace(' ', '\\n') for s in systems], fontsize=9)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed breakdown\n",
    "print(f\"\\nüìã Detailed Question Type Performance:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'System':<25} {'Factual R@5':<12} {'Factual EM':<11} {'Multi-hop R@5':<14} {'Multi-hop EM':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for system in systems:\n",
    "    perf = type_performance[system]\n",
    "    print(f\"{system:<25} {perf['factual']['recall5']:<12.3f} {perf['factual']['em']:<11.3f} \"\n",
    "          f\"{perf['multihop']['recall5']:<14.3f} {perf['multihop']['em']:<12.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133fe83",
   "metadata": {},
   "source": [
    "## üíæ Save Results and Create Download Package\n",
    "\n",
    "Let's save all our experimental results and create a downloadable package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617705e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Experimental Results\n",
    "print(\"üíæ Saving experimental results...\")\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path(\"rag_comparison_results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Prepare comprehensive results data\n",
    "results_data = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"experiment_type\": \"dynamic_rag_comparison\",\n",
    "    \"environment\": \"Google Colab\" if IN_COLAB else \"Local\",\n",
    "    \"dataset_info\": {\n",
    "        \"num_questions\": len(questions),\n",
    "        \"num_documents\": len(documents),\n",
    "        \"question_types\": {\n",
    "            \"factual\": len([q for q in questions if q.get(\"type\") == \"factual\"]),\n",
    "            \"multi_hop\": len([q for q in questions if q.get(\"type\") == \"multi-hop\"])\n",
    "        }\n",
    "    },\n",
    "    \"systems_evaluated\": list(all_results.keys()),\n",
    "    \"evaluation_metrics\": {\n",
    "        \"recall_at_k\": k_values,\n",
    "        \"precision_at_k\": k_values,\n",
    "        \"exact_match\": True,\n",
    "        \"f1_score\": True,\n",
    "        \"latency\": True\n",
    "    },\n",
    "    \"initialization_times\": init_times,\n",
    "    \"detailed_results\": all_results,\n",
    "    \"question_type_performance\": type_performance\n",
    "}\n",
    "\n",
    "# Convert numpy arrays to lists for JSON serialization\n",
    "def convert_numpy(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy(item) for item in obj]\n",
    "    return obj\n",
    "\n",
    "# Save JSON data\n",
    "json_path = results_dir / f\"rag_comparison_results_{timestamp}.json\"\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(convert_numpy(results_data), f, indent=2)\n",
    "\n",
    "print(f\"üìä Results saved to: {json_path}\")\n",
    "\n",
    "# Save main figure\n",
    "fig.savefig(results_dir / f\"rag_comparison_visualization_{timestamp}.png\", \n",
    "           dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Save question type analysis figure\n",
    "plt.savefig(results_dir / f\"question_type_analysis_{timestamp}.png\", \n",
    "           dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(f\"üìà Visualizations saved to: {results_dir}/\")\n",
    "\n",
    "# Create summary CSV\n",
    "summary_data = []\n",
    "for system in systems:\n",
    "    summary_data.append({\n",
    "        \"System\": system,\n",
    "        \"Recall@1\": np.mean(all_results[system][\"recall_at_k\"][1]),\n",
    "        \"Recall@3\": np.mean(all_results[system][\"recall_at_k\"][3]),\n",
    "        \"Recall@5\": np.mean(all_results[system][\"recall_at_k\"][5]),\n",
    "        \"Precision@5\": np.mean(all_results[system][\"precision_at_k\"][5]),\n",
    "        \"Exact_Match\": np.mean(all_results[system][\"exact_matches\"]),\n",
    "        \"F1_Score\": np.mean(all_results[system][\"f1_scores\"]),\n",
    "        \"Latency_ms\": np.mean(all_results[system][\"latencies\"]) * 1000,\n",
    "        \"Factual_Recall@5\": type_performance[system][\"factual\"][\"recall5\"],\n",
    "        \"Factual_EM\": type_performance[system][\"factual\"][\"em\"],\n",
    "        \"MultiHop_Recall@5\": type_performance[system][\"multihop\"][\"recall5\"],\n",
    "        \"MultiHop_EM\": type_performance[system][\"multihop\"][\"em\"]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "csv_path = results_dir / f\"rag_summary_results_{timestamp}.csv\"\n",
    "summary_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"üìÑ Summary CSV saved to: {csv_path}\")\n",
    "print(\"\\n‚úÖ All results saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f0f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Results (for Colab users)\n",
    "if IN_COLAB:\n",
    "    print(\"üì• Preparing files for download...\")\n",
    "    \n",
    "    # Create a zip file with all results\n",
    "    import zipfile\n",
    "    \n",
    "    zip_path = f\"dynamic_rag_comparison_results_{timestamp}.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "        # Add all files from results directory\n",
    "        for file_path in results_dir.glob(\"*\"):\n",
    "            zipf.write(file_path, file_path.name)\n",
    "        \n",
    "        # Add the experiment script\n",
    "        zipf.write(\"experiments/colab_experiments/dynamic_rag_comparison/dynamic_rag_experiment.py\", \n",
    "                   \"dynamic_rag_experiment.py\")\n",
    "        \n",
    "        # Add this notebook\n",
    "        try:\n",
    "            zipf.write(\"experiments/colab_experiments/dynamic_rag_comparison/dynamic_rag_colab.ipynb\", \n",
    "                       \"dynamic_rag_colab.ipynb\")\n",
    "        except:\n",
    "            pass  # File might not exist in Colab\n",
    "    \n",
    "    print(f\"üì¶ Created zip file: {zip_path}\")\n",
    "    \n",
    "    # Download files\n",
    "    from google.colab import files\n",
    "    \n",
    "    try:\n",
    "        files.download(zip_path)\n",
    "        print(\"‚úÖ Download initiated! Check your browser's download folder.\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Automatic download failed. You can manually download the files from the file browser.\")\n",
    "        print(\"üìÅ Available files:\")\n",
    "        !ls -la rag_comparison_results/\n",
    "        !ls -la *.zip\n",
    "else:\n",
    "    print(\"üìÅ Results saved locally in the rag_comparison_results/ directory\")\n",
    "    print(\"üìã Available files:\")\n",
    "    !ls -la rag_comparison_results/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ddc43",
   "metadata": {},
   "source": [
    "## üì¶ Experiment Results Download\n",
    "\n",
    "Download your experimental results for further analysis or sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030026ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Experiment Results\n",
    "print(\"üì¶ Preparing experiment results for download...\")\n",
    "\n",
    "def create_downloadable_results():\n",
    "    \"\"\"Create a downloadable package of all experimental results\"\"\"\n",
    "    import zipfile\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Create download directory\n",
    "    download_dir = Path(\"downloads\")\n",
    "    download_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    zip_filename = f\"rag_experiment_results_{timestamp}.zip\"\n",
    "    zip_path = download_dir / zip_filename\n",
    "    \n",
    "    print(f\"üìù Creating results package: {zip_filename}\")\n",
    "    \n",
    "    # Create comprehensive results package\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        \n",
    "        # Add experiment results\n",
    "        results_dir = Path(\"data/rag_experiments/results\")\n",
    "        if results_dir.exists():\n",
    "            for file_path in results_dir.rglob(\"*\"):\n",
    "                if file_path.is_file():\n",
    "                    arcname = f\"results/{file_path.relative_to(results_dir)}\"\n",
    "                    zipf.write(file_path, arcname)\n",
    "                    print(f\"   üìÑ Added: {arcname}\")\n",
    "        \n",
    "        # Add visualizations\n",
    "        viz_dir = Path(\"data/rag_experiments/visualizations\")\n",
    "        if viz_dir.exists():\n",
    "            for file_path in viz_dir.rglob(\"*.png\"):\n",
    "                if file_path.is_file():\n",
    "                    arcname = f\"visualizations/{file_path.name}\"\n",
    "                    zipf.write(file_path, arcname)\n",
    "                    print(f\"   üñºÔ∏è  Added: {arcname}\")\n",
    "        \n",
    "        # Add baseline comparisons\n",
    "        baselines_dir = Path(\"data/rag_experiments/baselines\")\n",
    "        if baselines_dir.exists():\n",
    "            for baseline_dir in baselines_dir.iterdir():\n",
    "                if baseline_dir.is_dir():\n",
    "                    results_files = baseline_dir.rglob(\"*.json\")\n",
    "                    for file_path in results_files:\n",
    "                        arcname = f\"baselines/{baseline_dir.name}/{file_path.name}\"\n",
    "                        zipf.write(file_path, arcname)\n",
    "                        print(f\"   üìä Added: {arcname}\")\n",
    "        \n",
    "        # Add experiment summary\n",
    "        summary = {\n",
    "            \"experiment_type\": \"Dynamic RAG Comparison\",\n",
    "            \"timestamp\": timestamp,\n",
    "            \"notebook_version\": \"v1.0.0\",\n",
    "            \"description\": \"Comparison of InsightSpike-AI dynamic RAG against baseline methods\",\n",
    "            \"datasets\": [\"NaturalQuestions_sample\", \"HotpotQA_sample\"],\n",
    "            \"methods_compared\": [\"BM25\", \"Static Embeddings\", \"DPR\", \"InsightSpike RAG\"],\n",
    "            \"metrics\": [\"Recall@k\", \"Precision@k\", \"Exact Match\", \"F1 Score\", \"Latency\"]\n",
    "        }\n",
    "        \n",
    "        summary_path = download_dir / \"experiment_summary.json\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        zipf.write(summary_path, \"experiment_summary.json\")\n",
    "        \n",
    "        print(f\"   üìã Added: experiment_summary.json\")\n",
    "    \n",
    "    file_size = zip_path.stat().st_size / (1024 * 1024)  # MB\n",
    "    print(f\"\\n‚úÖ Results package created successfully!\")\n",
    "    print(f\"üì¶ File: {zip_path}\")\n",
    "    print(f\"üìè Size: {file_size:.2f} MB\")\n",
    "    \n",
    "    return zip_path\n",
    "\n",
    "# Create and prepare results for download\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        # Create downloadable package\n",
    "        zip_path = create_downloadable_results()\n",
    "        \n",
    "        # Download in Colab\n",
    "        from google.colab import files\n",
    "        files.download(str(zip_path))\n",
    "        print(\"‚¨áÔ∏è  Download started in Colab!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating download package: {e}\")\n",
    "        print(\"üí° You can manually download files from the file browser\")\n",
    "        \n",
    "        # Show available files for manual download\n",
    "        results_dir = Path(\"data/rag_experiments/results\")\n",
    "        if results_dir.exists():\n",
    "            print(f\"\\nüìã Available result files:\")\n",
    "            for file_path in results_dir.rglob(\"*\"):\n",
    "                if file_path.is_file():\n",
    "                    print(f\"   üìÑ {file_path}\")\n",
    "else:\n",
    "    # Local environment - just create the package\n",
    "    zip_path = create_downloadable_results()\n",
    "    print(f\"üíæ Results saved locally: {zip_path}\")\n",
    "    print(\"üìÅ Open the 'downloads' folder to access your results\")\n",
    "\n",
    "print(f\"\\nüéâ Experiment complete! Your results are ready for analysis.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
