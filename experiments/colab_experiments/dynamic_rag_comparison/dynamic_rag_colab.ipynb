{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3759305",
   "metadata": {},
   "source": [
    "# ğŸ” InsightSpike-AI: Dynamic RAG Comparison Experiment\n",
    "## Evaluating Dynamic RAG Construction vs Existing Methods\n",
    "\n",
    "This notebook compares InsightSpike-AI's dynamic RAG construction capabilities against established baselines using standard question-answering benchmarks.\n",
    "\n",
    "### Experimental Design\n",
    "- **Datasets**: Simulated NaturalQuestions & HotpotQA samples\n",
    "- **Baselines**: BM25, Static Embeddings, DPR (Dense Passage Retrieval)\n",
    "- **Metrics**: Recall@k, Exact Match (EM), F1 Score, Inference Latency\n",
    "\n",
    "### InsightSpike-AI Dynamic RAG Features\n",
    "- **Adaptive Weighting**: Dynamically adjusts retrieval strategy based on query characteristics\n",
    "- **Intrinsic Motivation**: Uses Î”GED Ã— Î”IG for document selection enhancement\n",
    "- **Multi-Strategy Fusion**: Combines lexical, semantic, and learned retrieval methods\n",
    "- **Context-Aware Memory**: Maintains retrieval history for improved performance\n",
    "\n",
    "### Expected Outcomes\n",
    "We expect InsightSpike-AI's dynamic approach to show:\n",
    "1. Higher recall and precision across different k values\n",
    "2. Better handling of both factual and multi-hop questions\n",
    "3. Competitive or superior latency performance\n",
    "4. More robust performance across question types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e718812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš¨ STEP 1: Environment Setup and Package Installation\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"ğŸ”§ Running in Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"ğŸ”§ Running in local environment\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸ“¦ Installing required packages for Colab...\")\n",
    "    print(\"âš ï¸  IMPORTANT: This will trigger a runtime restart - this is EXPECTED and REQUIRED!\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Install compatible versions to avoid compatibility issues\n",
    "    print(\"ğŸ”§ Installing PyTorch and core dependencies...\")\n",
    "    !pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cpu\n",
    "    !pip install sentence-transformers==2.7.0  # Compatible with PyTorch 2.2.2\n",
    "    !pip install transformers==4.30.0  # Ensure compatibility\n",
    "    !pip install numpy==1.26.4  # Avoid NumPy 2.x compatibility issues\n",
    "    \n",
    "    print(\"ğŸ”§ Installing ML and visualization packages...\")\n",
    "    !pip install scikit-learn pandas matplotlib seaborn\n",
    "    !pip install plotly kaleido\n",
    "    !pip install faiss-cpu networkx\n",
    "    \n",
    "    print(\"âœ… Package installation complete\")\n",
    "    print(\"\")\n",
    "    print(\"ğŸš¨ CRITICAL: RESTART RUNTIME NOW!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ“‹ Required steps:\")\n",
    "    print(\"   1. Look for the popup warning 'ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å†èµ·å‹•ã™ã‚‹'\")\n",
    "    print(\"   2. Click 'å†èµ·å‹•ã™ã‚‹' or 'RESTART RUNTIME' button\")\n",
    "    print(\"   3. OR manually: Runtime menu â†’ Restart runtime\")\n",
    "    print(\"   4. After restart, run STEP 2 cell to continue setup\")\n",
    "    print(\"\")\n",
    "    print(\"ğŸ”„ Why restart is essential:\")\n",
    "    print(\"   - NumPy downgrade 2.x â†’ 1.26.4 (ML compatibility)\")\n",
    "    print(\"   - PyTorch version alignment 2.6.1 â†’ 2.2.2 (meta tensor fix)\")\n",
    "    print(\"   - Fresh Python session prevents import conflicts\")\n",
    "    print(\"\")\n",
    "    print(\"âš ï¸  DO NOT run the next cell until AFTER restart!\")\n",
    "    print(\"   Next cell will clone repository and setup InsightSpike-AI\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "else:\n",
    "    print(\"ğŸ  Local environment detected\")\n",
    "    print(\"ğŸ“‹ For local development:\")\n",
    "    print(\"   1. Ensure Poetry is installed: curl -sSL https://install.python-poetry.org | python3 -\")\n",
    "    print(\"   2. Install dependencies: poetry install\")\n",
    "    print(\"   3. Activate environment: poetry shell\")\n",
    "    print(\"   4. Or run in environment: poetry run jupyter lab\")\n",
    "    print(\"\")\n",
    "    print(\"âœ… Ready for local development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš¨ STEP 2: Repository Setup and InsightSpike-AI Installation (AFTER RESTART)\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Re-check environment after restart\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"ğŸ”§ Colab environment confirmed after restart\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"ğŸ”§ Local environment detected\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸ“¥ Setting up InsightSpike-AI in Colab...\")\n",
    "    \n",
    "    # Get GitHub token from Colab secrets\n",
    "    from google.colab import userdata\n",
    "    try:\n",
    "        github_token = userdata.get('GITHUB_TOKEN')\n",
    "        print(\"âœ… GitHub token retrieved from secrets\")\n",
    "    except Exception as e:\n",
    "        print(\"âŒ GitHub token not found in Colab secrets\")\n",
    "        print(\"ğŸ“ Setup instructions:\")\n",
    "        print(\"   1. Click the key icon (ğŸ”‘) in left sidebar\")\n",
    "        print(\"   2. Add new secret: Name='GITHUB_TOKEN'\")\n",
    "        print(\"   3. Value = your GitHub personal access token\")\n",
    "        print(\"   4. Get token: https://github.com/settings/tokens\")\n",
    "        raise e\n",
    "    \n",
    "    # Handle existing repository directory\n",
    "    repo_path = '/content/InsightSpike-AI'\n",
    "    if os.path.exists(repo_path):\n",
    "        print(\"ğŸ“‚ Repository already exists - updating...\")\n",
    "        os.chdir(repo_path)\n",
    "        !git pull origin main\n",
    "    else:\n",
    "        print(\"ğŸ“‚ Cloning InsightSpike-AI repository...\")\n",
    "        clone_url = f\"https://{github_token}@github.com/miyauchikazuyoshi/InsightSpike-AI.git\"\n",
    "        !git clone $clone_url\n",
    "        os.chdir(repo_path)\n",
    "    \n",
    "    print(f\"ğŸ“ Working directory: {os.getcwd()}\")\n",
    "    \n",
    "    # Install Poetry if not available\n",
    "    print(\"\\nğŸ—ï¸  Setting up Poetry for package management...\")\n",
    "    try:\n",
    "        poetry_path = \"/root/.local/bin/poetry\"\n",
    "        !{poetry_path} --version\n",
    "        print(\"âœ… Poetry already available\")\n",
    "    except:\n",
    "        print(\"ğŸ“¦ Installing Poetry...\")\n",
    "        !curl -sSL https://install.python-poetry.org | python3 -\n",
    "        !{poetry_path} --version\n",
    "    \n",
    "    # Use Colab-specific dependencies\n",
    "    print(\"ğŸ“‹ Configuring for Colab environment...\")\n",
    "    !cp pyproject_colab.toml pyproject.toml  # Colab-optimized dependencies\n",
    "    \n",
    "    # Install InsightSpike-AI package with Poetry (safe for all configurations)\n",
    "    print(\"ğŸ“¦ Installing InsightSpike-AI package...\")\n",
    "    print(\"   Note: Using Poetry for reliable development installs\")\n",
    "    !{poetry_path} install --only main --no-interaction\n",
    "    \n",
    "    # Add project to Python path\n",
    "    sys.path.insert(0, '/content/InsightSpike-AI/src')\n",
    "    \n",
    "    # Verify InsightSpike-AI installation\n",
    "    print(\"\\nğŸ” Verifying InsightSpike-AI installation...\")\n",
    "    installation_verified = False\n",
    "    \n",
    "    try:\n",
    "        import insightspike\n",
    "        print(f\"âœ… SUCCESS: InsightSpike-AI v{insightspike.__version__}\")\n",
    "        print(f\"   ğŸ“ Package location: {insightspike.__file__}\")\n",
    "        installation_verified = True\n",
    "        \n",
    "        # Test core components (with graceful fallback)\n",
    "        try:\n",
    "            from insightspike.core.rag import RAGSystem  # Updated import path\n",
    "            print(\"âœ… RAG components accessible\")\n",
    "        except ImportError:\n",
    "            try:\n",
    "                from insightspike.core.system import InsightSpikeSystem\n",
    "                print(\"âœ… Core system accessible\")\n",
    "            except ImportError:\n",
    "                print(\"âœ… Base components accessible - using fallback implementations\")\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"âš ï¸  Poetry install issue: {e}\")\n",
    "        print(\"ğŸ”„ Using fallback strategy...\")\n",
    "        \n",
    "        # Fallback: Direct path addition\n",
    "        sys.path.append('/content/InsightSpike-AI/src')\n",
    "        try:\n",
    "            import insightspike\n",
    "            print(\"âœ… SUCCESS: InsightSpike-AI available via PYTHONPATH\")\n",
    "            installation_verified = True\n",
    "        except ImportError as e2:\n",
    "            print(f\"âŒ Fallback failed: {e2}\")\n",
    "            print(\"ğŸ“‹ Will use simplified implementations for this experiment\")\n",
    "    \n",
    "    if installation_verified:\n",
    "        print(\"\\nâœ… InsightSpike-AI setup complete!\")\n",
    "        print(\"ğŸ“Š Ready to run dynamic RAG experiments\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  Package not available - using fallback mode\")\n",
    "        print(\"ğŸ“Š Experiment will use simplified implementations\")\n",
    "\n",
    "else:\n",
    "    # Local environment setup\n",
    "    print(\"ğŸ  Setting up local environment...\")\n",
    "    \n",
    "    # Find project root\n",
    "    project_root = Path.cwd()\n",
    "    while not (project_root / \"pyproject.toml\").exists() and project_root != project_root.parent:\n",
    "        project_root = project_root.parent\n",
    "    \n",
    "    if (project_root / \"pyproject.toml\").exists():\n",
    "        print(f\"ğŸ“ Project root found: {project_root}\")\n",
    "        \n",
    "        # Add src to path\n",
    "        src_path = project_root / \"src\"\n",
    "        if src_path.exists():\n",
    "            sys.path.insert(0, str(src_path))\n",
    "            print(f\"ğŸ“ Added to PYTHONPATH: {src_path}\")\n",
    "        \n",
    "        # Verify installation\n",
    "        try:\n",
    "            import insightspike\n",
    "            print(f\"âœ… InsightSpike-AI available: v{insightspike.__version__}\")\n",
    "            print(\"ğŸ“Š Ready for local experimentation\")\n",
    "        except ImportError:\n",
    "            print(\"âš ï¸  InsightSpike-AI not installed locally\")\n",
    "            print(\"ğŸ“‹ Install with: poetry install && poetry shell\")\n",
    "            print(\"ğŸ“Š Will use simplified implementations\")\n",
    "    else:\n",
    "        print(\"âŒ Project root not found - check working directory\")\n",
    "\n",
    "# Add experiment module to path\n",
    "experiment_path = 'experiments/colab_experiments/dynamic_rag_comparison'\n",
    "if os.path.exists(experiment_path):\n",
    "    sys.path.append(experiment_path)\n",
    "    print(f\"ğŸ“ Added experiment path: {experiment_path}\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Experiment path not found: {experiment_path}\")\n",
    "\n",
    "print(\"\\nğŸ¯ Environment setup complete! Ready for experiments.\")\n",
    "\n",
    "# Test actual import to verify everything works\n",
    "print(\"\\nğŸ§ª Testing core imports...\")\n",
    "try:\n",
    "    # Test basic imports\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    print(f\"âœ… PyTorch: {torch.__version__}\")\n",
    "    print(f\"âœ… NumPy: {np.__version__}\")\n",
    "    \n",
    "    # Test InsightSpike imports\n",
    "    import insightspike\n",
    "    print(f\"âœ… InsightSpike-AI: {insightspike.__version__}\")\n",
    "    \n",
    "    print(\"ğŸš€ All systems ready for experimentation!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Import test failed: {e}\")\n",
    "    print(\"ğŸ“‹ Some components may use fallback implementations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f49aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ğŸ¯ Environment setup complete!\")\n",
    "\n",
    "# Check and fix sentence-transformers compatibility\n",
    "print(\"\\nğŸ”§ Checking sentence-transformers compatibility...\")\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"âœ… Sentence Transformers available\")\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Sentence Transformers not available: {e}\")\n",
    "    print(\"ğŸ”„ Attempting to reinstall compatible version...\")\n",
    "    \n",
    "    if IN_COLAB:\n",
    "        try:\n",
    "            # Reinstall sentence-transformers compatible with NumPy 1.26.4\n",
    "            !pip install --force-reinstall sentence-transformers==2.7.0\n",
    "            print(\"ğŸ“¦ Reinstalled sentence-transformers\")\n",
    "            \n",
    "            # Test import again\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            print(\"âœ… Sentence Transformers now available\")\n",
    "            SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
    "        except Exception as e2:\n",
    "            print(f\"âŒ Reinstall failed: {e2}\")\n",
    "            print(\"ğŸ“‹ Will use TF-IDF fallback for embeddings\")\n",
    "            SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
    "    else:\n",
    "        print(\"ğŸ“‹ Using TF-IDF fallback for embeddings\")\n",
    "        SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    print(\"âœ… Scikit-learn available\")\n",
    "    SKLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Scikit-learn not available - using simplified methods\")\n",
    "    SKLEARN_AVAILABLE = False\n",
    "\n",
    "# Display final package status\n",
    "print(f\"\\nğŸ“Š Package Availability Summary:\")\n",
    "print(f\"   ğŸ”¢ NumPy: {np.__version__}\")\n",
    "print(f\"   ğŸ§  Sentence Transformers: {'âœ…' if SENTENCE_TRANSFORMERS_AVAILABLE else 'âŒ'}\")\n",
    "print(f\"   ğŸ“ Scikit-learn: {'âœ…' if SKLEARN_AVAILABLE else 'âŒ'}\")\n",
    "\n",
    "if not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "    print(f\"\\nğŸ’¡ Note: Using TF-IDF embeddings as fallback for dense retrieval\")\n",
    "    print(f\"   This may slightly affect DPR performance but won't impact comparison validity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed29f0",
   "metadata": {},
   "source": [
    "## ğŸ“Š Dataset Preparation and Preview\n",
    "\n",
    "Let's examine the evaluation dataset we'll be using for this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and Examine the Evaluation Dataset\n",
    "print(\"ğŸ“Š Creating evaluation dataset...\")\n",
    "\n",
    "# Load the expanded dataset\n",
    "questions, documents = create_expanded_dataset()\n",
    "\n",
    "print(f\"âœ… Dataset created:\")\n",
    "print(f\"   ğŸ“ Questions: {len(questions)}\")\n",
    "print(f\"   ğŸ“„ Documents: {len(documents)}\")\n",
    "\n",
    "# Display dataset statistics\n",
    "question_types = {}\n",
    "for q in questions:\n",
    "    qtype = q.get(\"type\", \"unknown\")\n",
    "    question_types[qtype] = question_types.get(qtype, 0) + 1\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Question Type Distribution:\")\n",
    "for qtype, count in question_types.items():\n",
    "    print(f\"   {qtype}: {count} questions\")\n",
    "\n",
    "# Show sample questions\n",
    "print(f\"\\nğŸ” Sample Questions:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, q in enumerate(questions[:3]):\n",
    "    print(f\"Q{i+1} [{q.get('type', 'unknown')}]: {q['question']}\")\n",
    "    print(f\"   Answer: {q['answer']}\")\n",
    "    print(f\"   Context: {q['context'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15264526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Analysis\n",
    "print(\"ğŸ“„ Document Corpus Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Calculate document statistics\n",
    "doc_lengths = [len(doc.split()) for doc in documents]\n",
    "total_tokens = sum(doc_lengths)\n",
    "avg_length = np.mean(doc_lengths)\n",
    "std_length = np.std(doc_lengths)\n",
    "\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "print(f\"Total tokens: {total_tokens:,}\")\n",
    "print(f\"Average doc length: {avg_length:.1f} Â± {std_length:.1f} tokens\")\n",
    "print(f\"Min doc length: {min(doc_lengths)} tokens\")\n",
    "print(f\"Max doc length: {max(doc_lengths)} tokens\")\n",
    "\n",
    "# Visualize document length distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(doc_lengths, bins=15, alpha=0.7, color='skyblue')\n",
    "plt.xlabel('Document Length (tokens)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Document Length Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show sample documents\n",
    "plt.subplot(1, 2, 2)\n",
    "sample_docs = documents[:5]\n",
    "doc_indices = range(1, len(sample_docs) + 1)\n",
    "sample_lengths = [len(doc.split()) for doc in sample_docs]\n",
    "\n",
    "plt.bar(doc_indices, sample_lengths, alpha=0.7, color='lightcoral')\n",
    "plt.xlabel('Document Index')\n",
    "plt.ylabel('Length (tokens)')\n",
    "plt.title('Sample Document Lengths')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display sample documents\n",
    "print(f\"\\nğŸ“„ Sample Documents:\")\n",
    "print(\"-\" * 50)\n",
    "for i, doc in enumerate(documents[:3]):\n",
    "    print(f\"Doc {i+1}: {doc[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed42b14",
   "metadata": {},
   "source": [
    "## ğŸ”§ Retrieval System Initialization\n",
    "\n",
    "Now let's initialize and test all the retrieval systems we'll be comparing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb80048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize All Retrieval Systems\n",
    "print(\"ğŸ”§ Initializing retrieval systems...\")\n",
    "\n",
    "# Track initialization time for each system\n",
    "init_times = {}\n",
    "\n",
    "# 1. BM25 Retriever\n",
    "print(\"\\nğŸ“Š Initializing BM25 Retriever...\")\n",
    "start_time = time.time()\n",
    "bm25_retriever = BM25Retriever(documents)\n",
    "init_times[\"BM25\"] = time.time() - start_time\n",
    "print(f\"   âœ… BM25 initialized in {init_times['BM25']:.3f}s\")\n",
    "\n",
    "# 2. Static Embedding Retriever\n",
    "print(\"\\nğŸ”¢ Initializing Static Embedding Retriever...\")\n",
    "start_time = time.time()\n",
    "static_retriever = StaticEmbeddingRetriever(documents)\n",
    "init_times[\"Static Embeddings\"] = time.time() - start_time\n",
    "print(f\"   âœ… Static Embeddings initialized in {init_times['Static Embeddings']:.3f}s\")\n",
    "\n",
    "# 3. DPR Retriever (if available)\n",
    "if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "    print(\"\\nğŸ§  Initializing DPR-style Dense Retriever...\")\n",
    "    start_time = time.time()\n",
    "    dpr_retriever = DPRRetriever(documents)\n",
    "    init_times[\"DPR (Dense)\"] = time.time() - start_time\n",
    "    print(f\"   âœ… DPR initialized in {init_times['DPR (Dense)']:.3f}s\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ DPR not available - skipping dense retrieval\")\n",
    "\n",
    "# 4. InsightSpike Dynamic RAG\n",
    "print(\"\\nğŸš€ Initializing InsightSpike Dynamic RAG...\")\n",
    "start_time = time.time()\n",
    "insightspike_rag = InsightSpikeRAG(documents)\n",
    "init_times[\"InsightSpike Dynamic RAG\"] = time.time() - start_time\n",
    "print(f\"   âœ… InsightSpike RAG initialized in {init_times['InsightSpike Dynamic RAG']:.3f}s\")\n",
    "\n",
    "# Display initialization summary\n",
    "print(f\"\\nâ±ï¸ Initialization Times Summary:\")\n",
    "print(\"-\" * 40)\n",
    "for system, init_time in init_times.items():\n",
    "    print(f\"{system:<25}: {init_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6012383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Retrieval Systems with Sample Query\n",
    "print(\"ğŸ§ª Testing retrieval systems with sample query...\")\n",
    "\n",
    "sample_query = \"When was the Declaration of Independence signed?\"\n",
    "print(f\"Test Query: '{sample_query}'\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Test each retriever\n",
    "retrievers = {\n",
    "    \"BM25\": bm25_retriever,\n",
    "    \"Static Embeddings\": static_retriever,\n",
    "    \"InsightSpike Dynamic RAG\": insightspike_rag\n",
    "}\n",
    "\n",
    "if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "    retrievers[\"DPR (Dense)\"] = dpr_retriever\n",
    "\n",
    "for name, retriever in retrievers.items():\n",
    "    print(f\"\\nğŸ” {name} Results:\")\n",
    "    start_time = time.time()\n",
    "    results = retriever.retrieve(sample_query, k=3)\n",
    "    query_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   Query time: {query_time*1000:.1f}ms\")\n",
    "    \n",
    "    for i, (doc_idx, score) in enumerate(results):\n",
    "        doc_preview = documents[doc_idx][:100] + \"...\" if len(documents[doc_idx]) > 100 else documents[doc_idx]\n",
    "        print(f\"   {i+1}. Score: {score:.3f} | Doc: {doc_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd20e70",
   "metadata": {},
   "source": [
    "## ğŸš€ Running the Complete Evaluation\n",
    "\n",
    "Now let's run the comprehensive evaluation across all systems and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f53b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Complete Evaluation\n",
    "print(\"ğŸš€ Starting comprehensive RAG evaluation...\")\n",
    "print(\"â° This will take a few minutes to complete...\")\n",
    "\n",
    "# Configure evaluation parameters\n",
    "k_values = [1, 3, 5]\n",
    "print(f\"ğŸ“Š Evaluating with k values: {k_values}\")\n",
    "\n",
    "# Initialize results storage\n",
    "all_results = {}\n",
    "\n",
    "# Evaluate each system\n",
    "for name, retriever in retrievers.items():\n",
    "    print(f\"\\nğŸ” Evaluating {name}...\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluate_retrieval_system(retriever, questions, documents, k_values)\n",
    "    all_results[name] = results\n",
    "    \n",
    "    # Display quick summary\n",
    "    avg_recall_5 = np.mean(results[\"recall_at_k\"][5])\n",
    "    avg_precision_5 = np.mean(results[\"precision_at_k\"][5])\n",
    "    avg_em = np.mean(results[\"exact_matches\"])\n",
    "    avg_f1 = np.mean(results[\"f1_scores\"])\n",
    "    avg_latency = np.mean(results[\"latencies\"])\n",
    "    \n",
    "    print(f\"   ğŸ“ˆ Quick Summary:\")\n",
    "    print(f\"      Recall@5: {avg_recall_5:.3f}\")\n",
    "    print(f\"      Precision@5: {avg_precision_5:.3f}\")\n",
    "    print(f\"      Exact Match: {avg_em:.3f}\")\n",
    "    print(f\"      F1 Score: {avg_f1:.3f}\")\n",
    "    print(f\"      Avg Latency: {avg_latency*1000:.1f}ms\")\n",
    "\n",
    "print(\"\\nâœ… Evaluation completed for all systems!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116daed0",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Results Visualization and Analysis\n",
    "\n",
    "Let's create comprehensive visualizations to understand the performance differences between systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23036021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Main Visualization\n",
    "print(\"ğŸ“ˆ Creating comprehensive results visualization...\")\n",
    "\n",
    "# Generate the main comparison visualization\n",
    "fig = create_rag_visualization(all_results, questions)\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Main visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b40d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Performance Analysis\n",
    "print(\"ğŸ“Š Detailed Performance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "systems = list(all_results.keys())\n",
    "\n",
    "# Create detailed comparison table\n",
    "comparison_data = []\n",
    "for system in systems:\n",
    "    results = all_results[system]\n",
    "    \n",
    "    row = {\n",
    "        \"System\": system,\n",
    "        \"Recall@1\": f\"{np.mean(results['recall_at_k'][1]):.3f} Â± {np.std(results['recall_at_k'][1]):.3f}\",\n",
    "        \"Recall@3\": f\"{np.mean(results['recall_at_k'][3]):.3f} Â± {np.std(results['recall_at_k'][3]):.3f}\",\n",
    "        \"Recall@5\": f\"{np.mean(results['recall_at_k'][5]):.3f} Â± {np.std(results['recall_at_k'][5]):.3f}\",\n",
    "        \"Precision@5\": f\"{np.mean(results['precision_at_k'][5]):.3f} Â± {np.std(results['precision_at_k'][5]):.3f}\",\n",
    "        \"Exact Match\": f\"{np.mean(results['exact_matches']):.3f} Â± {np.std(results['exact_matches']):.3f}\",\n",
    "        \"F1 Score\": f\"{np.mean(results['f1_scores']):.3f} Â± {np.std(results['f1_scores']):.3f}\",\n",
    "        \"Latency (ms)\": f\"{np.mean(results['latencies'])*1000:.1f} Â± {np.std(results['latencies'])*1000:.1f}\"\n",
    "    }\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "display(HTML(comparison_df.to_html(index=False, table_id=\"comparison_table\")))\n",
    "\n",
    "# Statistical Significance Testing\n",
    "print(f\"\\nğŸ”¬ Statistical Significance Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Compare InsightSpike against each baseline\n",
    "insightspike_name = \"InsightSpike Dynamic RAG\"\n",
    "if insightspike_name in all_results:\n",
    "    insightspike_recall5 = all_results[insightspike_name][\"recall_at_k\"][5]\n",
    "    insightspike_em = all_results[insightspike_name][\"exact_matches\"]\n",
    "    \n",
    "    for system in systems:\n",
    "        if system != insightspike_name:\n",
    "            system_recall5 = all_results[system][\"recall_at_k\"][5]\n",
    "            system_em = all_results[system][\"exact_matches\"]\n",
    "            \n",
    "            # T-test for Recall@5\n",
    "            _, p_recall = stats.ttest_ind(insightspike_recall5, system_recall5)\n",
    "            \n",
    "            # T-test for Exact Match\n",
    "            _, p_em = stats.ttest_ind(insightspike_em, system_em)\n",
    "            \n",
    "            # Calculate effect sizes (Cohen's d)\n",
    "            def cohens_d(group1, group2):\n",
    "                n1, n2 = len(group1), len(group2)\n",
    "                pooled_std = np.sqrt(((n1 - 1) * np.var(group1, ddof=1) + \n",
    "                                     (n2 - 1) * np.var(group2, ddof=1)) / (n1 + n2 - 2))\n",
    "                return (np.mean(group1) - np.mean(group2)) / pooled_std\n",
    "            \n",
    "            recall_effect = cohens_d(insightspike_recall5, system_recall5)\n",
    "            em_effect = cohens_d(insightspike_em, system_em)\n",
    "            \n",
    "            print(f\"\\nInsightSpike vs {system}:\")\n",
    "            print(f\"  Recall@5: p={p_recall:.4f}, Cohen's d={recall_effect:.3f}\")\n",
    "            print(f\"  Exact Match: p={p_em:.4f}, Cohen's d={em_effect:.3f}\")\n",
    "            \n",
    "            # Interpretation\n",
    "            if p_recall < 0.05:\n",
    "                print(f\"  Recall@5: Statistically significant difference âœ…\")\n",
    "            else:\n",
    "                print(f\"  Recall@5: No significant difference âŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ffa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by Question Type Analysis\n",
    "print(\"ğŸ¯ Performance by Question Type\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Separate results by question type\n",
    "factual_questions = [(i, q) for i, q in enumerate(questions) if q.get(\"type\") == \"factual\"]\n",
    "multihop_questions = [(i, q) for i, q in enumerate(questions) if q.get(\"type\") == \"multi-hop\"]\n",
    "\n",
    "print(f\"Factual questions: {len(factual_questions)}\")\n",
    "print(f\"Multi-hop questions: {len(multihop_questions)}\")\n",
    "\n",
    "# Calculate performance by question type\n",
    "type_performance = {}\n",
    "\n",
    "for system in systems:\n",
    "    results = all_results[system]\n",
    "    \n",
    "    # Factual performance\n",
    "    factual_recall5 = [results[\"recall_at_k\"][5][i] for i, _ in factual_questions]\n",
    "    factual_em = [results[\"exact_matches\"][i] for i, _ in factual_questions]\n",
    "    \n",
    "    # Multi-hop performance\n",
    "    multihop_recall5 = [results[\"recall_at_k\"][5][i] for i, _ in multihop_questions]\n",
    "    multihop_em = [results[\"exact_matches\"][i] for i, _ in multihop_questions]\n",
    "    \n",
    "    type_performance[system] = {\n",
    "        \"factual\": {\n",
    "            \"recall5\": np.mean(factual_recall5) if factual_recall5 else 0,\n",
    "            \"em\": np.mean(factual_em) if factual_em else 0\n",
    "        },\n",
    "        \"multihop\": {\n",
    "            \"recall5\": np.mean(multihop_recall5) if multihop_recall5 else 0,\n",
    "            \"em\": np.mean(multihop_em) if multihop_em else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Visualize question type performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Recall@5 by question type\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(systems))\n",
    "width = 0.35\n",
    "\n",
    "factual_recall = [type_performance[sys][\"factual\"][\"recall5\"] for sys in systems]\n",
    "multihop_recall = [type_performance[sys][\"multihop\"][\"recall5\"] for sys in systems]\n",
    "\n",
    "ax1.bar(x - width/2, factual_recall, width, label='Factual', alpha=0.8)\n",
    "ax1.bar(x + width/2, multihop_recall, width, label='Multi-hop', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('System')\n",
    "ax1.set_ylabel('Recall@5')\n",
    "ax1.set_title('Recall@5 by Question Type')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([s.replace(' ', '\\n') for s in systems], fontsize=9)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Exact Match by question type\n",
    "ax2 = axes[1]\n",
    "factual_em = [type_performance[sys][\"factual\"][\"em\"] for sys in systems]\n",
    "multihop_em = [type_performance[sys][\"multihop\"][\"em\"] for sys in systems]\n",
    "\n",
    "ax2.bar(x - width/2, factual_em, width, label='Factual', alpha=0.8)\n",
    "ax2.bar(x + width/2, multihop_em, width, label='Multi-hop', alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('System')\n",
    "ax2.set_ylabel('Exact Match')\n",
    "ax2.set_title('Exact Match by Question Type')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([s.replace(' ', '\\n') for s in systems], fontsize=9)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed breakdown\n",
    "print(f\"\\nğŸ“‹ Detailed Question Type Performance:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'System':<25} {'Factual R@5':<12} {'Factual EM':<11} {'Multi-hop R@5':<14} {'Multi-hop EM':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for system in systems:\n",
    "    perf = type_performance[system]\n",
    "    print(f\"{system:<25} {perf['factual']['recall5']:<12.3f} {perf['factual']['em']:<11.3f} \"\n",
    "          f\"{perf['multihop']['recall5']:<14.3f} {perf['multihop']['em']:<12.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133fe83",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Save Results and Create Download Package\n",
    "\n",
    "Let's save all our experimental results and create a downloadable package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617705e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Experimental Results\n",
    "print(\"ğŸ’¾ Saving experimental results...\")\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path(\"rag_comparison_results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Prepare comprehensive results data\n",
    "results_data = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"experiment_type\": \"dynamic_rag_comparison\",\n",
    "    \"environment\": \"Google Colab\" if IN_COLAB else \"Local\",\n",
    "    \"dataset_info\": {\n",
    "        \"num_questions\": len(questions),\n",
    "        \"num_documents\": len(documents),\n",
    "        \"question_types\": {\n",
    "            \"factual\": len([q for q in questions if q.get(\"type\") == \"factual\"]),\n",
    "            \"multi_hop\": len([q for q in questions if q.get(\"type\") == \"multi-hop\"])\n",
    "        }\n",
    "    },\n",
    "    \"systems_evaluated\": list(all_results.keys()),\n",
    "    \"evaluation_metrics\": {\n",
    "        \"recall_at_k\": k_values,\n",
    "        \"precision_at_k\": k_values,\n",
    "        \"exact_match\": True,\n",
    "        \"f1_score\": True,\n",
    "        \"latency\": True\n",
    "    },\n",
    "    \"initialization_times\": init_times,\n",
    "    \"detailed_results\": all_results,\n",
    "    \"question_type_performance\": type_performance\n",
    "}\n",
    "\n",
    "# Convert numpy arrays to lists for JSON serialization\n",
    "def convert_numpy(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy(item) for item in obj]\n",
    "    return obj\n",
    "\n",
    "# Save JSON data\n",
    "json_path = results_dir / f\"rag_comparison_results_{timestamp}.json\"\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(convert_numpy(results_data), f, indent=2)\n",
    "\n",
    "print(f\"ğŸ“Š Results saved to: {json_path}\")\n",
    "\n",
    "# Save main figure\n",
    "fig.savefig(results_dir / f\"rag_comparison_visualization_{timestamp}.png\", \n",
    "           dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Save question type analysis figure\n",
    "plt.savefig(results_dir / f\"question_type_analysis_{timestamp}.png\", \n",
    "           dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(f\"ğŸ“ˆ Visualizations saved to: {results_dir}/\")\n",
    "\n",
    "# Create summary CSV\n",
    "summary_data = []\n",
    "for system in systems:\n",
    "    summary_data.append({\n",
    "        \"System\": system,\n",
    "        \"Recall@1\": np.mean(all_results[system][\"recall_at_k\"][1]),\n",
    "        \"Recall@3\": np.mean(all_results[system][\"recall_at_k\"][3]),\n",
    "        \"Recall@5\": np.mean(all_results[system][\"recall_at_k\"][5]),\n",
    "        \"Precision@5\": np.mean(all_results[system][\"precision_at_k\"][5]),\n",
    "        \"Exact_Match\": np.mean(all_results[system][\"exact_matches\"]),\n",
    "        \"F1_Score\": np.mean(all_results[system][\"f1_scores\"]),\n",
    "        \"Latency_ms\": np.mean(all_results[system][\"latencies\"]) * 1000,\n",
    "        \"Factual_Recall@5\": type_performance[system][\"factual\"][\"recall5\"],\n",
    "        \"Factual_EM\": type_performance[system][\"factual\"][\"em\"],\n",
    "        \"MultiHop_Recall@5\": type_performance[system][\"multihop\"][\"recall5\"],\n",
    "        \"MultiHop_EM\": type_performance[system][\"multihop\"][\"em\"]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "csv_path = results_dir / f\"rag_summary_results_{timestamp}.csv\"\n",
    "summary_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"ğŸ“„ Summary CSV saved to: {csv_path}\")\n",
    "print(\"\\nâœ… All results saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f0f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Results (for Colab users)\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸ“¥ Preparing files for download...\")\n",
    "    \n",
    "    # Create a zip file with all results\n",
    "    import zipfile\n",
    "    \n",
    "    zip_path = f\"dynamic_rag_comparison_results_{timestamp}.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "        # Add all files from results directory\n",
    "        for file_path in results_dir.glob(\"*\"):\n",
    "            zipf.write(file_path, file_path.name)\n",
    "        \n",
    "        # Add the experiment script\n",
    "        zipf.write(\"experiments/colab_experiments/dynamic_rag_comparison/dynamic_rag_experiment.py\", \n",
    "                   \"dynamic_rag_experiment.py\")\n",
    "        \n",
    "        # Add this notebook\n",
    "        try:\n",
    "            zipf.write(\"experiments/colab_experiments/dynamic_rag_comparison/dynamic_rag_colab.ipynb\", \n",
    "                       \"dynamic_rag_colab.ipynb\")\n",
    "        except:\n",
    "            pass  # File might not exist in Colab\n",
    "    \n",
    "    print(f\"ğŸ“¦ Created zip file: {zip_path}\")\n",
    "    \n",
    "    # Download files\n",
    "    from google.colab import files\n",
    "    \n",
    "    try:\n",
    "        files.download(zip_path)\n",
    "        print(\"âœ… Download initiated! Check your browser's download folder.\")\n",
    "    except:\n",
    "        print(\"âš ï¸ Automatic download failed. You can manually download the files from the file browser.\")\n",
    "        print(\"ğŸ“ Available files:\")\n",
    "        !ls -la rag_comparison_results/\n",
    "        !ls -la *.zip\n",
    "else:\n",
    "    print(\"ğŸ“ Results saved locally in the rag_comparison_results/ directory\")\n",
    "    print(\"ğŸ“‹ Available files:\")\n",
    "    !ls -la rag_comparison_results/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ddc43",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Experiment Results Download\n",
    "\n",
    "Download your experimental results for further analysis or sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030026ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Experiment Results\n",
    "print(\"ğŸ“¦ Preparing experiment results for download...\")\n",
    "\n",
    "def create_downloadable_results():\n",
    "    \"\"\"Create a downloadable package of all experimental results\"\"\"\n",
    "    import zipfile\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Create download directory\n",
    "    download_dir = Path(\"downloads\")\n",
    "    download_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    zip_filename = f\"rag_experiment_results_{timestamp}.zip\"\n",
    "    zip_path = download_dir / zip_filename\n",
    "    \n",
    "    print(f\"ğŸ“ Creating results package: {zip_filename}\")\n",
    "    \n",
    "    # Create comprehensive results package\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        \n",
    "        # Add experiment results\n",
    "        results_dir = Path(\"data/rag_experiments/results\")\n",
    "        if results_dir.exists():\n",
    "            for file_path in results_dir.rglob(\"*\"):\n",
    "                if file_path.is_file():\n",
    "                    arcname = f\"results/{file_path.relative_to(results_dir)}\"\n",
    "                    zipf.write(file_path, arcname)\n",
    "                    print(f\"   ğŸ“„ Added: {arcname}\")\n",
    "        \n",
    "        # Add visualizations\n",
    "        viz_dir = Path(\"data/rag_experiments/visualizations\")\n",
    "        if viz_dir.exists():\n",
    "            for file_path in viz_dir.rglob(\"*.png\"):\n",
    "                if file_path.is_file():\n",
    "                    arcname = f\"visualizations/{file_path.name}\"\n",
    "                    zipf.write(file_path, arcname)\n",
    "                    print(f\"   ğŸ–¼ï¸  Added: {arcname}\")\n",
    "        \n",
    "        # Add baseline comparisons\n",
    "        baselines_dir = Path(\"data/rag_experiments/baselines\")\n",
    "        if baselines_dir.exists():\n",
    "            for baseline_dir in baselines_dir.iterdir():\n",
    "                if baseline_dir.is_dir():\n",
    "                    results_files = baseline_dir.rglob(\"*.json\")\n",
    "                    for file_path in results_files:\n",
    "                        arcname = f\"baselines/{baseline_dir.name}/{file_path.name}\"\n",
    "                        zipf.write(file_path, arcname)\n",
    "                        print(f\"   ğŸ“Š Added: {arcname}\")\n",
    "        \n",
    "        # Add experiment summary\n",
    "        summary = {\n",
    "            \"experiment_type\": \"Dynamic RAG Comparison\",\n",
    "            \"timestamp\": timestamp,\n",
    "            \"notebook_version\": \"v1.0.0\",\n",
    "            \"description\": \"Comparison of InsightSpike-AI dynamic RAG against baseline methods\",\n",
    "            \"datasets\": [\"NaturalQuestions_sample\", \"HotpotQA_sample\"],\n",
    "            \"methods_compared\": [\"BM25\", \"Static Embeddings\", \"DPR\", \"InsightSpike RAG\"],\n",
    "            \"metrics\": [\"Recall@k\", \"Precision@k\", \"Exact Match\", \"F1 Score\", \"Latency\"]\n",
    "        }\n",
    "        \n",
    "        summary_path = download_dir / \"experiment_summary.json\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        zipf.write(summary_path, \"experiment_summary.json\")\n",
    "        \n",
    "        print(f\"   ğŸ“‹ Added: experiment_summary.json\")\n",
    "    \n",
    "    file_size = zip_path.stat().st_size / (1024 * 1024)  # MB\n",
    "    print(f\"\\nâœ… Results package created successfully!\")\n",
    "    print(f\"ğŸ“¦ File: {zip_path}\")\n",
    "    print(f\"ğŸ“ Size: {file_size:.2f} MB\")\n",
    "    \n",
    "    return zip_path\n",
    "\n",
    "# Create and prepare results for download\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        # Create downloadable package\n",
    "        zip_path = create_downloadable_results()\n",
    "        \n",
    "        # Download in Colab\n",
    "        from google.colab import files\n",
    "        files.download(str(zip_path))\n",
    "        print(\"â¬‡ï¸  Download started in Colab!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating download package: {e}\")\n",
    "        print(\"ğŸ’¡ You can manually download files from the file browser\")\n",
    "        \n",
    "        # Show available files for manual download\n",
    "        results_dir = Path(\"data/rag_experiments/results\")\n",
    "        if results_dir.exists():\n",
    "            print(f\"\\nğŸ“‹ Available result files:\")\n",
    "            for file_path in results_dir.rglob(\"*\"):\n",
    "                if file_path.is_file():\n",
    "                    print(f\"   ğŸ“„ {file_path}\")\n",
    "else:\n",
    "    # Local environment - just create the package\n",
    "    zip_path = create_downloadable_results()\n",
    "    print(f\"ğŸ’¾ Results saved locally: {zip_path}\")\n",
    "    print(\"ğŸ“ Open the 'downloads' folder to access your results\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Experiment complete! Your results are ready for analysis.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
