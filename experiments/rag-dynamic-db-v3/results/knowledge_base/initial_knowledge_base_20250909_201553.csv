id,text,domain,concepts,depth,text_length,n_concepts
1,"Python's Global Interpreter Lock (GIL) prevents multiple threads from executing Python bytecode simultaneously, ensuring thread safety but limiting true parallelism in CPU-bound tasks.",programming,"python, gil, parallelization, memory",technical,184,4
2,"Python uses reference counting with cycle detection for memory management, automatically deallocating objects when their reference count reaches zero.",programming,"python, garbage_collection, memory",technical,150,3
3,"Overfitting occurs when a model learns noise in training data, resulting in poor generalization. Regularization techniques like L1/L2 penalties help prevent this.",ml,"machine_learning, overfitting, regularization",conceptual,162,3
4,"Gradient descent optimizes model parameters by iteratively moving in the direction of steepest decrease in the loss function, with learning rate controlling step size.",ml,"machine_learning, gradient_descent, optimization",technical,167,3
5,"Convolutional Neural Networks (CNNs) use convolution operations to extract hierarchical features from spatial data, with pooling layers reducing dimensionality.",dl,"deep_learning, cnn, neural_network",technical,160,3
6,"Transformers revolutionized NLP by using self-attention mechanisms to process sequences in parallel, eliminating the sequential bottleneck of RNNs.",dl,"transformer, attention, nlp, deep_learning",conceptual,147,4
7,"Backpropagation calculates gradients through automatic differentiation, propagating error signals backward through the network to update weights.",dl,"deep_learning, backpropagation, neural_network, gradient_descent",technical,145,4
8,"BERT uses bidirectional transformers with masked language modeling pre-training, enabling deep contextualized word representations for downstream tasks.",nlp,"bert, transformer, nlp, embedding",technical,152,4
9,"Tokenization strategies like BPE and WordPiece balance vocabulary size with representation quality, enabling subword modeling for handling OOV words.",nlp,"tokenization, nlp, preprocessing",practical,149,3
10,"GPU acceleration leverages thousands of CUDA cores for parallel matrix operations, providing 10-100x speedups for deep learning training.",performance,"gpu, optimization, deep_learning, parallelization",practical,137,4
11,"Mixed precision training uses FP16 for forward/backward passes while maintaining FP32 master weights, reducing memory usage and improving throughput.",performance,"optimization, gpu, memory, deep_learning",practical,149,4
12,"Feature engineering transforms raw data into informative representations, with techniques like one-hot encoding, scaling, and polynomial features improving model performance.",data,"feature_engineering, preprocessing, data, machine_learning",practical,174,4
13,"Data normalization techniques like StandardScaler and MinMaxScaler ensure features have similar scales, preventing gradient descent from being dominated by large-scale features.",data,"normalization, preprocessing, data, optimization",practical,177,4
