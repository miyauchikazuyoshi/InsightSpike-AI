#!/usr/bin/env python3
"""æœ€çµ‚ç‰ˆgeDIGãƒŠãƒ“ã‚²ãƒ¼ã‚¿ãƒ¼ï¼šå®Ÿç”¨çš„ãªçµ±åˆå®Ÿè£…"""

import sys
from pathlib import Path
import numpy as np
from typing import Dict, List, Tuple, Optional, Set, Any
from dataclasses import dataclass, field
from collections import defaultdict, deque
import matplotlib.pyplot as plt

sys.path.append(str(Path(__file__).parent.parent.parent))

from insightspike.environments.maze import SimpleMaze
from insightspike.maze_experimental.maze_config import MazeNavigatorConfig


@dataclass
class ActionMemory:
    """è¡Œå‹•ã®è¨˜æ†¶ï¼šä½ç½®ã§ã®è¡Œå‹•ã¨ãã®çµæœ"""
    position: Tuple[int, int]
    action: int
    result_position: Tuple[int, int]
    success: bool
    is_dead_end: bool = False
    is_goal: bool = False
    goal_progress: float = 0.0
    visit_time: int = 0


@dataclass
class PositionNode:
    """ä½ç½®ãƒãƒ¼ãƒ‰ï¼šè¡Œå‹•è¨˜æ†¶ã¨çµŒè·¯æƒ…å ±ã‚’æŒã¤"""
    position: Tuple[int, int]
    action_memories: List[ActionMemory] = field(default_factory=list)
    visit_count: int = 0
    is_junction: bool = False
    is_dead_end: bool = False
    is_goal: bool = False
    dead_end_actions: Set[int] = field(default_factory=set)  # è¡Œãæ­¢ã¾ã‚Šã«ã¤ãªãŒã‚‹è¡Œå‹•
    goal_path_actions: Set[int] = field(default_factory=set)  # ã‚´ãƒ¼ãƒ«ã«ã¤ãªãŒã‚‹è¡Œå‹•
    
    def get_action_value(self, action: int) -> float:
        """è¡Œå‹•ã®ä¾¡å€¤ã‚’è¨ˆç®—"""
        # ã‚´ãƒ¼ãƒ«ã¸ã®çµŒè·¯ãªã‚‰é«˜è©•ä¾¡
        if action in self.goal_path_actions:
            return 10.0
            
        # è¡Œãæ­¢ã¾ã‚Šãªã‚‰ä½è©•ä¾¡
        if action in self.dead_end_actions:
            return -10.0
            
        # è¡Œå‹•è¨˜æ†¶ã‹ã‚‰è©•ä¾¡
        memories = [m for m in self.action_memories if m.action == action]
        if memories:
            # æˆåŠŸç‡ã¨é€²æ—åº¦ã§è©•ä¾¡
            success_rate = sum(1 for m in memories if m.success) / len(memories)
            avg_progress = np.mean([m.goal_progress for m in memories])
            return success_rate * 2.0 + avg_progress
        
        return 0.0
        
    def get_action_count(self, action: int) -> int:
        """è¡Œå‹•ã®å®Ÿè¡Œå›æ•°"""
        return sum(1 for m in self.action_memories if m.action == action)


class FinalGeDIGNavigator:
    """æœ€çµ‚ç‰ˆgeDIGãƒŠãƒ“ã‚²ãƒ¼ã‚¿ãƒ¼"""
    
    def __init__(self, config: MazeNavigatorConfig):
        self.config = config
        self.nodes: Dict[Tuple[int, int], PositionNode] = {}
        self.goal_pos: Optional[Tuple[int, int]] = None
        self.time_step = 0
        
        # çµŒè·¯è¿½è·¡
        self.path_history: List[Tuple[int, int]] = []
        self.current_path_start: Optional[Tuple[int, int]] = None
        self.current_path_action: Optional[int] = None
        
        # ãƒ‡ãƒƒãƒ‰ã‚¨ãƒ³ãƒ‰å›é¿
        self.recent_positions = deque(maxlen=10)  # æœ€è¿‘ã®ä½ç½®å±¥æ­´
        
    def _get_or_create_node(self, pos: Tuple[int, int]) -> PositionNode:
        """ãƒãƒ¼ãƒ‰ã‚’å–å¾—ã¾ãŸã¯ä½œæˆ"""
        if pos not in self.nodes:
            self.nodes[pos] = PositionNode(position=pos)
        return self.nodes[pos]
        
    def _manhattan_distance(self, pos1: Tuple[int, int], pos2: Tuple[int, int]) -> float:
        """ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢"""
        return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])
        
    def _detect_loop(self) -> bool:
        """ãƒ«ãƒ¼ãƒ—ã‚’æ¤œå‡º"""
        if len(self.recent_positions) < 4:
            return False
        # åŒã˜ä½ç½®ã«ç¹°ã‚Šè¿”ã—æˆ»ã£ã¦ã„ã‚‹ã‹
        position_counts = defaultdict(int)
        for pos in self.recent_positions:
            position_counts[pos] += 1
        return any(count >= 3 for count in position_counts.values())
        
    def _propagate_dead_end(self, path: List[Tuple[int, int]], start_action: int):
        """è¡Œãæ­¢ã¾ã‚Šæƒ…å ±ã‚’çµŒè·¯ä¸Šã«ä¼æ’­"""
        if len(path) < 2:
            return
            
        # é–‹å§‹ç‚¹ã«è¡Œãæ­¢ã¾ã‚Šæƒ…å ±ã‚’è¨˜éŒ²
        start_node = self._get_or_create_node(path[0])
        start_node.dead_end_actions.add(start_action)
        
        # çµŒè·¯ä¸Šã®å„ãƒãƒ¼ãƒ‰ã«æƒ…å ±ã‚’ä¼æ’­
        for i in range(len(path) - 1):
            node = self._get_or_create_node(path[i])
            next_pos = path[i + 1]
            
            # ã©ã®è¡Œå‹•ã§æ¬¡ã®ä½ç½®ã«ç§»å‹•ã—ãŸã‹åˆ¤å®š
            for action in range(4):
                dx, dy = SimpleMaze.ACTIONS[action]
                if (path[i][0] + dx, path[i][1] + dy) == next_pos:
                    node.dead_end_actions.add(action)
                    break
                    
    def _propagate_goal_path(self, path: List[Tuple[int, int]], start_action: int):
        """ã‚´ãƒ¼ãƒ«çµŒè·¯æƒ…å ±ã‚’ä¼æ’­"""
        if len(path) < 2:
            return
            
        # é–‹å§‹ç‚¹ã«ã‚´ãƒ¼ãƒ«çµŒè·¯æƒ…å ±ã‚’è¨˜éŒ²
        start_node = self._get_or_create_node(path[0])
        start_node.goal_path_actions.add(start_action)
        
        # çµŒè·¯ä¸Šã®å„ãƒãƒ¼ãƒ‰ã«æƒ…å ±ã‚’ä¼æ’­
        for i in range(len(path) - 1):
            node = self._get_or_create_node(path[i])
            next_pos = path[i + 1]
            
            # ã©ã®è¡Œå‹•ã§æ¬¡ã®ä½ç½®ã«ç§»å‹•ã—ãŸã‹åˆ¤å®š
            for action in range(4):
                dx, dy = SimpleMaze.ACTIONS[action]
                if (path[i][0] + dx, path[i][1] + dy) == next_pos:
                    node.goal_path_actions.add(action)
                    break
                    
    def decide_action(self, obs, maze) -> int:
        """è¦³æ¸¬ã‹ã‚‰è¡Œå‹•ã‚’æ±ºå®š"""
        current_pos = obs.position
        current_node = self._get_or_create_node(current_pos)
        current_node.visit_count += 1
        
        # ãƒãƒ¼ãƒ‰å±æ€§æ›´æ–°
        current_node.is_junction = obs.is_junction
        current_node.is_dead_end = obs.is_dead_end
        current_node.is_goal = obs.is_goal
        
        # ä½ç½®å±¥æ­´æ›´æ–°
        self.recent_positions.append(current_pos)
        
        # ã‚´ãƒ¼ãƒ«ç™ºè¦‹
        if obs.is_goal and not self.goal_pos:
            self.goal_pos = current_pos
            print(f"ğŸ¯ ã‚´ãƒ¼ãƒ«ç™ºè¦‹ï¼ä½ç½®: {self.goal_pos}")
            # ã‚´ãƒ¼ãƒ«ã¾ã§ã®çµŒè·¯ã‚’è¨˜éŒ²
            if self.current_path_start and self.current_path_action is not None:
                full_path = [self.current_path_start] + self.path_history[self.path_history.index(self.current_path_start)+1:]
                self._propagate_goal_path(full_path, self.current_path_action)
                
        # è¡Œãæ­¢ã¾ã‚Šåˆ°é”
        if obs.is_dead_end:
            print(f"ğŸ’€ è¡Œãæ­¢ã¾ã‚Šåˆ°é”: {current_pos}")
            if self.current_path_start and self.current_path_action is not None:
                # ç¾åœ¨ã®çµŒè·¯ã‚’è¡Œãæ­¢ã¾ã‚Šã¨ã—ã¦è¨˜éŒ²
                start_idx = self.path_history.index(self.current_path_start) if self.current_path_start in self.path_history else 0
                dead_path = self.path_history[start_idx:]
                self._propagate_dead_end(dead_path, self.current_path_action)
                
        # æ–°ã—ã„çµŒè·¯ã®é–‹å§‹åˆ¤å®š
        if obs.is_junction or current_node.visit_count == 1 or obs.is_dead_end:
            self.current_path_start = current_pos
            
        # å„è¡Œå‹•ã®è©•ä¾¡
        action_scores = {}
        
        for action in obs.possible_moves:
            # åŸºæœ¬ä¾¡å€¤
            base_value = current_node.get_action_value(action)
            
            # è©¦è¡Œå›æ•°ã«ã‚ˆã‚‹æƒ…å ±åˆ©å¾—
            action_count = current_node.get_action_count(action)
            ig = 2.0 / (action_count + 1)
            
            # ãƒ«ãƒ¼ãƒ—æ¤œå‡ºæ™‚ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
            loop_penalty = 0.0
            if self._detect_loop():
                # æœ€ã‚‚è©¦è¡Œã•ã‚Œã¦ã„ãªã„è¡Œå‹•ã‚’å„ªé‡
                loop_penalty = -action_count * 2.0
                
            # æœ€çµ‚ã‚¹ã‚³ã‚¢
            score = base_value + self.config.k_ig * ig + loop_penalty
            action_scores[action] = score
            
        # æœ€é©è¡Œå‹•ã‚’é¸æŠ
        if action_scores:
            # Îµ-greedyæˆ¦ç•¥
            if np.random.random() < 0.1:  # 10%ã®ç¢ºç‡ã§ãƒ©ãƒ³ãƒ€ãƒ 
                best_action = np.random.choice(obs.possible_moves)
            else:
                best_action = max(action_scores.items(), key=lambda x: x[1])[0]
                
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
            if current_node.visit_count <= 2 or obs.is_junction:
                print(f"\nä½ç½®{current_pos}ã§ã®æ„æ€æ±ºå®š (è¨ªå•{current_node.visit_count}å›ç›®):")
                for a in obs.possible_moves:
                    print(f"  {['ä¸Š','å³','ä¸‹','å·¦'][a]}: {action_scores[a]:.2f}")
                print(f"  â†’ é¸æŠ: {['ä¸Š','å³','ä¸‹','å·¦'][best_action]}")
                
            # æ–°çµŒè·¯ã®é–‹å§‹è¡Œå‹•ã‚’è¨˜éŒ²
            if current_pos == self.current_path_start:
                self.current_path_action = best_action
                
            return best_action
        else:
            return np.random.choice([0, 1, 2, 3])  # ç·Šæ€¥æ™‚
            
    def update_after_action(self, old_pos: Tuple[int, int], action: int, 
                           new_pos: Tuple[int, int], obs):
        """è¡Œå‹•å¾Œã®æ›´æ–°"""
        # çµŒè·¯å±¥æ­´æ›´æ–°
        if new_pos not in self.path_history or self.path_history[-1] != new_pos:
            self.path_history.append(new_pos)
            
        # è¡Œå‹•è¨˜æ†¶ã‚’ä½œæˆ
        success = old_pos != new_pos
        goal_progress = 0.0
        
        if self.goal_pos and success:
            dist_before = self._manhattan_distance(old_pos, self.goal_pos)
            dist_after = self._manhattan_distance(new_pos, self.goal_pos)
            goal_progress = dist_before - dist_after
            
        memory = ActionMemory(
            position=old_pos,
            action=action,
            result_position=new_pos,
            success=success,
            is_dead_end=obs.is_dead_end,
            is_goal=obs.is_goal,
            goal_progress=goal_progress,
            visit_time=self.time_step
        )
        
        # ãƒãƒ¼ãƒ‰ã«è¨˜æ†¶ã‚’è¿½åŠ 
        node = self._get_or_create_node(old_pos)
        node.action_memories.append(memory)
        
        self.time_step += 1
        
    def get_statistics(self) -> Dict:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        total_memories = sum(len(n.action_memories) for n in self.nodes.values())
        dead_ends = sum(1 for n in self.nodes.values() if n.is_dead_end)
        junctions = sum(1 for n in self.nodes.values() if n.is_junction)
        
        return {
            'nodes': len(self.nodes),
            'memories': total_memories,
            'dead_ends': dead_ends,
            'junctions': junctions,
            'goal_found': self.goal_pos is not None
        }


def demonstrate_final_gedig():
    """æœ€çµ‚ç‰ˆgeDIGãƒŠãƒ“ã‚²ãƒ¼ã‚¿ãƒ¼ã®ãƒ‡ãƒ¢"""
    print("æœ€çµ‚ç‰ˆgeDIGãƒŠãƒ“ã‚²ãƒ¼ã‚¿ãƒ¼ï¼šå®Ÿç”¨çš„ãªçµ±åˆå®Ÿè£…")
    print("=" * 60)
    print("ç‰¹å¾´ï¼š")
    print("- è¡Œå‹•è¨˜æ†¶ã«ã‚ˆã‚‹çµŒé¨“ã®è“„ç©")
    print("- è¡Œãæ­¢ã¾ã‚Šæƒ…å ±ã®å³åº§ã®ä¼æ’­")
    print("- ãƒ«ãƒ¼ãƒ—æ¤œå‡ºã¨å›é¿")
    print("- ã‚·ãƒ³ãƒ—ãƒ«ã§åŠ¹ç‡çš„ãªå®Ÿè£…")
    print("=" * 60)
    
    config = MazeNavigatorConfig()
    config.w_ged = 1.0
    config.k_ig = 2.0
    
    # è¤‡æ•°è©¦è¡Œã§æ€§èƒ½è©•ä¾¡
    n_trials = 5
    results = []
    
    for trial in range(n_trials):
        print(f"\nè©¦è¡Œ {trial + 1}/{n_trials}")
        print("-" * 40)
        
        np.random.seed(trial + 200)
        maze = SimpleMaze(size=(15, 15), maze_type='dfs')
        navigator = FinalGeDIGNavigator(config)
        
        print(f"è¿·è·¯: {maze.size}")
        print(f"ã‚¹ã‚¿ãƒ¼ãƒˆ: {maze.start_pos} â†’ ã‚´ãƒ¼ãƒ«: {maze.goal_pos}")
        
        obs = maze.reset()
        steps = 0
        
        for _ in range(1000):  # ã‚ˆã‚Šé•·ã„åˆ¶é™æ™‚é–“
            old_pos = obs.position
            action = navigator.decide_action(obs, maze)
            obs, reward, done, info = maze.step(action)
            navigator.update_after_action(old_pos, action, obs.position, obs)
            steps += 1
            
            # é€²æ—è¡¨ç¤º
            if steps % 100 == 0:
                stats = navigator.get_statistics()
                print(f"  ã‚¹ãƒ†ãƒƒãƒ—{steps}: ãƒãƒ¼ãƒ‰{stats['nodes']}, è¡Œãæ­¢ã¾ã‚Š{stats['dead_ends']}")
                
            if done and maze.agent_pos == maze.goal_pos:
                print(f"\nâœ… ã‚´ãƒ¼ãƒ«åˆ°é”ï¼ã‚¹ãƒ†ãƒƒãƒ—æ•°: {steps}")
                stats = navigator.get_statistics()
                results.append({
                    'success': True,
                    'steps': steps,
                    **stats
                })
                break
        else:
            print(f"\nâŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ{steps}ã‚¹ãƒ†ãƒƒãƒ—ï¼‰")
            stats = navigator.get_statistics()
            results.append({
                'success': False,
                'steps': steps,
                **stats
            })
            
        # çµ±è¨ˆè¡¨ç¤º
        print(f"\næ¢ç´¢çµ±è¨ˆ:")
        print(f"  è¨ªå•ãƒãƒ¼ãƒ‰æ•°: {stats['nodes']}")
        print(f"  è¡Œå‹•è¨˜æ†¶æ•°: {stats['memories']}")
        print(f"  ç™ºè¦‹ã—ãŸè¡Œãæ­¢ã¾ã‚Š: {stats['dead_ends']}")
        print(f"  åˆ†å²ç‚¹: {stats['junctions']}")
        
    # å…¨ä½“çµ±è¨ˆ
    print("\n" + "=" * 60)
    print("å…¨è©¦è¡Œã®çµæœ:")
    success_count = sum(1 for r in results if r['success'])
    success_results = [r for r in results if r['success']]
    
    print(f"æˆåŠŸç‡: {success_count}/{n_trials} ({success_count/n_trials*100:.0f}%)")
    
    if success_results:
        print(f"å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆæˆåŠŸæ™‚ï¼‰: {np.mean([r['steps'] for r in success_results]):.1f}")
        print(f"å¹³å‡ãƒãƒ¼ãƒ‰æ•°: {np.mean([r['nodes'] for r in success_results]):.1f}")
        print(f"å¹³å‡è¡Œãæ­¢ã¾ã‚Šç™ºè¦‹æ•°: {np.mean([r['dead_ends'] for r in success_results]):.1f}")
        
    print("\n" + "=" * 60)
    print("âœ¨ æœ€çµ‚ç‰ˆgeDIGã®å®Ÿè£…ï¼š")
    print("âœ¨ ã‚·ãƒ³ãƒ—ãƒ«ãªãŒã‚‰åŠ¹æœçš„ãªè¡Œå‹•è¨˜æ†¶")
    print("âœ¨ è¡Œãæ­¢ã¾ã‚Šæƒ…å ±ã®å³åº§ã®æ´»ç”¨")
    print("âœ¨ ãƒ«ãƒ¼ãƒ—æ¤œå‡ºã«ã‚ˆã‚‹ç„¡é™ãƒ«ãƒ¼ãƒ—å›é¿")
    print("âœ¨ å®Ÿç”¨çš„ã§æ‹¡å¼µå¯èƒ½ãªè¨­è¨ˆ")


if __name__ == "__main__":
    demonstrate_final_gedig()