#!/usr/bin/env python3
"""ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã«åŸºã¥ãè‡ªå¾‹çš„geDIGãƒŠãƒ“ã‚²ãƒ¼ã‚¿ãƒ¼ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""

import sys
from pathlib import Path
import numpy as np
from typing import Dict, List, Tuple, Optional, Set
from dataclasses import dataclass, field
from collections import defaultdict
import matplotlib.pyplot as plt

sys.path.append(str(Path(__file__).parent.parent.parent))

from insightspike.environments.maze import SimpleMaze
from insightspike.maze_experimental.maze_config import MazeNavigatorConfig


@dataclass
class NavigationEpisode:
    """ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼šä½ç½®ã§ã®è¡Œå‹•ã¨ãã®çµæœ"""
    position: Tuple[int, int]
    action: int
    result_position: Tuple[int, int]
    success: bool
    goal_distance_before: float
    goal_distance_after: float
    timestamp: int
    
    @property
    def goal_progress(self) -> float:
        """ã‚´ãƒ¼ãƒ«ã¸ã®æ¥è¿‘åº¦ï¼ˆæ­£ã®å€¤ãŒè‰¯ã„ï¼‰"""
        return self.goal_distance_before - self.goal_distance_after


@dataclass
class PositionMemory:
    """ä½ç½®ã”ã¨ã®è¨˜æ†¶"""
    position: Tuple[int, int]
    episodes: List[NavigationEpisode] = field(default_factory=list)
    visits: int = 0
    last_visit: int = 0
    
    def get_action_statistics(self, action: int) -> Dict:
        """ç‰¹å®šã®è¡Œå‹•ã®çµ±è¨ˆã‚’å–å¾—"""
        action_episodes = [e for e in self.episodes if e.action == action]
        if not action_episodes:
            return {'count': 0, 'success_rate': 0.0, 'avg_progress': 0.0}
            
        success_count = sum(1 for e in action_episodes if e.success)
        avg_progress = np.mean([e.goal_progress for e in action_episodes])
        
        return {
            'count': len(action_episodes),
            'success_rate': success_count / len(action_episodes),
            'avg_progress': avg_progress
        }


class EpisodicGeDIGNavigator:
    """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã§è‡ªå¾‹çš„ã«è¡Œå‹•ã‚’æ±ºå®šã™ã‚‹ãƒŠãƒ“ã‚²ãƒ¼ã‚¿ãƒ¼"""
    
    def __init__(self, config: MazeNavigatorConfig):
        self.config = config
        self.position_memories: Dict[Tuple[int, int], PositionMemory] = {}
        self.goal_position: Optional[Tuple[int, int]] = None
        self.current_position: Optional[Tuple[int, int]] = None
        self.time_step = 0
        self.path_history: List[Tuple[int, int]] = []
        
    def _get_or_create_memory(self, position: Tuple[int, int]) -> PositionMemory:
        """ä½ç½®ã®è¨˜æ†¶ã‚’å–å¾—ã¾ãŸã¯ä½œæˆ"""
        if position not in self.position_memories:
            self.position_memories[position] = PositionMemory(position=position)
        return self.position_memories[position]
        
    def _manhattan_distance(self, pos1: Tuple[int, int], pos2: Tuple[int, int]) -> float:
        """ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢"""
        return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])
        
    def add_episode(self, position: Tuple[int, int], action: int, 
                   result_position: Tuple[int, int], success: bool):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¨˜æ†¶ã«è¿½åŠ """
        memory = self._get_or_create_memory(position)
        
        # ã‚´ãƒ¼ãƒ«ã¾ã§ã®è·é›¢è¨ˆç®—
        if self.goal_position:
            dist_before = self._manhattan_distance(position, self.goal_position)
            dist_after = self._manhattan_distance(result_position, self.goal_position)
        else:
            # ã‚´ãƒ¼ãƒ«ãŒæœªçŸ¥ã®å ´åˆã¯æ¢ç´¢ç¯„å›²ã®æ‹¡å¤§ã‚’è©•ä¾¡
            dist_before = 0
            dist_after = -1 if success else 0  # æ–°ã—ã„å ´æ‰€ã¸ã®ç§»å‹•ã‚’ä¿ƒé€²
            
        episode = NavigationEpisode(
            position=position,
            action=action,
            result_position=result_position,
            success=success,
            goal_distance_before=dist_before,
            goal_distance_after=dist_after,
            timestamp=self.time_step
        )
        
        memory.episodes.append(episode)
        memory.visits += 1
        memory.last_visit = self.time_step
        self.time_step += 1
        
    def query_best_action(self, position: Tuple[int, int], 
                         possible_actions: List[int]) -> int:
        """ä½ç½®ã§ã®æœ€é©è¡Œå‹•ã‚’ã‚¯ã‚¨ãƒª"""
        memory = self._get_or_create_memory(position)
        
        # å„è¡Œå‹•ã®geDIGã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        action_scores = {}
        
        for action in range(4):
            if action not in possible_actions:
                action_scores[action] = float('-inf')
                continue
                
            stats = memory.get_action_statistics(action)
            
            # ã‚°ãƒ©ãƒ•ç·¨é›†è·é›¢ï¼ˆGEDï¼‰çš„ãªè©•ä¾¡
            if stats['count'] == 0:
                # æœªæ¢ç´¢ã¯é«˜ã„æƒ…å ±åˆ©å¾—
                ged_score = 0.0
                ig_score = 3.0  # é«˜ã„æƒ…å ±åˆ©å¾—
            else:
                # æˆåŠŸç‡ã¨ã‚´ãƒ¼ãƒ«æ¥è¿‘åº¦ã‹ã‚‰è©•ä¾¡
                ged_score = stats['success_rate'] * (1 + stats['avg_progress'])
                # æ¢ç´¢å›æ•°ãŒå¤šã„ã»ã©æƒ…å ±åˆ©å¾—ã¯ä½ã„
                ig_score = 1.0 / (stats['count'] + 1)
                
            # æ™‚é–“çš„æ¸›è¡°ã‚’è€ƒæ…®ï¼ˆæœ€è¿‘ã®è¨˜æ†¶ã‚’é‡è¦–ï¼‰
            recency_factor = 1.0
            if memory.last_visit > 0:
                recency_factor = np.exp(-(self.time_step - memory.last_visit) * 0.01)
                
            # geDIGç›®çš„é–¢æ•°
            gediq_score = self.config.w_ged * ged_score - self.config.k_ig * ig_score
            gediq_score *= recency_factor
            
            action_scores[action] = gediq_score
            
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®è¡Œå‹•ã‚’é¸æŠï¼ˆÎµ-greedyï¼‰
        if np.random.random() < self.config.exploration_epsilon:
            return np.random.choice(possible_actions)
        else:
            valid_actions = [(a, s) for a, s in action_scores.items() 
                           if a in possible_actions]
            if valid_actions:
                return max(valid_actions, key=lambda x: x[1])[0]
            else:
                return np.random.choice(possible_actions)
                
    def decide_action(self, obs, maze) -> int:
        """è¦³æ¸¬ã«åŸºã¥ã„ã¦è¡Œå‹•ã‚’æ±ºå®š"""
        self.current_position = obs.position
        self.path_history.append(self.current_position)
        
        # ã‚´ãƒ¼ãƒ«ç™ºè¦‹
        if obs.is_goal and not self.goal_position:
            self.goal_position = self.current_position
            print(f"ğŸ¯ ã‚´ãƒ¼ãƒ«ç™ºè¦‹ï¼ä½ç½®: {self.goal_position}")
            
        # æœ€é©è¡Œå‹•ã‚’ã‚¯ã‚¨ãƒª
        return self.query_best_action(self.current_position, obs.possible_moves)
        
    def update_after_move(self, old_pos: Tuple[int, int], 
                         new_pos: Tuple[int, int], action: int):
        """ç§»å‹•å¾Œã®æ›´æ–°"""
        success = old_pos != new_pos
        self.add_episode(old_pos, action, new_pos, success)
        
    def get_exploration_statistics(self) -> Dict:
        """æ¢ç´¢çµ±è¨ˆã‚’å–å¾—"""
        total_episodes = sum(len(m.episodes) for m in self.position_memories.values())
        successful_episodes = sum(
            sum(1 for e in m.episodes if e.success) 
            for m in self.position_memories.values()
        )
        
        return {
            'positions_visited': len(self.position_memories),
            'total_episodes': total_episodes,
            'successful_episodes': successful_episodes,
            'failure_episodes': total_episodes - successful_episodes,
            'average_visits_per_position': total_episodes / len(self.position_memories) if self.position_memories else 0
        }


def visualize_episodic_navigation():
    """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã®å¯è¦–åŒ–"""
    print("ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã«ã‚ˆã‚‹è‡ªå¾‹çš„geDIGãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³")
    print("=" * 60)
    
    config = MazeNavigatorConfig()
    
    # è¤‡æ•°ã®è¿·è·¯ã§ãƒ†ã‚¹ãƒˆ
    n_trials = 5
    results = []
    
    for trial in range(n_trials):
        print(f"\nè©¦è¡Œ {trial + 1}/{n_trials}")
        print("-" * 40)
        
        np.random.seed(trial)
        maze = SimpleMaze(size=(15, 15), maze_type='dfs')
        navigator = EpisodicGeDIGNavigator(config)
        
        obs = maze.reset()
        steps = 0
        
        for _ in range(500):
            old_pos = obs.position
            action = navigator.decide_action(obs, maze)
            obs, reward, done, info = maze.step(action)
            new_pos = obs.position
            
            navigator.update_after_move(old_pos, new_pos, action)
            steps += 1
            
            if done and maze.agent_pos == maze.goal_pos:
                print(f"âœ… ã‚´ãƒ¼ãƒ«åˆ°é”ï¼ã‚¹ãƒ†ãƒƒãƒ—æ•°: {steps}")
                break
                
        stats = navigator.get_exploration_statistics()
        stats['steps'] = steps
        stats['success'] = maze.agent_pos == maze.goal_pos
        results.append(stats)
        
        print(f"æ¢ç´¢çµ±è¨ˆ:")
        print(f"  è¨ªå•ä½ç½®æ•°: {stats['positions_visited']}")
        print(f"  ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {stats['total_episodes']}")
        print(f"  æˆåŠŸ/å¤±æ•—: {stats['successful_episodes']}/{stats['failure_episodes']}")
    
    # çµæœã®ã¾ã¨ã‚
    print("\n" + "=" * 60)
    print("å…¨è©¦è¡Œã®çµæœ:")
    
    success_count = sum(1 for r in results if r['success'])
    avg_steps = np.mean([r['steps'] for r in results if r['success']])
    avg_positions = np.mean([r['positions_visited'] for r in results])
    
    print(f"æˆåŠŸç‡: {success_count}/{n_trials} ({success_count/n_trials*100:.0f}%)")
    print(f"å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆæˆåŠŸæ™‚ï¼‰: {avg_steps:.1f}")
    print(f"å¹³å‡æ¢ç´¢ä½ç½®æ•°: {avg_positions:.1f}")
    
    print("\né‡è¦ãªç‰¹å¾´:")
    print("âœ¨ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã‹ã‚‰è‡ªå¾‹çš„ã«è¡Œå‹•æ±ºå®š")
    print("âœ¨ ã‚´ãƒ¼ãƒ«ä½ç½®ã¯æ¢ç´¢ä¸­ã«ç™ºè¦‹")
    print("âœ¨ å¤±æ•—çµŒé¨“ã‚‚è²´é‡ãªè¨˜æ†¶ã¨ã—ã¦æ´»ç”¨")
    print("âœ¨ geDIGç›®çš„é–¢æ•°ã«ã‚ˆã‚ŠåŠ¹ç‡çš„ãªæ¢ç´¢ã‚’å®Ÿç¾")
    print("=" * 60)


if __name__ == "__main__":
    visualize_episodic_navigation()