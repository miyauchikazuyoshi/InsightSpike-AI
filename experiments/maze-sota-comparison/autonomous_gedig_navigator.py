#!/usr/bin/env python3
"""è‡ªå¾‹çš„ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã«åŸºã¥ãgeDIGãƒŠãƒ“ã‚²ãƒ¼ã‚¿ãƒ¼"""

import sys
from pathlib import Path
import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, field
import json

sys.path.append(str(Path(__file__).parent.parent.parent))

from insightspike.environments.maze import SimpleMaze
from insightspike.maze_experimental.maze_config import MazeNavigatorConfig


@dataclass
class MovementEpisode:
    """ç§»å‹•ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼šä½ç½®Aã‹ã‚‰ä½ç½®Bã¸ã®ç§»å‹•è©¦è¡Œã®è¨˜éŒ²"""
    from_pos: Tuple[int, int]
    to_pos: Tuple[int, int]
    action: int  # 0:ä¸Š, 1:å³, 2:ä¸‹, 3:å·¦
    success: bool
    distance_to_goal: float  # ã‚´ãƒ¼ãƒ«ã¾ã§ã®è·é›¢å¤‰åŒ–
    timestamp: int


@dataclass 
class PositionQuery:
    """ä½ç½®ã‚¯ã‚¨ãƒªï¼šç‰¹å®šã®ä½ç½®ã§æ¬¡ã«é€²ã‚€ã¹ãæ–¹å‘ã‚’å•ã†"""
    current_pos: Tuple[int, int]
    goal_pos: Tuple[int, int]
    context: str = "ã‚´ãƒ¼ãƒ«ã«åˆ°é”ã™ã‚‹ãŸã‚ã«æ¬¡ã«é€²ã‚€ã¹ãæ–¹å‘ã¯ï¼Ÿ"


class AutonomousGeDIGNavigator:
    """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã‹ã‚‰è‡ªå¾‹çš„ã«è¡Œå‹•ã‚’æ±ºå®šã™ã‚‹ãƒŠãƒ“ã‚²ãƒ¼ã‚¿ãƒ¼"""
    
    def __init__(self, config: MazeNavigatorConfig):
        self.config = config
        self.episode_memory: List[MovementEpisode] = []
        self.position_history: List[Tuple[int, int]] = []
        self.goal_pos: Optional[Tuple[int, int]] = None
        self.time_step = 0
        
    def add_movement_episode(self, from_pos: Tuple[int, int], 
                           to_pos: Tuple[int, int], 
                           action: int, 
                           success: bool):
        """ç§»å‹•ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¨˜æ†¶ã«è¿½åŠ """
        if self.goal_pos:
            # ã‚´ãƒ¼ãƒ«ã¾ã§ã®è·é›¢å¤‰åŒ–ã‚’è¨ˆç®—
            dist_before = self._manhattan_distance(from_pos, self.goal_pos)
            dist_after = self._manhattan_distance(to_pos, self.goal_pos)
            distance_change = dist_before - dist_after  # æ­£ã®å€¤ãªã‚‰ã‚´ãƒ¼ãƒ«ã«è¿‘ã¥ã„ãŸ
        else:
            distance_change = 0.0
            
        episode = MovementEpisode(
            from_pos=from_pos,
            to_pos=to_pos,
            action=action,
            success=success,
            distance_to_goal=distance_change,
            timestamp=self.time_step
        )
        
        self.episode_memory.append(episode)
        self.time_step += 1
        
    def query_next_action(self, current_pos: Tuple[int, int]) -> int:
        """ç¾åœ¨ä½ç½®ã§ã®ã‚¯ã‚¨ãƒªï¼šæ¬¡ã«é€²ã‚€ã¹ãæ–¹å‘ã¯ï¼Ÿ"""
        
        # é–¢é€£ã™ã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’æ¤œç´¢
        relevant_episodes = self._find_relevant_episodes(current_pos)
        
        if not relevant_episodes:
            # è¨˜æ†¶ã«ãªã„å ´åˆã¯æ¢ç´¢ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰
            return np.random.randint(0, 4)
            
        # geDIGçš„ãªè©•ä¾¡ï¼šæ§‹é€ çš„é¡ä¼¼æ€§ã¨æƒ…å ±åˆ©å¾—ã®ãƒãƒ©ãƒ³ã‚¹
        best_action = self._evaluate_actions_gedig(current_pos, relevant_episodes)
        
        return best_action
        
    def _find_relevant_episodes(self, pos: Tuple[int, int]) -> List[MovementEpisode]:
        """ç¾åœ¨ä½ç½®ã«é–¢é€£ã™ã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’æ¤œç´¢"""
        relevant = []
        
        for episode in self.episode_memory:
            # åŒã˜ä½ç½®ã‹ã‚‰ã®ç§»å‹•
            if episode.from_pos == pos:
                relevant.append(episode)
            # è¿‘ã„ä½ç½®ã‹ã‚‰ã®ç§»å‹•ï¼ˆæ§‹é€ çš„é¡ä¼¼æ€§ï¼‰
            elif self._manhattan_distance(episode.from_pos, pos) <= 2:
                relevant.append(episode)
                
        return relevant
        
    def _evaluate_actions_gedig(self, pos: Tuple[int, int], 
                               episodes: List[MovementEpisode]) -> int:
        """geDIGçš„ãªè¡Œå‹•è©•ä¾¡"""
        action_scores = {}
        
        for action in range(4):
            # ã“ã®è¡Œå‹•ã«é–¢ã™ã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’åé›†
            action_episodes = [e for e in episodes if e.action == action]
            
            if not action_episodes:
                # æœªæ¢ç´¢ã®è¡Œå‹•ã¯é«˜ã„æƒ…å ±åˆ©å¾—
                action_scores[action] = self.config.k_ig * 2.0
                continue
                
            # GEDï¼ˆã‚°ãƒ©ãƒ•ç·¨é›†è·é›¢ï¼‰çš„ãªè©•ä¾¡
            # æˆåŠŸã—ãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã»ã©ä¾¡å€¤ãŒé«˜ã„
            success_rate = sum(1 for e in action_episodes if e.success) / len(action_episodes)
            
            # ã‚´ãƒ¼ãƒ«ã¸ã®æ¥è¿‘åº¦
            avg_goal_progress = np.mean([e.distance_to_goal for e in action_episodes])
            
            # æ™‚é–“çš„æ¸›è¡°ï¼ˆæ–°ã—ã„è¨˜æ†¶ã»ã©é‡è¦ï¼‰
            recency_weight = np.mean([
                np.exp(-(self.time_step - e.timestamp) * 0.1) 
                for e in action_episodes
            ])
            
            # geDIGè©•ä¾¡é–¢æ•°
            ged_score = success_rate * avg_goal_progress * recency_weight
            ig_score = 1.0 / (len(action_episodes) + 1)  # è©¦è¡Œå›æ•°ãŒå°‘ãªã„ã»ã©æƒ…å ±åˆ©å¾—å¤§
            
            action_scores[action] = (
                self.config.w_ged * ged_score - 
                self.config.k_ig * ig_score
            )
            
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®è¡Œå‹•ã‚’é¸æŠ
        return max(action_scores.items(), key=lambda x: x[1])[0]
        
    def _manhattan_distance(self, pos1: Tuple[int, int], 
                           pos2: Tuple[int, int]) -> float:
        """ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢"""
        return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])
        
    def decide_action(self, obs, maze) -> int:
        """è¦³æ¸¬ã«åŸºã¥ã„ã¦è¡Œå‹•ã‚’æ±ºå®šï¼ˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹äº’æ›æ€§ã®ãŸã‚ï¼‰"""
        current_pos = obs.position
        
        # ã‚´ãƒ¼ãƒ«ä½ç½®ã‚’è¨˜éŒ²
        if obs.is_goal:
            self.goal_pos = current_pos
            
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã«åŸºã¥ã„ã¦è¡Œå‹•ã‚’æ±ºå®š
        action = self.query_next_action(current_pos)
        
        # å¯èƒ½ãªè¡Œå‹•ã®ã¿ã‚’é¸æŠ
        if action not in obs.possible_moves:
            # å£ã«ã¶ã¤ã‹ã‚‹ã“ã¨ã‚‚è¨˜æ†¶ã¨ã—ã¦é‡è¦
            self.add_movement_episode(
                from_pos=current_pos,
                to_pos=current_pos,  # ç§»å‹•ã§ããªã‹ã£ãŸ
                action=action,
                success=False
            )
            # åˆ¥ã®è¡Œå‹•ã‚’é¸æŠ
            if obs.possible_moves:
                action = np.random.choice(obs.possible_moves)
            else:
                action = 0
                
        return action
        
    def update_after_action(self, old_pos: Tuple[int, int], 
                           new_pos: Tuple[int, int], 
                           action: int):
        """è¡Œå‹•å¾Œã®æ›´æ–°"""
        success = old_pos != new_pos  # ç§»å‹•ã§ããŸã‹ã©ã†ã‹
        self.add_movement_episode(old_pos, new_pos, action, success)
        
    def explain_decision(self, pos: Tuple[int, int]) -> str:
        """æ„æ€æ±ºå®šã®èª¬æ˜ï¼ˆè§£é‡ˆå¯èƒ½æ€§ï¼‰"""
        episodes = self._find_relevant_episodes(pos)
        
        explanation = f"ä½ç½®{pos}ã§ã®æ„æ€æ±ºå®š:\n"
        explanation += f"é–¢é€£ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {len(episodes)}\n"
        
        for action in range(4):
            action_name = ['ä¸Š', 'å³', 'ä¸‹', 'å·¦'][action]
            action_eps = [e for e in episodes if e.action == action]
            
            if action_eps:
                success_rate = sum(1 for e in action_eps if e.success) / len(action_eps)
                explanation += f"  {action_name}: æˆåŠŸç‡{success_rate:.1%}, è©¦è¡Œ{len(action_eps)}å›\n"
            else:
                explanation += f"  {action_name}: æœªæ¢ç´¢\n"
                
        return explanation


def demonstrate_autonomous_navigation():
    """è‡ªå¾‹çš„ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("è‡ªå¾‹çš„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã«ã‚ˆã‚‹geDIGãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³")
    print("=" * 60)
    
    config = MazeNavigatorConfig(
        ged_weight=1.0,
        ig_weight=2.0,
        temperature=1.0,
        exploration_epsilon=0.0
    )
    
    # è¿·è·¯ã‚’ä½œæˆ
    np.random.seed(42)
    maze = SimpleMaze(size=(10, 10), maze_type='dfs')
    
    # ãƒŠãƒ“ã‚²ãƒ¼ã‚¿ãƒ¼ä½œæˆ
    navigator = AutonomousGeDIGNavigator(config)
    
    # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
    obs = maze.reset()
    path = [obs.position]
    
    print(f"ã‚¹ã‚¿ãƒ¼ãƒˆ: {maze.start_pos}")
    print(f"ã‚´ãƒ¼ãƒ«: {maze.goal_pos}")
    print("-" * 40)
    
    for step in range(200):
        old_pos = obs.position
        
        # è¡Œå‹•æ±ºå®šï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã‹ã‚‰è‡ªå¾‹çš„ã«ï¼‰
        action = navigator.decide_action(obs, maze)
        
        # è¡Œå‹•å®Ÿè¡Œ
        obs, reward, done, info = maze.step(action)
        new_pos = obs.position
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã‚’æ›´æ–°
        navigator.update_after_action(old_pos, new_pos, action)
        
        path.append(new_pos)
        
        # é‡è¦ãªæ™‚ç‚¹ã§ã®èª¬æ˜
        if step % 20 == 0 or obs.is_junction or obs.is_dead_end:
            print(f"\nã‚¹ãƒ†ãƒƒãƒ— {step}: ä½ç½®{old_pos}")
            print(navigator.explain_decision(old_pos))
            
        if done and maze.agent_pos == maze.goal_pos:
            print(f"\nğŸ‰ ã‚´ãƒ¼ãƒ«åˆ°é”ï¼ã‚¹ãƒ†ãƒƒãƒ—æ•°: {step + 1}")
            break
            
    # æœ€çµ‚çµ±è¨ˆ
    print("\n" + "=" * 60)
    print("ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶çµ±è¨ˆ:")
    print(f"ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {len(navigator.episode_memory)}")
    
    success_episodes = [e for e in navigator.episode_memory if e.success]
    print(f"æˆåŠŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {len(success_episodes)}")
    print(f"å¤±æ•—ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {len(navigator.episode_memory) - len(success_episodes)}")
    
    # ä½ç½®ã”ã¨ã®å­¦ç¿’çŠ¶æ³
    position_visits = {}
    for episode in navigator.episode_memory:
        pos = episode.from_pos
        if pos not in position_visits:
            position_visits[pos] = 0
        position_visits[pos] += 1
        
    print(f"è¨ªå•ä½ç½®æ•°: {len(position_visits)}")
    print(f"å¹³å‡è¨ªå•å›æ•°: {np.mean(list(position_visits.values())):.1f}")
    
    print("\né‡è¦ãªæ´å¯Ÿ:")
    print("- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ãŒè‡ªå¾‹çš„ã«è¡Œå‹•ã‚’æ±ºå®š")
    print("- å¤±æ•—ã‚‚é‡è¦ãªè¨˜æ†¶ã¨ã—ã¦æ´»ç”¨")
    print("- æ§‹é€ çš„é¡ä¼¼æ€§ã«ã‚ˆã‚ŠæœªçŸ¥ã®çŠ¶æ³ã§ã‚‚å¯¾å¿œå¯èƒ½")
    print("- geDIGçš„è©•ä¾¡ã«ã‚ˆã‚ŠåŠ¹ç‡çš„ãªæ¢ç´¢ã‚’å®Ÿç¾")


if __name__ == "__main__":
    demonstrate_autonomous_navigation()