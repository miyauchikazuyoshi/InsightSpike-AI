#!/usr/bin/env python3
"""çœŸã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã«ã‚ˆã‚‹geDIGãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³"""

import sys
from pathlib import Path
import numpy as np
from typing import Dict, List, Tuple, Optional, Set
from dataclasses import dataclass, field
import matplotlib.pyplot as plt
from collections import defaultdict

sys.path.append(str(Path(__file__).parent.parent.parent))

from insightspike.environments.maze import SimpleMaze
from insightspike.maze_experimental.maze_config import MazeNavigatorConfig


@dataclass
class Episode:
    """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼šçŠ¶æ³-è¡Œå‹•-çµæœã®è¨˜éŒ²"""
    query: str  # "ä½ç½®(x,y)ã§ã‚´ãƒ¼ãƒ«ã«å‘ã‹ã†ã«ã¯ï¼Ÿ"
    context: Dict  # ä½ç½®ã€ã‚´ãƒ¼ãƒ«ã¾ã§ã®è·é›¢ãªã©
    action: int  # é¸æŠã—ãŸè¡Œå‹•
    result: Dict  # çµæœï¼ˆæ–°ä½ç½®ã€æˆåŠŸ/å¤±æ•—ã€ã‚´ãƒ¼ãƒ«ã¸ã®æ¥è¿‘åº¦ï¼‰
    value: float  # ã“ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ä¾¡å€¤


class TrueEpisodicGeDIGNavigator:
    """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã‹ã‚‰è‡ªå¾‹çš„ã«è¡Œå‹•ã‚’æ±ºå®š"""
    
    def __init__(self, config: MazeNavigatorConfig):
        self.config = config
        self.episodes: List[Episode] = []
        self.goal_pos: Optional[Tuple[int, int]] = None
        self.current_pos: Optional[Tuple[int, int]] = None
        self.time_step = 0
        
    def _manhattan_distance(self, pos1: Tuple[int, int], pos2: Tuple[int, int]) -> float:
        """ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢"""
        return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])
        
    def create_episode(self, old_pos: Tuple[int, int], action: int, 
                      new_pos: Tuple[int, int], possible_actions: List[int]):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦è¨˜æ†¶"""
        # ã‚¯ã‚¨ãƒªã®ç”Ÿæˆ
        if self.goal_pos:
            query = f"ä½ç½®{old_pos}ã‹ã‚‰ã‚´ãƒ¼ãƒ«{self.goal_pos}ã«å‘ã‹ã†ã«ã¯ï¼Ÿ"
            goal_dist_before = self._manhattan_distance(old_pos, self.goal_pos)
            goal_dist_after = self._manhattan_distance(new_pos, self.goal_pos)
        else:
            query = f"ä½ç½®{old_pos}ã‹ã‚‰æ¢ç´¢ã‚’é€²ã‚ã‚‹ã«ã¯ï¼Ÿ"
            goal_dist_before = 0
            goal_dist_after = 0
            
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        context = {
            'position': old_pos,
            'goal_known': self.goal_pos is not None,
            'possible_actions': possible_actions,
            'time': self.time_step
        }
        
        # çµæœ
        success = old_pos != new_pos
        result = {
            'new_position': new_pos,
            'success': success,
            'goal_progress': goal_dist_before - goal_dist_after if self.goal_pos else (1 if success else 0)
        }
        
        # ä¾¡å€¤ã®è¨ˆç®—ï¼ˆæˆåŠŸåº¦ Ã— ã‚´ãƒ¼ãƒ«æ¥è¿‘åº¦ï¼‰
        value = result['goal_progress'] if success else -0.5
        
        episode = Episode(
            query=query,
            context=context,
            action=action,
            result=result,
            value=value
        )
        
        self.episodes.append(episode)
        self.time_step += 1
        
    def query_action(self, position: Tuple[int, int], 
                    possible_actions: List[int]) -> int:
        """ç¾åœ¨çŠ¶æ³ã«åŸºã¥ã„ã¦è¡Œå‹•ã‚’æ±ºå®š"""
        
        # ç¾åœ¨ã®ã‚¯ã‚¨ãƒª
        if self.goal_pos:
            current_query = f"ä½ç½®{position}ã‹ã‚‰ã‚´ãƒ¼ãƒ«{self.goal_pos}ã«å‘ã‹ã†ã«ã¯ï¼Ÿ"
        else:
            current_query = f"ä½ç½®{position}ã‹ã‚‰æ¢ç´¢ã‚’é€²ã‚ã‚‹ã«ã¯ï¼Ÿ"
            
        # é–¢é€£ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’æ¤œç´¢ï¼ˆgeDIGçš„ãªé¡ä¼¼åº¦è¨ˆç®—ï¼‰
        relevant_episodes = self._find_relevant_episodes(position, current_query)
        
        if not relevant_episodes:
            # æœªçŸ¥ã®çŠ¶æ³ã§ã¯æ¢ç´¢ï¼ˆé«˜ã„æƒ…å ±åˆ©å¾—ï¼‰
            return np.random.choice(possible_actions)
            
        # å„è¡Œå‹•ã®è©•ä¾¡
        action_scores = defaultdict(list)
        
        for episode in relevant_episodes:
            # æ§‹é€ çš„é¡ä¼¼åº¦ï¼ˆåŒã˜ä½ç½®ã‹ã‚‰ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¯é«˜ã„é‡ã¿ï¼‰
            if episode.context['position'] == position:
                similarity = 1.0
            else:
                dist = self._manhattan_distance(episode.context['position'], position)
                similarity = 1.0 / (1.0 + dist)
                
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ä¾¡å€¤ã‚’è¡Œå‹•ã”ã¨ã«é›†è¨ˆ
            action_scores[episode.action].append(episode.value * similarity)
            
        # å„è¡Œå‹•ã®geDIGè©•ä¾¡
        best_action = None
        best_score = float('-inf')
        
        for action in possible_actions:
            if action in action_scores:
                # æ—¢çŸ¥ã®è¡Œå‹•ï¼šå¹³å‡ä¾¡å€¤
                avg_value = np.mean(action_scores[action])
                # æƒ…å ±åˆ©å¾—ã¯è©¦è¡Œå›æ•°ã«åæ¯”ä¾‹
                ig = 1.0 / (len(action_scores[action]) + 1)
            else:
                # æœªçŸ¥ã®è¡Œå‹•ï¼šé«˜ã„æƒ…å ±åˆ©å¾—
                avg_value = 0.0
                ig = 2.0
                
            # geDIGç›®çš„é–¢æ•°
            score = self.config.w_ged * avg_value - self.config.k_ig * ig
            
            if score > best_score:
                best_score = score
                best_action = action
                
        return best_action if best_action is not None else np.random.choice(possible_actions)
        
    def _find_relevant_episodes(self, position: Tuple[int, int], 
                               query: str) -> List[Episode]:
        """é–¢é€£ã™ã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’æ¤œç´¢"""
        relevant = []
        
        for episode in self.episodes:
            # åŒã˜ä½ç½®ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
            if episode.context['position'] == position:
                relevant.append(episode)
            # è¿‘ã„ä½ç½®ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼ˆæ§‹é€ çš„é¡ä¼¼æ€§ï¼‰
            elif self._manhattan_distance(episode.context['position'], position) <= 2:
                relevant.append(episode)
                
        # æ™‚é–“çš„ã«æ–°ã—ã„ã‚‚ã®ã‚’å„ªå…ˆ
        relevant.sort(key=lambda e: e.context['time'], reverse=True)
        
        return relevant[:20]  # æœ€æ–°20ä»¶ã¾ã§
        
    def decide_action(self, obs, maze) -> int:
        """è¦³æ¸¬ã‹ã‚‰è¡Œå‹•ã‚’æ±ºå®š"""
        self.current_pos = obs.position
        
        # ã‚´ãƒ¼ãƒ«ç™ºè¦‹
        if obs.is_goal and not self.goal_pos:
            self.goal_pos = obs.position
            print(f"ğŸ¯ ã‚´ãƒ¼ãƒ«ç™ºè¦‹ï¼ä½ç½®: {self.goal_pos}")
            
        return self.query_action(obs.position, obs.possible_moves)
        
    def learn_from_experience(self, old_pos: Tuple[int, int], 
                            action: int, new_pos: Tuple[int, int], 
                            possible_actions: List[int]):
        """çµŒé¨“ã‹ã‚‰å­¦ç¿’ï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã«è¿½åŠ ï¼‰"""
        self.create_episode(old_pos, action, new_pos, possible_actions)


def demonstrate_true_episodic_gedig():
    """çœŸã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶geDIGã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã«ã‚ˆã‚‹è‡ªå¾‹çš„geDIGãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³")
    print("=" * 60)
    print("é‡è¦ãªæ¦‚å¿µï¼š")
    print("- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ = ã‚¯ã‚¨ãƒªï¼ˆçŠ¶æ³ã§ã®å•ã„ï¼‰+ è¡Œå‹• + çµæœ")
    print("- é¡ä¼¼çŠ¶æ³ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‹ã‚‰è¡Œå‹•ã‚’æ±ºå®š")
    print("- geDIGç›®çš„é–¢æ•°ã§æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹")
    print("=" * 60)
    
    config = MazeNavigatorConfig()
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    np.random.seed(42)
    maze = SimpleMaze(size=(10, 10), maze_type='dfs')
    navigator = TrueEpisodicGeDIGNavigator(config)
    
    print(f"\nè¿·è·¯: {maze.size}")
    print(f"ã‚¹ã‚¿ãƒ¼ãƒˆ: {maze.start_pos} â†’ ã‚´ãƒ¼ãƒ«: {maze.goal_pos}")
    print("-" * 40)
    
    obs = maze.reset()
    path = [obs.position]
    
    for step in range(200):
        old_pos = obs.position
        possible_actions = obs.possible_moves.copy()
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã‹ã‚‰è¡Œå‹•ã‚’æ±ºå®š
        action = navigator.decide_action(obs, maze)
        
        # è¡Œå‹•å®Ÿè¡Œ
        obs, reward, done, info = maze.step(action)
        new_pos = obs.position
        
        # çµŒé¨“ã‹ã‚‰å­¦ç¿’
        navigator.learn_from_experience(old_pos, action, new_pos, possible_actions)
        
        path.append(new_pos)
        
        # é‡è¦ãªæ™‚ç‚¹ã§ã®çŠ¶æ³
        if step % 20 == 0:
            print(f"ã‚¹ãƒ†ãƒƒãƒ— {step}: ä½ç½®{old_pos} â†’ {new_pos}")
            print(f"  ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {len(navigator.episodes)}")
            
        if done and maze.agent_pos == maze.goal_pos:
            print(f"\nâœ… ã‚´ãƒ¼ãƒ«åˆ°é”ï¼ã‚¹ãƒ†ãƒƒãƒ—æ•°: {step + 1}")
            break
            
    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã®åˆ†æ
    print("\n" + "=" * 60)
    print("ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã®åˆ†æ:")
    print(f"ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {len(navigator.episodes)}")
    
    # ä¾¡å€¤ã®é«˜ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
    valuable_episodes = sorted(navigator.episodes, key=lambda e: e.value, reverse=True)[:5]
    print("\nä¾¡å€¤ã®é«˜ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼ˆä¸Šä½5ä»¶ï¼‰:")
    for i, ep in enumerate(valuable_episodes):
        print(f"{i+1}. {ep.query}")
        print(f"   è¡Œå‹•: {['ä¸Š', 'å³', 'ä¸‹', 'å·¦'][ep.action]}, ä¾¡å€¤: {ep.value:.2f}")
        
    # ä½ç½®ã”ã¨ã®è¨ªå•å›æ•°
    position_counts = defaultdict(int)
    for ep in navigator.episodes:
        position_counts[ep.context['position']] += 1
        
    print(f"\nè¨ªå•ä½ç½®æ•°: {len(position_counts)}")
    print(f"å¹³å‡è¨ªå•å›æ•°: {np.mean(list(position_counts.values())):.1f}")
    
    # çµŒè·¯ã®åŠ¹ç‡æ€§
    if maze.agent_pos == maze.goal_pos:
        optimal_dist = abs(maze.start_pos[0] - maze.goal_pos[0]) + abs(maze.start_pos[1] - maze.goal_pos[1])
        actual_dist = len(path) - 1
        efficiency = optimal_dist / actual_dist if actual_dist > 0 else 0
        print(f"\nçµŒè·¯åŠ¹ç‡: {efficiency:.1%} (æœ€é©{optimal_dist}æ­© / å®Ÿéš›{actual_dist}æ­©)")
    
    print("\n" + "=" * 60)
    print("ã¾ã¨ã‚ï¼š")
    print("âœ¨ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ãŒè‡ªå¾‹çš„ã«æ¬¡ã®è¡Œå‹•ã‚’æ±ºå®š")
    print("âœ¨ éå»ã®çµŒé¨“ã‹ã‚‰é¡ä¼¼çŠ¶æ³ã§ã®æœ€é©è¡Œå‹•ã‚’å­¦ç¿’")
    print("âœ¨ geDIGç›®çš„é–¢æ•°ã«ã‚ˆã‚Šæ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å®Ÿç¾")
    print("âœ¨ ã“ã‚ŒãŒæœ¬æ¥ã®geDIGã®å§¿ï¼")


if __name__ == "__main__":
    demonstrate_true_episodic_gedig()