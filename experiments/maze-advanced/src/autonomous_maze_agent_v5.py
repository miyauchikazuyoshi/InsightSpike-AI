#!/usr/bin/env python3
"""
Autonomous Maze Agent V5 with Initial Episodes
=============================================

åˆæœŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼ˆã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã®4æ–¹å‘ï¼‹ã‚´ãƒ¼ãƒ«ï¼‰ã‚’æŒã¤è‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
"""

import sys
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from insightspike.config.presets import ConfigPresets
from insightspike.core.episode import Episode
from insightspike.environments.maze import SimpleMaze
from donut_search_maze import DonutSearchMaze

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class MazeState:
    """è¿·è·¯ã®ç¾åœ¨çŠ¶æ…‹"""
    position: Tuple[int, int]
    visited_positions: List[Tuple[int, int]]
    last_action: Optional[int] = None
    last_result: Optional[str] = None
    step_count: int = 0


class CompactVectorSpace:
    """5æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã§ã®å‡¦ç†"""
    
    def __init__(self, maze_size: Tuple[int, int]):
        self.maze_width, self.maze_height = maze_size
        
    def state_to_vector(self, position: Tuple[int, int], 
                       action: Optional[int] = None,
                       result: Optional[str] = None,
                       visits: int = 0) -> np.ndarray:
        """ä½ç½®ã¨çŠ¶æ…‹ã‚’5æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
        # ä½ç½®ã‚’æ­£è¦åŒ–
        norm_x = position[0] / (self.maze_width - 1) if self.maze_width > 1 else 0
        norm_y = position[1] / (self.maze_height - 1) if self.maze_height > 1 else 0
        
        # è¡Œå‹•ã‚’æ­£è¦åŒ–
        if action is not None:
            norm_action = action * 0.25  # 0, 0.25, 0.5, 0.75
        else:
            norm_action = 0.5
        
        # çµæœã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        result_map = {'wall': -1.0, 'empty': 0.0, 'goal': 1.0, None: 0.0}
        norm_result = result_map.get(result, 0.0)
        
        # è¨ªå•å›æ•°ã‚’æ­£è¦åŒ–
        norm_visits = min(visits / 10.0, 1.0)
        
        return np.array([norm_x, norm_y, norm_action, norm_result, norm_visits])


class AutonomousMazeAgentV5:
    """åˆæœŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’æŒã¤è‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, config=None):
        self.config = config or ConfigPresets.experiment()
        self.donut_search = DonutSearchMaze(dimension=5)
        self.vector_space = None
        self.maze_env = None
        self.current_state = None
        self.goal_episode_id = "GOAL_EPISODE"  # ç‰¹åˆ¥ãªID
        self.initial_episodes = {}  # åˆæœŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ä¿å­˜
        
    def setup_maze(self, maze_query: Dict):
        """è¿·è·¯ç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        maze_array = np.array(maze_query['maze'])
        
        self.maze_env = SimpleMaze(
            size=maze_array.shape[::-1],
            maze_type='custom',
            maze_layout=maze_array,
            start_pos=maze_query['start'],
            goal_pos=maze_query['goal']
        )
        
        self.vector_space = CompactVectorSpace(self.maze_env.size)
        
        self.current_state = MazeState(
            position=self.maze_env.start_pos,
            visited_positions=[self.maze_env.start_pos]
        )
        
        # åˆæœŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¿½åŠ 
        self._add_initial_episodes()
        
    def _add_initial_episodes(self):
        """åˆæœŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼ˆã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã®4æ–¹å‘ï¼‹ã‚´ãƒ¼ãƒ«ï¼‰ã‚’è¿½åŠ """
        start_x, start_y = self.maze_env.start_pos
        goal_x, goal_y = self.maze_env.goal_pos
        
        # 1. ã‚´ãƒ¼ãƒ«ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
        goal_vector = self.vector_space.state_to_vector(
            position=(goal_x, goal_y),
            action=None,
            result='goal',
            visits=1
        )
        self.donut_search.add_episode(self.goal_episode_id, goal_vector, (goal_x, goal_y))
        self.initial_episodes[self.goal_episode_id] = goal_vector
        logger.info(f"Added goal episode at {self.maze_env.goal_pos}")
        
        # 2. ã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã‹ã‚‰ã®4æ–¹å‘ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
        directions = [(0, -1), (1, 0), (0, 1), (-1, 0)]
        action_names = ['up', 'right', 'down', 'left']
        
        for action, ((dx, dy), name) in enumerate(zip(directions, action_names)):
            nx, ny = start_x + dx, start_y + dy
            
            # çµæœã‚’åˆ¤å®š
            if (0 <= nx < self.maze_env.size[0] and 
                0 <= ny < self.maze_env.size[1]):
                # SimpleMazeã®ã‚´ãƒ¼ãƒ«ä½ç½®ã¯ç‰¹åˆ¥ãªå€¤ã‚’æŒã¤å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§æ³¨æ„
                if (nx, ny) == self.maze_env.goal_pos:
                    result = 'goal'
                elif self._is_passable(nx, ny):
                    result = 'empty'
                else:
                    result = 'wall'
            else:
                result = 'wall'
            
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ™ã‚¯ãƒˆãƒ«ä½œæˆ
            episode_vector = self.vector_space.state_to_vector(
                position=(start_x, start_y),
                action=action,
                result=result,
                visits=1
            )
            
            episode_id = f"INITIAL_{name.upper()}"
            self.donut_search.add_episode(episode_id, episode_vector, (start_x, start_y))
            self.initial_episodes[episode_id] = episode_vector
            
            logger.info(f"Added initial episode: {name} from start -> {result}")
        
        logger.info(f"Total initial episodes: {len(self.initial_episodes)}")
        
    def _is_passable(self, x: int, y: int) -> bool:
        """æŒ‡å®šä½ç½®ãŒé€šè¡Œå¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆã‚´ãƒ¼ãƒ«ä½ç½®ã‚‚é€šè¡Œå¯èƒ½ã¨ã™ã‚‹ï¼‰"""
        if (x, y) == self.maze_env.goal_pos:
            return True
        return self.maze_env.grid[y, x] == 0
        
    def process_state(self, state: MazeState) -> int:
        """ãƒ‰ãƒ¼ãƒŠãƒ„æ¤œç´¢ã¨åˆæœŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’æ´»ç”¨ã—ã¦æ–¹å‘ã‚’æ±ºå®š"""
        # ç¾åœ¨çŠ¶æ…‹ã®ãƒ™ã‚¯ãƒˆãƒ«
        current_vector = self.vector_space.state_to_vector(
            position=state.position,
            action=state.last_action,
            result=state.last_result,
            visits=state.visited_positions.count(state.position)
        )
        
        # ç¾åœ¨ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¨˜éŒ²
        episode_id = f"step_{state.step_count}"
        self.donut_search.add_episode(episode_id, current_vector, state.position)
        
        # ã‚´ãƒ¼ãƒ«ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¾ã§ã®è·é›¢ã‚’è¨ˆç®—
        goal_vector = self.donut_search.episode_vectors[self.goal_episode_id]
        goal_distance = np.linalg.norm(current_vector - goal_vector)
        logger.info(f"Distance to goal episode: {goal_distance:.3f}")
        
        # ãƒ‰ãƒ¼ãƒŠãƒ„æ¤œç´¢ï¼ˆã‚´ãƒ¼ãƒ«ã«è¿‘ã„é ˜åŸŸã‚’æ¢ç´¢ï¼‰
        result = self.donut_search.donut_search(
            current_vector,
            inner_radius=0.1,  # æ—¢ã«æ¢ç´¢ã—ãŸè¿‘ã„é ˜åŸŸ
            outer_radius=goal_distance + 0.2  # ã‚´ãƒ¼ãƒ«ã¾ã§ã®è·é›¢ã‚ˆã‚Šå°‘ã—é ã
        )
        
        logger.info(f"Donut search: inner={len(result.inner_nodes)}, " +
                   f"candidates={len(result.candidates)}, " +
                   f"outer={len(result.outer_nodes)}")
        
        # å¯èƒ½ãªè¡Œå‹•ã‚’å–å¾—
        possible_actions = self._get_possible_actions()
        if not possible_actions:
            return 0
            
        # å„è¡Œå‹•ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        action_scores = {}
        
        for action in possible_actions:
            # ã“ã®è¡Œå‹•ã‚’å–ã£ãŸå ´åˆã®äºˆæ¸¬ä½ç½®
            dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0)][action]
            next_pos = (state.position[0] + dx, state.position[1] + dy)
            
            # äºˆæ¸¬ãƒ™ã‚¯ãƒˆãƒ«
            predicted_vector = self.vector_space.state_to_vector(
                position=next_pos,
                action=action,
                result='empty',  # ä»®å®š
                visits=state.visited_positions.count(next_pos)
            )
            
            # ã‚´ãƒ¼ãƒ«ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¸ã®è·é›¢
            predicted_distance = np.linalg.norm(predicted_vector - goal_vector)
            
            # ã‚¹ã‚³ã‚¢ï¼ˆè·é›¢ãŒå°ã•ã„ã»ã©è‰¯ã„ï¼‰
            score = -predicted_distance
            
            # è¨ªå•å›æ•°ã«ã‚ˆã‚‹ãƒšãƒŠãƒ«ãƒ†ã‚£
            visit_penalty = state.visited_positions.count(next_pos) * 0.1
            score -= visit_penalty
            
            # åˆæœŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã®é¡ä¼¼æ€§ãƒœãƒ¼ãƒŠã‚¹
            for ep_id, ep_vec in self.initial_episodes.items():
                if 'INITIAL_' in ep_id:  # åˆæœŸæ–¹å‘ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å ´åˆ
                    similarity = 1.0 - np.linalg.norm(predicted_vector - ep_vec)
                    if similarity > 0.7:  # é«˜ã„é¡ä¼¼æ€§ãŒã‚ã‚‹å ´åˆ
                        score += similarity * 0.2
            
            action_scores[action] = score
            
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®è¡Œå‹•ã‚’é¸æŠ
        best_action = max(action_scores.keys(), key=lambda a: action_scores[a])
        
        logger.info(f"Action scores: {action_scores}")
        logger.info(f"Selected action: {best_action}")
        
        return best_action
        
    def _get_possible_actions(self) -> List[int]:
        """å¯èƒ½ãªè¡Œå‹•ã®ãƒªã‚¹ãƒˆï¼ˆã‚´ãƒ¼ãƒ«ä½ç½®ã‚‚é€šè¡Œå¯èƒ½ã¨ã™ã‚‹ï¼‰"""
        actions = []
        x, y = self.current_state.position
        
        for action, (dx, dy) in enumerate([(0, -1), (1, 0), (0, 1), (-1, 0)]):
            nx, ny = x + dx, y + dy
            if (0 <= nx < self.maze_env.size[0] and 
                0 <= ny < self.maze_env.size[1] and
                self._is_passable(nx, ny)):
                actions.append(action)
                
        return actions
        
    def execute_action(self, action: int) -> str:
        """è¡Œå‹•ã‚’å®Ÿè¡Œ"""
        old_pos = self.current_state.position
        dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0)][action]
        new_x, new_y = old_pos[0] + dx, old_pos[1] + dy
        
        if (0 <= new_x < self.maze_env.size[0] and 
            0 <= new_y < self.maze_env.size[1]):
            
            if self._is_passable(new_x, new_y):
                # ç§»å‹•æˆåŠŸ
                self.current_state.position = (new_x, new_y)
                if (new_x, new_y) == self.maze_env.goal_pos:
                    result = 'goal'
                else:
                    result = 'empty'
            else:
                result = 'wall'
        else:
            result = 'wall'
            
        self.current_state.last_action = action
        self.current_state.last_result = result
        self.current_state.visited_positions.append(self.current_state.position)
        self.current_state.step_count += 1
        
        return result
        
    def solve_maze(self, maze_query: Dict, max_steps: int = 50) -> Dict:
        """è¿·è·¯ã‚’è§£ã"""
        self.setup_maze(maze_query)
        
        print(f"\n=== Autonomous Maze Solving with Initial Episodes ===")
        print(f"Start: {self.maze_env.start_pos}, Goal: {self.maze_env.goal_pos}")
        print(f"Initial episodes: {len(self.initial_episodes)}")
        print(f"  - Goal episode")
        print(f"  - 4 directional episodes from start\n")
        
        path_for_viz = []
        
        while self.current_state.step_count < max_steps:
            print(f"Step {self.current_state.step_count}: Position {self.current_state.position}", end=" ")
            
            # è¡Œå‹•æ±ºå®š
            action = self.process_state(self.current_state)
            
            # è¡Œå‹•å®Ÿè¡Œ
            result = self.execute_action(action)
            actions = ['â†‘', 'â†’', 'â†“', 'â†']
            print(f"â†’ {actions[action]} â†’ {result}")
            
            path_for_viz.append(self.current_state.position)
            
            # ã‚´ãƒ¼ãƒ«åˆ¤å®š
            if self.current_state.position == self.maze_env.goal_pos:
                print(f"\nğŸ‰ Goal reached in {self.current_state.step_count} steps!")
                break
                
        # ç°¡å˜ãªå¯è¦–åŒ–
        self._visualize_result(path_for_viz)
        
        return {
            'success': self.current_state.position == self.maze_env.goal_pos,
            'steps': self.current_state.step_count,
            'path': self.current_state.visited_positions,
            'unique_positions': len(set(self.current_state.visited_positions))
        }
        
    def _visualize_result(self, path: List[Tuple[int, int]]):
        """çµæœã‚’å¯è¦–åŒ–"""
        fig, ax = plt.subplots(figsize=(6, 6))
        
        # è¿·è·¯ã‚’æç”»ï¼ˆã‚´ãƒ¼ãƒ«ä½ç½®ã‚’ç‰¹åˆ¥æ‰±ã„ï¼‰
        maze_display = self.maze_env.grid.copy()
        gx, gy = self.maze_env.goal_pos
        if maze_display[gy, gx] != 0:
            maze_display[gy, gx] = 0  # ã‚´ãƒ¼ãƒ«ä½ç½®ã‚’é€šè¡Œå¯èƒ½ã¨ã—ã¦è¡¨ç¤º
        
        ax.imshow(maze_display, cmap='binary')
        
        # çµŒè·¯ã‚’æç”»
        if path:
            path_array = np.array(path)
            ax.plot(path_array[:, 0], path_array[:, 1], 'g-', linewidth=2, alpha=0.7)
            
            # è¨ªå•é †ã‚’ç•ªå·ã§è¡¨ç¤º
            for i, pos in enumerate(path[:20]):  # æœ€åˆã®20ã‚¹ãƒ†ãƒƒãƒ—ã¾ã§
                ax.text(pos[0], pos[1], str(i), fontsize=8, ha='center', va='center')
        
        # ã‚¹ã‚¿ãƒ¼ãƒˆã¨ã‚´ãƒ¼ãƒ«
        ax.plot(*self.maze_env.start_pos, 'go', markersize=15, label='Start')
        ax.plot(*self.maze_env.goal_pos, 'r*', markersize=20, label='Goal')
        
        ax.set_title(f'Maze Solution (Steps: {self.current_state.step_count})')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.savefig('results/initial_episodes_solution.png')
        plt.close()
        print("\nVisualization saved to results/initial_episodes_solution.png")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    # 3x3ã®ç°¡å˜ãªè¿·è·¯
    test_maze_3x3 = {
        "maze": [
            [0, 0, 0],
            [0, 1, 0],
            [0, 0, 0]
        ],
        "start": (0, 0),
        "goal": (2, 2)
    }
    
    # 5x5ã®è¿·è·¯
    test_maze_5x5 = {
        "maze": [
            [0, 0, 1, 0, 0],
            [0, 1, 1, 0, 0],
            [0, 0, 0, 0, 1],
            [1, 0, 1, 0, 0],
            [0, 0, 0, 0, 0]
        ],
        "start": (0, 0),
        "goal": (4, 4)
    }
    
    # 3x3ã§ãƒ†ã‚¹ãƒˆ
    print("Testing with 3x3 maze...")
    agent = AutonomousMazeAgentV5()
    result = agent.solve_maze(test_maze_3x3)
    
    print(f"\n=== Result ===")
    print(f"Success: {result['success']}")
    print(f"Steps: {result['steps']}")
    print(f"Efficiency: {result['unique_positions'] / result['steps']:.2%}")
    
    # æˆåŠŸã—ãŸã‚‰5x5ã§ã‚‚ãƒ†ã‚¹ãƒˆ
    if result['success']:
        print("\n\nTesting with 5x5 maze...")
        agent2 = AutonomousMazeAgentV5()
        result2 = agent2.solve_maze(test_maze_5x5)
        
        print(f"\n=== Result ===")
        print(f"Success: {result2['success']}")
        print(f"Steps: {result2['steps']}")
        print(f"Efficiency: {result2['unique_positions'] / result2['steps']:.2%}")


if __name__ == "__main__":
    main()