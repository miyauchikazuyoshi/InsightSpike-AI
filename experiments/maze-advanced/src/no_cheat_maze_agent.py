#!/usr/bin/env python3
"""
No-Cheat Maze Agent
===================

ã‚´ãƒ¼ãƒ«ä½ç½®ã‚’äº‹å‰ã«çŸ¥ã‚‰ãªã„ã€ã‚ˆã‚Šç¾å®Ÿçš„ãªè¿·è·¯æ¢ç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
"""

import sys
from pathlib import Path
import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import logging

sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from insightspike.environments.maze import SimpleMaze
from donut_search_maze import DonutSearchMaze

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ExplorationMemory:
    """æ¢ç´¢ã®è¨˜æ†¶"""
    position: Tuple[int, int]
    action: int
    result: str
    vector: np.ndarray
    step: int
    value: float = 0.0  # ã“ã®çµŒè·¯ã®ä¾¡å€¤


class NoCheatMazeAgent:
    """ãƒãƒ¼ãƒˆãªã—è¿·è·¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self):
        self.memories: List[ExplorationMemory] = []
        self.position = (0, 0)
        self.step_count = 0
        self.found_goal = False
        self.goal_memory: Optional[ExplorationMemory] = None
        
        # æ¢ç´¢æˆ¦ç•¥
        self.exploration_rate = 0.8  # åˆæœŸã¯æ¢ç´¢é‡è¦–
        self.visited_positions = {}  # ä½ç½® -> è¨ªå•å›æ•°
        
    def add_memory(self, position: Tuple[int, int], action: int, 
                   result: str, vector: np.ndarray):
        """æ¢ç´¢ã®è¨˜æ†¶ã‚’è¿½åŠ """
        memory = ExplorationMemory(
            position=position,
            action=action,
            result=result,
            vector=vector,
            step=self.step_count
        )
        
        # ã‚´ãƒ¼ãƒ«ã‚’ç™ºè¦‹ã—ãŸã‚‰è¨˜éŒ²
        if result == 'goal' and not self.found_goal:
            self.found_goal = True
            self.goal_memory = memory
            memory.value = 100.0  # ã‚´ãƒ¼ãƒ«ã¯æœ€é«˜ä¾¡å€¤
            logger.info(f"ğŸ¯ Goal discovered at {position}!")
        elif result == 'empty':
            # æ–°ã—ã„å ´æ‰€ã®ç™ºè¦‹ã¯ä¾¡å€¤ãŒã‚ã‚‹
            if position not in self.visited_positions:
                memory.value = 10.0
            else:
                memory.value = 1.0 / (1 + self.visited_positions[position])
        else:  # wall
            memory.value = -5.0
            
        self.memories.append(memory)
        
        # è¨ªå•å›æ•°ã‚’æ›´æ–°
        if position not in self.visited_positions:
            self.visited_positions[position] = 0
        self.visited_positions[position] += 1
        
    def decide_action(self, possible_actions: List[int]) -> int:
        """æ¬¡ã®è¡Œå‹•ã‚’æ±ºå®šï¼ˆã‚´ãƒ¼ãƒ«ä½ç½®ã‚’çŸ¥ã‚‰ãªã„ï¼‰"""
        
        if not possible_actions:
            return 0
            
        # æ¢ç´¢ç‡ã«åŸºã¥ã„ã¦æˆ¦ç•¥ã‚’é¸æŠ
        if np.random.random() < self.exploration_rate:
            # æ¢ç´¢ãƒ¢ãƒ¼ãƒ‰ï¼šæœªè¨ªå•ã®æ–¹å‘ã‚’å„ªå…ˆ
            return self._exploration_strategy(possible_actions)
        else:
            # æ´»ç”¨ãƒ¢ãƒ¼ãƒ‰ï¼šéå»ã®è‰¯ã„çµŒé¨“ã‚’æ´»ç”¨
            return self._exploitation_strategy(possible_actions)
    
    def _exploration_strategy(self, possible_actions: List[int]) -> int:
        """æ¢ç´¢æˆ¦ç•¥ï¼šæœªçŸ¥ã®é ˜åŸŸã‚’å„ªå…ˆ"""
        action_scores = {}
        
        for action in possible_actions:
            # ã“ã®è¡Œå‹•ã®äºˆæ¸¬ä½ç½®
            dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0)][action]
            next_pos = (self.position[0] + dx, self.position[1] + dy)
            
            # æœªè¨ªå•ãªã‚‰é«˜ã‚¹ã‚³ã‚¢
            if next_pos not in self.visited_positions:
                action_scores[action] = 100.0
            else:
                # è¨ªå•å›æ•°ãŒå°‘ãªã„ã»ã©é«˜ã‚¹ã‚³ã‚¢
                visit_count = self.visited_positions[next_pos]
                action_scores[action] = 10.0 / (1 + visit_count)
                
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®è¡Œå‹•ã‚’é¸æŠ
        if action_scores:
            return max(action_scores.keys(), key=lambda a: action_scores[a])
        else:
            return np.random.choice(possible_actions)
    
    def _exploitation_strategy(self, possible_actions: List[int]) -> int:
        """æ´»ç”¨æˆ¦ç•¥ï¼šéå»ã®è‰¯ã„çµŒé¨“ã‚’æ´»ç”¨"""
        if not self.memories:
            return np.random.choice(possible_actions)
            
        action_scores = {action: 0.0 for action in possible_actions}
        
        # é¡ä¼¼ã—ãŸçŠ¶æ³ã§ã®æˆåŠŸä½“é¨“ã‚’æ¢ã™
        current_visits = self.visited_positions.get(self.position, 0)
        
        for memory in self.memories:
            # åŒã˜ä½ç½®ã§ã®è¨˜æ†¶
            if memory.position == self.position:
                if memory.action in possible_actions:
                    # æˆåŠŸä½“é¨“ï¼ˆç‰¹ã«ã‚´ãƒ¼ãƒ«ç™ºè¦‹ï¼‰ã¯é«˜ãè©•ä¾¡
                    action_scores[memory.action] += memory.value
                    
                    # æœ€è¿‘ã®è¨˜æ†¶ã»ã©é‡è¦–
                    recency_bonus = 1.0 / (1 + self.step_count - memory.step)
                    action_scores[memory.action] += recency_bonus
        
        # ã‚´ãƒ¼ãƒ«ã‚’è¦‹ã¤ã‘ãŸå¾Œã¯ã€ã‚´ãƒ¼ãƒ«ã¸ã®çµŒè·¯ã‚’é€†ç®—
        if self.found_goal and self.goal_memory:
            # ç°¡å˜ãªçµŒè·¯é€†ç®—ï¼ˆæœ¬æ¥ã¯ã‚‚ã£ã¨è³¢ã„æ–¹æ³•ãŒå¿…è¦ï¼‰
            goal_x, goal_y = self.goal_memory.position
            curr_x, curr_y = self.position
            
            # ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢ã§æ–¹å‘ã‚’æ¨å®š
            dx = goal_x - curr_x
            dy = goal_y - curr_y
            
            if dx > 0 and 1 in possible_actions:  # å³
                action_scores[1] += 50.0
            elif dx < 0 and 3 in possible_actions:  # å·¦
                action_scores[3] += 50.0
                
            if dy > 0 and 2 in possible_actions:  # ä¸‹
                action_scores[2] += 50.0
            elif dy < 0 and 0 in possible_actions:  # ä¸Š
                action_scores[0] += 50.0
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®è¡Œå‹•ã‚’é¸æŠ
        best_action = max(action_scores.keys(), key=lambda a: action_scores[a])
        
        # å…¨ã¦ã®ã‚¹ã‚³ã‚¢ãŒ0ãªã‚‰æ¢ç´¢ã«æˆ»ã‚‹
        if all(score == 0 for score in action_scores.values()):
            return self._exploration_strategy(possible_actions)
            
        return best_action
    
    def update_exploration_rate(self):
        """æ¢ç´¢ç‡ã‚’æ›´æ–°"""
        # ã‚´ãƒ¼ãƒ«ã‚’è¦‹ã¤ã‘ãŸã‚‰æ¢ç´¢ç‡ã‚’ä¸‹ã’ã‚‹
        if self.found_goal:
            self.exploration_rate = 0.2
        else:
            # æ™‚é–“ã¨ã¨ã‚‚ã«æ¢ç´¢ç‡ã‚’ä¸‹ã’ã‚‹ï¼ˆã§ã‚‚æœ€ä½0.3ã¯ä¿ã¤ï¼‰
            self.exploration_rate = max(0.3, 0.8 - self.step_count * 0.01)


def demonstrate_no_cheat():
    """ãƒãƒ¼ãƒˆãªã—æ¢ç´¢ã®ãƒ‡ãƒ¢"""
    print("=== ãƒãƒ¼ãƒˆãªã—è¿·è·¯æ¢ç´¢ãƒ‡ãƒ¢ ===\n")
    
    agent = NoCheatMazeAgent()
    
    # ä»®æƒ³çš„ãªè¿·è·¯ã§ã®å‹•ä½œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    print("ã‚´ãƒ¼ãƒ«ä½ç½®ã‚’çŸ¥ã‚‰ãªã„çŠ¶æ…‹ã§æ¢ç´¢é–‹å§‹...\n")
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: åˆæœŸä½ç½®ã‹ã‚‰æ¢ç´¢
    agent.position = (0, 0)
    possible_actions = [1, 2]  # å³ã¨ä¸‹ãŒå¯èƒ½
    
    action = agent.decide_action(possible_actions)
    action_names = ['â†‘', 'â†’', 'â†“', 'â†']
    print(f"Step 0: ä½ç½®{agent.position} â†’ è¡Œå‹•: {action_names[action]}")
    print(f"  æ¢ç´¢ç‡: {agent.exploration_rate:.2f}")
    print(f"  æˆ¦ç•¥: {'æ¢ç´¢' if np.random.random() < agent.exploration_rate else 'æ´»ç”¨'}")
    
    # å³ã«ç§»å‹•ã—ãŸã¨ã™ã‚‹
    agent.add_memory((0, 0), action, 'empty', np.array([0.0, 0.0, 0.25, 0.0, 0.1]))
    agent.position = (1, 0)
    agent.step_count += 1
    agent.update_exploration_rate()
    
    # ã‚¹ãƒ†ãƒƒãƒ—2
    possible_actions = [1, 2, 3]
    action = agent.decide_action(possible_actions)
    print(f"\nStep 1: ä½ç½®{agent.position} â†’ è¡Œå‹•: {action_names[action]}")
    print(f"  è¨ªå•æ¸ˆã¿ä½ç½®: {list(agent.visited_positions.keys())}")
    
    # ã•ã‚‰ã«æ¢ç´¢...
    print("\n... æ¢ç´¢ã‚’ç¶šã‘ã‚‹ ...")
    
    # ã‚´ãƒ¼ãƒ«ç™ºè¦‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    agent.position = (2, 2)
    agent.add_memory((2, 2), 1, 'goal', np.array([1.0, 1.0, 0.25, 1.0, 0.1]))
    print(f"\nğŸ‰ ã‚´ãƒ¼ãƒ«ç™ºè¦‹ï¼ä½ç½®: {agent.position}")
    print(f"  æ¢ç´¢ç‡ãŒ {0.8:.2f} ã‹ã‚‰ {agent.exploration_rate:.2f} ã«ä½ä¸‹")
    print(f"  ä»Šå¾Œã¯ã‚´ãƒ¼ãƒ«ã¸ã®çµŒè·¯ã‚’æ´»ç”¨ã—ã¦ç§»å‹•")


if __name__ == "__main__":
    demonstrate_no_cheat()