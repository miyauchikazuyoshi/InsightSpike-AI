#!/usr/bin/env python3
"""
Post-Action Query Agent
=======================

è¡Œå‹•å¾Œã®çŠ¶æ…‹ã§ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
6æ¬¡å…ƒ: [X, Y, null(action), null(result), visits, goal_info]
"""

import sys
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import logging

sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from insightspike.config.presets import ConfigPresets
from insightspike.environments.maze import SimpleMaze
from donut_search_maze import DonutSearchMaze

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class MazeState:
    """è¿·è·¯ã®ç¾åœ¨çŠ¶æ…‹"""
    position: Tuple[int, int]
    visited_positions: List[Tuple[int, int]]
    discovered_cells: Dict[Tuple[int, int], float]  # ä½ç½® -> goal_info
    last_action: Optional[int] = None
    last_result: Optional[str] = None
    step_count: int = 0


class PostActionVectorSpace:
    """è¡Œå‹•å¾Œã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“"""
    
    def __init__(self, maze_size: Tuple[int, int]):
        self.maze_width, self.maze_height = maze_size
        
    def create_query_vector(self, 
                          position: Tuple[int, int], 
                          visits: int = 0,
                          goal_info: float = -1.0) -> np.ndarray:
        """
        è¡Œå‹•å¾Œã®çŠ¶æ…‹ã‹ã‚‰ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
        action=null, result=nullã®çŠ¶æ…‹
        """
        # ä½ç½®ã‚’æ­£è¦åŒ–
        norm_x = position[0] / (self.maze_width - 1) if self.maze_width > 1 else 0
        norm_y = position[1] / (self.maze_height - 1) if self.maze_height > 1 else 0
        
        # action, resultã¯nullçŠ¶æ…‹ï¼ˆ0.5ã§è¡¨ç¾ï¼‰
        norm_action = 0.5
        norm_result = 0.5
        
        # è¨ªå•å›æ•°ã‚’æ­£è¦åŒ–
        norm_visits = min(visits / 10.0, 1.0)
        
        # 6æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«: [X, Y, action(null), result(null), visits, goal_info]
        return np.array([norm_x, norm_y, norm_action, norm_result, norm_visits, goal_info])
    
    def create_response_vector(self,
                             position: Tuple[int, int],
                             action: int,
                             result: str,
                             visits: int = 0,
                             goal_info: float = -1.0) -> np.ndarray:
        """
        è¡Œå‹•å®Ÿè¡Œå¾Œã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
        """
        # ä½ç½®ã‚’æ­£è¦åŒ–
        norm_x = position[0] / (self.maze_width - 1) if self.maze_width > 1 else 0
        norm_y = position[1] / (self.maze_height - 1) if self.maze_height > 1 else 0
        
        # è¡Œå‹•ã‚’æ­£è¦åŒ–
        norm_action = action * 0.25  # 0, 0.25, 0.5, 0.75
        
        # çµæœã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        result_map = {'wall': -1.0, 'empty': 0.0, 'goal': 1.0}
        norm_result = result_map.get(result, 0.0)
        
        # è¨ªå•å›æ•°ã‚’æ­£è¦åŒ–
        norm_visits = min(visits / 10.0, 1.0)
        
        return np.array([norm_x, norm_y, norm_action, norm_result, norm_visits, goal_info])


class PostActionQueryAgent:
    """è¡Œå‹•å¾Œã‚¯ã‚¨ãƒªã‚’ä½¿ã†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, config=None):
        self.config = config or ConfigPresets.experiment()
        self.donut_search = DonutSearchMaze(dimension=6)
        self.vector_space = None
        self.maze_env = None
        self.current_state = None
        self.goal_discovered = False
        self.goal_position = None
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶
        self.query_response_pairs = []  # (query_vector, action, response_vector)ã®ãƒªã‚¹ãƒˆ
        
    def setup_maze(self, maze_query: Dict):
        """è¿·è·¯ç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        maze_array = np.array(maze_query['maze'])
        
        self.maze_env = SimpleMaze(
            size=maze_array.shape[::-1],
            maze_type='custom',
            maze_layout=maze_array,
            start_pos=maze_query['start'],
            goal_pos=maze_query['goal']
        )
        
        self.vector_space = PostActionVectorSpace(self.maze_env.size)
        
        self.current_state = MazeState(
            position=self.maze_env.start_pos,
            visited_positions=[self.maze_env.start_pos],
            discovered_cells={self.maze_env.start_pos: 0.0}  # ã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã¯é€šå¸¸ãƒã‚¹
        )
        
    def decide_action(self, state: MazeState) -> int:
        """ç¾åœ¨çŠ¶æ…‹ã‹ã‚‰ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¦è¡Œå‹•ã‚’æ±ºå®š"""
        
        # ç¾åœ¨ä½ç½®ã®ã‚´ãƒ¼ãƒ«æƒ…å ±
        current_goal_info = state.discovered_cells.get(state.position, -1.0)
        
        # ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆï¼ˆè¡Œå‹•å¾Œã®çŠ¶æ…‹ï¼‰
        query_vector = self.vector_space.create_query_vector(
            position=state.position,
            visits=state.visited_positions.count(state.position),
            goal_info=current_goal_info
        )
        
        logger.info(f"Query vector at {state.position}: {query_vector}")
        
        # ãƒ‰ãƒ¼ãƒŠãƒ„æ¤œç´¢ã§é¡ä¼¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’æ¢ã™
        if len(self.query_response_pairs) > 0:
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ãƒ‰ãƒ¼ãƒŠãƒ„æ¤œç´¢ã«è¿½åŠ 
            for i, (q_vec, action, r_vec) in enumerate(self.query_response_pairs):
                self.donut_search.add_episode(f"episode_{i}", q_vec, None)
            
            result = self.donut_search.donut_search(
                query_vector,
                inner_radius=0.05,  # ã‚ˆã‚Šå³å¯†ãªé¡ä¼¼æ€§
                outer_radius=0.5
            )
            
            logger.info(f"Donut search: inner={len(result.inner_nodes)}, " +
                       f"candidates={len(result.candidates)}, " +
                       f"outer={len(result.outer_nodes)}")
        
        # å¯èƒ½ãªè¡Œå‹•ã‚’å–å¾—
        possible_actions = self._get_possible_actions()
        if not possible_actions:
            return 0
            
        # å„è¡Œå‹•ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        action_scores = {}
        
        for action in possible_actions:
            score = 0.0
            
            # éå»ã®çµŒé¨“ã‹ã‚‰å­¦ç¿’
            for q_vec, past_action, r_vec in self.query_response_pairs:
                if past_action == action:
                    # åŒã˜è¡Œå‹•ã®çµæœã‚’å‚ç…§
                    similarity = 1.0 - np.linalg.norm(query_vector - q_vec)
                    
                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ™ã‚¯ãƒˆãƒ«ã®ã‚´ãƒ¼ãƒ«æƒ…å ±ã§é‡ã¿ä»˜ã‘
                    if r_vec[5] == 1.0:  # ã‚´ãƒ¼ãƒ«ç™ºè¦‹
                        score += similarity * 100.0
                    elif r_vec[3] == 1.0:  # ã‚´ãƒ¼ãƒ«ã¸ã®ç§»å‹•ï¼ˆresult='goal'ï¼‰
                        score += similarity * 50.0
                    elif r_vec[3] == 0.0:  # é€šå¸¸ã®ç§»å‹•ï¼ˆresult='empty'ï¼‰
                        score += similarity * 10.0
                    elif r_vec[3] == -1.0:  # å£ï¼ˆresult='wall'ï¼‰
                        score -= similarity * 20.0
            
            # æ¢ç´¢ãƒœãƒ¼ãƒŠã‚¹ï¼ˆæœªçŸ¥ã®æ–¹å‘ã‚’å„ªå…ˆï¼‰
            dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0)][action]
            next_pos = (state.position[0] + dx, state.position[1] + dy)
            if next_pos not in state.discovered_cells:
                score += 20.0
            
            # è¨ªå•å›æ•°ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆè»½æ¸›ï¼‰
            visit_penalty = state.visited_positions.count(next_pos) * 2.0
            score -= visit_penalty
            
            action_scores[action] = score
            
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®è¡Œå‹•ã‚’é¸æŠï¼ˆåŒç‚¹ã®å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
        max_score = max(action_scores.values())
        best_actions = [a for a, s in action_scores.items() if s == max_score]
        best_action = np.random.choice(best_actions)
        
        action_names = ['â†‘', 'â†’', 'â†“', 'â†']
        logger.info(f"Action scores: {[(action_names[a], round(s, 2)) for a, s in action_scores.items()]}")
        logger.info(f"Selected action: {best_action} ({action_names[best_action]})")
        
        return best_action
        
    def _get_possible_actions(self) -> List[int]:
        """å¯èƒ½ãªè¡Œå‹•ã®ãƒªã‚¹ãƒˆ"""
        actions = []
        x, y = self.current_state.position
        
        for action, (dx, dy) in enumerate([(0, -1), (1, 0), (0, 1), (-1, 0)]):
            nx, ny = x + dx, y + dy
            if (0 <= nx < self.maze_env.size[0] and 
                0 <= ny < self.maze_env.size[1] and
                (self.maze_env.grid[ny, nx] == 0 or (nx, ny) == self.maze_env.goal_pos)):
                actions.append(action)
                
        return actions
        
    def execute_action(self, action: int) -> str:
        """è¡Œå‹•ã‚’å®Ÿè¡Œã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨˜éŒ²"""
        old_pos = self.current_state.position
        old_goal_info = self.current_state.discovered_cells.get(old_pos, -1.0)
        old_visits = self.current_state.visited_positions.count(old_pos)
        
        # ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«ï¼ˆè¡Œå‹•å‰ï¼‰
        query_vector = self.vector_space.create_query_vector(
            position=old_pos,
            visits=old_visits,
            goal_info=old_goal_info
        )
        
        # è¡Œå‹•å®Ÿè¡Œ
        dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0)][action]
        new_x, new_y = old_pos[0] + dx, old_pos[1] + dy
        
        if (0 <= new_x < self.maze_env.size[0] and 
            0 <= new_y < self.maze_env.size[1]):
            
            if self.maze_env.grid[new_y, new_x] == 0 or (new_x, new_y) == self.maze_env.goal_pos:
                # ç§»å‹•æˆåŠŸ
                self.current_state.position = (new_x, new_y)
                
                # ã‚´ãƒ¼ãƒ«åˆ¤å®šã¨æƒ…å ±æ›´æ–°
                if (new_x, new_y) == self.maze_env.goal_pos:
                    result = 'goal'
                    self.current_state.discovered_cells[(new_x, new_y)] = 1.0  # ã‚´ãƒ¼ãƒ«
                    if not self.goal_discovered:
                        self.goal_discovered = True
                        self.goal_position = (new_x, new_y)
                        logger.info(f"ğŸ¯ Goal discovered at {self.goal_position}!")
                else:
                    result = 'empty'
                    self.current_state.discovered_cells[(new_x, new_y)] = 0.0  # é€šå¸¸ãƒã‚¹
            else:
                result = 'wall'
        else:
            result = 'wall'
            
        # ç§»å‹•æˆåŠŸæ™‚ã®ã¿è¨ªå•è¨˜éŒ²
        if result != 'wall':
            self.current_state.visited_positions.append(self.current_state.position)
            
        self.current_state.last_action = action
        self.current_state.last_result = result
        self.current_state.step_count += 1
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ™ã‚¯ãƒˆãƒ«ä½œæˆ
        new_goal_info = self.current_state.discovered_cells.get(self.current_state.position, old_goal_info)
        response_vector = self.vector_space.create_response_vector(
            position=old_pos,  # è¡Œå‹•é–‹å§‹ä½ç½®
            action=action,
            result=result,
            visits=old_visits,
            goal_info=new_goal_info  # æ›´æ–°ã•ã‚ŒãŸã‚´ãƒ¼ãƒ«æƒ…å ±
        )
        
        # ã‚¯ã‚¨ãƒªãƒ»ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒšã‚¢ã‚’è¨˜éŒ²
        self.query_response_pairs.append((query_vector, action, response_vector))
        
        logger.info(f"Response vector: {response_vector}")
        
        return result
        
    def solve_maze(self, maze_query: Dict, max_steps: int = 50) -> Dict:
        """è¿·è·¯ã‚’è§£ã"""
        self.setup_maze(maze_query)
        
        print(f"\n=== Post-Action Query Agent ===")
        print(f"Start: {self.maze_env.start_pos}, Goal: Unknown initially")
        print(f"Query: [X, Y, null, null, visits, goal_info]")
        print(f"Response: [X, Y, action, result, visits, goal_info]\n")
        
        path_for_viz = []
        
        while self.current_state.step_count < max_steps:
            print(f"Step {self.current_state.step_count}: Position {self.current_state.position}", end=" ")
            
            # è¡Œå‹•æ±ºå®š
            action = self.decide_action(self.current_state)
            
            # è¡Œå‹•å®Ÿè¡Œ
            result = self.execute_action(action)
            actions = ['â†‘', 'â†’', 'â†“', 'â†']
            print(f"â†’ {actions[action]} â†’ {result}")
            
            path_for_viz.append(self.current_state.position)
            
            # ã‚´ãƒ¼ãƒ«åˆ¤å®š
            if self.current_state.position == self.maze_env.goal_pos:
                print(f"\nğŸ‰ Goal reached in {self.current_state.step_count} steps!")
                
                # ã‚´ãƒ¼ãƒ«åˆ°é”æ™‚ã®ç‰¹åˆ¥ãªã‚¯ã‚¨ãƒªãƒ»ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒšã‚¢
                final_query = self.vector_space.create_query_vector(
                    position=self.current_state.position,
                    visits=self.current_state.visited_positions.count(self.current_state.position),
                    goal_info=1.0
                )
                # ã‚´ãƒ¼ãƒ«ã§ã®ã€Œæ»åœ¨ã€ã‚’è¡¨ã™ãƒ¬ã‚¹ãƒãƒ³ã‚¹
                final_response = self.vector_space.create_response_vector(
                    position=self.current_state.position,
                    action=4,  # ç‰¹åˆ¥ãªå€¤ï¼šæ»åœ¨
                    result='goal',
                    visits=self.current_state.visited_positions.count(self.current_state.position),
                    goal_info=1.0
                )
                self.query_response_pairs.append((final_query, 4, final_response))
                
                break
                
        # ç°¡å˜ãªå¯è¦–åŒ–
        self._visualize_result(path_for_viz)
        
        return {
            'success': self.current_state.position == self.maze_env.goal_pos,
            'steps': self.current_state.step_count,
            'path': self.current_state.visited_positions,
            'unique_positions': len(set(self.current_state.visited_positions)),
            'discovered_cells': len(self.current_state.discovered_cells),
            'episodes_recorded': len(self.query_response_pairs)
        }
        
    def _visualize_result(self, path: List[Tuple[int, int]]):
        """çµæœã‚’å¯è¦–åŒ–"""
        fig, ax = plt.subplots(figsize=(6, 6))
        
        # è¿·è·¯ã‚’æç”»
        maze_display = self.maze_env.grid.copy().astype(float)
        ax.imshow(maze_display, cmap='binary', alpha=0.3)
        
        # çµŒè·¯ã‚’æç”»
        if path:
            path_array = np.array(path)
            ax.plot(path_array[:, 0], path_array[:, 1], 'g-', linewidth=2, alpha=0.7)
            
            # è¨ªå•é †ã‚’ç•ªå·ã§è¡¨ç¤º
            for i, pos in enumerate(path[:20]):
                ax.text(pos[0], pos[1], str(i), fontsize=8, ha='center', va='center')
        
        # ã‚¹ã‚¿ãƒ¼ãƒˆã¨ã‚´ãƒ¼ãƒ«
        ax.plot(*self.maze_env.start_pos, 'go', markersize=15, label='Start')
        ax.plot(*self.maze_env.goal_pos, 'r*', markersize=20, label='Goal')
        
        ax.set_title(f'Post-Action Query Agent (Steps: {self.current_state.step_count})')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('results/post_action_query_solution.png')
        plt.close()
        print("\nVisualization saved to results/post_action_query_solution.png")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    # 3x3ã®ç°¡å˜ãªè¿·è·¯
    test_maze_3x3 = {
        "maze": [
            [0, 0, 0],
            [0, 1, 0],
            [0, 0, 0]
        ],
        "start": (0, 0),
        "goal": (2, 2)
    }
    
    # 5x5ã®è¿·è·¯
    test_maze_5x5 = {
        "maze": [
            [0, 0, 1, 0, 0],
            [0, 1, 1, 0, 0],
            [0, 0, 0, 0, 1],
            [1, 0, 1, 0, 0],
            [0, 0, 0, 0, 0]
        ],
        "start": (0, 0),
        "goal": (4, 4)
    }
    
    # 3x3ã§ãƒ†ã‚¹ãƒˆ
    print("Testing with 3x3 maze...")
    agent = PostActionQueryAgent()
    result = agent.solve_maze(test_maze_3x3)
    
    print(f"\n=== Result ===")
    print(f"Success: {result['success']}")
    print(f"Steps: {result['steps']}")
    print(f"Episodes recorded: {result['episodes_recorded']}")
    print(f"Efficiency: {result['unique_positions'] / result['steps']:.2%}")
    
    # æˆåŠŸã—ãŸã‚‰5x5ã§ã‚‚ãƒ†ã‚¹ãƒˆ
    if result['success']:
        print("\n\nTesting with 5x5 maze...")
        agent2 = PostActionQueryAgent()
        result2 = agent2.solve_maze(test_maze_5x5)
        
        print(f"\n=== Result ===")
        print(f"Success: {result2['success']}")
        print(f"Steps: {result2['steps']}")
        print(f"Episodes recorded: {result2['episodes_recorded']}")
        print(f"Efficiency: {result2['unique_positions'] / result2['steps']:.2%}")


if __name__ == "__main__":
    main()