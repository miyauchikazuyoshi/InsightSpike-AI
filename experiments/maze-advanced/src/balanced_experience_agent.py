#!/usr/bin/env python3
"""
Balanced Experience-Based Agent
===============================

çµŒé¨“ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’ã«æœ€å°é™ã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’è¿½åŠ 
- åŸºæœ¬ã¯é¡ä¼¼æ€§ãƒ™ãƒ¼ã‚¹
- æ¢ç´¢ä¿ƒé€²ã®ãŸã‚ã®æ–°è¦æ€§ãƒœãƒ¼ãƒŠã‚¹
- æ–¹å‘æ€§ã®é€£ç¶šæ€§ã‚’è€ƒæ…®
"""

import sys
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import logging
import json
from datetime import datetime
from collections import deque

sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from insightspike.environments.maze import SimpleMaze
from donut_search_maze import DonutSearchMaze

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class MazeState:
    position: Tuple[int, int]
    visited_positions: List[Tuple[int, int]]
    last_action: Optional[int] = None
    last_result: Optional[str] = None
    step_count: int = 0
    
    # è¿½åŠ ï¼šæœ€è¿‘ã®è¡Œå‹•å±¥æ­´
    recent_actions: deque = None
    
    def __post_init__(self):
        if self.recent_actions is None:
            self.recent_actions = deque(maxlen=5)


class BalancedExperienceAgent:
    """ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸçµŒé¨“ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, maze_size: int):
        self.maze_size = maze_size
        self.vector_space = None
        self.maze_env = None
        self.current_state = None
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶
        self.episodes = []  # (state_vector, action, result_vector)
        
        # æ¢ç´¢ç”¨ã®ãƒ‰ãƒ¼ãƒŠãƒ„ã‚µãƒ¼ãƒ
        self.donut_search = None
        
        # è¨˜éŒ²ç”¨
        self.decision_log = []
        
    def create_maze(self) -> np.ndarray:
        """è¿·è·¯ã‚’ç”Ÿæˆ"""
        if self.maze_size == 5:
            maze = np.array([
                [0, 0, 1, 0, 0],
                [0, 1, 1, 0, 0],
                [0, 0, 0, 0, 1],
                [1, 0, 1, 0, 0],
                [0, 0, 0, 0, 0]
            ])
        else:
            # 10x10
            maze = np.zeros((10, 10))
            maze[1:4, 2] = 1
            maze[2, 2:7] = 1
            maze[4:7, 4] = 1
            maze[6, 4:8] = 1
            maze[8, 1:5] = 1
            return maze
        return maze
    
    def setup_maze(self):
        """è¿·è·¯ç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        maze_array = self.create_maze()
        
        self.maze_env = SimpleMaze(
            size=(self.maze_size, self.maze_size),
            maze_type='custom',
            maze_layout=maze_array,
            start_pos=(0, 0),
            goal_pos=(self.maze_size-1, self.maze_size-1)
        )
        
        self.vector_space = EnhancedVectorSpace(self.maze_env.size)
        
        self.current_state = MazeState(
            position=self.maze_env.start_pos,
            visited_positions=[self.maze_env.start_pos]
        )
        
        # ãƒ‰ãƒ¼ãƒŠãƒ„ã‚µãƒ¼ãƒã®åˆæœŸåŒ–
        self.donut_search = DonutSearchMaze(self.maze_env)
        
        # åˆæœŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¿½åŠ 
        self._add_initial_episodes()
        
    def _add_initial_episodes(self):
        """åˆæœŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¿½åŠ ï¼ˆã‚´ãƒ¼ãƒ«ï¼‹åˆæœŸæ¢ç´¢ï¼‰"""
        # ã‚´ãƒ¼ãƒ«ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
        goal_x, goal_y = self.maze_env.goal_pos
        goal_vector = self.vector_space.create_state_vector(
            position=(goal_x, goal_y),
            action=None,
            result='goal',
            visit_count=1
        )
        self.episodes.append((goal_vector, 4, goal_vector))  # ç‰¹åˆ¥ãªæ»åœ¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        
        # ã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã‹ã‚‰ã®4æ–¹å‘æ¢ç´¢
        start_x, start_y = self.maze_env.start_pos
        for action, (dx, dy) in enumerate([(0, -1), (1, 0), (0, 1), (-1, 0)]):
            nx, ny = start_x + dx, start_y + dy
            
            start_vector = self.vector_space.create_state_vector(
                position=(start_x, start_y),
                action=None,
                result=None,
                visit_count=1
            )
            
            if (0 <= nx < self.maze_env.size[0] and 
                0 <= ny < self.maze_env.size[1] and
                (self.maze_env.grid[ny, nx] == 0 or (nx, ny) == self.maze_env.goal_pos)):
                result = 'goal' if (nx, ny) == self.maze_env.goal_pos else 'empty'
                result_vector = self.vector_space.create_state_vector(
                    position=(nx, ny),
                    action=action,
                    result=result,
                    visit_count=1
                )
            else:
                result = 'wall'
                result_vector = self.vector_space.create_state_vector(
                    position=(start_x, start_y),
                    action=action,
                    result=result,
                    visit_count=1
                )
            
            self.episodes.append((start_vector, action, result_vector))
        
        logger.info(f"Added {len(self.episodes)} initial episodes")
        
    def decide_action_balanced(self, state: MazeState) -> int:
        """ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸè¡Œå‹•æ±ºå®š"""
        
        # ç¾åœ¨çŠ¶æ…‹ã®ãƒ™ã‚¯ãƒˆãƒ«
        visit_count = state.visited_positions.count(state.position)
        current_vector = self.vector_space.create_state_vector(
            position=state.position,
            action=state.last_action,
            result=state.last_result,
            visit_count=visit_count
        )
        
        # å¯èƒ½ãªè¡Œå‹•
        possible_actions = self._get_possible_actions()
        if not possible_actions:
            return 0
        
        # å„è¡Œå‹•ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        action_scores = {}
        
        for action in possible_actions:
            # 1. åŸºæœ¬ã®é¡ä¼¼æ€§ã‚¹ã‚³ã‚¢
            similarity_score = self._calculate_similarity_score(
                current_vector, action
            )
            
            # 2. æ–°è¦æ€§ãƒœãƒ¼ãƒŠã‚¹ï¼ˆæœªæ¢ç´¢æ–¹å‘ã¸ã®ä¿ƒé€²ï¼‰
            novelty_bonus = self._calculate_novelty_bonus(
                state, action
            )
            
            # 3. æ–¹å‘æ€§ã®é€£ç¶šæ€§ï¼ˆåŒã˜æ–¹å‘ã¸ã®ç¶™ç¶šã‚’ä¿ƒé€²ï¼‰
            continuity_bonus = self._calculate_continuity_bonus(
                state, action
            )
            
            # 4. ãƒ‰ãƒ¼ãƒŠãƒ„ã‚µãƒ¼ãƒã«ã‚ˆã‚‹æ¢ç´¢ä¾¡å€¤
            exploration_value = self._calculate_exploration_value(
                state.position, action
            )
            
            # ç·åˆã‚¹ã‚³ã‚¢
            total_score = (
                similarity_score * 0.4 +
                novelty_bonus * 0.3 +
                continuity_bonus * 0.1 +
                exploration_value * 0.2
            )
            
            action_scores[action] = {
                'total': total_score,
                'similarity': similarity_score,
                'novelty': novelty_bonus,
                'continuity': continuity_bonus,
                'exploration': exploration_value
            }
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®è¡Œå‹•ã‚’é¸æŠï¼ˆÎµ-greedyï¼‰
        epsilon = max(0.1, 0.3 - state.step_count * 0.001)  # æ™‚é–“ã¨ã¨ã‚‚ã«æ¸›å°‘
        
        if np.random.random() < epsilon:
            selected_action = np.random.choice(possible_actions)
            reason = 'exploration'
        else:
            max_score = max(scores['total'] for scores in action_scores.values())
            best_actions = [a for a, scores in action_scores.items() 
                           if scores['total'] == max_score]
            selected_action = np.random.choice(best_actions)
            reason = 'exploitation'
        
        # æ±ºå®šã‚’ãƒ­ã‚°ã«è¨˜éŒ²
        action_names = ['â†‘', 'â†’', 'â†“', 'â†']
        self.decision_log.append({
            'step': state.step_count,
            'position': state.position,
            'state_vector': current_vector.tolist(),
            'action_scores': {
                action_names[a]: {k: round(v, 3) for k, v in scores.items()}
                for a, scores in action_scores.items()
            },
            'selected_action': action_names[selected_action],
            'reason': reason,
            'epsilon': round(epsilon, 3)
        })
        
        return selected_action
    
    def _calculate_similarity_score(self, current_vector: np.ndarray, action: int) -> float:
        """é¡ä¼¼æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        max_similarity = -1.0
        
        for ep_state, ep_action, ep_result in self.episodes:
            if ep_action == action:
                # æˆåŠŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’é‡è¦–
                result_weight = 1.0
                if hasattr(ep_result, '__iter__') and len(ep_result) >= 4:
                    if ep_result[3] == 1.0:  # goal
                        result_weight = 2.0
                    elif ep_result[3] == -1.0:  # wall
                        result_weight = 0.5
                
                similarity = np.dot(current_vector, ep_state) / (
                    np.linalg.norm(current_vector) * np.linalg.norm(ep_state) + 1e-10
                )
                
                weighted_similarity = similarity * result_weight
                max_similarity = max(max_similarity, weighted_similarity)
        
        return max_similarity
    
    def _calculate_novelty_bonus(self, state: MazeState, action: int) -> float:
        """æ–°è¦æ€§ãƒœãƒ¼ãƒŠã‚¹ã®è¨ˆç®—"""
        dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0)][action]
        nx, ny = state.position[0] + dx, state.position[1] + dy
        
        if (nx, ny) not in state.visited_positions:
            return 1.0  # æœªè¨ªå•åœ°ç‚¹ã¸ã®ãƒœãƒ¼ãƒŠã‚¹
        else:
            # è¨ªå•å›æ•°ã«å¿œã˜ã¦æ¸›è¡°
            visit_count = state.visited_positions.count((nx, ny))
            return 1.0 / (1.0 + visit_count)
    
    def _calculate_continuity_bonus(self, state: MazeState, action: int) -> float:
        """æ–¹å‘æ€§ã®é€£ç¶šæ€§ãƒœãƒ¼ãƒŠã‚¹"""
        if not state.recent_actions:
            return 0.5
        
        # æœ€è¿‘ã®è¡Œå‹•ã¨åŒã˜æ–¹å‘ãªã‚‰é«˜ã‚¹ã‚³ã‚¢
        if state.last_action == action:
            return 1.0
        
        # é€†æ–¹å‘ãªã‚‰ä½ã‚¹ã‚³ã‚¢
        opposite = (action + 2) % 4
        if state.last_action == opposite:
            return 0.0
        
        return 0.5
    
    def _calculate_exploration_value(self, position: Tuple[int, int], action: int) -> float:
        """ãƒ‰ãƒ¼ãƒŠãƒ„ã‚µãƒ¼ãƒã«ã‚ˆã‚‹æ¢ç´¢ä¾¡å€¤"""
        dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0)][action]
        nx, ny = position[0] + dx, position[1] + dy
        
        if not (0 <= nx < self.maze_env.size[0] and 0 <= ny < self.maze_env.size[1]):
            return 0.0
        
        # ç¾åœ¨ä½ç½®ã‹ã‚‰ã®è·é›¢
        current_distance = np.sqrt((nx - position[0])**2 + (ny - position[1])**2)
        
        # ã‚´ãƒ¼ãƒ«ã¾ã§ã®è·é›¢
        goal_distance = np.sqrt(
            (nx - self.maze_env.goal_pos[0])**2 + 
            (ny - self.maze_env.goal_pos[1])**2
        )
        
        # ãƒ‰ãƒ¼ãƒŠãƒ„é ˜åŸŸå†…ãªã‚‰é«˜è©•ä¾¡
        if 0.2 < goal_distance / max(self.maze_env.size) < 0.8:
            return 0.8
        else:
            return 0.2
    
    def _get_possible_actions(self) -> List[int]:
        """å¯èƒ½ãªè¡Œå‹•ã®ãƒªã‚¹ãƒˆ"""
        actions = []
        x, y = self.current_state.position
        
        for action, (dx, dy) in enumerate([(0, -1), (1, 0), (0, 1), (-1, 0)]):
            nx, ny = x + dx, y + dy
            if (0 <= nx < self.maze_env.size[0] and 
                0 <= ny < self.maze_env.size[1] and
                (self.maze_env.grid[ny, nx] == 0 or (nx, ny) == self.maze_env.goal_pos)):
                actions.append(action)
                
        return actions
    
    def execute_action(self, action: int) -> str:
        """è¡Œå‹•ã‚’å®Ÿè¡Œã—ã¦çµŒé¨“ã‚’è¨˜éŒ²"""
        old_pos = self.current_state.position
        visit_count = self.current_state.visited_positions.count(old_pos)
        
        old_vector = self.vector_space.create_state_vector(
            position=old_pos,
            action=self.current_state.last_action,
            result=self.current_state.last_result,
            visit_count=visit_count
        )
        
        # è¡Œå‹•å®Ÿè¡Œ
        dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0)][action]
        new_x, new_y = old_pos[0] + dx, old_pos[1] + dy
        
        if (0 <= new_x < self.maze_env.size[0] and 
            0 <= new_y < self.maze_env.size[1]):
            
            if self.maze_env.grid[new_y, new_x] == 0 or (new_x, new_y) == self.maze_env.goal_pos:
                # ç§»å‹•æˆåŠŸ
                self.current_state.position = (new_x, new_y)
                
                if (new_x, new_y) == self.maze_env.goal_pos:
                    result = 'goal'
                else:
                    result = 'empty'
            else:
                result = 'wall'
        else:
            result = 'wall'
            
        if result != 'wall':
            self.current_state.visited_positions.append(self.current_state.position)
            
        self.current_state.last_action = action
        self.current_state.last_result = result
        self.current_state.recent_actions.append(action)
        self.current_state.step_count += 1
        
        # çµæœãƒ™ã‚¯ãƒˆãƒ«
        new_visit_count = self.current_state.visited_positions.count(self.current_state.position)
        result_vector = self.vector_space.create_state_vector(
            position=self.current_state.position,
            action=action,
            result=result,
            visit_count=new_visit_count
        )
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã—ã¦è¨˜éŒ²
        self.episodes.append((old_vector, action, result_vector))
        
        # ãƒ‰ãƒ¼ãƒŠãƒ„ã‚µãƒ¼ãƒã®æ›´æ–°
        if self.donut_search:
            self.donut_search.update_position(self.current_state.position)
        
        return result
    
    def solve_maze(self, max_steps: int = 100) -> Dict:
        """è¿·è·¯ã‚’è§£ã"""
        self.setup_maze()
        
        print(f"\n=== Balanced Experience {self.maze_size}x{self.maze_size} Maze ===")
        print(f"Similarity + Novelty + Continuity + Exploration")
        print(f"Start: (0, 0), Goal: ({self.maze_size-1}, {self.maze_size-1})\n")
        
        path_history = [self.current_state.position]
        
        while self.current_state.step_count < max_steps:
            if self.current_state.step_count % 10 == 0:
                print(f"Step {self.current_state.step_count}: Position {self.current_state.position}")
            
            # ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸè¡Œå‹•æ±ºå®š
            action = self.decide_action_balanced(self.current_state)
            
            # è¡Œå‹•å®Ÿè¡Œ
            result = self.execute_action(action)
            
            path_history.append(self.current_state.position)
            
            # ã‚´ãƒ¼ãƒ«åˆ¤å®š
            if self.current_state.position == self.maze_env.goal_pos:
                print(f"\nğŸ‰ Goal reached in {self.current_state.step_count} steps!")
                break
        
        # çµæœã‚’ä¿å­˜
        self._save_results(path_history)
        self._visualize_path(path_history)
        
        return {
            'success': self.current_state.position == self.maze_env.goal_pos,
            'steps': self.current_state.step_count,
            'path_length': len(path_history),
            'unique_positions': len(set(path_history)),
            'total_episodes': len(self.episodes)
        }
    
    def _save_results(self, path_history: List[Tuple[int, int]]):
        """çµæœã‚’JSONã§ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"results/balanced_experience_{self.maze_size}x{self.maze_size}_{timestamp}.json"
        
        data = {
            'maze_size': self.maze_size,
            'timestamp': timestamp,
            'total_steps': self.current_state.step_count,
            'success': self.current_state.position == self.maze_env.goal_pos,
            'path': [list(pos) for pos in path_history],
            'unique_positions': len(set(path_history)),
            'decision_log': self.decision_log[:20],  # æœ€åˆã®20ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿
            'total_episodes': len(self.episodes)
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
            
        print(f"\nDecision log saved to: {filename}")
    
    def _visualize_path(self, path_history: List[Tuple[int, int]]):
        """çµŒè·¯ã‚’å¯è¦–åŒ–"""
        fig, ax = plt.subplots(figsize=(8, 8))
        
        # è¿·è·¯ã‚’æç”»
        maze_display = self.maze_env.grid.copy().astype(float)
        ax.imshow(maze_display, cmap='binary', alpha=0.3)
        
        # è¨ªå•é »åº¦ãƒãƒƒãƒ—
        visit_map = np.zeros_like(maze_display)
        for pos in path_history:
            x, y = pos
            visit_map[y, x] += 1
        
        # æ­£è¦åŒ–ã—ã¦ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤
        if visit_map.max() > 0:
            visit_map = visit_map / visit_map.max()
        
        combined = np.where(maze_display == 1, -0.5, visit_map)
        im = ax.imshow(combined, cmap='RdYlGn', vmin=-0.5, vmax=1, alpha=0.8)
        
        # ãƒ‘ã‚¹ã‚’ç·šã§æç”»
        if len(path_history) > 1:
            path_array = np.array(path_history)
            ax.plot(path_array[:, 0], path_array[:, 1], 'b-', linewidth=1, alpha=0.5)
        
        # ã‚¹ã‚¿ãƒ¼ãƒˆã¨ã‚´ãƒ¼ãƒ«
        ax.plot(0, 0, 'go', markersize=15, label='Start')
        ax.plot(self.maze_size-1, self.maze_size-1, 'r*', markersize=20, label='Goal')
        
        ax.set_title(f'Balanced Experience Path (Steps: {self.current_state.step_count})')
        ax.legend()
        
        plt.colorbar(im, ax=ax, label='Visit Frequency')
        plt.tight_layout()
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'results/balanced_experience_path_{self.maze_size}x{self.maze_size}_{timestamp}.png'
        plt.savefig(filename, dpi=150)
        plt.close()
        
        print(f"Path visualization saved to: {filename}")


class EnhancedVectorSpace:
    """æ‹¡å¼µã•ã‚ŒãŸçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ï¼ˆ5æ¬¡å…ƒï¼‰"""
    
    def __init__(self, maze_size: Tuple[int, int]):
        self.maze_width, self.maze_height = maze_size
        
    def create_state_vector(self, position: Tuple[int, int], 
                          action: Optional[int] = None,
                          result: Optional[str] = None,
                          visit_count: int = 0) -> np.ndarray:
        """çŠ¶æ…‹ã‚’5æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
        # ä½ç½®ã‚’æ­£è¦åŒ–
        norm_x = position[0] / (self.maze_width - 1) if self.maze_width > 1 else 0
        norm_y = position[1] / (self.maze_height - 1) if self.maze_height > 1 else 0
        
        # è¡Œå‹•ã‚’æ­£è¦åŒ–
        if action is not None:
            norm_action = action * 0.25
        else:
            norm_action = 0.5
        
        # çµæœã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        result_map = {'wall': -1.0, 'empty': 0.0, 'goal': 1.0, None: 0.0}
        norm_result = result_map.get(result, 0.0)
        
        # è¨ªå•å›æ•°ã‚’æ­£è¦åŒ–ï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        norm_visits = np.log1p(visit_count) / 10.0
        
        return np.array([norm_x, norm_y, norm_action, norm_result, norm_visits])


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 60)
    print("Testing Balanced Experience-Based Agent")
    print("Similarity + Novelty + Continuity + Exploration")
    print("=" * 60)
    
    # 5x5è¿·è·¯ã§ãƒ†ã‚¹ãƒˆ
    agent_5 = BalancedExperienceAgent(maze_size=5)
    result_5 = agent_5.solve_maze(max_steps=100)
    
    print(f"\n=== 5x5 Maze Result ===")
    print(f"Success: {result_5['success']}")
    print(f"Steps: {result_5['steps']}")
    print(f"Unique positions: {result_5['unique_positions']}")
    print(f"Path efficiency: {result_5['unique_positions'] / result_5['path_length']:.2%}")
    print(f"Total episodes learned: {result_5['total_episodes']}")
    
    # 10x10è¿·è·¯ã§ãƒ†ã‚¹ãƒˆ
    print("\n" + "=" * 60)
    
    agent_10 = BalancedExperienceAgent(maze_size=10)
    result_10 = agent_10.solve_maze(max_steps=200)
    
    print(f"\n=== 10x10 Maze Result ===")
    print(f"Success: {result_10['success']}")
    print(f"Steps: {result_10['steps']}")
    print(f"Unique positions: {result_10['unique_positions']}")
    print(f"Path efficiency: {result_10['unique_positions'] / result_10['path_length']:.2%}")
    print(f"Total episodes learned: {result_10['total_episodes']}")


if __name__ == "__main__":
    main()