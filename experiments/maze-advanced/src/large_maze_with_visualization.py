#!/usr/bin/env python3
"""
Large Maze with Memory Visualization
====================================

10x10, 20x20è¿·è·¯ã§ã®å®Ÿé¨“
è¨˜æ†¶ãƒãƒ¼ãƒ‰ã®å¢—åŠ ã¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é¸æŠã‚’å¯è¦–åŒ–ãƒ»è¨˜éŒ²
"""

import sys
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import matplotlib
matplotlib.use('Agg')
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
import logging

sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from insightspike.config.presets import ConfigPresets
from insightspike.environments.maze import SimpleMaze
from donut_search_maze import DonutSearchMaze

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class EpisodeRecord:
    """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜éŒ²"""
    step: int
    episode_id: str
    position: Tuple[int, int]
    query_vector: List[float]
    selected_action: int
    action_name: str
    result: str
    response_vector: List[float]
    similar_episodes: List[Dict]  # é¡ä¼¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
    action_scores: Dict[str, float]
    memory_node_count: int


@dataclass
class MazeState:
    """è¿·è·¯ã®ç¾åœ¨çŠ¶æ…‹"""
    position: Tuple[int, int]
    visited_positions: List[Tuple[int, int]]
    discovered_cells: Dict[Tuple[int, int], float]
    last_action: Optional[int] = None
    last_result: Optional[str] = None
    step_count: int = 0


class MemoryVisualizingAgent:
    """è¨˜æ†¶ã®æˆé•·ã‚’å¯è¦–åŒ–ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, maze_size: int):
        self.maze_size = maze_size
        self.vector_space = None
        self.maze_env = None
        self.current_state = None
        self.donut_search = DonutSearchMaze(dimension=6)
        
        # è¨˜éŒ²ç”¨
        self.episode_records: List[EpisodeRecord] = []
        self.memory_nodes: List[Dict] = []  # è¨˜æ†¶ãƒãƒ¼ãƒ‰ã®å±¥æ­´
        self.query_response_pairs = []
        
        # å¯è¦–åŒ–ç”¨
        self.node_positions = {}  # episode_id -> (x, y) in graph
        
    def create_maze(self) -> np.ndarray:
        """ã‚µã‚¤ã‚ºã«å¿œã˜ãŸè¿·è·¯ã‚’ç”Ÿæˆ"""
        if self.maze_size == 10:
            # 10x10ã®è¿·è·¯ï¼ˆæ‰‹å‹•è¨­è¨ˆï¼‰
            maze = np.zeros((10, 10))
            # å£ã‚’è¿½åŠ 
            maze[1:4, 2] = 1
            maze[2, 2:7] = 1
            maze[4:7, 4] = 1
            maze[6, 4:8] = 1
            maze[8, 1:5] = 1
            maze[5:9, 7] = 1
            return maze
        else:
            # 20x20ã®è¿·è·¯ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ç”Ÿæˆï¼‰
            np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
            maze = np.zeros((20, 20))
            
            # ãƒ©ãƒ³ãƒ€ãƒ ã«å£ã‚’é…ç½®ï¼ˆé€šè¡Œå¯èƒ½æ€§ã‚’ä¿è¨¼ï¼‰
            for i in range(1, 19):
                for j in range(1, 19):
                    if np.random.random() < 0.25:  # 25%ã®ç¢ºç‡ã§å£
                        # ã‚¹ã‚¿ãƒ¼ãƒˆã¨ã‚´ãƒ¼ãƒ«ä»˜è¿‘ã¯ç©ºã‘ã‚‹
                        if not ((i < 3 and j < 3) or (i > 16 and j > 16)):
                            maze[i, j] = 1
            
            return maze
    
    def setup_maze(self):
        """è¿·è·¯ç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        maze_array = self.create_maze()
        
        self.maze_env = SimpleMaze(
            size=(self.maze_size, self.maze_size),
            maze_type='custom',
            maze_layout=maze_array,
            start_pos=(0, 0),
            goal_pos=(self.maze_size-1, self.maze_size-1)
        )
        
        self.vector_space = PostActionVectorSpace(self.maze_env.size)
        
        self.current_state = MazeState(
            position=self.maze_env.start_pos,
            visited_positions=[self.maze_env.start_pos],
            discovered_cells={self.maze_env.start_pos: 0.0}
        )
        
        # æœ€åˆã®ã‚´ãƒ¼ãƒ«ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¿½åŠ ï¼ˆç›®çš„é–¢æ•°ã®ä»£ã‚ã‚Šï¼‰
        self._add_goal_episode()
        
    def _add_goal_episode(self):
        """ã‚´ãƒ¼ãƒ«ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¿½åŠ ï¼ˆå”¯ä¸€ã®ãƒãƒ¼ãƒˆï¼‰"""
        goal_x, goal_y = self.maze_env.goal_pos
        
        goal_query = self.vector_space.create_query_vector(
            position=(goal_x, goal_y),
            visits=1,
            goal_info=1.0
        )
        
        goal_response = self.vector_space.create_response_vector(
            position=(goal_x, goal_y),
            action=4,  # ç‰¹åˆ¥ãªå€¤ï¼šæ»åœ¨
            result='goal',
            visits=1,
            goal_info=1.0
        )
        
        episode_id = "GOAL_EPISODE"
        self.donut_search.add_episode(episode_id, goal_query, (goal_x, goal_y))
        self.query_response_pairs.append((goal_query, 4, goal_response))
        
        # ã‚°ãƒ©ãƒ•ç”¨ã®ä½ç½®ã‚’è¨˜éŒ²
        self.node_positions[episode_id] = (0.5, 0.9)  # ä¸Šéƒ¨ä¸­å¤®
        
        logger.info(f"Added goal episode at {self.maze_env.goal_pos}")
        
    def decide_action(self, state: MazeState) -> Tuple[int, Dict]:
        """è¡Œå‹•æ±ºå®šã¨è©³ç´°è¨˜éŒ²"""
        current_goal_info = state.discovered_cells.get(state.position, -1.0)
        
        # ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
        query_vector = self.vector_space.create_query_vector(
            position=state.position,
            visits=state.visited_positions.count(state.position),
            goal_info=current_goal_info
        )
        
        # é¡ä¼¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ¤œç´¢
        similar_episodes = []
        if len(self.query_response_pairs) > 0:
            for i, (q_vec, action, r_vec) in enumerate(self.query_response_pairs):
                similarity = 1.0 - np.linalg.norm(query_vector - q_vec)
                similar_episodes.append({
                    'episode_id': f"episode_{i}" if i > 0 else "GOAL_EPISODE",
                    'similarity': float(similarity),
                    'action': action,
                    'result_value': float(r_vec[3]),  # result
                    'goal_info': float(r_vec[5])      # goal_info
                })
            
            # é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆ
            similar_episodes.sort(key=lambda x: x['similarity'], reverse=True)
            similar_episodes = similar_episodes[:5]  # ä¸Šä½5å€‹
        
        # å¯èƒ½ãªè¡Œå‹•ã‚’å–å¾—
        possible_actions = self._get_possible_actions()
        if not possible_actions:
            return 0, {'similar_episodes': similar_episodes}
            
        # å„è¡Œå‹•ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        action_scores = {}
        
        for action in possible_actions:
            score = 0.0
            
            # é¡ä¼¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‹ã‚‰ã®å­¦ç¿’
            for ep in similar_episodes:
                if ep['action'] == action:
                    weight = ep['similarity']
                    if ep['goal_info'] == 1.0:
                        score += weight * 100.0
                    elif ep['result_value'] == 1.0:
                        score += weight * 50.0
                    elif ep['result_value'] == 0.0:
                        score += weight * 10.0
                    else:  # wall
                        score -= weight * 20.0
            
            # æ¢ç´¢ãƒœãƒ¼ãƒŠã‚¹
            dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0)][action]
            next_pos = (state.position[0] + dx, state.position[1] + dy)
            if next_pos not in state.discovered_cells:
                score += 15.0
            
            # è¨ªå•å›æ•°ãƒšãƒŠãƒ«ãƒ†ã‚£
            visit_penalty = state.visited_positions.count(next_pos) * 3.0
            score -= visit_penalty
            
            action_scores[action] = score
            
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®è¡Œå‹•ã‚’é¸æŠ
        max_score = max(action_scores.values())
        best_actions = [a for a, s in action_scores.items() if s == max_score]
        best_action = np.random.choice(best_actions)
        
        action_names = ['â†‘', 'â†’', 'â†“', 'â†']
        
        return best_action, {
            'similar_episodes': similar_episodes,
            'action_scores': {action_names[a]: round(s, 2) for a, s in action_scores.items()}
        }
        
    def _get_possible_actions(self) -> List[int]:
        """å¯èƒ½ãªè¡Œå‹•ã®ãƒªã‚¹ãƒˆ"""
        actions = []
        x, y = self.current_state.position
        
        for action, (dx, dy) in enumerate([(0, -1), (1, 0), (0, 1), (-1, 0)]):
            nx, ny = x + dx, y + dy
            if (0 <= nx < self.maze_env.size[0] and 
                0 <= ny < self.maze_env.size[1] and
                (self.maze_env.grid[ny, nx] == 0 or (nx, ny) == self.maze_env.goal_pos)):
                actions.append(action)
                
        return actions
        
    def execute_action(self, action: int, decision_info: Dict) -> str:
        """è¡Œå‹•å®Ÿè¡Œã¨è¨˜éŒ²"""
        old_pos = self.current_state.position
        old_goal_info = self.current_state.discovered_cells.get(old_pos, -1.0)
        old_visits = self.current_state.visited_positions.count(old_pos)
        
        # ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«
        query_vector = self.vector_space.create_query_vector(
            position=old_pos,
            visits=old_visits,
            goal_info=old_goal_info
        )
        
        # è¡Œå‹•å®Ÿè¡Œ
        dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0)][action]
        new_x, new_y = old_pos[0] + dx, old_pos[1] + dy
        
        if (0 <= new_x < self.maze_env.size[0] and 
            0 <= new_y < self.maze_env.size[1]):
            
            if self.maze_env.grid[new_y, new_x] == 0 or (new_x, new_y) == self.maze_env.goal_pos:
                # ç§»å‹•æˆåŠŸ
                self.current_state.position = (new_x, new_y)
                
                if (new_x, new_y) == self.maze_env.goal_pos:
                    result = 'goal'
                    self.current_state.discovered_cells[(new_x, new_y)] = 1.0
                else:
                    result = 'empty'
                    self.current_state.discovered_cells[(new_x, new_y)] = 0.0
            else:
                result = 'wall'
        else:
            result = 'wall'
            
        if result != 'wall':
            self.current_state.visited_positions.append(self.current_state.position)
            
        self.current_state.last_action = action
        self.current_state.last_result = result
        self.current_state.step_count += 1
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ™ã‚¯ãƒˆãƒ«
        new_goal_info = self.current_state.discovered_cells.get(self.current_state.position, old_goal_info)
        response_vector = self.vector_space.create_response_vector(
            position=old_pos,
            action=action,
            result=result,
            visits=old_visits,
            goal_info=new_goal_info
        )
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜éŒ²
        episode_id = f"step_{self.current_state.step_count}"
        self.query_response_pairs.append((query_vector, action, response_vector))
        self.donut_search.add_episode(episode_id, query_vector, old_pos)
        
        # è¨˜éŒ²ã‚’ä¿å­˜
        action_names = ['â†‘', 'â†’', 'â†“', 'â†']
        record = EpisodeRecord(
            step=self.current_state.step_count,
            episode_id=episode_id,
            position=old_pos,
            query_vector=query_vector.tolist(),
            selected_action=action,
            action_name=action_names[action],
            result=result,
            response_vector=response_vector.tolist(),
            similar_episodes=decision_info['similar_episodes'],
            action_scores=decision_info['action_scores'],
            memory_node_count=len(self.query_response_pairs)
        )
        self.episode_records.append(record)
        
        # ãƒ¡ãƒ¢ãƒªãƒãƒ¼ãƒ‰ã®çŠ¶æ…‹ã‚’è¨˜éŒ²
        self.memory_nodes.append({
            'step': self.current_state.step_count,
            'total_episodes': len(self.query_response_pairs),
            'discovered_cells': len(self.current_state.discovered_cells),
            'unique_positions': len(set(self.current_state.visited_positions))
        })
        
        return result
        
    def solve_maze(self, max_steps: int = 200) -> Dict:
        """è¿·è·¯ã‚’è§£ã"""
        self.setup_maze()
        
        print(f"\n=== {self.maze_size}x{self.maze_size} Maze Solving ===")
        print(f"Start: (0, 0), Goal: ({self.maze_size-1}, {self.maze_size-1})")
        
        # ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ•ãƒ¬ãƒ¼ãƒ ä¿å­˜
        frames = []
        
        while self.current_state.step_count < max_steps:
            if self.current_state.step_count % 10 == 0:
                print(f"Step {self.current_state.step_count}: Position {self.current_state.position}")
            
            # è¡Œå‹•æ±ºå®š
            action, decision_info = self.decide_action(self.current_state)
            
            # è¡Œå‹•å®Ÿè¡Œ
            result = self.execute_action(action, decision_info)
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ ä¿å­˜ï¼ˆ10ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ï¼‰
            if self.current_state.step_count % 10 == 0:
                frames.append(self._create_frame())
            
            # ã‚´ãƒ¼ãƒ«åˆ¤å®š
            if self.current_state.position == self.maze_env.goal_pos:
                print(f"\nğŸ‰ Goal reached in {self.current_state.step_count} steps!")
                frames.append(self._create_frame())  # æœ€çµ‚ãƒ•ãƒ¬ãƒ¼ãƒ 
                break
                
        # çµæœã‚’ä¿å­˜
        self._save_results()
        self._create_memory_growth_visualization()
        
        return {
            'success': self.current_state.position == self.maze_env.goal_pos,
            'steps': self.current_state.step_count,
            'path_length': len(self.current_state.visited_positions),
            'unique_positions': len(set(self.current_state.visited_positions)),
            'total_episodes': len(self.query_response_pairs),
            'discovered_cells': len(self.current_state.discovered_cells)
        }
        
    def _create_frame(self) -> np.ndarray:
        """ç¾åœ¨çŠ¶æ…‹ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ"""
        # ç°¡å˜ãªå¯è¦–åŒ–ã®ãŸã‚ã€è¨ªå•ãƒãƒƒãƒ—ã‚’è¿”ã™
        visit_map = np.zeros(self.maze_env.grid.shape)
        for pos in self.current_state.visited_positions:
            x, y = pos
            visit_map[y, x] += 1
        return visit_map
        
    def _convert_to_json_serializable(self, obj):
        """NumPyå‹ã‚’å«ã‚€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’JSON serializableã«å¤‰æ›"""
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: self._convert_to_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_json_serializable(item) for item in obj]
        elif isinstance(obj, tuple):
            return list(obj)
        else:
            return obj
    
    def _save_results(self):
        """çµæœã‚’JSONã§ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"results/maze_{self.maze_size}x{self.maze_size}_episodes_{timestamp}.json"
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜éŒ²ã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
        records_dict = []
        for record in self.episode_records:
            record_dict = asdict(record)
            # å…¨ä½“ã‚’å†å¸°çš„ã«å¤‰æ›
            record_dict = self._convert_to_json_serializable(record_dict)
            records_dict.append(record_dict)
        
        data = {
            'maze_size': self.maze_size,
            'timestamp': timestamp,
            'total_steps': self.current_state.step_count,
            'success': self.current_state.position == self.maze_env.goal_pos,
            'episode_records': records_dict,
            'memory_growth': self.memory_nodes,
            'final_path': [list(pos) for pos in self.current_state.visited_positions]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
            
        print(f"\nEpisode records saved to: {filename}")
        
    def _create_memory_growth_visualization(self):
        """è¨˜æ†¶ãƒãƒ¼ãƒ‰ã®æˆé•·ã‚’å¯è¦–åŒ–"""
        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12))
        
        steps = [node['step'] for node in self.memory_nodes]
        total_episodes = [node['total_episodes'] for node in self.memory_nodes]
        discovered_cells = [node['discovered_cells'] for node in self.memory_nodes]
        unique_positions = [node['unique_positions'] for node in self.memory_nodes]
        
        # 1. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã®æˆé•·
        ax1.plot(steps, total_episodes, 'b-', linewidth=2)
        ax1.set_xlabel('Steps')
        ax1.set_ylabel('Total Episodes')
        ax1.set_title(f'Memory Growth in {self.maze_size}x{self.maze_size} Maze')
        ax1.grid(True, alpha=0.3)
        
        # 2. ç™ºè¦‹ã—ãŸã‚»ãƒ«ã¨è¨ªå•ä½ç½®
        ax2.plot(steps, discovered_cells, 'g-', label='Discovered Cells', linewidth=2)
        ax2.plot(steps, unique_positions, 'r-', label='Unique Positions', linewidth=2)
        ax2.set_xlabel('Steps')
        ax2.set_ylabel('Count')
        ax2.set_title('Exploration Progress')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. è¿·è·¯ã¨æœ€çµ‚çµŒè·¯
        maze_display = self.maze_env.grid.copy().astype(float)
        
        # è¨ªå•é »åº¦ãƒãƒƒãƒ—
        visit_freq = np.zeros_like(maze_display)
        for pos in self.current_state.visited_positions:
            x, y = pos
            visit_freq[y, x] += 1
            
        # æ­£è¦åŒ–
        if visit_freq.max() > 0:
            visit_freq = visit_freq / visit_freq.max()
            
        # ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤
        combined = np.where(maze_display == 1, -1, visit_freq)
        
        im = ax3.imshow(combined, cmap='RdYlGn', vmin=-1, vmax=1)
        
        # ã‚¹ã‚¿ãƒ¼ãƒˆã¨ã‚´ãƒ¼ãƒ«
        ax3.plot(0, 0, 'go', markersize=10, label='Start')
        ax3.plot(self.maze_size-1, self.maze_size-1, 'r*', markersize=15, label='Goal')
        
        # ç¾åœ¨ä½ç½®
        if self.current_state.position != self.maze_env.goal_pos:
            x, y = self.current_state.position
            ax3.plot(x, y, 'bo', markersize=10, label='Current')
            
        ax3.set_title(f'Visit Frequency Map (Total Steps: {self.current_state.step_count})')
        ax3.legend()
        
        plt.colorbar(im, ax=ax3, label='Visit Frequency')
        
        plt.tight_layout()
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'results/memory_growth_{self.maze_size}x{self.maze_size}_{timestamp}.png'
        plt.savefig(filename, dpi=150)
        plt.close()
        
        print(f"Memory growth visualization saved to: {filename}")


class PostActionVectorSpace:
    """è¡Œå‹•å¾Œã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ï¼ˆå‰å›ã®å®Ÿè£…ã‹ã‚‰ï¼‰"""
    
    def __init__(self, maze_size: Tuple[int, int]):
        self.maze_width, self.maze_height = maze_size
        
    def create_query_vector(self, position: Tuple[int, int], 
                          visits: int = 0,
                          goal_info: float = -1.0) -> np.ndarray:
        norm_x = position[0] / (self.maze_width - 1) if self.maze_width > 1 else 0
        norm_y = position[1] / (self.maze_height - 1) if self.maze_height > 1 else 0
        norm_action = 0.5
        norm_result = 0.5
        norm_visits = min(visits / 10.0, 1.0)
        
        return np.array([norm_x, norm_y, norm_action, norm_result, norm_visits, goal_info])
    
    def create_response_vector(self, position: Tuple[int, int],
                             action: int, result: str,
                             visits: int = 0,
                             goal_info: float = -1.0) -> np.ndarray:
        norm_x = position[0] / (self.maze_width - 1) if self.maze_width > 1 else 0
        norm_y = position[1] / (self.maze_height - 1) if self.maze_height > 1 else 0
        norm_action = action * 0.25
        result_map = {'wall': -1.0, 'empty': 0.0, 'goal': 1.0}
        norm_result = result_map.get(result, 0.0)
        norm_visits = min(visits / 10.0, 1.0)
        
        return np.array([norm_x, norm_y, norm_action, norm_result, norm_visits, goal_info])


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    # 10x10è¿·è·¯
    print("=" * 60)
    print("Testing with 10x10 maze...")
    print("=" * 60)
    
    agent_10 = MemoryVisualizingAgent(maze_size=10)
    result_10 = agent_10.solve_maze(max_steps=200)
    
    print(f"\n=== 10x10 Maze Result ===")
    print(f"Success: {result_10['success']}")
    print(f"Steps: {result_10['steps']}")
    print(f"Path efficiency: {result_10['unique_positions'] / result_10['path_length']:.2%}")
    print(f"Total episodes: {result_10['total_episodes']}")
    print(f"Discovered cells: {result_10['discovered_cells']}")
    
    # 20x20è¿·è·¯
    print("\n" + "=" * 60)
    print("Testing with 20x20 maze...")
    print("=" * 60)
    
    agent_20 = MemoryVisualizingAgent(maze_size=20)
    result_20 = agent_20.solve_maze(max_steps=500)
    
    print(f"\n=== 20x20 Maze Result ===")
    print(f"Success: {result_20['success']}")
    print(f"Steps: {result_20['steps']}")
    print(f"Path efficiency: {result_20['unique_positions'] / result_20['path_length']:.2%}")
    print(f"Total episodes: {result_20['total_episodes']}")
    print(f"Discovered cells: {result_20['discovered_cells']}")


if __name__ == "__main__":
    main()