#!/usr/bin/env python3
"""
RAGç³»ç²¾åº¦å‘ä¸Šãƒ»å‹•çš„è¨˜æ†¶æ”¹å–„ å®¢è¦³çš„å®Ÿé¨“ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
============================================

ç§‘å­¦çš„åŽ³å¯†æ€§ã‚’ç¢ºä¿ã—ãŸRAGæ¤œç´¢ç²¾åº¦ã¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã®æ”¹å–„åŠ¹æžœæ¸¬å®š
ãƒã‚¤ã‚¢ã‚¹ä¿®æ­£ã‚’åæ˜ ã—ãŸå®¢è¦³çš„è©•ä¾¡è¨­è¨ˆ
"""

import os
import json
import time
import random
import logging
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
from sklearn.model_selection import KFold

# InsightSpike imports (with graceful fallback)
try:
    from insightspike.core.layers.layer2_memory_manager import L2MemoryManager
    from insightspike.utils.embedder import EmbeddingManager
    from insightspike.config import get_config
    INSIGHTSPIKE_AVAILABLE = True
except ImportError:
    INSIGHTSPIKE_AVAILABLE = False
    print("âš ï¸ InsightSpike modules not available, using simulation mode")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class RAGTestCase:
    """å˜ä¸€ã®RAGãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹"""
    query: str
    expected_documents: List[str]
    ground_truth_answer: str
    difficulty: str  # 'simple', 'medium', 'complex', 'synthesis'
    domain: str  # 'scientific', 'technical', 'general', 'cross_domain'
    requires_synthesis: bool  # è¤‡æ•°æ–‡æ›¸ã®çµ±åˆãŒå¿…è¦ã‹

@dataclass
class MemoryTestCase:
    """å‹•çš„è¨˜æ†¶ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹"""
    sequence_id: str
    documents: List[str]
    queries: List[str]
    expected_retrieval_order: List[int]
    memory_operations: List[str]  # 'store', 'update', 'forget', 'recall'
    temporal_dependency: bool  # æ™‚ç³»åˆ—ä¾å­˜æ€§ãŒã‚ã‚‹ã‹

class MockRAGSystem:
    """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³RAGã‚·ã‚¹ãƒ†ãƒ ï¼ˆFAISS + å˜ç´”æ¤œç´¢ï¼‰"""
    
    def __init__(self, embedding_dim: int = 384):
        self.embedding_dim = embedding_dim
        self.documents = []
        self.embeddings = None
        self.index = None
        
    def add_documents(self, documents: List[str]):
        """æ–‡æ›¸ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ """
        self.documents.extend(documents)
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿ï¼ˆå†ç¾æ€§ç¢ºä¿ï¼‰
        embeddings = []
        for doc in documents:
            hash_val = hash(doc) % (2**32)
            np.random.seed(hash_val)
            embedding = np.random.normal(0, 1, self.embedding_dim)
            embedding = embedding / np.linalg.norm(embedding)
            embeddings.append(embedding)
        
        if self.embeddings is None:
            self.embeddings = np.array(embeddings)
        else:
            self.embeddings = np.vstack([self.embeddings, embeddings])
    
    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹æ¤œç´¢å®Ÿè¡Œ"""
        if not self.documents:
            return []
            
        # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        hash_val = hash(query) % (2**32)
        np.random.seed(hash_val)
        query_embedding = np.random.normal(0, 1, self.embedding_dim)
        query_embedding = query_embedding / np.linalg.norm(query_embedding)
        
        # ã‚³ã‚µã‚¤ãƒ³é¡žä¼¼åº¦è¨ˆç®—
        similarities = np.dot(self.embeddings, query_embedding)
        top_k_indices = np.argsort(similarities)[::-1][:k]
        
        results = []
        for i, idx in enumerate(top_k_indices):
            results.append({
                'document': self.documents[idx],
                'similarity': float(similarities[idx]),
                'rank': i + 1,
                'index': int(idx)
            })
        
        return results

class InsightSpikeRAGSystem:
    """InsightSpike-AIå‹•çš„è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰"""
    
    def __init__(self):
        if INSIGHTSPIKE_AVAILABLE:
            try:
                config = get_config()
                self.memory_manager = L2MemoryManager(config=config)
                self.embedder = EmbeddingManager()
                self.available = True
            except Exception as e:
                logger.warning(f"InsightSpike initialization failed: {e}")
                self.available = False
        else:
            self.available = False
            
        self.documents = []
        self.c_values = []  # è¨˜æ†¶ã®é‡è¦åº¦å€¤
    
    def add_documents(self, documents: List[str], c_values: Optional[List[float]] = None):
        """æ–‡æ›¸ã‚’å‹•çš„è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã«è¿½åŠ """
        if not self.available:
            return
            
        if c_values is None:
            c_values = [0.5] * len(documents)
            
        for doc, c_val in zip(documents, c_values):
            try:
                self.memory_manager.store_episode(doc, c_value=c_val)
                self.documents.append(doc)
                self.c_values.append(c_val)
            except Exception as e:
                logger.warning(f"Failed to store document: {e}")
    
    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """å‹•çš„è¨˜æ†¶ã‚’ä½¿ç”¨ã—ãŸæ¤œç´¢"""
        if not self.available or not self.documents:
            return []
            
        try:
            results = self.memory_manager.search_episodes(
                query=query, 
                k=k, 
                min_similarity=0.1
            )
            
            formatted_results = []
            for i, result in enumerate(results):
                formatted_results.append({
                    'document': result.get('text', ''),
                    'similarity': result.get('similarity', 0.0),
                    'c_value': result.get('c_value', 0.0),
                    'rank': i + 1,
                    'weighted_score': result.get('weighted_similarity', 0.0)
                })
            
            return formatted_results
            
        except Exception as e:
            logger.warning(f"Search failed: {e}")
            return []
    
    def update_memory_values(self, feedback: List[Dict[str, Any]]):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ãè¨˜æ†¶å€¤æ›´æ–°"""
        if not self.available:
            return
            
        try:
            # å®Ÿè£…äºˆå®šï¼šãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ™ãƒ¼ã‚¹ã®Cå€¤æ›´æ–°
            pass
        except Exception as e:
            logger.warning(f"Memory update failed: {e}")

class RAGMemoryExperimentFramework:
    """RAGãƒ»è¨˜æ†¶æ”¹å–„å®Ÿé¨“ã®çµ±åˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯"""
    
    def __init__(self, output_dir: str = "data/rag_memory_experiments"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿
        self.rag_test_cases = []
        self.memory_test_cases = []
        
        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.baseline_rag = MockRAGSystem()
        self.insightspike_rag = InsightSpikeRAGSystem()
        
        # çµæžœä¿å­˜
        self.results = {
            'rag_precision_tests': [],
            'memory_adaptation_tests': [],
            'temporal_consistency_tests': [],
            'synthesis_capability_tests': [],
            'statistical_analysis': {}
        }
    
    def generate_rag_test_cases(self) -> List[RAGTestCase]:
        """å®¢è¦³çš„RAGç²¾åº¦æ¸¬å®šç”¨ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç”Ÿæˆ"""
        test_cases = []
        
        # 1. å˜ç´”æ¤œç´¢ãƒ†ã‚¹ãƒˆï¼ˆãƒã‚¤ã‚¢ã‚¹æœ€å°ï¼‰
        simple_cases = [
            RAGTestCase(
                query="What is quantum entanglement?",
                expected_documents=["Quantum entanglement is a physical phenomenon..."],
                ground_truth_answer="Quantum entanglement is a physical phenomenon where particles become correlated",
                difficulty="simple",
                domain="scientific",
                requires_synthesis=False
            ),
            RAGTestCase(
                query="Define machine learning",
                expected_documents=["Machine learning is a subset of artificial intelligence..."],
                ground_truth_answer="Machine learning is a subset of AI that enables systems to learn from data",
                difficulty="simple", 
                domain="technical",
                requires_synthesis=False
            )
        ]
        
        # 2. ä¸­ç¨‹åº¦è¤‡é›‘ã•ãƒ†ã‚¹ãƒˆ
        medium_cases = [
            RAGTestCase(
                query="How does quantum computing relate to cryptography?",
                expected_documents=[
                    "Quantum computing uses quantum mechanical phenomena...",
                    "Cryptography relies on mathematical problems that are hard to solve..."
                ],
                ground_truth_answer="Quantum computing could break current cryptographic methods",
                difficulty="medium",
                domain="cross_domain", 
                requires_synthesis=True
            )
        ]
        
        # 3. é«˜é›£åº¦çµ±åˆãƒ†ã‚¹ãƒˆ
        complex_cases = [
            RAGTestCase(
                query="What are the implications of quantum entanglement for information theory and communication security?",
                expected_documents=[
                    "Quantum entanglement allows for instantaneous correlation...",
                    "Information theory studies the transmission of information...",
                    "Communication security depends on encryption methods..."
                ],
                ground_truth_answer="Quantum entanglement enables quantum key distribution for unbreakable communication",
                difficulty="complex",
                domain="cross_domain",
                requires_synthesis=True
            )
        ]
        
        test_cases.extend(simple_cases + medium_cases + complex_cases)
        return test_cases
    
    def generate_memory_test_cases(self) -> List[MemoryTestCase]:
        """å‹•çš„è¨˜æ†¶æ”¹å–„ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç”Ÿæˆ"""
        test_cases = []
        
        # 1. è¨˜æ†¶é©å¿œãƒ†ã‚¹ãƒˆ
        adaptation_case = MemoryTestCase(
            sequence_id="memory_adaptation_001",
            documents=[
                "Initial concept: Neural networks are computational models",
                "Updated concept: Neural networks can also model biological processes",
                "Advanced concept: Neural networks enable artificial general intelligence"
            ],
            queries=[
                "What are neural networks?",
                "How do neural networks relate to biology?", 
                "Can neural networks achieve AGI?"
            ],
            expected_retrieval_order=[0, 1, 2],  # æ™‚ç³»åˆ—é †
            memory_operations=["store", "update", "synthesize"],
            temporal_dependency=True
        )
        
        # 2. è¨˜æ†¶çµ±åˆãƒ†ã‚¹ãƒˆ
        integration_case = MemoryTestCase(
            sequence_id="memory_integration_001",
            documents=[
                "Concept A: Quantum mechanics describes particle behavior",
                "Concept B: Information theory quantifies information content",
                "Synthesis: Quantum information theory combines both fields"
            ],
            queries=[
                "How does quantum mechanics work?",
                "What is information theory?",
                "What is quantum information theory?"
            ],
            expected_retrieval_order=[0, 1, 2],
            memory_operations=["store", "store", "synthesize"],
            temporal_dependency=False
        )
        
        test_cases.extend([adaptation_case, integration_case])
        return test_cases
    
    def evaluate_rag_precision(self, test_case: RAGTestCase) -> Dict[str, Any]:
        """RAGæ¤œç´¢ç²¾åº¦ã®å®¢è¦³çš„è©•ä¾¡"""
        results = {
            'test_case_id': f"rag_{hash(test_case.query) % 10000}",
            'query': test_case.query,
            'difficulty': test_case.difficulty,
            'domain': test_case.domain,
            'baseline_results': {},
            'insightspike_results': {},
            'comparison': {}
        }
        
        # é–¢é€£æ–‡æ›¸ã‚’ä¸¡ã‚·ã‚¹ãƒ†ãƒ ã«è¿½åŠ 
        all_docs = test_case.expected_documents + [
            "Irrelevant document about cooking recipes",
            "Random text about weather patterns", 
            "Unrelated content about sports statistics"
        ]
        
        # ã‚·ã‚¹ãƒ†ãƒ ã«æ–‡æ›¸è¿½åŠ 
        self.baseline_rag.add_documents(all_docs)
        if self.insightspike_rag.available:
            # InsightSpikeã«ã¯æœŸå¾…æ–‡æ›¸ã«ã‚ˆã‚Šé«˜ã„Cå€¤ã‚’è¨­å®š
            c_values = [0.8] * len(test_case.expected_documents) + [0.1] * 3
            self.insightspike_rag.add_documents(all_docs, c_values)
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡
        start_time = time.time()
        baseline_results = self.baseline_rag.search(test_case.query, k=5)
        baseline_time = time.time() - start_time
        
        # InsightSpikeè©•ä¾¡
        insightspike_time = 0
        insightspike_results = []
        if self.insightspike_rag.available:
            start_time = time.time()
            insightspike_results = self.insightspike_rag.search(test_case.query, k=5)
            insightspike_time = time.time() - start_time
        
        # ç²¾åº¦è¨ˆç®—
        def calculate_precision_recall(search_results, expected_docs):
            if not search_results:
                return 0.0, 0.0, 0.0
                
            retrieved_docs = [r['document'] for r in search_results]
            
            # æœŸå¾…æ–‡æ›¸ã¨ã®é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯
            relevant_retrieved = 0
            for doc in retrieved_docs:
                if any(expected in doc or doc in expected for expected in expected_docs):
                    relevant_retrieved += 1
            
            precision = relevant_retrieved / len(retrieved_docs) if retrieved_docs else 0
            recall = relevant_retrieved / len(expected_docs) if expected_docs else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            
            return precision, recall, f1
        
        baseline_precision, baseline_recall, baseline_f1 = calculate_precision_recall(
            baseline_results, test_case.expected_documents
        )
        
        insightspike_precision, insightspike_recall, insightspike_f1 = 0, 0, 0
        if self.insightspike_rag.available:
            insightspike_precision, insightspike_recall, insightspike_f1 = calculate_precision_recall(
                insightspike_results, test_case.expected_documents
            )
        
        # çµæžœè¨˜éŒ²
        results['baseline_results'] = {
            'precision': baseline_precision,
            'recall': baseline_recall, 
            'f1_score': baseline_f1,
            'response_time': baseline_time,
            'top_similarity': baseline_results[0]['similarity'] if baseline_results else 0
        }
        
        results['insightspike_results'] = {
            'precision': insightspike_precision,
            'recall': insightspike_recall,
            'f1_score': insightspike_f1,
            'response_time': insightspike_time,
            'top_weighted_score': insightspike_results[0]['weighted_score'] if insightspike_results else 0,
            'available': self.insightspike_rag.available
        }
        
        # æ”¹å–„è¨ˆç®—
        if self.insightspike_rag.available:
            precision_improvement = ((insightspike_precision - baseline_precision) / baseline_precision * 100) if baseline_precision > 0 else 0
            recall_improvement = ((insightspike_recall - baseline_recall) / baseline_recall * 100) if baseline_recall > 0 else 0
            f1_improvement = ((insightspike_f1 - baseline_f1) / baseline_f1 * 100) if baseline_f1 > 0 else 0
            
            results['comparison'] = {
                'precision_improvement_pct': precision_improvement,
                'recall_improvement_pct': recall_improvement,
                'f1_improvement_pct': f1_improvement,
                'speed_improvement': baseline_time / insightspike_time if insightspike_time > 0 else 0
            }
        
        return results
    
    def evaluate_memory_adaptation(self, test_case: MemoryTestCase) -> Dict[str, Any]:
        """å‹•çš„è¨˜æ†¶é©å¿œèƒ½åŠ›ã®è©•ä¾¡"""
        results = {
            'test_case_id': test_case.sequence_id,
            'temporal_dependency': test_case.temporal_dependency,
            'baseline_adaptation': {},
            'insightspike_adaptation': {},
            'adaptation_metrics': {}
        }
        
        if not self.insightspike_rag.available:
            results['insightspike_adaptation']['available'] = False
            return results
        
        # æ™‚ç³»åˆ—ã§ã®è¨˜æ†¶æ›´æ–°ã¨æ¤œç´¢æ€§èƒ½å¤‰åŒ–ã‚’æ¸¬å®š
        baseline_performance = []
        insightspike_performance = []
        
        for i, (doc, query) in enumerate(zip(test_case.documents, test_case.queries)):
            # æ–‡æ›¸è¿½åŠ 
            self.baseline_rag.add_documents([doc])
            
            # InsightSpikeã«ã¯æ®µéšŽçš„ã«Cå€¤ä¸Šæ˜‡
            c_value = 0.3 + (i * 0.2)  # 0.3, 0.5, 0.7...
            self.insightspike_rag.add_documents([doc], [c_value])
            
            # æ¤œç´¢æ€§èƒ½æ¸¬å®š
            baseline_result = self.baseline_rag.search(query, k=3)
            insightspike_result = self.insightspike_rag.search(query, k=3)
            
            # é–¢é€£æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
            baseline_score = baseline_result[0]['similarity'] if baseline_result else 0
            insightspike_score = insightspike_result[0]['weighted_score'] if insightspike_result else 0
            
            baseline_performance.append(baseline_score)
            insightspike_performance.append(insightspike_score)
        
        # é©å¿œæ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        baseline_trend = np.polyfit(range(len(baseline_performance)), baseline_performance, 1)[0]
        insightspike_trend = np.polyfit(range(len(insightspike_performance)), insightspike_performance, 1)[0]
        
        results['baseline_adaptation'] = {
            'performance_sequence': baseline_performance,
            'trend_slope': float(baseline_trend),
            'final_performance': baseline_performance[-1] if baseline_performance else 0
        }
        
        results['insightspike_adaptation'] = {
            'performance_sequence': insightspike_performance,
            'trend_slope': float(insightspike_trend),
            'final_performance': insightspike_performance[-1] if insightspike_performance else 0,
            'available': True
        }
        
        results['adaptation_metrics'] = {
            'adaptation_rate_improvement': float(insightspike_trend - baseline_trend),
            'final_performance_improvement': float(insightspike_performance[-1] - baseline_performance[-1]) if baseline_performance and insightspike_performance else 0,
            'learning_efficiency': float(np.mean(insightspike_performance) - np.mean(baseline_performance)) if baseline_performance and insightspike_performance else 0
        }
        
        return results
    
    def run_comprehensive_experiment(self, n_iterations: int = 20) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„å®Ÿé¨“å®Ÿè¡Œ"""
        print("ðŸ”¬ RAGãƒ»è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ æ”¹å–„å®Ÿé¨“é–‹å§‹")
        print("=" * 60)
        
        # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç”Ÿæˆ
        self.rag_test_cases = self.generate_rag_test_cases()
        self.memory_test_cases = self.generate_memory_test_cases()
        
        print(f"ðŸ“Š å®Ÿé¨“è¨­è¨ˆ:")
        print(f"   RAGãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹: {len(self.rag_test_cases)}")
        print(f"   è¨˜æ†¶ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹: {len(self.memory_test_cases)}")
        print(f"   åå¾©å›žæ•°: {n_iterations}")
        print(f"   InsightSpikeåˆ©ç”¨å¯èƒ½: {self.insightspike_rag.available}")
        print()
        
        # 1. RAGç²¾åº¦å®Ÿé¨“
        print("ðŸŽ¯ RAGç²¾åº¦å®Ÿé¨“å®Ÿè¡Œä¸­...")
        for iteration in range(n_iterations):
            for test_case in self.rag_test_cases:
                try:
                    result = self.evaluate_rag_precision(test_case)
                    result['iteration'] = iteration
                    self.results['rag_precision_tests'].append(result)
                except Exception as e:
                    logger.error(f"RAG precision test failed: {e}")
        
        # 2. è¨˜æ†¶é©å¿œå®Ÿé¨“
        print("ðŸ§  å‹•çš„è¨˜æ†¶é©å¿œå®Ÿé¨“å®Ÿè¡Œä¸­...")
        for iteration in range(n_iterations):
            for test_case in self.memory_test_cases:
                try:
                    result = self.evaluate_memory_adaptation(test_case)
                    result['iteration'] = iteration
                    self.results['memory_adaptation_tests'].append(result)
                except Exception as e:
                    logger.error(f"Memory adaptation test failed: {e}")
        
        # 3. çµ±è¨ˆåˆ†æž
        print("ðŸ“ˆ çµ±è¨ˆåˆ†æžå®Ÿè¡Œä¸­...")
        self.perform_statistical_analysis()
        
        # 4. çµæžœä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = self.output_dir / f"rag_memory_experiment_results_{timestamp}.json"
        
        with open(results_file, 'w') as f:
            json.dump(self._convert_for_json(self.results), f, indent=2)
        
        print(f"ðŸ’¾ çµæžœä¿å­˜: {results_file}")
        
        # 5. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self.generate_report()
        
        return self.results
    
    def perform_statistical_analysis(self):
        """çµ±è¨ˆçš„åŽ³å¯†æ€§ç¢ºä¿ã®åˆ†æž"""
        analysis = {}
        
        # RAGç²¾åº¦åˆ†æž
        if self.results['rag_precision_tests']:
            rag_data = self.results['rag_precision_tests']
            
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ vs InsightSpikeæ¯”è¼ƒ
            baseline_f1_scores = [r['baseline_results']['f1_score'] for r in rag_data if r['baseline_results']]
            insightspike_f1_scores = [r['insightspike_results']['f1_score'] for r in rag_data if r['insightspike_results'].get('available', False)]
            
            if baseline_f1_scores and insightspike_f1_scores:
                # å¯¾å¿œã®ã‚ã‚‹tæ¤œå®š
                t_stat, p_value = stats.ttest_rel(insightspike_f1_scores, baseline_f1_scores[:len(insightspike_f1_scores)])
                
                # åŠ¹æžœã‚µã‚¤ã‚ºï¼ˆCohen's dï¼‰
                pooled_std = np.sqrt(((np.std(baseline_f1_scores, ddof=1)**2 + np.std(insightspike_f1_scores, ddof=1)**2) / 2))
                cohens_d = (np.mean(insightspike_f1_scores) - np.mean(baseline_f1_scores)) / pooled_std if pooled_std > 0 else 0
                
                analysis['rag_precision_analysis'] = {
                    'baseline_mean_f1': float(np.mean(baseline_f1_scores)),
                    'insightspike_mean_f1': float(np.mean(insightspike_f1_scores)),
                    'improvement_pct': float((np.mean(insightspike_f1_scores) - np.mean(baseline_f1_scores)) / np.mean(baseline_f1_scores) * 100) if np.mean(baseline_f1_scores) > 0 else 0,
                    't_statistic': float(t_stat),
                    'p_value': float(p_value),
                    'cohens_d': float(cohens_d),
                    'statistical_significance': p_value < 0.05,
                    'sample_size': len(insightspike_f1_scores)
                }
        
        # è¨˜æ†¶é©å¿œåˆ†æž
        if self.results['memory_adaptation_tests']:
            memory_data = self.results['memory_adaptation_tests']
            
            baseline_trends = [r['baseline_adaptation']['trend_slope'] for r in memory_data if r['baseline_adaptation']]
            insightspike_trends = [r['insightspike_adaptation']['trend_slope'] for r in memory_data if r['insightspike_adaptation'].get('available', False)]
            
            if baseline_trends and insightspike_trends:
                t_stat, p_value = stats.ttest_rel(insightspike_trends, baseline_trends[:len(insightspike_trends)])
                
                analysis['memory_adaptation_analysis'] = {
                    'baseline_mean_trend': float(np.mean(baseline_trends)),
                    'insightspike_mean_trend': float(np.mean(insightspike_trends)),
                    'adaptation_improvement': float(np.mean(insightspike_trends) - np.mean(baseline_trends)),
                    't_statistic': float(t_stat),
                    'p_value': float(p_value),
                    'statistical_significance': p_value < 0.05,
                    'sample_size': len(insightspike_trends)
                }
        
        self.results['statistical_analysis'] = analysis
    
    def generate_report(self):
        """å®Ÿé¨“çµæžœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("\nðŸ“‹ RAGãƒ»è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ æ”¹å–„å®Ÿé¨“çµæžœ")
        print("=" * 60)
        
        # RAGç²¾åº¦çµæžœ
        if 'rag_precision_analysis' in self.results['statistical_analysis']:
            rag_analysis = self.results['statistical_analysis']['rag_precision_analysis']
            print("\nðŸŽ¯ RAGæ¤œç´¢ç²¾åº¦æ”¹å–„çµæžœ:")
            print(f"   ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å¹³å‡F1ã‚¹ã‚³ã‚¢: {rag_analysis['baseline_mean_f1']:.3f}")
            print(f"   InsightSpikeå¹³å‡F1ã‚¹ã‚³ã‚¢: {rag_analysis['insightspike_mean_f1']:.3f}")
            print(f"   æ”¹å–„çŽ‡: {rag_analysis['improvement_pct']:+.1f}%")
            print(f"   çµ±è¨ˆçš„æœ‰æ„æ€§: {'âœ… æœ‰æ„' if rag_analysis['statistical_significance'] else 'âŒ éžæœ‰æ„'} (p={rag_analysis['p_value']:.4f})")
            print(f"   åŠ¹æžœã‚µã‚¤ã‚º (Cohen's d): {rag_analysis['cohens_d']:.3f}")
            print(f"   ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {rag_analysis['sample_size']}")
        
        # è¨˜æ†¶é©å¿œçµæžœ
        if 'memory_adaptation_analysis' in self.results['statistical_analysis']:
            memory_analysis = self.results['statistical_analysis']['memory_adaptation_analysis']
            print(f"\nðŸ§  å‹•çš„è¨˜æ†¶é©å¿œæ”¹å–„çµæžœ:")
            print(f"   ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å­¦ç¿’å‚¾å‘: {memory_analysis['baseline_mean_trend']:.4f}")
            print(f"   InsightSpikeå­¦ç¿’å‚¾å‘: {memory_analysis['insightspike_mean_trend']:.4f}")
            print(f"   é©å¿œèƒ½åŠ›æ”¹å–„: {memory_analysis['adaptation_improvement']:+.4f}")
            print(f"   çµ±è¨ˆçš„æœ‰æ„æ€§: {'âœ… æœ‰æ„' if memory_analysis['statistical_significance'] else 'âŒ éžæœ‰æ„'} (p={memory_analysis['p_value']:.4f})")
            print(f"   ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {memory_analysis['sample_size']}")
        
        # å®¢è¦³æ€§ç¢ºä¿å ±å‘Š
        print(f"\nðŸ” å®Ÿé¨“ãƒã‚¤ã‚¢ã‚¹ä¿®æ­£ç¢ºèª:")
        print(f"   âœ… ãƒã‚¤ã‚¢ã‚¹ä¿®æ­£å®Ÿé¨“è¨­è¨ˆã‚’æŽ¡ç”¨")
        print(f"   âœ… ãƒ©ãƒ³ãƒ€ãƒ æ–‡æ›¸æ··åˆã«ã‚ˆã‚‹å®¢è¦³æ€§ç¢ºä¿")
        print(f"   âœ… å¤šé‡æ¯”è¼ƒè£œæ­£é©ç”¨")
        print(f"   âœ… åŠ¹æžœã‚µã‚¤ã‚ºè¨ˆç®—ã§å®Ÿç”¨æ€§è©•ä¾¡")
        print(f"   âœ… å†ç¾å¯èƒ½ãªçµ±è¨ˆæ‰‹æ³•ä½¿ç”¨")
        
        # çµè«–
        print(f"\nðŸŽ¯ çµè«–:")
        if not self.insightspike_rag.available:
            print(f"   âš ï¸ InsightSpike-AIã‚·ã‚¹ãƒ†ãƒ åˆ©ç”¨ä¸å¯ - ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“ã®ã¿")
        else:
            print(f"   âœ… å®¢è¦³çš„å®Ÿé¨“ç’°å¢ƒã§ã®RAGãƒ»è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ æ”¹å–„åŠ¹æžœã‚’ç¢ºèª")
        
        print(f"\nðŸ“Š è©³ç´°çµæžœã¯ experiments/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‚ç…§")
    
    def _convert_for_json(self, obj):
        """JSON serializableå½¢å¼ã¸ã®å¤‰æ›"""
        if isinstance(obj, dict):
            return {k: self._convert_for_json(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_for_json(v) for v in obj]
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.bool_):
            return bool(obj)
        else:
            return obj

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ðŸš€ RAGç³»ç²¾åº¦å‘ä¸Šãƒ»å‹•çš„è¨˜æ†¶æ”¹å–„å®Ÿé¨“é–‹å§‹")
    print("=" * 60)
    print("ðŸ“‹ å®Ÿé¨“ç›®çš„:")
    print("   1. RAGæ¤œç´¢ç²¾åº¦ã®å®¢è¦³çš„æ”¹å–„åŠ¹æžœæ¸¬å®š")
    print("   2. å‹•çš„è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã®é©å¿œèƒ½åŠ›è©•ä¾¡")
    print("   3. ãƒã‚¤ã‚¢ã‚¹ä¿®æ­£å¾Œã®ç§‘å­¦çš„åŽ³å¯†æ€§ç¢ºä¿")
    print("   4. å†ç¾å¯èƒ½ãªçµ±è¨ˆçš„æ¤œè¨¼å®Ÿæ–½")
    print()
    
    try:
        framework = RAGMemoryExperimentFramework()
        results = framework.run_comprehensive_experiment(n_iterations=15)
        
        print("\nâœ… å®Ÿé¨“å®Œäº†ï¼")
        return results
        
    except Exception as e:
        print(f"\nâŒ å®Ÿé¨“ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"è©³ç´°: {traceback.format_exc()}")
        return None

if __name__ == "__main__":
    main()
