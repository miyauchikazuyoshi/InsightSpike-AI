#!/usr/bin/env python3
"""
geDIG Embeddingå®Ÿé¨“
==================

Î”GED Ã— Î”IG ã‚’åŸºç›¤ã¨ã—ãŸè„³ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ‰embeddingæ‰‹æ³•ã®å®Ÿè£…ã¨è©•ä¾¡
å¾“æ¥ã®embeddingæ‰‹æ³•ã¨ã®æ€§èƒ½æ¯”è¼ƒã‚’680å•ã§å®Ÿæ–½
"""

import sys
import os
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import json
import time
import warnings
from datetime import datetime
from typing import Dict, List, Tuple, Any
from collections import defaultdict, Counter
import re
import math
from scipy import stats
from scipy.spatial.distance import cosine

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®srcãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

warnings.filterwarnings('ignore')

# Import required libraries
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.decomposition import PCA
    SKLEARN_AVAILABLE = True
    print("âœ… Scikit-learn available")
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âš ï¸ Scikit-learn not available")

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
    print("âœ… Sentence Transformers available")
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ Sentence Transformers not available")

# Import InsightSpike-AI components
try:
    from insightspike.algorithms.graph_edit_distance import GraphEditDistance, OptimizationLevel
    from insightspike.algorithms.information_gain import InformationGain, EntropyMethod
    print("âœ… InsightSpike-AI components imported successfully")
    INSIGHTSPIKE_AVAILABLE = True
except ImportError as e:
    print(f"âŒ InsightSpike-AI import error: {e}")
    INSIGHTSPIKE_AVAILABLE = False

# Load HuggingFace datasets
try:
    from datasets import Dataset
    DATASETS_AVAILABLE = True
    print("âœ… HuggingFace datasets library available")
except ImportError:
    DATASETS_AVAILABLE = False
    print("âš ï¸ HuggingFace datasets library not available")

class GeDIGEmbedding:
    """
    geDIG (Graph Edit Distance Ã— Information Gain) Embedding
    è„³ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ‰ãªæ–°ã—ã„embeddingæ‰‹æ³•
    """
    
    def __init__(self, embedding_dim: int = 128):
        self.embedding_dim = embedding_dim
        
        if INSIGHTSPIKE_AVAILABLE:
            self.ged_calculator = GraphEditDistance(optimization_level=OptimizationLevel.FAST)
            self.ig_calculator = InformationGain(method=EntropyMethod.SHANNON)
            print(f"âœ… geDIG Embedding initialized (dim={embedding_dim})")
        else:
            print("âš ï¸ Using fallback geDIG implementation")
    
    def text_to_graph(self, text: str, max_nodes: int = 20) -> Dict:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚°ãƒ©ãƒ•æ§‹é€ ã«å¤‰æ›"""
        words = text.lower().split()[:max_nodes]
        
        # ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ï¼ˆå˜èªã®ç‰¹æ€§ï¼‰
        nodes = []
        for word in words:
            node_features = {
                'word': word,
                'length': len(word),
                'vowel_count': sum(1 for c in word if c in 'aeiou'),
                'consonant_count': len(word) - sum(1 for c in word if c in 'aeiou'),
                'first_char': ord(word[0]) if word else 0,
                'last_char': ord(word[-1]) if word else 0
            }
            nodes.append(node_features)
        
        # ã‚¨ãƒƒã‚¸ï¼ˆå˜èªé–“ã®é–¢ä¿‚ï¼‰
        edges = []
        for i in range(len(words)-1):
            # éš£æ¥é–¢ä¿‚
            edges.append((i, i+1, {'type': 'adjacent'}))
            
            # é¡ä¼¼æ€§é–¢ä¿‚ï¼ˆå…±é€šæ–‡å­—ãŒå¤šã„ï¼‰
            if len(words[i]) > 2 and len(words[i+1]) > 2:
                common_chars = set(words[i]).intersection(set(words[i+1]))
                if len(common_chars) >= 2:
                    edges.append((i, i+1, {'type': 'similar'}))
        
        return {'nodes': nodes, 'edges': edges}
    
    def calculate_gedig_vector(self, text1: str, text2: str) -> np.ndarray:
        """2ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆé–“ã®geDIGãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—"""
        
        if INSIGHTSPIKE_AVAILABLE:
            try:
                # ã‚°ãƒ©ãƒ•ä½œæˆ
                graph1 = self.text_to_graph(text1)
                graph2 = self.text_to_graph(text2)
                
                # Î”GEDè¨ˆç®—
                ged_result = self.ged_calculator.calculate(graph1, graph2)
                delta_ged = ged_result.ged_value
                
                # Î”IGè¨ˆç®—ï¼ˆè¤‡æ•°ã®ç‰¹å¾´é‡ã§ï¼‰
                features1 = self.extract_text_features(text1)
                features2 = self.extract_text_features(text2)
                
                ig_values = []
                for feature_name in features1.keys():
                    if feature_name in features2:
                        try:
                            ig_result = self.ig_calculator.calculate(features1[feature_name], features2[feature_name])
                            ig_values.append(ig_result.ig_value)
                        except:
                            ig_values.append(0.5)  # fallback
                
                delta_ig = np.mean(ig_values) if ig_values else 0.5
                
            except Exception as e:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨ˆç®—
                delta_ged = self.fallback_ged(text1, text2)
                delta_ig = self.fallback_ig(text1, text2)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨ˆç®—
            delta_ged = self.fallback_ged(text1, text2)
            delta_ig = self.fallback_ig(text1, text2)
        
        # geDIGãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
        gedig_vector = self.generate_gedig_vector(text1, text2, delta_ged, delta_ig)
        
        return gedig_vector
    
    def extract_text_features(self, text: str) -> Dict[str, List[int]]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è¤‡æ•°ã®ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        features = {}
        
        # æ–‡å­—ãƒ¬ãƒ™ãƒ«ç‰¹å¾´
        char_codes = [ord(c) % 256 for c in text[:50]]
        features['char_codes'] = char_codes
        
        # å˜èªãƒ¬ãƒ™ãƒ«ç‰¹å¾´
        words = text.split()[:20]
        word_lengths = [len(word) for word in words]
        features['word_lengths'] = word_lengths + [0] * (20 - len(word_lengths))  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        
        # æ§‹æ–‡ãƒ¬ãƒ™ãƒ«ç‰¹å¾´
        sentence_lengths = [len(sent.split()) for sent in text.split('.') if sent.strip()][:10]
        features['sentence_lengths'] = sentence_lengths + [0] * (10 - len(sentence_lengths))
        
        # èªå½™ãƒ¬ãƒ™ãƒ«ç‰¹å¾´
        unique_words = len(set(words))
        total_words = len(words)
        vocabulary_diversity = unique_words / total_words if total_words > 0 else 0
        features['vocab_features'] = [int(vocabulary_diversity * 100)] * 10  # æ­£è¦åŒ–
        
        return features
    
    def fallback_ged(self, text1: str, text2: str) -> float:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯GEDè¨ˆç®—"""
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        # Jaccardè·é›¢ãƒ™ãƒ¼ã‚¹ã®GEDè¿‘ä¼¼
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        jaccard_similarity = intersection / union if union > 0 else 0
        
        # GEDã¨ã—ã¦è¿”ã™ï¼ˆè·é›¢ãªã®ã§1-similarityã‚’ä½¿ç”¨ï¼‰
        ged_estimate = (1 - jaccard_similarity) * (len(text1) + len(text2)) / 2
        
        return ged_estimate
    
    def fallback_ig(self, text1: str, text2: str) -> float:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯IGè¨ˆç®—"""
        # æ–‡å­—é »åº¦ãƒ™ãƒ¼ã‚¹ã®IGè¿‘ä¼¼
        chars1 = Counter(text1.lower())
        chars2 = Counter(text2.lower())
        
        all_chars = set(chars1.keys()).union(set(chars2.keys()))
        
        if not all_chars:
            return 0.5
        
        # ç°¡æ˜“ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
        entropy1 = -sum((count/len(text1)) * np.log2(count/len(text1)) 
                        for count in chars1.values() if count > 0)
        entropy2 = -sum((count/len(text2)) * np.log2(count/len(text2)) 
                        for count in chars2.values() if count > 0)
        
        # IGè¿‘ä¼¼ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å·®åˆ†ï¼‰
        ig_estimate = abs(entropy1 - entropy2) / max(entropy1, entropy2, 1.0)
        
        return ig_estimate
    
    def generate_gedig_vector(self, text1: str, text2: str, delta_ged: float, delta_ig: float) -> np.ndarray:
        """Î”GED Ã— Î”IG ã‹ã‚‰é«˜æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
        
        # åŸºæœ¬geDIGå€¤
        gedig_core = delta_ged * delta_ig
        
        # æ‹¡å¼µç‰¹å¾´é‡
        text_features = {
            'length_ratio': len(text1) / max(len(text2), 1),
            'word_count_ratio': len(text1.split()) / max(len(text2.split()), 1),
            'char_diversity1': len(set(text1.lower())) / max(len(text1), 1),
            'char_diversity2': len(set(text2.lower())) / max(len(text2), 1),
            'common_words': len(set(text1.split()).intersection(set(text2.split()))),
            'delta_ged_norm': delta_ged / (len(text1) + len(text2) + 1),
            'delta_ig_norm': delta_ig,
            'gedig_core': gedig_core
        }
        
        # ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆï¼ˆæ•°å­¦çš„å¤‰æ›ï¼‰
        vector_components = []
        
        # ã‚³ã‚¢æˆåˆ†
        vector_components.extend([
            gedig_core,
            np.sqrt(gedig_core),
            np.log(gedig_core + 1),
            np.sin(gedig_core),
            np.cos(gedig_core)
        ])
        
        # Î”GEDæˆåˆ†
        vector_components.extend([
            delta_ged,
            delta_ged ** 0.5,
            delta_ged ** 2,
            np.exp(-delta_ged / 100),
            np.tanh(delta_ged / 50)
        ])
        
        # Î”IGæˆåˆ†
        vector_components.extend([
            delta_ig,
            delta_ig ** 2,
            delta_ig ** 3,
            np.exp(-delta_ig),
            self.sigmoid(delta_ig * 10)
        ])
        
        # ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´æˆåˆ†
        for feature_name, value in text_features.items():
            vector_components.extend([
                value,
                np.sqrt(abs(value)),
                np.log(abs(value) + 1),
                np.sin(value * np.pi),
                np.cos(value * np.pi)
            ])
        
        # ç›¸äº’ä½œç”¨æˆåˆ†
        for i in range(min(10, len(vector_components))):
            for j in range(i+1, min(10, len(vector_components))):
                vector_components.append(vector_components[i] * vector_components[j])
        
        # æ¬¡å…ƒèª¿æ•´
        if len(vector_components) > self.embedding_dim:
            vector_components = vector_components[:self.embedding_dim]
        elif len(vector_components) < self.embedding_dim:
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            padding_size = self.embedding_dim - len(vector_components)
            vector_components.extend([0.0] * padding_size)
        
        return np.array(vector_components, dtype=np.float32)
    
    def sigmoid(self, x):
        """ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def embed_corpus(self, texts: List[str], reference_text: str = None) -> np.ndarray:
        """ã‚³ãƒ¼ãƒ‘ã‚¹å…¨ä½“ã‚’embedding"""
        
        if reference_text is None:
            # å¹³å‡çš„ãªãƒ†ã‚­ã‚¹ãƒˆã‚’å‚ç…§ã¨ã—ã¦ä½¿ç”¨
            avg_length = int(np.mean([len(text) for text in texts]))
            reference_text = " ".join(texts[0].split()[:avg_length])
        
        embeddings = []
        
        print(f"ğŸ§  Generating geDIG embeddings for {len(texts)} texts...")
        
        for i, text in enumerate(texts):
            if i % 100 == 0:
                print(f"   Processing {i}/{len(texts)}...")
            
            gedig_vector = self.calculate_gedig_vector(text, reference_text)
            embeddings.append(gedig_vector)
        
        return np.array(embeddings)

def load_mega_datasets_for_embedding():
    """Embeddingå®Ÿé¨“ç”¨ã«MEGAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿"""
    print("ğŸ“¥ Loading MEGA datasets for embedding experiment...")
    
    data_dir = Path("data/mega_huggingface_datasets")
    
    questions = []
    documents = []
    
    # ç°¡ç•¥åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆå‰å›ã®å®Ÿé¨“çµæœã‚’åˆ©ç”¨ï¼‰
    dataset_files = [
        ("squad_300", 300),
        ("squad_200", 200),
        ("drop_50", 50),
        ("boolq_50", 50),
        ("commonsense_qa_20", 20),
        ("hotpot_qa_60", 60)
    ]
    
    for dataset_name, expected_count in dataset_files:
        dataset_path = data_dir / dataset_name
        if dataset_path.exists():
            print(f"   ğŸ“š Loading {dataset_name}...")
            try:
                dataset = Dataset.load_from_disk(str(dataset_path))
                
                for i, example in enumerate(dataset):
                    if dataset_name.startswith("squad"):
                        question = example.get('question', '')
                        context = example.get('context', '')
                        answers = example.get('answers', {})
                        
                        if question and context:
                            answer = "Unknown"
                            if isinstance(answers, dict) and 'text' in answers:
                                answer_list = answers['text']
                                answer = answer_list[0] if isinstance(answer_list, list) and answer_list else "Unknown"
                            
                            questions.append({
                                "question": question,
                                "answer": answer,
                                "context": context,
                                "dataset": "squad"
                            })
                            documents.append(context)
                    
                    # ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚åŒæ§˜ã«å‡¦ç†ï¼ˆç°¡ç•¥ç‰ˆï¼‰
                    elif dataset_name.startswith("boolq"):
                        question = example.get('question', '')
                        passage = example.get('passage', '')
                        answer = example.get('answer', False)
                        
                        if question and passage:
                            questions.append({
                                "question": question,
                                "answer": "Yes" if answer else "No",
                                "context": passage,
                                "dataset": "boolq"
                            })
                            documents.append(passage)
                
                print(f"      âœ… Loaded data from {dataset_name}")
                
            except Exception as e:
                print(f"      âŒ Loading failed: {e}")
    
    print(f"   âœ… Total loaded: {len(questions)} questions, {len(documents)} documents")
    
    return questions, documents

def run_gedig_embedding_experiment():
    """geDIG embeddingå®Ÿé¨“å®Ÿè¡Œ"""
    
    print("ğŸš€ Starting geDIG Embedding Experiment")
    print("ğŸ§  Brain-Inspired Î”GED Ã— Î”IG Embedding vs Traditional Methods")
    print("=" * 80)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
    questions, documents = load_mega_datasets_for_embedding()
    
    if len(questions) < 50:
        print(f"âŒ Insufficient data: {len(questions)} questions")
        return
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºèª¿æ•´ï¼ˆåŠ¹ç‡åŒ–ï¼‰
    max_samples = 200  # è¨ˆç®—åŠ¹ç‡ã®ãŸã‚
    if len(questions) > max_samples:
        questions = questions[:max_samples]
        documents = documents[:max_samples]
    
    print(f"ğŸ¯ Experiment scale: {len(questions)} questions")
    
    # 1. geDIG EmbeddingåˆæœŸåŒ–
    print("\nğŸ§  Initializing geDIG Embedding...")
    gedig_embedder = GeDIGEmbedding(embedding_dim=128)
    
    # 2. å¾“æ¥æ‰‹æ³•ã®åˆæœŸåŒ–
    print("ğŸ“Š Initializing traditional embedding methods...")
    
    # TF-IDF
    if SKLEARN_AVAILABLE:
        tfidf_vectorizer = TfidfVectorizer(max_features=128, stop_words='english')
        tfidf_doc_vectors = tfidf_vectorizer.fit_transform(documents)
        print("   âœ… TF-IDF initialized")
    
    # Sentence-BERT
    sbert_available = SENTENCE_TRANSFORMERS_AVAILABLE
    if sbert_available:
        try:
            sbert_model = SentenceTransformer('all-MiniLM-L6-v2')
            sbert_doc_vectors = sbert_model.encode(documents, show_progress_bar=True)
            print("   âœ… Sentence-BERT initialized")
        except Exception as e:
            print(f"   âŒ Sentence-BERT failed: {e}")
            sbert_available = False
    
    # 3. geDIG embeddingç”Ÿæˆ
    print("\nğŸ§  Generating geDIG embeddings...")
    start_time = time.time()
    
    # å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå¹³å‡çš„ãªæ–‡æ›¸ï¼‰
    avg_doc_length = int(np.mean([len(doc.split()) for doc in documents]))
    reference_doc = " ".join(documents[0].split()[:avg_doc_length])
    
    gedig_doc_vectors = gedig_embedder.embed_corpus(documents, reference_doc)
    gedig_embedding_time = time.time() - start_time
    
    print(f"   âœ… geDIG embeddings generated in {gedig_embedding_time:.1f}s")
    print(f"   ğŸ“Š Shape: {gedig_doc_vectors.shape}")
    
    # 4. æ¤œç´¢æ€§èƒ½è©•ä¾¡
    print("\nğŸ“ˆ Evaluating retrieval performance...")
    
    results = {}
    
    # geDIG embedding evaluation
    print("   ğŸ§  Evaluating geDIG embedding...")
    gedig_scores = []
    gedig_latencies = []
    
    for i, question_data in enumerate(questions[:50]):  # ã‚µãƒ³ãƒ—ãƒ«è©•ä¾¡
        question = question_data["question"]
        answer = question_data["answer"]
        
        start_time = time.time()
        
        # è³ªå•ã®geDIG embedding
        question_vector = gedig_embedder.calculate_gedig_vector(question, reference_doc)
        
        # æ–‡æ›¸ã¨ã®é¡ä¼¼åº¦è¨ˆç®—
        similarities = []
        for doc_vector in gedig_doc_vectors:
            similarity = 1 - cosine(question_vector, doc_vector)
            similarities.append(similarity)
        
        latency = (time.time() - start_time) * 1000
        gedig_latencies.append(latency)
        
        # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        top_5_indices = np.argsort(similarities)[-5:][::-1]
        top_5_docs = [documents[idx] for idx in top_5_indices]
        
        # é–¢é€£æ€§è©•ä¾¡
        relevance_score = 0.0
        answer_words = set(answer.lower().split())
        for doc in top_5_docs:
            doc_words = set(doc.lower().split())
            if answer_words:
                overlap = len(answer_words.intersection(doc_words)) / len(answer_words)
                relevance_score = max(relevance_score, overlap)
        
        gedig_scores.append(relevance_score)
    
    results["geDIG Embedding"] = {
        "relevance_score": np.mean(gedig_scores),
        "relevance_std": np.std(gedig_scores),
        "latency": np.mean(gedig_latencies),
        "embedding_time": gedig_embedding_time
    }
    
    # TF-IDF evaluation
    if SKLEARN_AVAILABLE:
        print("   ğŸ“Š Evaluating TF-IDF...")
        tfidf_scores = []
        tfidf_latencies = []
        
        for i, question_data in enumerate(questions[:50]):
            question = question_data["question"]
            answer = question_data["answer"]
            
            start_time = time.time()
            
            question_vector = tfidf_vectorizer.transform([question])
            similarities = cosine_similarity(question_vector, tfidf_doc_vectors)[0]
            
            latency = (time.time() - start_time) * 1000
            tfidf_latencies.append(latency)
            
            top_5_indices = np.argsort(similarities)[-5:][::-1]
            top_5_docs = [documents[idx] for idx in top_5_indices]
            
            relevance_score = 0.0
            answer_words = set(answer.lower().split())
            for doc in top_5_docs:
                doc_words = set(doc.lower().split())
                if answer_words:
                    overlap = len(answer_words.intersection(doc_words)) / len(answer_words)
                    relevance_score = max(relevance_score, overlap)
            
            tfidf_scores.append(relevance_score)
        
        results["TF-IDF"] = {
            "relevance_score": np.mean(tfidf_scores),
            "relevance_std": np.std(tfidf_scores),
            "latency": np.mean(tfidf_latencies),
            "embedding_time": 0.0  # Already computed
        }
    
    # Sentence-BERT evaluation
    if sbert_available:
        print("   ğŸ¤— Evaluating Sentence-BERT...")
        sbert_scores = []
        sbert_latencies = []
        
        for i, question_data in enumerate(questions[:50]):
            question = question_data["question"]
            answer = question_data["answer"]
            
            start_time = time.time()
            
            question_vector = sbert_model.encode([question])
            similarities = cosine_similarity(question_vector, sbert_doc_vectors)[0]
            
            latency = (time.time() - start_time) * 1000
            sbert_latencies.append(latency)
            
            top_5_indices = np.argsort(similarities)[-5:][::-1]
            top_5_docs = [documents[idx] for idx in top_5_indices]
            
            relevance_score = 0.0
            answer_words = set(answer.lower().split())
            for doc in top_5_docs:
                doc_words = set(doc.lower().split())
                if answer_words:
                    overlap = len(answer_words.intersection(doc_words)) / len(answer_words)
                    relevance_score = max(relevance_score, overlap)
            
            sbert_scores.append(relevance_score)
        
        results["Sentence-BERT"] = {
            "relevance_score": np.mean(sbert_scores),
            "relevance_std": np.std(sbert_scores),
            "latency": np.mean(sbert_latencies),
            "embedding_time": 0.0  # Pre-computed
        }
    
    # 5. çµæœè¡¨ç¤º
    print("\nğŸ“Š geDIG Embedding Experiment Results:")
    print("=" * 60)
    
    for method, metrics in results.items():
        print(f"\nğŸ” {method}:")
        print(f"   Relevance Score: {metrics['relevance_score']:.3f} Â± {metrics['relevance_std']:.3f}")
        print(f"   Latency: {metrics['latency']:.1f}ms")
        if metrics['embedding_time'] > 0:
            print(f"   Embedding Time: {metrics['embedding_time']:.1f}s")
    
    # 6. çµ±è¨ˆçš„åˆ†æ
    perform_gedig_statistical_analysis(results)
    
    # 7. å¯è¦–åŒ–
    create_gedig_visualization(results, len(questions))
    
    print("\nâœ… geDIG Embedding experiment completed!")
    print("ğŸ§  Brain-inspired Î”GED Ã— Î”IG embedding evaluated successfully!")

def perform_gedig_statistical_analysis(results: Dict):
    """geDIG embeddingçµ±è¨ˆåˆ†æ"""
    
    print(f"\nğŸ“Š geDIG Embedding Statistical Analysis:")
    print("=" * 50)
    
    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆTF-IDFï¼‰ã¨ã®æ¯”è¼ƒ
    if "TF-IDF" in results:
        baseline = results["TF-IDF"]
        
        for method, metrics in results.items():
            if method != "TF-IDF":
                improvement = (metrics["relevance_score"] - baseline["relevance_score"]) / baseline["relevance_score"] * 100
                latency_change = (metrics["latency"] - baseline["latency"]) / baseline["latency"] * 100
                
                print(f"\nğŸ§  {method} vs TF-IDF:")
                print(f"   Relevance Improvement: {improvement:+.1f}%")
                print(f"   Latency Change: {latency_change:+.1f}%")
                
                # åŠ¹æœåˆ¤å®š
                if improvement > 5:
                    significance = "âœ… Significant Improvement"
                elif improvement > 0:
                    significance = "ğŸ”„ Marginal Improvement"
                elif improvement > -5:
                    significance = "â¡ï¸ Comparable Performance"
                else:
                    significance = "âŒ Performance Degradation"
                
                print(f"   Assessment: {significance}")

def create_gedig_visualization(results: Dict, num_questions: int):
    """geDIG embeddingçµæœå¯è¦–åŒ–"""
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))
    
    methods = list(results.keys())
    
    # 1. Relevance Scoreæ¯”è¼ƒ
    relevance_scores = [results[method]["relevance_score"] for method in methods]
    relevance_stds = [results[method]["relevance_std"] for method in methods]
    
    bars1 = ax1.bar(methods, relevance_scores, yerr=relevance_stds, capsize=5, 
                   color=['gold', 'skyblue', 'lightcoral'][:len(methods)])
    ax1.set_ylabel('Relevance Score')
    ax1.set_title(f'geDIG Embedding Performance ({num_questions} questions)')
    ax1.tick_params(axis='x', rotation=45)
    
    for bar, score in zip(bars1, relevance_scores):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{score:.3f}', ha='center', va='bottom')
    
    # 2. Latencyæ¯”è¼ƒ
    latencies = [results[method]["latency"] for method in methods]
    bars2 = ax2.bar(methods, latencies, color=['gold', 'skyblue', 'lightcoral'][:len(methods)])
    ax2.set_ylabel('Latency (ms)')
    ax2.set_title('Query Latency Comparison')
    ax2.tick_params(axis='x', rotation=45)
    
    for bar, latency in zip(bars2, latencies):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                f'{latency:.1f}ms', ha='center', va='bottom')
    
    # 3. æ”¹å–„ç‡ï¼ˆTF-IDFãƒ™ãƒ¼ã‚¹ï¼‰
    if "TF-IDF" in results:
        baseline_score = results["TF-IDF"]["relevance_score"]
        improvements = []
        method_names = []
        
        for method in methods:
            if method != "TF-IDF":
                improvement = (results[method]["relevance_score"] - baseline_score) / baseline_score * 100
                improvements.append(improvement)
                method_names.append(method)
        
        colors = ['green' if x > 0 else 'red' for x in improvements]
        bars3 = ax3.bar(method_names, improvements, color=colors, alpha=0.7)
        ax3.set_ylabel('Improvement over TF-IDF (%)')
        ax3.set_title('Relative Performance')
        ax3.tick_params(axis='x', rotation=45)
        ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        
        for bar, improvement in zip(bars3, improvements):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + (1 if improvement > 0 else -3),
                    f'{improvement:+.1f}%', ha='center', va='bottom' if improvement > 0 else 'top')
    
    # 4. åŠ¹ç‡æ€§åˆ†æ
    if len(methods) >= 2:
        scores = [results[method]["relevance_score"] for method in methods]
        latencies = [results[method]["latency"] for method in methods]
        
        scatter = ax4.scatter(latencies, scores, s=100, alpha=0.7, 
                            c=['gold', 'skyblue', 'lightcoral'][:len(methods)])
        
        for i, method in enumerate(methods):
            ax4.annotate(method, (latencies[i], scores[i]), 
                        xytext=(5, 5), textcoords='offset points')
        
        ax4.set_xlabel('Latency (ms)')
        ax4.set_ylabel('Relevance Score')
        ax4.set_title('Efficiency Analysis (Higher-Left is Better)')
        ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('gedig_embedding_results.png', dpi=300, bbox_inches='tight')
    print("ğŸ“ˆ geDIG embedding visualization saved: gedig_embedding_results.png")

if __name__ == "__main__":
    run_gedig_embedding_experiment()