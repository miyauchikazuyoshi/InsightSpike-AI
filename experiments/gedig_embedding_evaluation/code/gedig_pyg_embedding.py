#!/usr/bin/env python3
"""
PyTorch Geometricç‰ˆ geDIG Embedding
==================================

æ—¢å­˜ã®InsightSpike-AI PyGå®Ÿè£…ã‚’æ´»ç”¨ã—ãŸé«˜é€Ÿã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹embedding
"""

import sys
import os
from pathlib import Path
import numpy as np
import torch
import torch.nn as nn
import torch_geometric as pyg
from torch_geometric.data import Data, Batch
from torch_geometric.nn import GCNConv, global_mean_pool
import time
import warnings
from typing import Dict, List, Tuple, Any

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®srcãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

warnings.filterwarnings('ignore')

# InsightSpike-AI components
try:
    from insightspike.algorithms.graph_edit_distance import GraphEditDistance, OptimizationLevel
    from insightspike.algorithms.information_gain import InformationGain, EntropyMethod
    from insightspike.core.layers.layer3_graph_reasoner import L3GraphReasoner
    from insightspike.utils.graph_metrics import GraphMetrics
    print("âœ… InsightSpike-AI PyG components imported successfully")
    INSIGHTSPIKE_AVAILABLE = True
except ImportError as e:
    print(f"âŒ InsightSpike-AI import error: {e}")
    INSIGHTSPIKE_AVAILABLE = False

class PyGGeDIGEmbedding(nn.Module):
    """
    PyTorch Geometricç‰ˆ geDIG Embedding
    GPUå¯¾å¿œãƒ»GNNãƒ™ãƒ¼ã‚¹ã®é«˜é€Ÿå®Ÿè£…
    """
    
    def __init__(self, input_dim: int = 64, hidden_dim: int = 128, output_dim: int = 128):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        
        # GNNã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆInsightSpike-AIã¨åŒã˜ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼‰
        self.gnn_encoder = nn.Sequential(
            GCNConv(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            GCNConv(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            GCNConv(hidden_dim, output_dim),
        )
        
        # Î”GED Ã— Î”IG çµ±åˆå±¤
        self.gedig_fusion = nn.Sequential(
            nn.Linear(output_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
            nn.Tanh()
        )
        
        # GPUåˆ©ç”¨å¯èƒ½ãƒã‚§ãƒƒã‚¯
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.to(self.device)
        
        print(f"âœ… PyG geDIG Embedding initialized on {self.device}")
        
        # InsightSpike-AI components
        if INSIGHTSPIKE_AVAILABLE:
            self.ged_calculator = GraphEditDistance(optimization_level=OptimizationLevel.FAST)
            self.ig_calculator = InformationGain(method=EntropyMethod.SHANNON)
    
    def text_to_pyg_graph(self, text: str) -> Data:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’PyTorch Geometricã‚°ãƒ©ãƒ•ã«å¤‰æ›"""
        
        words = text.lower().split()[:30]  # æœ€å¤§30ãƒãƒ¼ãƒ‰
        
        # ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ï¼ˆå˜èªåŸ‹ã‚è¾¼ã¿ï¼‰
        node_features = []
        for word in words:
            # ç°¡æ˜“åŸ‹ã‚è¾¼ã¿ï¼ˆå®Ÿéš›ã¯BERTã‚„Word2Vecä½¿ç”¨æ¨å¥¨ï¼‰
            features = [
                len(word) / 10.0,  # æ­£è¦åŒ–ã•ã‚ŒãŸé•·ã•
                sum(1 for c in word if c in 'aeiou') / max(len(word), 1),  # æ¯éŸ³æ¯”ç‡
                sum(1 for c in word if c.isdigit()) / max(len(word), 1),  # æ•°å­—æ¯”ç‡
                ord(word[0]) / 255.0 if word else 0,  # æœ€åˆã®æ–‡å­—
            ]
            # input_dimã¾ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            features.extend([0.0] * (self.input_dim - len(features)))
            node_features.append(features[:self.input_dim])
        
        x = torch.tensor(node_features, dtype=torch.float32)
        
        # ã‚¨ãƒƒã‚¸æ§‹ç¯‰ï¼ˆéš£æ¥é–¢ä¿‚ + é¡ä¼¼æ€§ï¼‰
        edge_list = []
        
        # éš£æ¥ã‚¨ãƒƒã‚¸
        for i in range(len(words) - 1):
            edge_list.append([i, i + 1])
            edge_list.append([i + 1, i])  # åŒæ–¹å‘
        
        # é¡ä¼¼æ€§ã‚¨ãƒƒã‚¸ï¼ˆæ–‡å­—ã®é‡è¤‡ãŒå¤šã„å ´åˆï¼‰
        for i in range(len(words)):
            for j in range(i + 1, len(words)):
                if i != j and len(set(words[i]).intersection(set(words[j]))) >= 2:
                    edge_list.append([i, j])
                    edge_list.append([j, i])
        
        if edge_list:
            edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()
        else:
            # ã‚¨ãƒƒã‚¸ãŒãªã„å ´åˆã®å‡¦ç†
            edge_index = torch.zeros((2, 0), dtype=torch.long)
        
        # PyG Dataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
        data = Data(x=x, edge_index=edge_index)
        data.num_nodes = len(words)
        
        return data.to(self.device)
    
    def forward(self, graph1: Data, graph2: Data) -> torch.Tensor:
        """2ã¤ã®ã‚°ãƒ©ãƒ•ã‹ã‚‰geDIG embeddingã‚’ç”Ÿæˆ"""
        
        # GNNã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆå„å±¤ã‚’å€‹åˆ¥ã«å‘¼ã³å‡ºã—ï¼‰
        x1 = graph1.x
        x2 = graph2.x
        
        for layer in self.gnn_encoder:
            if isinstance(layer, GCNConv):
                x1 = layer(x1, graph1.edge_index)
                x2 = layer(x2, graph2.edge_index)
            else:
                x1 = layer(x1)
                x2 = layer(x2)
        
        # ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«è¡¨ç¾ï¼ˆå¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼‰
        batch1 = torch.zeros(graph1.num_nodes, dtype=torch.long).to(self.device)
        batch2 = torch.zeros(graph2.num_nodes, dtype=torch.long).to(self.device)
        
        graph1_repr = global_mean_pool(x1, batch1)
        graph2_repr = global_mean_pool(x2, batch2)
        
        # Î”GED Ã— Î”IG èåˆ
        combined = torch.cat([graph1_repr, graph2_repr], dim=1)
        gedig_embedding = self.gedig_fusion(combined)
        
        return gedig_embedding
    
    def calculate_fast_ged_ig(self, graph1: Data, graph2: Data) -> Tuple[float, float]:
        """é«˜é€ŸÎ”GED Ã— Î”IGè¨ˆç®—ï¼ˆGPUå¯¾å¿œï¼‰"""
        
        # ã‚°ãƒ©ãƒ•ç‰¹å¾´é‡ã‚’ä½¿ã£ãŸè¿‘ä¼¼è¨ˆç®—
        with torch.no_grad():
            # ãƒãƒ¼ãƒ‰æ•°ã®å·®
            node_diff = abs(graph1.num_nodes - graph2.num_nodes)
            
            # ã‚¨ãƒƒã‚¸æ•°ã®å·®
            edge_diff = abs(graph1.edge_index.size(1) - graph2.edge_index.size(1))
            
            # ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã®å·®ï¼ˆã‚³ã‚µã‚¤ãƒ³è·é›¢ï¼‰
            x1_mean = graph1.x.mean(dim=0)
            x2_mean = graph2.x.mean(dim=0)
            feature_sim = torch.cosine_similarity(x1_mean, x2_mean, dim=0)
            
            # è¿‘ä¼¼Î”GED
            delta_ged = node_diff + edge_diff * 0.5 + (1 - feature_sim) * 10
            
            # è¿‘ä¼¼Î”IGï¼ˆç‰¹å¾´é‡ã®åˆ†æ•£å·®ï¼‰
            var1 = graph1.x.var()
            var2 = graph2.x.var()
            delta_ig = torch.abs(var1 - var2) / (var1 + var2 + 1e-6)
            
            return delta_ged.item(), delta_ig.item()
    
    def embed_texts(self, texts: List[str], reference_text: str = None) -> np.ndarray:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã‚’geDIG embeddingã«å¤‰æ›"""
        
        if reference_text is None:
            reference_text = texts[0]
        
        reference_graph = self.text_to_pyg_graph(reference_text)
        embeddings = []
        
        print(f"ğŸ§  Generating PyG geDIG embeddings for {len(texts)} texts...")
        
        with torch.no_grad():
            for i, text in enumerate(texts):
                if i % 50 == 0:
                    print(f"   Processing {i}/{len(texts)}...")
                
                text_graph = self.text_to_pyg_graph(text)
                
                # geDIG embeddingç”Ÿæˆ
                embedding = self.forward(text_graph, reference_graph)
                
                # Î”GED Ã— Î”IGè¨ˆç®—
                delta_ged, delta_ig = self.calculate_fast_ged_ig(text_graph, reference_graph)
                
                # æœ€çµ‚embeddingï¼ˆÎ”GEDÃ—Î”IGé‡ã¿ä»˜ã‘ï¼‰
                gedig_weight = delta_ged * delta_ig
                weighted_embedding = embedding * gedig_weight
                
                embeddings.append(weighted_embedding.cpu().numpy())
        
        return np.vstack(embeddings)

def benchmark_pyg_vs_original():
    """PyGç‰ˆã¨å…ƒã®geDIG embeddingæ€§èƒ½æ¯”è¼ƒ"""
    
    print("ğŸš€ PyTorch Geometric geDIG Embedding Benchmark")
    print("=" * 60)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_texts = [
        "The quick brown fox jumps over the lazy dog",
        "Machine learning is transforming artificial intelligence",
        "Quantum computing will revolutionize cryptography",
        "Natural language processing enables human-computer interaction",
        "Deep neural networks learn hierarchical representations"
    ] * 20  # 100ãƒ†ã‚­ã‚¹ãƒˆ
    
    reference = "Artificial intelligence and machine learning research"
    
    # 1. PyGç‰ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    print("\nğŸ§  PyG geDIG Embedding:")
    pyg_embedder = PyGGeDIGEmbedding()
    
    start_time = time.time()
    pyg_embeddings = pyg_embedder.embed_texts(test_texts[:10], reference)
    pyg_time = time.time() - start_time
    
    print(f"   âœ… Time: {pyg_time:.3f}s")
    print(f"   ğŸ“Š Shape: {pyg_embeddings.shape}")
    print(f"   ğŸš€ Speed: {len(test_texts[:10])/pyg_time:.1f} texts/sec")
    print(f"   ğŸ’¾ Device: {pyg_embedder.device}")
    
    # 2. å…ƒã®å®Ÿè£…ã¨ã®æ¯”è¼ƒï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
    try:
        from gedig_embedding_experiment import GeDIGEmbedding
        
        print("\nğŸ“Š Original geDIG Embedding:")
        original_embedder = GeDIGEmbedding(embedding_dim=128)
        
        start_time = time.time()
        original_embeddings = original_embedder.embed_corpus(test_texts[:10], reference)
        original_time = time.time() - start_time
        
        print(f"   âœ… Time: {original_time:.3f}s")
        print(f"   ğŸ“Š Shape: {original_embeddings.shape}")
        print(f"   ğŸš€ Speed: {len(test_texts[:10])/original_time:.1f} texts/sec")
        
        # é€Ÿåº¦æ¯”è¼ƒ
        speedup = original_time / pyg_time
        print(f"\nâš¡ PyG Speedup: {speedup:.2f}x faster!")
        
    except ImportError:
        print("\nâš ï¸ Original implementation not found for comparison")
    
    # 3. GPU vs CPUæ¯”è¼ƒï¼ˆGPUåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
    if torch.cuda.is_available():
        print("\nğŸ”¥ GPU Performance Test:")
        
        # ã‚ˆã‚Šå¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ†ã‚¹ãƒˆ
        large_texts = test_texts * 10  # 500ãƒ†ã‚­ã‚¹ãƒˆ
        
        start_time = time.time()
        gpu_embeddings = pyg_embedder.embed_texts(large_texts[:50], reference)
        gpu_time = time.time() - start_time
        
        print(f"   âœ… GPU Time (50 texts): {gpu_time:.3f}s")
        print(f"   ğŸš€ GPU Speed: {50/gpu_time:.1f} texts/sec")
        
        # ç†è«–ä¸Šã®1000å•å‡¦ç†æ™‚é–“
        estimated_1000 = (1000 / 50) * gpu_time
        print(f"   ğŸ“ˆ Estimated time for 1000 texts: {estimated_1000:.1f}s")
    
    print("\nâœ… PyG geDIG Embedding benchmark completed!")

if __name__ == "__main__":
    benchmark_pyg_vs_original()