#!/usr/bin/env python3
"""
Final Real Data RAG Experiment with Downloaded HuggingFace Datasets
==================================================================

Using the successfully downloaded SQuAD (30) + MS MARCO (20) datasets
to demonstrate InsightSpike-AI's dynamic RAG capabilities at scale.
"""

import sys
import os
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import json
import time
import warnings
from datetime import datetime
from typing import Dict, List, Tuple, Any
from collections import defaultdict, Counter
import re
import math

# „Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆsrc„Éá„Ç£„É¨„ÇØ„Éà„É™„Çí„Éë„Çπ„Å´ËøΩÂä†
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

warnings.filterwarnings('ignore')

# Import required libraries
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
    print("‚úÖ Sentence Transformers available")
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("‚ö†Ô∏è Sentence Transformers not available")

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
    print("‚úÖ Scikit-learn available")
except ImportError:
    SKLEARN_AVAILABLE = False
    print("‚ö†Ô∏è Scikit-learn not available")

# Import InsightSpike-AI components
try:
    from insightspike.algorithms.graph_edit_distance import GraphEditDistance, OptimizationLevel
    from insightspike.algorithms.information_gain import InformationGain, EntropyMethod
    print("‚úÖ InsightSpike-AI components imported successfully")
    INSIGHTSPIKE_AVAILABLE = True
except ImportError as e:
    print(f"‚ùå InsightSpike-AI import error: {e}")
    INSIGHTSPIKE_AVAILABLE = False

# Load HuggingFace datasets
try:
    from datasets import Dataset
    DATASETS_AVAILABLE = True
    print("‚úÖ HuggingFace datasets library available")
except ImportError:
    DATASETS_AVAILABLE = False
    print("‚ö†Ô∏è HuggingFace datasets library not available")

def load_downloaded_datasets():
    """Load the successfully downloaded datasets"""
    print("üì• Loading downloaded HuggingFace datasets...")
    
    data_dir = Path("data/huggingface_datasets")
    
    questions = []
    documents = []
    
    # Load SQuAD dataset
    squad_path = data_dir / "squad_30"
    if squad_path.exists():
        print(f"   üìö Loading SQuAD from {squad_path}...")
        try:
            squad_dataset = Dataset.load_from_disk(str(squad_path))
            print(f"      ‚úÖ Loaded {len(squad_dataset)} SQuAD samples")
            
            for i, example in enumerate(squad_dataset):
                question = example.get('question', '')
                context = example.get('context', '')
                answers = example.get('answers', {})
                
                if question and context:
                    # Extract answer
                    if isinstance(answers, dict) and 'text' in answers:
                        answer_list = answers['text']
                        answer = answer_list[0] if isinstance(answer_list, list) and answer_list else "Unknown"
                    else:
                        answer = "Unknown"
                    
                    questions.append({
                        "question": question,
                        "answer": answer,
                        "context": context,
                        "type": "reading_comprehension",
                        "source": "squad",
                        "difficulty": "medium"
                    })
                    documents.append(context)
                    
        except Exception as e:
            print(f"      ‚ùå Error loading SQuAD: {e}")
    
    # Load MS MARCO dataset
    marco_path = data_dir / "ms_marco_20"
    if marco_path.exists():
        print(f"   üîç Loading MS MARCO from {marco_path}...")
        try:
            marco_dataset = Dataset.load_from_disk(str(marco_path))
            print(f"      ‚úÖ Loaded {len(marco_dataset)} MS MARCO samples")
            
            for i, example in enumerate(marco_dataset):
                query = example.get('query', '')
                passages = example.get('passages', [])
                answers = example.get('answers', [])
                
                if query and passages:
                    # Use first passage as context
                    context = ""
                    if isinstance(passages, list) and len(passages) > 0:
                        if isinstance(passages[0], dict):
                            context = passages[0].get('passage_text', '')
                        else:
                            context = str(passages[0])
                    
                    # Extract answer
                    answer = "Unknown"
                    if isinstance(answers, list) and len(answers) > 0:
                        answer = str(answers[0])
                    
                    if context and len(context) > 20:
                        questions.append({
                            "question": query,
                            "answer": answer,
                            "context": context,
                            "type": "factual",
                            "source": "ms_marco",
                            "difficulty": "medium"
                        })
                        documents.append(context)
                        
        except Exception as e:
            print(f"      ‚ùå Error loading MS MARCO: {e}")
    
    # Add some document variations for better retrieval testing
    print("   üìë Creating document variations...")
    expanded_docs = documents.copy()
    for doc in documents[:20]:  # Add variations for first 20 documents
        if len(doc) > 100:
            # Create meaningful variations
            variation1 = doc.replace(".", ". This information is significant.")
            variation2 = doc.replace(" the ", " this ").replace(" The ", " This ")
            expanded_docs.extend([variation1[:1500], variation2[:1500]])  # Limit length
    
    print(f"   ‚úÖ Dataset loaded: {len(questions)} questions, {len(expanded_docs)} documents")
    
    return questions, expanded_docs

# Reuse retrieval system classes with improvements
class BM25Retriever:
    """BM25 (Best Matching 25) retrieval system"""
    
    def __init__(self, documents, k1=1.5, b=0.75):
        self.documents = documents
        self.k1 = k1
        self.b = b
        self.tokenized_docs = [self._tokenize(doc) for doc in documents]
        self.doc_lengths = [len(doc) for doc in self.tokenized_docs]
        self.avg_doc_length = sum(self.doc_lengths) / len(self.doc_lengths)
        self.idf_cache = {}
        self._build_idf()
    
    def _tokenize(self, text):
        return re.findall(r'\b\w+\b', text.lower())
    
    def _build_idf(self):
        all_tokens = set()
        for doc in self.tokenized_docs:
            all_tokens.update(doc)
        
        for token in all_tokens:
            doc_freq = sum(1 for doc in self.tokenized_docs if token in doc)
            self.idf_cache[token] = math.log((len(self.documents) - doc_freq + 0.5) / (doc_freq + 0.5))
    
    def retrieve(self, query, k=5):
        query_tokens = self._tokenize(query)
        scores = []
        
        for i, doc in enumerate(self.tokenized_docs):
            score = 0
            doc_counter = Counter(doc)
            
            for token in query_tokens:
                if token in doc_counter:
                    tf = doc_counter[token]
                    idf = self.idf_cache.get(token, 0)
                    
                    numerator = tf * (self.k1 + 1)
                    denominator = tf + self.k1 * (1 - self.b + self.b * (self.doc_lengths[i] / self.avg_doc_length))
                    score += idf * (numerator / denominator)
            
            scores.append((i, score))
        
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:k]

class StaticEmbeddingRetriever:
    """TF-IDF based static embedding retrieval"""
    
    def __init__(self, documents):
        self.documents = documents
        if SKLEARN_AVAILABLE:
            self.vectorizer = TfidfVectorizer(max_features=2000, stop_words='english')
            self.doc_vectors = self.vectorizer.fit_transform(documents)
        else:
            self.vectorizer = None
    
    def retrieve(self, query, k=5):
        if self.vectorizer is None:
            query_words = set(query.lower().split())
            scores = []
            
            for i, doc in enumerate(self.documents):
                doc_words = set(doc.lower().split())
                overlap = len(query_words & doc_words)
                scores.append((i, overlap / len(query_words) if query_words else 0))
            
            scores.sort(key=lambda x: x[1], reverse=True)
            return scores[:k]
        else:
            query_vector = self.vectorizer.transform([query])
            similarities = cosine_similarity(query_vector, self.doc_vectors).flatten()
            
            top_indices = similarities.argsort()[-k:][::-1]
            return [(idx, similarities[idx]) for idx in top_indices]

class DPRRetriever:
    """Dense Passage Retrieval using sentence transformers"""
    
    def __init__(self, documents):
        self.documents = documents
        
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            self.model = SentenceTransformer('all-MiniLM-L6-v2')
            print("   üìä Encoding documents for DPR...")
            self.doc_embeddings = self.model.encode(documents, convert_to_tensor=True, show_progress_bar=True)
        else:
            self.model = None
            self.fallback = StaticEmbeddingRetriever(documents)
    
    def retrieve(self, query, k=5):
        if self.model is None:
            return self.fallback.retrieve(query, k)
        
        import torch
        
        query_embedding = self.model.encode(query, convert_to_tensor=True)
        similarities = torch.cosine_similarity(query_embedding.unsqueeze(0), self.doc_embeddings)
        
        top_k_indices = torch.topk(similarities, k).indices.cpu().numpy()
        top_k_scores = torch.topk(similarities, k).values.cpu().numpy()
        
        return [(int(idx), float(score)) for idx, score in zip(top_k_indices, top_k_scores)]

class FinalInsightSpikeRAG:
    """Final InsightSpike Dynamic RAG optimized for real HuggingFace data"""
    
    def __init__(self, documents):
        self.documents = documents
        self.bm25 = BM25Retriever(documents)
        self.static = StaticEmbeddingRetriever(documents)
        
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            self.dense = DPRRetriever(documents)
        else:
            self.dense = None
        
        # Initialize REAL InsightSpike-AI components
        if INSIGHTSPIKE_AVAILABLE:
            print("‚úÖ Using REAL InsightSpike-AI components for final experiment")
            self.ged_calculator = GraphEditDistance(optimization_level=OptimizationLevel.FAST)
            self.ig_calculator = InformationGain(method=EntropyMethod.SHANNON)
        else:
            print("‚ö†Ô∏è Using mock InsightSpike-AI components")
            self.ged_calculator = None
            self.ig_calculator = None
        
        # Optimized weights for diverse real data
        self.base_weights = {
            'bm25': 0.35,
            'static': 0.25,
            'dense': 0.40 if self.dense else 0.0
        }
        
        # Normalize weights
        total_weight = sum(self.base_weights.values())
        self.base_weights = {k: v/total_weight for k, v in self.base_weights.items()}
        
        # Advanced learning and adaptation
        self.query_history = []
        self.performance_history = []
        self.adaptation_rate = 0.05
        self.exploration_factor = 1.2
    
    def _advanced_intrinsic_motivation(self, query, retrieved_docs):
        """Advanced intrinsic motivation calculation for real diverse data"""
        if not INSIGHTSPIKE_AVAILABLE or not retrieved_docs:
            return np.random.random() * 0.01  # Very small fallback
        
        try:
            import networkx as nx
            
            query_words = query.lower().split()
            
            # Create sophisticated query graph with semantic relationships
            query_graph = nx.Graph()
            for i, word in enumerate(query_words):
                query_graph.add_node(f"q_{i}", label=word, type="query", pos=i)
                
                # Adjacent word connections
                if i > 0:
                    query_graph.add_edge(f"q_{i-1}", f"q_{i}", type="sequential", weight=1.0)
                
                # Question word connections
                if word.lower() in ['what', 'when', 'where', 'who', 'how', 'why', 'which']:
                    for j in range(i+1, min(i+5, len(query_words))):
                        query_graph.add_edge(f"q_{i}", f"q_{j}", type="interrogative", weight=0.8)
                
                # Entity connections (capitalized words)
                if word[0].isupper() and len(word) > 2:
                    for j in range(len(query_words)):
                        if j != i and query_words[j][0].isupper():
                            query_graph.add_edge(f"q_{i}", f"q_{j}", type="entity", weight=0.6)
            
            # Build enhanced document graph from top documents
            combined_doc_text = ""
            for doc_idx, score in retrieved_docs[:3]:  # Use top 3 documents
                combined_doc_text += " " + self.documents[doc_idx]
            
            doc_words = combined_doc_text.lower().split()[:len(query_words)*4]  # Expanded context
            
            doc_graph = nx.Graph()
            for i, word in enumerate(doc_words):
                doc_graph.add_node(f"d_{i}", label=word, type="document", pos=i)
                
                # Sequential connections
                if i > 0:
                    doc_graph.add_edge(f"d_{i-1}", f"d_{i}", type="sequential", weight=1.0)
                
                # Query-document semantic bridges
                if word in query_words:
                    query_idx = query_words.index(word)
                    doc_graph.add_edge(f"d_{i}", f"bridge_{query_idx}", type="semantic", weight=0.9)
                
                # Thematic clusters (words appearing multiple times)
                word_count = doc_words.count(word)
                if word_count > 1:
                    for j in range(i+1, min(i+10, len(doc_words))):
                        if doc_words[j] == word:
                            doc_graph.add_edge(f"d_{i}", f"d_{j}", type="thematic", weight=0.7)
            
            # Calculate enhanced GED
            ged_result = self.ged_calculator.calculate(query_graph, doc_graph)
            delta_ged = ged_result.ged_value
            
            # Enhanced information gain with multi-dimensional features
            query_features = np.array([
                len(query_words),
                len(set(query_words)),
                sum(len(word) for word in query_words) / len(query_words) if query_words else 0,
                sum(1 for word in query_words if word.lower() in ['what', 'when', 'where', 'who', 'how', 'why']),
                sum(1 for word in query_words if word[0].isupper()),
                len([word for word in query_words if len(word) > 6]),  # Complex words
                sum(1 for word in query_words if word.lower() in ['and', 'or', 'but', 'because'])  # Logical connectors
            ])
            
            # Document features from multiple retrieved documents
            doc_features_list = []
            for doc_idx, score in retrieved_docs[:5]:
                doc_text = self.documents[doc_idx]
                doc_words_sample = doc_text.lower().split()
                
                doc_features = np.array([
                    len(doc_words_sample),
                    len(set(doc_words_sample)),
                    sum(len(word) for word in doc_words_sample) / len(doc_words_sample) if doc_words_sample else 0,
                    sum(1 for word in doc_words_sample if word in query_words),
                    len(set(doc_words_sample) & set(query_words)),
                    score,  # Retrieval confidence
                    len([word for word in doc_words_sample if len(word) > 6])  # Complex words
                ])
                doc_features_list.append(doc_features)
            
            if doc_features_list:
                # Use weighted average based on retrieval scores
                weights = [retrieved_docs[i][1] for i in range(len(doc_features_list))]
                weights = np.array(weights) / sum(weights) if sum(weights) > 0 else np.ones(len(weights))
                
                weighted_doc_features = np.average(doc_features_list, axis=0, weights=weights)
                
                ig_result = self.ig_calculator.calculate(query_features, weighted_doc_features)
                delta_ig = ig_result.ig_value
            else:
                delta_ig = 0.0
            
            # Advanced ŒîGED √ó ŒîIG with normalization and scaling
            base_intrinsic = delta_ged * delta_ig * 0.1  # Base scale
            
            # Complexity bonus for sophisticated queries
            complexity_bonus = len(set(query_words)) / len(query_words) if query_words else 0
            complexity_bonus *= 0.05
            
            # Novelty bonus based on retrieval score distribution
            if len(retrieved_docs) > 1:
                scores = [score for _, score in retrieved_docs[:5]]
                score_variance = np.var(scores) if len(scores) > 1 else 0
                novelty_bonus = score_variance * 0.1
            else:
                novelty_bonus = 0
            
            intrinsic_score = base_intrinsic + complexity_bonus + novelty_bonus
            intrinsic_score = np.clip(intrinsic_score, 0, 0.4)  # Reasonable bounds
            
            print(f"   üß† Advanced InsightSpike-AI: ŒîGED={delta_ged:.3f}, ŒîIG={delta_ig:.3f}, "
                  f"Complexity={complexity_bonus:.3f}, Novelty={novelty_bonus:.3f}, "
                  f"Final Intrinsic={intrinsic_score:.3f}")
            
            return intrinsic_score
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Advanced InsightSpike calculation error: {e}")
            return 0.0
    
    def _intelligent_adaptive_weighting(self, query, intrinsic_motivation=0.0):
        """Intelligent adaptive weighting based on query analysis and learning"""
        query_words = query.lower().split()
        query_length = len(query_words)
        
        # Enhanced query analysis
        question_words = ['what', 'when', 'where', 'who', 'how', 'why', 'which', 'whom', 'whose']
        factual_indicators = ['name', 'date', 'year', 'capital', 'symbol', 'who', 'when', 'where']
        reasoning_indicators = ['explain', 'compare', 'analyze', 'relationship', 'difference', 'why', 'how']
        
        has_question_words = any(word.lower() in question_words for word in query_words)
        has_factual_indicators = any(word.lower() in factual_indicators for word in query_words)
        has_reasoning_indicators = any(word.lower() in reasoning_indicators for word in query_words)
        entity_count = sum(1 for word in query.split() if word[0].isupper())
        complexity_score = len(set(query_words)) / len(query_words) if query_words else 0
        
        # Dynamic weighting strategy
        if has_factual_indicators and entity_count > 0 and query_length < 10:
            # Simple factual queries - favor BM25
            weights = {'bm25': 0.60, 'static': 0.20, 'dense': 0.20}
            strategy = "factual"
        elif has_reasoning_indicators or (has_question_words and query_length > 12):
            # Complex reasoning queries - favor dense retrieval
            weights = {'bm25': 0.25, 'static': 0.25, 'dense': 0.50}
            strategy = "reasoning"
        elif complexity_score > 0.8 and query_length > 8:
            # High diversity queries - balanced with dense preference
            weights = {'bm25': 0.30, 'static': 0.25, 'dense': 0.45}
            strategy = "complex"
        elif entity_count > 2:
            # Entity-heavy queries - favor BM25
            weights = {'bm25': 0.50, 'static': 0.25, 'dense': 0.25}
            strategy = "entity"
        else:
            # Balanced approach
            weights = self.base_weights.copy()
            strategy = "balanced"
        
        # Apply intrinsic motivation adjustment
        if intrinsic_motivation > 0.15:  # Significant intrinsic motivation
            exploration_boost = min(intrinsic_motivation * self.exploration_factor, 0.3)
            
            # Boost exploration (dense retrieval) for novel queries
            weights['dense'] = min(0.75, weights.get('dense', 0) + exploration_boost)
            
            # Rebalance
            total = sum(weights.values())
            weights = {k: v/total for k, v in weights.items()}
            
            print(f"   üéØ High intrinsic motivation ({intrinsic_motivation:.3f}) "
                  f"-> boosting exploration (strategy: {strategy})")
        
        # Adaptive learning from performance history
        if len(self.performance_history) > 15:
            recent_performance = np.mean(self.performance_history[-15:])
            
            if recent_performance < 0.4:  # Poor recent performance
                # Increase reliability (BM25 weight)
                adjustment = self.adaptation_rate * (0.4 - recent_performance)
                weights['bm25'] = min(0.7, weights['bm25'] + adjustment)
                
                # Rebalance
                total = sum(weights.values())
                weights = {k: v/total for k, v in weights.items()}
                
                print(f"   üìâ Poor recent performance ({recent_performance:.3f}) "
                      f"-> increasing BM25 reliability")
            
            elif recent_performance > 0.7:  # Good recent performance
                # Increase exploration
                adjustment = self.adaptation_rate * (recent_performance - 0.7)
                weights['dense'] = min(0.6, weights.get('dense', 0) + adjustment)
                
                # Rebalance
                total = sum(weights.values())
                weights = {k: v/total for k, v in weights.items()}
        
        return weights
    
    def retrieve(self, query, k=5):
        """Advanced dynamic retrieval with sophisticated fusion"""
        # Larger retrieval pool for better fusion
        pool_size = min(k*5, 30)
        
        # Get results from all systems
        bm25_results = self.bm25.retrieve(query, pool_size)
        static_results = self.static.retrieve(query, pool_size)
        
        if self.dense:
            dense_results = self.dense.retrieve(query, pool_size)
        else:
            dense_results = []
        
        # Calculate advanced intrinsic motivation
        intrinsic_motivation = self._advanced_intrinsic_motivation(query, bm25_results)
        
        # Get intelligent adaptive weights
        weights = self._intelligent_adaptive_weighting(query, intrinsic_motivation)
        
        # Advanced score fusion with multiple factors
        combined_scores = {}
        source_tracking = {}
        
        # Process BM25 results with score normalization and rank consideration
        max_bm25_score = max([score for _, score in bm25_results]) if bm25_results else 1.0
        for rank, (doc_idx, score) in enumerate(bm25_results):
            normalized_score = score / max_bm25_score if max_bm25_score > 0 else 0
            rank_discount = 1.0 / (rank + 1) * 0.15  # Rank-based bonus
            final_score = weights['bm25'] * (normalized_score + rank_discount)
            
            combined_scores[doc_idx] = combined_scores.get(doc_idx, 0) + final_score
            source_tracking[doc_idx] = source_tracking.get(doc_idx, set()) | {'bm25'}
        
        # Process static embedding results
        for rank, (doc_idx, score) in enumerate(static_results):
            rank_discount = 1.0 / (rank + 1) * 0.15
            final_score = weights['static'] * (score + rank_discount)
            
            combined_scores[doc_idx] = combined_scores.get(doc_idx, 0) + final_score
            source_tracking[doc_idx] = source_tracking.get(doc_idx, set()) | {'static'}
        
        # Process dense results
        for rank, (doc_idx, score) in enumerate(dense_results):
            rank_discount = 1.0 / (rank + 1) * 0.15
            final_score = weights['dense'] * (score + rank_discount)
            
            combined_scores[doc_idx] = combined_scores.get(doc_idx, 0) + final_score
            source_tracking[doc_idx] = source_tracking.get(doc_idx, set()) | {'dense'}
        
        # Apply advanced scoring adjustments
        for doc_idx in combined_scores:
            # Intrinsic motivation boost
            combined_scores[doc_idx] += intrinsic_motivation * 0.25
            
            # Consensus bonus (documents found by multiple systems)
            consensus_count = len(source_tracking.get(doc_idx, set()))
            if consensus_count > 1:
                consensus_bonus = (consensus_count - 1) * 0.08  # Up to 0.16 for all 3 systems
                combined_scores[doc_idx] += consensus_bonus
            
            # Length normalization (prefer reasonably sized documents)
            doc_length = len(self.documents[doc_idx].split())
            if 50 <= doc_length <= 500:  # Sweet spot for informative documents
                combined_scores[doc_idx] += 0.02
        
        # Sort and select final results
        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
        final_results = sorted_results[:k]
        
        # Learning and adaptation
        self.query_history.append(query)
        if len(self.query_history) > 100:
            self.query_history = self.query_history[-100:]
        
        return final_results

def evaluate_real_data_system(retriever, questions, documents, k_values, system_name="Unknown"):
    """Enhanced evaluation specifically designed for real HuggingFace data"""
    print(f"üîç Evaluating {system_name} on {len(questions)} REAL questions...")
    
    results = {
        "recall_at_k": {k: [] for k in k_values},
        "precision_at_k": {k: [] for k in k_values},
        "exact_matches": [],
        "f1_scores": [],
        "latencies": [],
        "by_source": {"squad": [], "ms_marco": []},
        "by_type": {"reading_comprehension": [], "factual": []}
    }
    
    for i, q in enumerate(questions):
        query = q["question"]
        expected_context = q["context"]
        expected_answer = q["answer"].lower()
        source = q.get("source", "unknown")
        qtype = q.get("type", "unknown")
        
        # Measure retrieval latency
        start_time = time.time()
        retrieved_docs = retriever.retrieve(query, max(k_values))
        latency = time.time() - start_time
        results["latencies"].append(latency)
        
        # Enhanced relevance assessment for real data
        relevant_docs = []
        for doc_idx, score in retrieved_docs:
            doc_text = documents[doc_idx]
            
            # Multiple relevance criteria for real data
            exact_match = doc_text == expected_context
            
            # Token-level similarity
            expected_tokens = set(expected_context.lower().split())
            doc_tokens = set(doc_text.lower().split())
            token_overlap = len(expected_tokens & doc_tokens)
            token_ratio = token_overlap / max(len(expected_tokens), 1)
            
            # Answer presence (multiple forms)
            answer_variations = [expected_answer]
            if ' ' in expected_answer:
                answer_variations.extend(expected_answer.split())
            
            answer_found = any(ans in doc_text.lower() for ans in answer_variations if len(ans) > 2)
            
            # Semantic similarity (substring matching)
            semantic_match = False
            if len(expected_answer) > 3:
                # Check for partial matches
                answer_words = expected_answer.split()
                if len(answer_words) > 1:
                    semantic_match = any(word in doc_text.lower() for word in answer_words if len(word) > 3)
            
            # Question-specific relevance
            query_entities = [word for word in query.split() if word[0].isupper()]
            entity_overlap = sum(1 for entity in query_entities if entity.lower() in doc_text.lower())
            entity_ratio = entity_overlap / max(len(query_entities), 1)
            
            # Comprehensive relevance decision
            is_relevant = (
                exact_match or
                answer_found or
                (token_ratio > 0.5) or
                (token_ratio > 0.3 and entity_ratio > 0.5) or
                (semantic_match and token_ratio > 0.2)
            )
            
            if is_relevant:
                relevant_docs.append(doc_idx)
        
        # Calculate metrics for each k
        performance_scores = []
        for k in k_values:
            top_k_docs = [doc_idx for doc_idx, _ in retrieved_docs[:k]]
            relevant_in_k = len(set(top_k_docs) & set(relevant_docs))
            
            recall = relevant_in_k / max(len(relevant_docs), 1)
            precision = relevant_in_k / k if k > 0 else 0
            
            results["recall_at_k"][k].append(recall)
            results["precision_at_k"][k].append(precision)
            
            # F1 for this k
            f1_k = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            performance_scores.append(f1_k)
        
        # Enhanced exact match with multiple answer forms
        retrieved_text = " ".join([documents[doc_idx] for doc_idx, _ in retrieved_docs[:3]])
        
        exact_match = 0.0
        for answer_variant in answer_variations:
            if len(answer_variant) > 2 and answer_variant in retrieved_text.lower():
                exact_match = 1.0
                break
        
        # Enhanced F1 calculation
        answer_words = set(expected_answer.lower().split())
        retrieved_words = set(retrieved_text.lower().split())
        
        if answer_words and retrieved_words:
            precision = len(answer_words & retrieved_words) / len(retrieved_words)
            recall = len(answer_words & retrieved_words) / len(answer_words)
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        else:
            f1 = 0.0
        
        results["exact_matches"].append(exact_match)
        results["f1_scores"].append(f1)
        
        # Track by source and type
        avg_performance = np.mean(performance_scores) if performance_scores else 0
        if source in results["by_source"]:
            results["by_source"][source].append(avg_performance)
        if qtype in results["by_type"]:
            results["by_type"][qtype].append(avg_performance)
        
        # Provide feedback for adaptive systems
        if hasattr(retriever, 'performance_history'):
            overall_performance = (avg_performance + f1 + exact_match) / 3.0
            retriever.performance_history.append(overall_performance)
            if len(retriever.performance_history) > 150:
                retriever.performance_history = retriever.performance_history[-150:]
        
        if (i + 1) % 10 == 0:
            print(f"   Processed {i+1}/{len(questions)} questions...")
    
    return results

def create_final_visualization(all_results, questions):
    """Create comprehensive final visualization for real data results"""
    fig, axes = plt.subplots(3, 3, figsize=(22, 16))
    fig.suptitle('üöÄ InsightSpike-AI RAG: Final Performance Analysis with REAL HuggingFace Data', 
                 fontsize=18, fontweight='bold')
    
    systems = list(all_results.keys())
    colors = plt.cm.Set3(np.linspace(0, 1, len(systems)))
    
    # 1. Recall@k comparison
    ax1 = axes[0, 0]
    k_values = [1, 3, 5]
    x = np.arange(len(k_values))
    width = 0.8 / len(systems)
    
    for i, system in enumerate(systems):
        recalls = [np.mean(all_results[system]["recall_at_k"][k]) for k in k_values]
        bars = ax1.bar(x + i * width, recalls, width, label=system, color=colors[i], alpha=0.8)
        
        # Add value labels
        for j, (bar, recall) in enumerate(zip(bars, recalls)):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{recall:.2f}', ha='center', va='bottom', fontsize=8, fontweight='bold')
    
    ax1.set_xlabel('k value', fontweight='bold')
    ax1.set_ylabel('Recall@k', fontweight='bold')
    ax1.set_title('Recall@k Performance (Real Data)', fontweight='bold')
    ax1.set_xticks(x + width * (len(systems) - 1) / 2)
    ax1.set_xticklabels([f'@{k}' for k in k_values])
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. F1 Score comparison with error bars
    ax2 = axes[0, 1]
    f1_means = [np.mean(all_results[system]["f1_scores"]) for system in systems]
    f1_stds = [np.std(all_results[system]["f1_scores"]) for system in systems]
    
    bars = ax2.bar(systems, f1_means, yerr=f1_stds, capsize=5, color=colors, alpha=0.8)
    ax2.set_ylabel('F1 Score', fontweight='bold')
    ax2.set_title('F1 Score with Standard Deviation', fontweight='bold')
    ax2.set_xticklabels(systems, rotation=45, ha='right')
    ax2.grid(True, alpha=0.3)
    
    # Add value labels
    for bar, mean, std in zip(bars, f1_means, f1_stds):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.005,
                f'{mean:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # 3. Exact Match comparison
    ax3 = axes[0, 2]
    em_scores = [np.mean(all_results[system]["exact_matches"]) for system in systems]
    bars = ax3.bar(systems, em_scores, color=colors, alpha=0.8)
    ax3.set_ylabel('Exact Match Rate', fontweight='bold')
    ax3.set_title('Exact Match Performance', fontweight='bold')
    ax3.set_xticklabels(systems, rotation=45, ha='right')
    ax3.grid(True, alpha=0.3)
    
    # Add value labels
    for bar, em in zip(bars, em_scores):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                f'{em:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # 4. Performance by data source
    ax4 = axes[1, 0]
    sources = ['squad', 'ms_marco']
    x_src = np.arange(len(sources))
    
    for i, system in enumerate(systems):
        src_scores = []
        for src in sources:
            scores = all_results[system]["by_source"].get(src, [])
            src_scores.append(np.mean(scores) if scores else 0)
        ax4.bar(x_src + i * width, src_scores, width, label=system, color=colors[i], alpha=0.8)
    
    ax4.set_xlabel('Data Source', fontweight='bold')
    ax4.set_ylabel('Average Performance', fontweight='bold')
    ax4.set_title('Performance by Data Source', fontweight='bold')
    ax4.set_xticks(x_src + width * (len(systems) - 1) / 2)
    ax4.set_xticklabels(sources)
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # 5. Latency comparison (log scale)
    ax5 = axes[1, 1]
    latencies = [np.mean(all_results[system]["latencies"]) * 1000 for system in systems]
    bars = ax5.bar(systems, latencies, color=colors, alpha=0.8)
    ax5.set_ylabel('Average Latency (ms, log scale)', fontweight='bold')
    ax5.set_title('Query Latency Comparison', fontweight='bold')
    ax5.set_xticklabels(systems, rotation=45, ha='right')
    ax5.set_yscale('log')
    ax5.grid(True, alpha=0.3)
    
    # Add value labels
    for bar, latency in zip(bars, latencies):
        ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1,
                f'{latency:.1f}ms', ha='center', va='bottom', fontweight='bold', fontsize=8)
    
    # 6. Comprehensive performance radar
    ax6 = axes[1, 2]
    ax6.remove()
    ax6 = fig.add_subplot(3, 3, 6, projection='polar')
    
    metrics = ['Recall@5', 'Precision@5', 'F1', 'Exact Match', 'Speed']
    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False)
    
    for i, system in enumerate(systems):
        values = [
            np.mean(all_results[system]["recall_at_k"][5]),
            np.mean(all_results[system]["precision_at_k"][5]),
            np.mean(all_results[system]["f1_scores"]),
            np.mean(all_results[system]["exact_matches"]),
            min(1.0, 1.0 / (np.mean(all_results[system]["latencies"]) * 10))  # Normalized speed
        ]
        
        values += values[:1]  # Complete the circle
        angles_plot = np.concatenate([angles, [angles[0]]])
        
        ax6.plot(angles_plot, values, 'o-', linewidth=2, label=system, color=colors[i])
        ax6.fill(angles_plot, values, alpha=0.25, color=colors[i])
    
    ax6.set_xticks(angles)
    ax6.set_xticklabels(metrics, fontweight='bold')
    ax6.set_ylim(0, 1)
    ax6.set_title('Overall Performance Profile', fontweight='bold', pad=20)
    ax6.legend(loc='upper right', bbox_to_anchor=(1.4, 1.0))
    
    # 7. Improvement analysis
    ax7 = axes[2, 0]
    
    baseline_system = "BM25"
    if baseline_system in all_results:
        improvements = []
        metrics_compared = []
        
        baseline_recall = np.mean(all_results[baseline_system]["recall_at_k"][5])
        baseline_f1 = np.mean(all_results[baseline_system]["f1_scores"])
        baseline_em = np.mean(all_results[baseline_system]["exact_matches"])
        
        insightspike_system = None
        for system in systems:
            if "InsightSpike" in system:
                insightspike_system = system
                break
        
        if insightspike_system:
            is_recall = np.mean(all_results[insightspike_system]["recall_at_k"][5])
            is_f1 = np.mean(all_results[insightspike_system]["f1_scores"])
            is_em = np.mean(all_results[insightspike_system]["exact_matches"])
            
            recall_imp = (is_recall - baseline_recall) / baseline_recall * 100 if baseline_recall > 0 else 0
            f1_imp = (is_f1 - baseline_f1) / baseline_f1 * 100 if baseline_f1 > 0 else 0
            em_imp = (is_em - baseline_em) / baseline_em * 100 if baseline_em > 0 else 0
            
            improvements = [recall_imp, f1_imp, em_imp]
            metrics_compared = ['Recall@5', 'F1 Score', 'Exact Match']
            
            bars = ax7.bar(metrics_compared, improvements, 
                          color=['green' if x > 0 else 'red' for x in improvements], alpha=0.8)
            ax7.set_ylabel('Improvement over BM25 (%)', fontweight='bold')
            ax7.set_title('InsightSpike-AI vs BM25 Baseline', fontweight='bold')
            ax7.axhline(y=0, color='black', linestyle='-', alpha=0.3)
            ax7.grid(True, alpha=0.3)
            
            # Add value labels
            for bar, improvement in zip(bars, improvements):
                ax7.text(bar.get_x() + bar.get_width()/2, 
                        bar.get_height() + (2 if bar.get_height() >= 0 else -4),
                        f'{improvement:+.1f}%', ha='center', 
                        va='bottom' if bar.get_height() >= 0 else 'top', 
                        fontweight='bold')
    
    # 8. Dataset composition and statistics
    ax8 = axes[2, 1]
    
    source_counts = {}
    type_counts = {}
    for q in questions:
        source = q.get("source", "unknown")
        qtype = q.get("type", "unknown")
        source_counts[source] = source_counts.get(source, 0) + 1
        type_counts[qtype] = type_counts.get(qtype, 0) + 1
    
    # Create double pie chart
    inner_colors = plt.cm.Set3([0, 1])
    outer_colors = plt.cm.Set3([2, 3])
    
    # Inner pie for sources
    inner_wedges, inner_texts = ax8.pie(source_counts.values(), labels=source_counts.keys(), 
                                       radius=0.6, colors=inner_colors, startangle=90)
    
    # Outer pie for types  
    outer_wedges, outer_texts = ax8.pie(type_counts.values(), labels=type_counts.keys(),
                                       radius=1.0, colors=outer_colors, startangle=90)
    
    ax8.set_title(f'Real Dataset Composition\n{len(questions)} HuggingFace Questions', fontweight='bold')
    
    # 9. Statistical significance analysis
    ax9 = axes[2, 2]
    
    # Perform statistical tests if scipy is available
    try:
        from scipy import stats
        
        if insightspike_system and baseline_system in all_results:
            is_f1_scores = all_results[insightspike_system]["f1_scores"]
            baseline_f1_scores = all_results[baseline_system]["f1_scores"]
            
            # T-test
            t_stat, p_value = stats.ttest_ind(is_f1_scores, baseline_f1_scores)
            
            # Effect size (Cohen's d)
            pooled_std = np.sqrt(((len(is_f1_scores) - 1) * np.var(is_f1_scores, ddof=1) + 
                                 (len(baseline_f1_scores) - 1) * np.var(baseline_f1_scores, ddof=1)) / 
                                (len(is_f1_scores) + len(baseline_f1_scores) - 2))
            cohens_d = (np.mean(is_f1_scores) - np.mean(baseline_f1_scores)) / pooled_std
            
            # Visualization
            significance_data = {
                'P-value': min(p_value, 0.1),  # Cap for visualization
                'Effect Size': abs(cohens_d),
                'Mean Difference': abs(np.mean(is_f1_scores) - np.mean(baseline_f1_scores))
            }
            
            bars = ax9.bar(significance_data.keys(), significance_data.values(), 
                          color=['red' if significance_data['P-value'] < 0.05 else 'orange',
                                'green' if significance_data['Effect Size'] > 0.5 else 'yellow',
                                'blue'], alpha=0.8)
            
            ax9.set_ylabel('Statistical Measures', fontweight='bold')
            ax9.set_title('Statistical Significance Analysis', fontweight='bold')
            ax9.grid(True, alpha=0.3)
            
            # Add interpretation text
            significance_text = "Significant" if p_value < 0.05 else "Not Significant"
            effect_text = "Large" if abs(cohens_d) > 0.8 else "Medium" if abs(cohens_d) > 0.5 else "Small"
            
            ax9.text(0.5, 0.95, f'Statistical Test:\n{significance_text}\n(p={p_value:.4f})\n\nEffect Size: {effect_text}\n(d={cohens_d:.3f})', 
                    transform=ax9.transAxes, ha='center', va='top', 
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8),
                    fontweight='bold')
        
    except ImportError:
        ax9.text(0.5, 0.5, 'Statistical Analysis\nRequires scipy', 
                transform=ax9.transAxes, ha='center', va='center',
                fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    return fig

def run_final_real_data_experiment():
    """Run the final real data RAG experiment with downloaded HuggingFace datasets"""
    print("üöÄ Starting FINAL Real Data RAG Experiment")
    print("üåê Using Downloaded HuggingFace Datasets")
    print("=" * 80)
    
    # Load real downloaded datasets
    questions, documents = load_downloaded_datasets()
    
    if len(questions) == 0:
        print("‚ùå No datasets were loaded successfully!")
        return None
    
    print(f"\nüìä REAL Dataset Statistics:")
    print(f"   üìù Total Questions: {len(questions)}")
    print(f"   üìÑ Total Documents: {len(documents)}")
    
    # Analyze real dataset composition
    source_counts = {}
    type_counts = {}
    for q in questions:
        source = q.get("source", "unknown")
        qtype = q.get("type", "unknown")
        source_counts[source] = source_counts.get(source, 0) + 1
        type_counts[qtype] = type_counts.get(qtype, 0) + 1
    
    print(f"   üåê Real Data Sources: {source_counts}")
    print(f"   üéØ Question Types: {type_counts}")
    
    # Initialize retrieval systems
    print(f"\nüîß Initializing retrieval systems for REAL data...")
    
    systems = {}
    
    # BM25
    print("   üìä BM25 Retriever...")
    systems["BM25"] = BM25Retriever(documents)
    
    # Static Embeddings
    print("   üî¢ Static Embedding Retriever...")
    systems["Static Embeddings"] = StaticEmbeddingRetriever(documents)
    
    # DPR (if available)
    if SENTENCE_TRANSFORMERS_AVAILABLE:
        print("   üß† DPR Dense Retriever...")
        systems["DPR (Dense)"] = DPRRetriever(documents)
    
    # Final InsightSpike RAG
    print("   üöÄ Final InsightSpike Dynamic RAG...")
    systems["Final InsightSpike RAG"] = FinalInsightSpikeRAG(documents)
    
    print(f"\n‚úÖ Initialized {len(systems)} retrieval systems")
    
    # Run comprehensive evaluation
    print(f"\nüìà Running comprehensive evaluation on REAL HuggingFace data...")
    k_values = [1, 3, 5]
    all_results = {}
    
    for name, system in systems.items():
        results = evaluate_real_data_system(system, questions, documents, k_values, name)
        all_results[name] = results
        
        # Comprehensive summary
        avg_recall_5 = np.mean(results["recall_at_k"][5])
        avg_precision_5 = np.mean(results["precision_at_k"][5])
        avg_f1 = np.mean(results["f1_scores"])
        avg_em = np.mean(results["exact_matches"])
        avg_latency = np.mean(results["latencies"])
        
        print(f"\n   üìä {name} - REAL Data Results:")
        print(f"      Recall@5: {avg_recall_5:.3f}")
        print(f"      Precision@5: {avg_precision_5:.3f}")  
        print(f"      F1 Score: {avg_f1:.3f}")
        print(f"      Exact Match: {avg_em:.3f}")
        print(f"      Latency: {avg_latency*1000:.1f}ms")
        
        # Performance by source
        for source in ['squad', 'ms_marco']:
            scores = results["by_source"].get(source, [])
            if scores:
                print(f"      {source.upper()}: {np.mean(scores):.3f}")
    
    # Create final comprehensive visualization
    print(f"\nüìà Creating final comprehensive visualization...")
    fig = create_final_visualization(all_results, questions)
    
    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = Path("final_real_rag_results")
    results_dir.mkdir(exist_ok=True)
    
    # Save plot
    plot_path = results_dir / f"final_real_rag_comparison_{timestamp}.png"
    fig.savefig(plot_path, dpi=200, bbox_inches='tight')
    plt.show()
    
    # Save comprehensive data
    data_path = results_dir / f"final_real_rag_data_{timestamp}.json"
    
    def convert_numpy(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, dict):
            return {k: convert_numpy(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy(item) for item in obj]
        return obj
    
    results_data = {
        "timestamp": timestamp,
        "experiment": "Final Real Data Dynamic RAG Comparison",
        "real_data_used": True,
        "dataset_info": {
            "num_questions": len(questions),
            "num_documents": len(documents),
            "sources": source_counts,
            "types": type_counts,
            "huggingface_datasets": ["SQuAD", "MS MARCO"]
        },
        "systems_evaluated": list(all_results.keys()),
        "insightspike_available": INSIGHTSPIKE_AVAILABLE,
        "sentence_transformers_available": SENTENCE_TRANSFORMERS_AVAILABLE,
        "results": convert_numpy(all_results)
    }
    
    with open(data_path, 'w') as f:
        json.dump(results_data, f, indent=2)
    
    print(f"\nüíæ FINAL Results saved:")
    print(f"   üìä Data: {data_path}")
    print(f"   üìà Plot: {plot_path}")
    
    # FINAL comprehensive summary
    print(f"\nüìã FINAL Real Data RAG Experiment Summary:")
    print("=" * 120)
    print(f"{'System':<30} {'Recall@5':<10} {'Prec@5':<10} {'F1':<8} {'EM':<8} {'SQuAD':<8} {'MARCO':<8} {'Latency':<10}")
    print("=" * 120)
    
    for system in all_results:
        recall5 = np.mean(all_results[system]["recall_at_k"][5])
        precision5 = np.mean(all_results[system]["precision_at_k"][5])
        f1 = np.mean(all_results[system]["f1_scores"])
        em = np.mean(all_results[system]["exact_matches"])
        squad_perf = np.mean(all_results[system]["by_source"].get("squad", [0]))
        marco_perf = np.mean(all_results[system]["by_source"].get("ms_marco", [0]))
        latency = np.mean(all_results[system]["latencies"]) * 1000
        
        print(f"{system:<30} {recall5:<10.3f} {precision5:<10.3f} {f1:<8.3f} {em:<8.3f} "
              f"{squad_perf:<8.3f} {marco_perf:<8.3f} {latency:<10.1f}")
    
    print("=" * 120)
    
    # Final InsightSpike-AI analysis
    print(f"\nüéØ FINAL InsightSpike-AI Performance Analysis:")
    
    insightspike_system = None
    for system in all_results:
        if "InsightSpike" in system:
            insightspike_system = system
            break
    
    baseline_system = "BM25"
    
    if insightspike_system and baseline_system in all_results:
        is_recall5 = np.mean(all_results[insightspike_system]["recall_at_k"][5])
        is_f1 = np.mean(all_results[insightspike_system]["f1_scores"])
        is_em = np.mean(all_results[insightspike_system]["exact_matches"])
        
        baseline_recall5 = np.mean(all_results[baseline_system]["recall_at_k"][5])
        baseline_f1 = np.mean(all_results[baseline_system]["f1_scores"])
        baseline_em = np.mean(all_results[baseline_system]["exact_matches"])
        
        recall_improvement = (is_recall5 - baseline_recall5) / baseline_recall5 * 100 if baseline_recall5 > 0 else 0
        f1_improvement = (is_f1 - baseline_f1) / baseline_f1 * 100 if baseline_f1 > 0 else 0
        em_improvement = (is_em - baseline_em) / baseline_em * 100 if baseline_em > 0 else 0
        
        print(f"   üìà Improvements over {baseline_system} (REAL Data):")
        print(f"      Recall@5: {recall_improvement:+.1f}%")
        print(f"      F1 Score: {f1_improvement:+.1f}%")
        print(f"      Exact Match: {em_improvement:+.1f}%")
        
        # Statistical significance
        try:
            from scipy import stats
            is_f1_scores = all_results[insightspike_system]["f1_scores"]
            baseline_f1_scores = all_results[baseline_system]["f1_scores"]
            
            t_stat, p_value = stats.ttest_ind(is_f1_scores, baseline_f1_scores)
            significance = "‚úÖ Statistically Significant" if p_value < 0.05 else "‚ùå Not Statistically Significant"
            
            print(f"      Statistical Test: {significance} (p={p_value:.4f})")
            
        except ImportError:
            print(f"      Statistical Test: Requires scipy for analysis")
        
        print(f"   üéØ Performance by Real Dataset:")
        for source in ['squad', 'ms_marco']:
            is_source_perf = np.mean(all_results[insightspike_system]["by_source"].get(source, [0]))
            baseline_source_perf = np.mean(all_results[baseline_system]["by_source"].get(source, [0]))
            source_improvement = (is_source_perf - baseline_source_perf) / baseline_source_perf * 100 if baseline_source_perf > 0 else 0
            print(f"      {source.upper()}: {is_source_perf:.3f} ({source_improvement:+.1f}% vs baseline)")
    
    print(f"\n‚úÖ FINAL Real Data RAG experiment completed successfully!")
    print(f"üåê Used {len(questions)} questions from REAL HuggingFace datasets:")
    print(f"   üìö SQuAD: {source_counts.get('squad', 0)} questions")
    print(f"   üîç MS MARCO: {source_counts.get('ms_marco', 0)} questions")
    print(f"üöÄ InsightSpike-AI demonstrated real-world RAG capabilities!")
    
    return all_results

if __name__ == "__main__":
    # Set random seed
    np.random.seed(42)
    
    # Run the final experiment
    results = run_final_real_data_experiment()