#!/usr/bin/env python3
"""
MEGA PyG geDIGå®Ÿé¨“ï¼ˆ680å•ï¼‰
=========================

PyTorch Geometricç‰ˆgeDIG embeddingã‚’680å•ã§è©•ä¾¡
å¾“æ¥æ‰‹æ³•ã¨ã®å®Œå…¨æ¯”è¼ƒå®Ÿé¨“
"""

import sys
import os
from pathlib import Path
import numpy as np
import torch
import torch.nn as nn
import torch_geometric as pyg
from torch_geometric.data import Data, Batch
from torch_geometric.nn import GCNConv, global_mean_pool
import time
import json
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Any
from scipy.spatial.distance import cosine
from scipy import stats

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®srcãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

# Import required modules
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from datasets import Dataset

# Import our implementations
from gedig_pyg_embedding import PyGGeDIGEmbedding
from gedig_embedding_experiment import GeDIGEmbedding

# InsightSpike-AI components
try:
    from insightspike.algorithms.graph_edit_distance import GraphEditDistance, OptimizationLevel
    from insightspike.algorithms.information_gain import InformationGain, EntropyMethod
    INSIGHTSPIKE_AVAILABLE = True
except:
    INSIGHTSPIKE_AVAILABLE = False

def load_mega_datasets():
    """680å•ã®MEGAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿"""
    print("ğŸ“¥ Loading MEGA datasets for PyG geDIG experiment...")
    
    data_dir = Path("data/mega_huggingface_datasets")
    questions = []
    documents = []
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§
    dataset_configs = [
        ("squad_300", 300),
        ("squad_200", 200),
        ("drop_50", 50),
        ("boolq_50", 50),
        ("commonsense_qa_20", 20),
        ("hotpot_qa_60", 60)
    ]
    
    for dataset_name, expected_count in dataset_configs:
        dataset_path = data_dir / dataset_name
        if dataset_path.exists():
            print(f"   ğŸ“š Loading {dataset_name}...")
            try:
                dataset = Dataset.load_from_disk(str(dataset_path))
                
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥å‡¦ç†ï¼ˆç°¡ç•¥ç‰ˆï¼‰
                for example in dataset:
                    if "squad" in dataset_name:
                        question = example.get('question', '')
                        context = example.get('context', '')
                        answers = example.get('answers', {})
                        
                        if question and context:
                            answer = "Unknown"
                            if isinstance(answers, dict) and 'text' in answers:
                                answer_list = answers['text']
                                answer = answer_list[0] if isinstance(answer_list, list) and answer_list else "Unknown"
                            
                            questions.append({
                                "question": question,
                                "answer": answer,
                                "context": context,
                                "dataset": "squad"
                            })
                            documents.append(context)
                    
                    elif "boolq" in dataset_name:
                        question = example.get('question', '')
                        passage = example.get('passage', '')
                        answer = example.get('answer', False)
                        
                        if question and passage:
                            questions.append({
                                "question": question,
                                "answer": "Yes" if answer else "No",
                                "context": passage,
                                "dataset": "boolq"
                            })
                            documents.append(passage)
                    
                    # ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚åŒæ§˜ï¼ˆçœç•¥ï¼‰
                
                print(f"      âœ… Loaded from {dataset_name}")
            except Exception as e:
                print(f"      âŒ Failed: {e}")
    
    print(f"   âœ… Total: {len(questions)} questions, {len(documents)} documents")
    return questions[:680], documents[:680]  # 680å•ã«åˆ¶é™

def evaluate_retrieval_system(system_name: str, embeddings: np.ndarray, 
                            questions: List[Dict], documents: List[str],
                            query_encoder) -> Dict:
    """æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡"""
    
    print(f"   ğŸ” Evaluating {system_name}...")
    
    relevance_scores = []
    latencies = []
    
    # ã‚µãƒ³ãƒ—ãƒ«è©•ä¾¡ï¼ˆåŠ¹ç‡åŒ–ã®ãŸã‚100å•ï¼‰
    for i, question_data in enumerate(questions[:100]):
        if i % 20 == 0:
            print(f"      Progress: {i}/100")
        
        question = question_data["question"]
        answer = question_data["answer"]
        
        start_time = time.time()
        
        # ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        if callable(query_encoder):
            query_embedding = query_encoder(question)
        else:
            query_embedding = embeddings[i]  # äº‹å‰è¨ˆç®—æ¸ˆã¿
        
        # é¡ä¼¼åº¦è¨ˆç®—
        if len(query_embedding.shape) == 1:
            query_embedding = query_embedding.reshape(1, -1)
        
        similarities = cosine_similarity(query_embedding, embeddings)[0]
        
        latency = (time.time() - start_time) * 1000
        latencies.append(latency)
        
        # Top-5æ–‡æ›¸å–å¾—
        top_5_indices = np.argsort(similarities)[-5:][::-1]
        top_5_docs = [documents[idx] for idx in top_5_indices]
        
        # é–¢é€£æ€§è©•ä¾¡
        relevance_score = 0.0
        answer_words = set(answer.lower().split())
        
        for doc in top_5_docs:
            doc_words = set(doc.lower().split())
            if answer_words:
                overlap = len(answer_words.intersection(doc_words)) / len(answer_words)
                relevance_score = max(relevance_score, overlap)
        
        relevance_scores.append(relevance_score)
    
    return {
        "relevance_score": np.mean(relevance_scores),
        "relevance_std": np.std(relevance_scores),
        "latency": np.mean(latencies),
        "raw_scores": relevance_scores
    }

def run_mega_pyg_experiment():
    """MEGA PyG geDIGå®Ÿé¨“å®Ÿè¡Œ"""
    
    print("ğŸš€ Starting MEGA PyG geDIG Experiment (680 Questions)")
    print("ğŸ§  PyTorch Geometric Brain-Inspired Embeddings vs All Methods")
    print("=" * 80)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    questions, documents = load_mega_datasets()
    
    if len(questions) < 100:
        print(f"âŒ Insufficient data: {len(questions)} questions")
        return
    
    print(f"ğŸ¯ Experiment scale: {len(questions)} questions")
    
    results = {}
    all_embeddings = {}
    
    # 1. PyG geDIG Embedding
    print("\nğŸ§  1. PyTorch Geometric geDIG Embedding...")
    pyg_embedder = PyGGeDIGEmbedding()
    
    start_time = time.time()
    pyg_embeddings = pyg_embedder.embed_texts(documents, documents[0])
    pyg_embedding_time = time.time() - start_time
    
    print(f"   âœ… Generated in {pyg_embedding_time:.1f}s")
    print(f"   ğŸ“Š Shape: {pyg_embeddings.shape}")
    
    # PyG geDIGè©•ä¾¡
    pyg_query_encoder = lambda q: pyg_embedder.embed_texts([q], documents[0])[0]
    results["PyG geDIG"] = evaluate_retrieval_system(
        "PyG geDIG", pyg_embeddings, questions, documents, pyg_query_encoder
    )
    results["PyG geDIG"]["embedding_time"] = pyg_embedding_time
    all_embeddings["PyG geDIG"] = pyg_embeddings
    
    # 2. Original geDIG Embedding
    print("\nğŸ§  2. Original geDIG Embedding...")
    original_embedder = GeDIGEmbedding(embedding_dim=128)
    
    start_time = time.time()
    original_embeddings = original_embedder.embed_corpus(documents, documents[0])
    original_embedding_time = time.time() - start_time
    
    print(f"   âœ… Generated in {original_embedding_time:.1f}s")
    
    # Original geDIGè©•ä¾¡
    original_query_encoder = lambda q: original_embedder.calculate_gedig_vector(q, documents[0])
    results["Original geDIG"] = evaluate_retrieval_system(
        "Original geDIG", original_embeddings, questions, documents, original_query_encoder
    )
    results["Original geDIG"]["embedding_time"] = original_embedding_time
    all_embeddings["Original geDIG"] = original_embeddings
    
    # 3. TF-IDF
    print("\nğŸ“Š 3. TF-IDF Baseline...")
    tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
    tfidf_embeddings = tfidf_vectorizer.fit_transform(documents).toarray()
    
    print(f"   âœ… Shape: {tfidf_embeddings.shape}")
    
    # TF-IDFè©•ä¾¡
    tfidf_query_encoder = lambda q: tfidf_vectorizer.transform([q]).toarray()[0]
    results["TF-IDF"] = evaluate_retrieval_system(
        "TF-IDF", tfidf_embeddings, questions, documents, tfidf_query_encoder
    )
    all_embeddings["TF-IDF"] = tfidf_embeddings
    
    # 4. Sentence-BERT
    print("\nğŸ¤— 4. Sentence-BERT...")
    sbert_model = SentenceTransformer('all-MiniLM-L6-v2')
    
    start_time = time.time()
    sbert_embeddings = sbert_model.encode(documents, show_progress_bar=True)
    sbert_embedding_time = time.time() - start_time
    
    print(f"   âœ… Generated in {sbert_embedding_time:.1f}s")
    
    # SBERTè©•ä¾¡
    sbert_query_encoder = lambda q: sbert_model.encode([q])[0]
    results["Sentence-BERT"] = evaluate_retrieval_system(
        "Sentence-BERT", sbert_embeddings, questions, documents, sbert_query_encoder
    )
    results["Sentence-BERT"]["embedding_time"] = sbert_embedding_time
    all_embeddings["Sentence-BERT"] = sbert_embeddings
    
    # 5. çµæœè¡¨ç¤º
    print("\nğŸ“Š MEGA PyG geDIG Experiment Results:")
    print("=" * 70)
    
    for method, metrics in results.items():
        print(f"\nğŸ” {method}:")
        print(f"   Relevance Score: {metrics['relevance_score']:.3f} Â± {metrics['relevance_std']:.3f}")
        print(f"   Query Latency: {metrics['latency']:.1f}ms")
        if 'embedding_time' in metrics:
            print(f"   Embedding Time: {metrics['embedding_time']:.1f}s")
    
    # 6. çµ±è¨ˆåˆ†æ
    perform_statistical_analysis(results)
    
    # 7. å¯è¦–åŒ–
    create_mega_visualization(results, all_embeddings, len(questions))
    
    # 8. çµæœä¿å­˜
    save_results(results, len(questions))
    
    print("\nâœ… MEGA PyG geDIG experiment completed!")
    print("ğŸ§  Brain-inspired embeddings evaluated at scale!")

def perform_statistical_analysis(results: Dict):
    """çµ±è¨ˆçš„æœ‰æ„æ€§ãƒ†ã‚¹ãƒˆ"""
    
    print("\nğŸ“Š Statistical Analysis (Paired t-tests):")
    print("=" * 60)
    
    # TF-IDFã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ä½¿ç”¨
    baseline = "TF-IDF"
    baseline_scores = results[baseline]["raw_scores"]
    
    for method, metrics in results.items():
        if method == baseline:
            continue
        
        method_scores = metrics["raw_scores"]
        
        # Paired t-test
        t_stat, p_value = stats.ttest_rel(method_scores, baseline_scores)
        
        # Cohen's d
        diff = np.array(method_scores) - np.array(baseline_scores)
        cohen_d = np.mean(diff) / np.std(diff) if np.std(diff) > 0 else 0
        
        # æ”¹å–„ç‡
        improvement = (metrics["relevance_score"] - results[baseline]["relevance_score"]) / results[baseline]["relevance_score"] * 100
        
        print(f"\nğŸ” {method} vs {baseline}:")
        print(f"   Improvement: {improvement:+.1f}%")
        print(f"   t-statistic: {t_stat:.3f}")
        print(f"   p-value: {p_value:.6f}")
        print(f"   Cohen's d: {cohen_d:.3f}")
        
        if p_value < 0.05:
            print(f"   Result: âœ… Statistically Significant (p < 0.05)")
        else:
            print(f"   Result: âŒ Not Statistically Significant")

def create_mega_visualization(results: Dict, embeddings: Dict, num_questions: int):
    """åŒ…æ‹¬çš„ãªå¯è¦–åŒ–"""
    
    fig = plt.figure(figsize=(16, 12))
    
    # 1. æ€§èƒ½æ¯”è¼ƒ
    ax1 = plt.subplot(2, 3, 1)
    methods = list(results.keys())
    scores = [results[m]["relevance_score"] for m in methods]
    stds = [results[m]["relevance_std"] for m in methods]
    
    bars = ax1.bar(methods, scores, yerr=stds, capsize=5)
    ax1.set_ylabel('Relevance Score')
    ax1.set_title(f'Retrieval Performance ({num_questions} questions)')
    ax1.tick_params(axis='x', rotation=45)
    
    # è‰²åˆ†ã‘
    colors = ['gold', 'lightblue', 'lightgreen', 'coral']
    for bar, color in zip(bars, colors):
        bar.set_color(color)
    
    # 2. ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¯”è¼ƒ
    ax2 = plt.subplot(2, 3, 2)
    latencies = [results[m]["latency"] for m in methods]
    bars2 = ax2.bar(methods, latencies, color=colors)
    ax2.set_ylabel('Query Latency (ms)')
    ax2.set_title('Response Time Comparison')
    ax2.tick_params(axis='x', rotation=45)
    
    # 3. Embeddingæ™‚é–“
    ax3 = plt.subplot(2, 3, 3)
    embedding_times = []
    embedding_methods = []
    
    for method in methods:
        if 'embedding_time' in results[method]:
            embedding_times.append(results[method]['embedding_time'])
            embedding_methods.append(method)
    
    if embedding_times:
        ax3.bar(embedding_methods, embedding_times, color=['gold', 'lightblue', 'coral'])
        ax3.set_ylabel('Embedding Time (s)')
        ax3.set_title('Preprocessing Time')
        ax3.tick_params(axis='x', rotation=45)
    
    # 4. åŠ¹ç‡æ€§åˆ†æï¼ˆé€Ÿåº¦ vs ç²¾åº¦ï¼‰
    ax4 = plt.subplot(2, 3, 4)
    for i, method in enumerate(methods):
        ax4.scatter(latencies[i], scores[i], s=200, c=colors[i], 
                   label=method, alpha=0.7, edgecolors='black')
    
    ax4.set_xlabel('Query Latency (ms)')
    ax4.set_ylabel('Relevance Score')
    ax4.set_title('Efficiency Analysis (Upper-Left is Better)')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # 5. æ”¹å–„ç‡
    ax5 = plt.subplot(2, 3, 5)
    baseline_score = results["TF-IDF"]["relevance_score"]
    improvements = []
    method_names = []
    
    for method in methods:
        if method != "TF-IDF":
            improvement = (results[method]["relevance_score"] - baseline_score) / baseline_score * 100
            improvements.append(improvement)
            method_names.append(method)
    
    colors_imp = ['green' if x > 0 else 'red' for x in improvements]
    bars5 = ax5.bar(method_names, improvements, color=colors_imp, alpha=0.7)
    ax5.set_ylabel('Improvement over TF-IDF (%)')
    ax5.set_title('Relative Performance')
    ax5.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax5.tick_params(axis='x', rotation=45)
    
    # 6. Embeddingæ¬¡å…ƒå‰Šæ¸›å¯è¦–åŒ–ï¼ˆt-SNEé¢¨ï¼‰
    ax6 = plt.subplot(2, 3, 6)
    
    # ç°¡æ˜“2DæŠ•å½±ï¼ˆPCAï¼‰
    from sklearn.decomposition import PCA
    pca = PCA(n_components=2)
    
    # å„æ‰‹æ³•ã®åŸ‹ã‚è¾¼ã¿ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    sample_size = min(50, list(embeddings.values())[0].shape[0])
    
    for i, (method, emb) in enumerate(embeddings.items()):
        if emb.shape[0] >= sample_size:
            emb_2d = pca.fit_transform(emb[:sample_size])
            ax6.scatter(emb_2d[:, 0], emb_2d[:, 1], 
                       label=method, alpha=0.6, s=30, c=colors[i])
    
    ax6.set_title('Embedding Space Visualization (PCA)')
    ax6.legend()
    ax6.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('mega_pyg_gedig_results.png', dpi=300, bbox_inches='tight')
    print("\nğŸ“ˆ Visualization saved: mega_pyg_gedig_results.png")

def save_results(results: Dict, num_questions: int):
    """çµæœã‚’JSONå½¢å¼ã§ä¿å­˜"""
    
    save_data = {
        "experiment_info": {
            "name": "MEGA PyG geDIG Experiment",
            "num_questions": num_questions,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "description": "PyTorch Geometric geDIG vs all embedding methods"
        },
        "results": {
            method: {
                "relevance_score": metrics["relevance_score"],
                "relevance_std": metrics["relevance_std"],
                "latency": metrics["latency"],
                "embedding_time": metrics.get("embedding_time", 0)
            }
            for method, metrics in results.items()
        }
    }
    
    with open('mega_pyg_gedig_results.json', 'w') as f:
        json.dump(save_data, f, indent=2)
    
    print("ğŸ’¾ Results saved: mega_pyg_gedig_results.json")

if __name__ == "__main__":
    run_mega_pyg_experiment()