#!/usr/bin/env python3
"""
1000å•ãƒ¬ãƒ™ãƒ«è¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåé›†
================================

è¤‡æ•°ã®HuggingFaceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰1000å•ã‚’åŠ¹ç‡çš„ã«åé›†ã—ã¦
InsightSpike-AIã®çœŸã®å®ŸåŠ›ã‚’è©•ä¾¡ã™ã‚‹
"""

from datasets import load_dataset
from pathlib import Path
import json
import time
from typing import Dict, List, Any
import concurrent.futures
from threading import Lock

def download_dataset_batch(dataset_config):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    name, config, split, samples = dataset_config
    
    try:
        print(f"ğŸ“š Downloading {name} ({samples} samples)...")
        start_time = time.time()
        
        if config:
            dataset = load_dataset(name, config, split=split)
        else:
            dataset = load_dataset(name, split=split)
        
        download_time = time.time() - start_time
        
        return {
            "name": name,
            "dataset": dataset,
            "samples": len(dataset),
            "download_time": download_time,
            "status": "success"
        }
    except Exception as e:
        print(f"âŒ {name} failed: {e}")
        return {
            "name": name,
            "status": "failed",
            "error": str(e)
        }

def download_1000_questions():
    """1000å•ã®è¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåé›†"""
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    data_dir = Path("data/mega_huggingface_datasets")
    data_dir.mkdir(parents=True, exist_ok=True)
    
    print("ğŸŒ 1000å•ãƒ¬ãƒ™ãƒ«è¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåé›†é–‹å§‹!")
    print("=" * 70)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šï¼ˆä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ï¼‰
    dataset_configs = [
        # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå¤§å®¹é‡ï¼‰
        ("squad", None, "validation[:300]", 300),
        ("squad", None, "train[:200]", 200),
        ("ms_marco", "v1.1", "validation[:150]", 150),
        
        # å¤šæ§˜ãªQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        ("natural_questions", None, "validation[:100]", 100),  # å†ãƒãƒ£ãƒ¬ãƒ³ã‚¸
        ("coqa", None, "validation[:80]", 80),
        ("drop", None, "validation[:50]", 50),
        
        # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        ("hotpot_qa", "fullwiki", "validation[:60]", 60),
        ("xnli", None, "validation[:50]", 50),
        ("boolq", None, "validation[:50]", 50),
        
        # è»½é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆç¢ºå®Ÿæ€§é‡è¦–ï¼‰
        ("piqa", None, "validation[:40]", 40),
        ("winogrande", "winogrande_xl", "validation[:30]", 30),
        ("hellaswag", None, "validation[:30]", 30),
        ("arc", "ARC-Easy", "validation[:30]", 30),
        ("arc", "ARC-Challenge", "validation[:20]", 20),
        ("commonsense_qa", None, "validation[:20]", 20),
    ]
    
    print(f"ğŸ¯ ç›®æ¨™: {sum(config[3] for config in dataset_configs)} å•")
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {len(dataset_configs)} ç¨®é¡")
    
    # ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
    print("\nğŸš€ ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
    
    datasets_info = {}
    total_samples = 0
    lock = Lock()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¸¦åˆ—å‡¦ç†ã§é–‹å§‹
        future_to_config = {
            executor.submit(download_dataset_batch, config): config 
            for config in dataset_configs
        }
        
        for future in concurrent.futures.as_completed(future_to_config):
            config = future_to_config[future]
            try:
                result = future.result()
                
                with lock:
                    dataset_name = result["name"]
                    if result["status"] == "success":
                        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜
                        save_path = data_dir / f"{dataset_name}_{result['samples']}"
                        result["dataset"].save_to_disk(str(save_path))
                        
                        datasets_info[f"{dataset_name}_{result['samples']}"] = {
                            "path": str(save_path),
                            "samples": result["samples"],
                            "download_time": result["download_time"],
                            "status": "success"
                        }
                        
                        total_samples += result["samples"]
                        print(f"âœ… {dataset_name}: {result['samples']} samples ({result['download_time']:.1f}s)")
                    else:
                        datasets_info[f"{dataset_name}_failed"] = {
                            "status": "failed",
                            "error": result["error"]
                        }
                        
            except Exception as e:
                print(f"âŒ Unexpected error: {e}")
    
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æƒ…å ±ä¿å­˜
    info_path = data_dir / "mega_download_info.json"
    with open(info_path, 'w') as f:
        json.dump(datasets_info, f, indent=2)
    
    # ã‚µãƒãƒªè¡¨ç¤º
    print(f"\nğŸ“Š è¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåé›†å®Œäº†!")
    print("=" * 70)
    successful_downloads = [k for k, v in datasets_info.items() if v.get("status") == "success"]
    
    print(f"âœ… æˆåŠŸ: {len(successful_downloads)} datasets")
    print(f"ğŸ“ ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {total_samples}")
    print(f"ğŸ’¾ ä¿å­˜å…ˆ: {data_dir}")
    print(f"ğŸ“‹ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æƒ…å ±: {info_path}")
    
    # è©³ç´°ãƒªã‚¹ãƒˆ
    print(f"\nğŸ“š ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:")
    for dataset_name in successful_downloads:
        info = datasets_info[dataset_name]
        print(f"   ğŸ“– {dataset_name}: {info['samples']} samples ({info['download_time']:.1f}s)")
    
    if total_samples >= 1000:
        print(f"\nğŸ¯ ç›®æ¨™é”æˆ! {total_samples}å•ã§è¶…å¤§è¦æ¨¡RAGå®Ÿé¨“æº–å‚™å®Œäº†!")
    elif total_samples >= 500:
        print(f"\nâš¡ éƒ¨åˆ†é”æˆ! {total_samples}å•ã§å¤§è¦æ¨¡RAGå®Ÿé¨“å¯èƒ½!")
    else:
        print(f"\nâš ï¸ ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³: {total_samples}å•ï¼ˆç›®æ¨™: 1000å•ä»¥ä¸Šï¼‰")
    
    # å¤±æ•—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±
    failed_downloads = [k for k, v in datasets_info.items() if v.get("status") == "failed"]
    if failed_downloads:
        print(f"\nâŒ å¤±æ•—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ({len(failed_downloads)}):")
        for dataset_name in failed_downloads:
            error = datasets_info[dataset_name].get("error", "Unknown error")
            print(f"   âŒ {dataset_name}: {error}")
    
    return datasets_info, total_samples

if __name__ == "__main__":
    download_1000_questions()