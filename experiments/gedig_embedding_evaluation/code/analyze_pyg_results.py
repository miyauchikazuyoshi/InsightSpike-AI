#!/usr/bin/env python3
"""
PyG geDIGå®Ÿé¨“çµæœåˆ†æ
===================
"""

import json
import matplotlib.pyplot as plt
import numpy as np

# å®Ÿé¨“çµæœï¼ˆå‡ºåŠ›ã‹ã‚‰æŠ½å‡ºï¼‰
results = {
    "PyG geDIG": {
        "relevance_score": 0.327,
        "relevance_std": 0.434,
        "latency": 5.0,
        "embedding_time": 1.9
    },
    "Original geDIG": {
        "relevance_score": 0.035,
        "relevance_std": 0.131,
        "latency": 0.9,
        "embedding_time": 0.3
    },
    "TF-IDF": {
        "relevance_score": 0.538,
        "relevance_std": 0.481,
        "latency": 2.1,
        "embedding_time": 0.0
    },
    "Sentence-BERT": {
        "relevance_score": 0.633,
        "relevance_std": 0.442,
        "latency": 45.2,
        "embedding_time": 4.5
    }
}

# çµ±è¨ˆçš„æœ‰æ„æ€§
statistical_results = {
    "PyG geDIG vs TF-IDF": {
        "improvement": -39.3,
        "p_value": 0.000876,
        "cohen_d": -0.345,
        "significant": True
    },
    "Original geDIG vs TF-IDF": {
        "improvement": -93.5,
        "p_value": 0.000000,
        "cohen_d": -1.032,
        "significant": True
    },
    "Sentence-BERT vs TF-IDF": {
        "improvement": 17.6,
        "p_value": 0.088451,
        "cohen_d": 0.173,
        "significant": False
    }
}

def analyze_pyg_results():
    """PyG geDIGçµæœã®è©³ç´°åˆ†æ"""
    
    print("ğŸ§  PyG geDIGå®Ÿé¨“çµæœåˆ†æï¼ˆ550å•ï¼‰")
    print("=" * 60)
    
    # 1. æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    print("\nğŸ“Š æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆRelevance Scoreï¼‰:")
    sorted_methods = sorted(results.items(), key=lambda x: x[1]['relevance_score'], reverse=True)
    
    for rank, (method, metrics) in enumerate(sorted_methods, 1):
        print(f"   {rank}. {method}: {metrics['relevance_score']:.3f} Â± {metrics['relevance_std']:.3f}")
    
    # 2. é€Ÿåº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    print("\nâš¡ é€Ÿåº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆQuery Latencyï¼‰:")
    sorted_speed = sorted(results.items(), key=lambda x: x[1]['latency'])
    
    for rank, (method, metrics) in enumerate(sorted_speed, 1):
        print(f"   {rank}. {method}: {metrics['latency']:.1f}ms")
    
    # 3. PyG geDIGç‰¹åˆ¥åˆ†æ
    print("\nğŸ§  PyG geDIGè©³ç´°åˆ†æ:")
    pyg_score = results["PyG geDIG"]["relevance_score"]
    original_score = results["Original geDIG"]["relevance_score"]
    
    pyg_improvement = (pyg_score - original_score) / original_score * 100
    print(f"   PyG vs Originalæ”¹å–„ç‡: +{pyg_improvement:.1f}%")
    print(f"   PyGã¯Originalã®{pyg_score/original_score:.1f}å€ã®æ€§èƒ½")
    
    # 4. çµ±è¨ˆçš„æœ‰æ„æ€§
    print("\nğŸ“Š çµ±è¨ˆçš„æœ‰æ„æ€§ã‚µãƒãƒª:")
    for comparison, stats in statistical_results.items():
        significance = "âœ… æœ‰æ„" if stats["significant"] else "âŒ éæœ‰æ„"
        print(f"   {comparison}:")
        print(f"      æ”¹å–„ç‡: {stats['improvement']:+.1f}%")
        print(f"      på€¤: {stats['p_value']:.6f}")
        print(f"      åŠ¹æœé‡: {stats['cohen_d']:.3f}")
        print(f"      çµæœ: {significance}")
    
    # 5. åŠ¹ç‡æ€§åˆ†æ
    print("\nâš¡ åŠ¹ç‡æ€§åˆ†æï¼ˆç²¾åº¦/é€Ÿåº¦æ¯”ï¼‰:")
    for method, metrics in results.items():
        efficiency = metrics['relevance_score'] / metrics['latency'] * 1000
        print(f"   {method}: {efficiency:.2f} (score/ms Ã— 1000)")
    
    # å¯è¦–åŒ–
    create_detailed_visualization()

def create_detailed_visualization():
    """è©³ç´°ãªå¯è¦–åŒ–"""
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))
    
    methods = list(results.keys())
    colors = ['gold', 'lightblue', 'lightgreen', 'coral']
    
    # 1. æ€§èƒ½æ¯”è¼ƒï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ¼ä»˜ãï¼‰
    scores = [results[m]['relevance_score'] for m in methods]
    stds = [results[m]['relevance_std'] for m in methods]
    
    bars1 = ax1.bar(methods, scores, yerr=stds, capsize=5, color=colors)
    ax1.set_ylabel('Relevance Score')
    ax1.set_title('Retrieval Performance Comparison (550 Questions)')
    ax1.tick_params(axis='x', rotation=45)
    
    for bar, score in zip(bars1, scores):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                f'{score:.3f}', ha='center', va='bottom')
    
    # 2. ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¯”è¼ƒï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
    latencies = [results[m]['latency'] for m in methods]
    bars2 = ax2.bar(methods, latencies, color=colors)
    ax2.set_ylabel('Query Latency (ms)')
    ax2.set_title('Response Time Comparison')
    ax2.set_yscale('log')
    ax2.tick_params(axis='x', rotation=45)
    
    for bar, latency in zip(bars2, latencies):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1,
                f'{latency:.1f}', ha='center', va='bottom')
    
    # 3. PyG vs Original geDIGæ¯”è¼ƒ
    gedig_methods = ['PyG geDIG', 'Original geDIG']
    gedig_scores = [results[m]['relevance_score'] for m in gedig_methods]
    gedig_latencies = [results[m]['latency'] for m in gedig_methods]
    
    x = np.arange(len(gedig_methods))
    width = 0.35
    
    bars3_1 = ax3.bar(x - width/2, gedig_scores, width, label='Relevance Score', color='gold')
    ax3_2 = ax3.twinx()
    bars3_2 = ax3_2.bar(x + width/2, gedig_latencies, width, label='Latency (ms)', color='lightblue')
    
    ax3.set_xlabel('geDIG Variants')
    ax3.set_ylabel('Relevance Score', color='gold')
    ax3_2.set_ylabel('Latency (ms)', color='lightblue')
    ax3.set_title('PyG vs Original geDIG Comparison')
    ax3.set_xticks(x)
    ax3.set_xticklabels(gedig_methods)
    ax3.tick_params(axis='y', labelcolor='gold')
    ax3_2.tick_params(axis='y', labelcolor='lightblue')
    
    # 4. åŠ¹ç‡æ€§ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
    scores_all = [results[m]['relevance_score'] for m in methods]
    latencies_all = [results[m]['latency'] for m in methods]
    
    for i, (method, score, latency) in enumerate(zip(methods, scores_all, latencies_all)):
        ax4.scatter(latency, score, s=300, c=colors[i], label=method, 
                   alpha=0.7, edgecolors='black', linewidth=2)
        ax4.annotate(method, (latency, score), xytext=(5, 5), 
                    textcoords='offset points', fontsize=9)
    
    ax4.set_xlabel('Query Latency (ms)')
    ax4.set_ylabel('Relevance Score')
    ax4.set_title('Efficiency Matrix (Upper-Left is Better)')
    ax4.grid(True, alpha=0.3)
    ax4.set_xlim(0, max(latencies_all) * 1.2)
    ax4.set_ylim(0, max(scores_all) * 1.1)
    
    # æœ€é©é ˜åŸŸã‚’å¼·èª¿
    ax4.axvspan(0, 5, alpha=0.1, color='green', label='Fast Zone')
    ax4.axhspan(0.5, 1.0, alpha=0.1, color='green', label='High Accuracy Zone')
    
    plt.tight_layout()
    plt.savefig('pyg_gedig_analysis.png', dpi=300, bbox_inches='tight')
    print("\nğŸ“ˆ å¯è¦–åŒ–ä¿å­˜: pyg_gedig_analysis.png")

def conclusions():
    """æœ€çµ‚çµè«–"""
    
    print("\nğŸ¯ PyG geDIGå®Ÿé¨“æœ€çµ‚çµè«–:")
    print("=" * 60)
    
    print("\nâœ… **ä¸»è¦ç™ºè¦‹:**")
    print("1. PyG geDIGã¯Original geDIGã®**9.3å€**ã®æ€§èƒ½å‘ä¸Š")
    print("2. Sentence-BERTãŒæœ€é«˜æ€§èƒ½ï¼ˆ0.633ï¼‰ã‚’é”æˆ")
    print("3. PyG geDIGï¼ˆ0.327ï¼‰ã¯TF-IDFï¼ˆ0.538ï¼‰ã«åŠã°ãš")
    print("4. çµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ã‚’ç¢ºèªï¼ˆp < 0.001ï¼‰")
    
    print("\nâš¡ **é€Ÿåº¦åˆ†æ:**")
    print("1. Original geDIG: 0.9msï¼ˆæœ€é€Ÿï¼‰")
    print("2. TF-IDF: 2.1ms")
    print("3. PyG geDIG: 5.0msï¼ˆä¸­é€Ÿï¼‰")
    print("4. Sentence-BERT: 45.2msï¼ˆæœ€é…ï¼‰")
    
    print("\nğŸ§  **PyG geDIGã®æ„ç¾©:**")
    print("1. ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å¯èƒ½æ€§ã‚’å®Ÿè¨¼")
    print("2. CPUç’°å¢ƒã§ã‚‚å®Ÿç”¨çš„ãªé€Ÿåº¦ï¼ˆ5msï¼‰")
    print("3. GPUç’°å¢ƒã§ã¯æ›´ãªã‚‹é«˜é€ŸåŒ–ãŒæœŸå¾…")
    print("4. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ”¹å–„ã§ç²¾åº¦å‘ä¸Šã®ä½™åœ°å¤§")
    
    print("\nğŸš€ **ä»Šå¾Œã®æ”¹å–„ææ¡ˆ:**")
    print("1. Graph Attention Networks (GAT) ã®å°å…¥")
    print("2. äº‹å‰å­¦ç¿’æ¸ˆã¿ã‚°ãƒ©ãƒ•è¡¨ç¾ã®æ´»ç”¨")
    print("3. Î”GEDÃ—Î”IGè¨ˆç®—ã®æœ€é©åŒ–")
    print("4. ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã‚°ãƒ©ãƒ•ç‰¹å¾´ã®çµ±åˆ")

if __name__ == "__main__":
    analyze_pyg_results()
    conclusions()