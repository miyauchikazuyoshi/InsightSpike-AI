#!/usr/bin/env python3
"""
å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
===============================

è¤‡æ•°ã®HuggingFaceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰100-200å•ã‚’åé›†ã—ã¦
InsightSpike-AIã®çœŸã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹
"""

from datasets import load_dataset
from pathlib import Path
import json
import time
from typing import Dict, List, Any

def download_large_datasets():
    """è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰å¤§è¦æ¨¡ã‚µãƒ³ãƒ—ãƒ«ã‚’åé›†"""
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    data_dir = Path("data/large_huggingface_datasets")
    data_dir.mkdir(parents=True, exist_ok=True)
    
    print("ğŸŒ å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåé›†é–‹å§‹...")
    print("=" * 60)
    
    datasets_info = {}
    total_samples = 0
    
    # 1. SQuADï¼ˆã‚ˆã‚Šå¤šãï¼‰
    print("\nğŸ“š Step 1: SQuAD (100 samples)...")
    try:
        start_time = time.time()
        squad_dataset = load_dataset("squad", split="validation[:100]")
        download_time = time.time() - start_time
        
        squad_path = data_dir / "squad_100"
        squad_dataset.save_to_disk(str(squad_path))
        
        print(f"âœ… SQuAD: {len(squad_dataset)} samples ({download_time:.1f}s)")
        
        datasets_info["squad_100"] = {
            "path": str(squad_path),
            "samples": len(squad_dataset),
            "download_time": download_time,
            "status": "success"
        }
        total_samples += len(squad_dataset)
        
    except Exception as e:
        print(f"âŒ SQuAD failed: {e}")
        datasets_info["squad_100"] = {"status": "failed", "error": str(e)}
    
    # 2. MS MARCOï¼ˆã‚ˆã‚Šå¤šãï¼‰
    print("\nğŸ” Step 2: MS MARCO (50 samples)...")
    try:
        start_time = time.time()
        marco_dataset = load_dataset("ms_marco", "v1.1", split="validation[:50]")
        download_time = time.time() - start_time
        
        marco_path = data_dir / "ms_marco_50"
        marco_dataset.save_to_disk(str(marco_path))
        
        print(f"âœ… MS MARCO: {len(marco_dataset)} samples ({download_time:.1f}s)")
        
        datasets_info["ms_marco_50"] = {
            "path": str(marco_path),
            "samples": len(marco_dataset),
            "download_time": download_time,
            "status": "success"
        }
        total_samples += len(marco_dataset)
        
    except Exception as e:
        print(f"âŒ MS MARCO failed: {e}")
        datasets_info["ms_marco_50"] = {"status": "failed", "error": str(e)}
    
    # 3. CoQAï¼ˆå¯¾è©±å‹QAï¼‰
    print("\nğŸ’¬ Step 3: CoQA (30 samples)...")
    try:
        start_time = time.time()
        coqa_dataset = load_dataset("coqa", split="validation[:30]")
        download_time = time.time() - start_time
        
        coqa_path = data_dir / "coqa_30"
        coqa_dataset.save_to_disk(str(coqa_path))
        
        print(f"âœ… CoQA: {len(coqa_dataset)} samples ({download_time:.1f}s)")
        
        datasets_info["coqa_30"] = {
            "path": str(coqa_path),
            "samples": len(coqa_dataset),
            "download_time": download_time,
            "status": "success"
        }
        total_samples += len(coqa_dataset)
        
    except Exception as e:
        print(f"âŒ CoQA failed: {e}")
        datasets_info["coqa_30"] = {"status": "failed", "error": str(e)}
    
    # 4. QuACï¼ˆæ–‡è„ˆQAï¼‰
    print("\nğŸ“– Step 4: QuAC (20 samples)...")
    try:
        start_time = time.time()
        quac_dataset = load_dataset("quac", split="validation[:20]")
        download_time = time.time() - start_time
        
        quac_path = data_dir / "quac_20"
        quac_dataset.save_to_disk(str(quac_path))
        
        print(f"âœ… QuAC: {len(quac_dataset)} samples ({download_time:.1f}s)")
        
        datasets_info["quac_20"] = {
            "path": str(quac_path),
            "samples": len(quac_dataset),
            "download_time": download_time,
            "status": "success"
        }
        total_samples += len(quac_dataset)
        
    except Exception as e:
        print(f"âŒ QuAC failed: {e}")
        datasets_info["quac_20"] = {"status": "failed", "error": str(e)}
    
    # 5. DROPï¼ˆæ•°å€¤æ¨è«–ï¼‰
    print("\nğŸ”¢ Step 5: DROP (20 samples)...")
    try:
        start_time = time.time()
        drop_dataset = load_dataset("drop", split="validation[:20]")
        download_time = time.time() - start_time
        
        drop_path = data_dir / "drop_20"
        drop_dataset.save_to_disk(str(drop_path))
        
        print(f"âœ… DROP: {len(drop_dataset)} samples ({download_time:.1f}s)")
        
        datasets_info["drop_20"] = {
            "path": str(drop_path),
            "samples": len(drop_dataset),
            "download_time": download_time,
            "status": "success"
        }
        total_samples += len(drop_dataset)
        
    except Exception as e:
        print(f"âŒ DROP failed: {e}")
        datasets_info["drop_20"] = {"status": "failed", "error": str(e)}
    
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æƒ…å ±ä¿å­˜
    info_path = data_dir / "large_download_info.json"
    with open(info_path, 'w') as f:
        json.dump(datasets_info, f, indent=2)
    
    # ã‚µãƒãƒªè¡¨ç¤º
    print(f"\nğŸ“Š å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåé›†å®Œäº†!")
    print("=" * 60)
    successful = [k for k, v in datasets_info.items() if v.get("status") == "success"]
    print(f"âœ… æˆåŠŸ: {len(successful)} datasets")
    print(f"ğŸ“ ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {total_samples}")
    print(f"ğŸ’¾ ä¿å­˜å…ˆ: {data_dir}")
    
    for dataset_name in successful:
        info = datasets_info[dataset_name]
        print(f"   ğŸ“š {dataset_name}: {info['samples']} samples ({info['download_time']:.1f}s)")
    
    if total_samples >= 100:
        print(f"\nğŸ¯ ç›®æ¨™é”æˆ! {total_samples}å•ã§å¤§è¦æ¨¡RAGå®Ÿé¨“æº–å‚™å®Œäº†!")
    else:
        print(f"\nâš ï¸ ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³: {total_samples}å•ï¼ˆç›®æ¨™: 100å•ä»¥ä¸Šï¼‰")
    
    return datasets_info, total_samples

if __name__ == "__main__":
    download_large_datasets()