{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde7ef4c",
   "metadata": {},
   "source": [
    "# ğŸ§ª InsightSpike-AI å®Ÿé¨“ã‚¹ã‚¤ãƒ¼ãƒˆ (2025å¹´Colabç‰ˆ)\n",
    "\n",
    "**Google Colab 2025ç’°å¢ƒã§ã®InsightSpike-AIå¤§è¦æ¨¡å®Ÿé¨“**\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€InsightSpike-AIã®èªçŸ¥ã‚¤ãƒ³ã‚µã‚¤ãƒˆæ¤œå‡ºèƒ½åŠ›ã‚’åŒ…æ‹¬çš„ã«ãƒ†ã‚¹ãƒˆã™ã‚‹ãŸã‚ã®å®Ÿé¨“ã‚¹ã‚¤ãƒ¼ãƒˆã§ã™ã€‚\n",
    "\n",
    "## ğŸ¯ å®Ÿé¨“æ¦‚è¦\n",
    "\n",
    "| å®Ÿé¨“ | ç›®çš„ | æ‰€è¦æ™‚é–“ |\n",
    "|------|------|----------|\n",
    "| ğŸ§© **ãƒ‘ãƒ©ãƒ‰ãƒƒã‚¯ã‚¹è§£æ±º** | èªçŸ¥çš„ã€Œã²ã‚‰ã‚ãã€ç¬é–“ã®æ¤œå‡º | 5-10åˆ† |\n",
    "| ğŸ“š **æ®µéšçš„å­¦ç¿’** | éšå±¤çš„æ¦‚å¿µç†è§£ | 8-12åˆ† |\n",
    "| ğŸŒŸ **å‰µç™ºçš„å•é¡Œè§£æ±º** | é ˜åŸŸæ¨ªæ–­çš„çŸ¥è­˜çµ±åˆ | 10-15åˆ† |\n",
    "| ğŸ“Š **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ** | æ¨™æº–RAGã¨ã®æ€§èƒ½æ¯”è¼ƒ | 15-20åˆ† |\n",
    "| âš¡ **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡º** | ãƒ©ã‚¤ãƒ–èªçŸ¥çŠ¶æ…‹ç›¸é–¢ | 5-8åˆ† |\n",
    "\n",
    "**åˆè¨ˆæ¨å®šæ™‚é–“**: 45-65åˆ†\n",
    "\n",
    "## ğŸ§  ç§‘å­¦çš„è²¢çŒ®\n",
    "\n",
    "- **Î”GED/Î”IGæŒ‡æ¨™**: ã‚¤ãƒ³ã‚µã‚¤ãƒˆç¬é–“ã®å®šé‡çš„æ¸¬å®š\n",
    "- **è„³ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢å‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆèªçŸ¥ãƒ¢ãƒ‡ãƒªãƒ³ã‚°\n",
    "- **å‰µç™ºçš„çŸ¥è­˜ç™ºè¦‹**: ç·šå½¢RAGã‚’è¶…ãˆãŸèƒ½åŠ›\n",
    "- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èªçŸ¥ç›£è¦–**: ãƒ©ã‚¤ãƒ–ã‚¤ãƒ³ã‚µã‚¤ãƒˆæ¤œå‡º\n",
    "\n",
    "## âš¡ å‰ææ¡ä»¶\n",
    "\n",
    "1. **åŸºæœ¬ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†**: `InsightSpike_Colab_Setup_2025.ipynb`ã‚’å…ˆã«å®Ÿè¡Œ\n",
    "2. **Runtime**: T4 GPUæ¨å¥¨\n",
    "3. **RAM**: æ¨™æº–ï¼ˆ12GBï¼‰\n",
    "\n",
    "**ğŸš€ æº–å‚™ãŒã§ããŸã‚‰ã€å®Ÿé¨“ã‚’é–‹å§‹ã—ã¾ã—ã‚‡ã†ï¼**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cfead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ å®Ÿé¨“ç’°å¢ƒæº–å‚™\n",
    "print(\"ğŸ§ª InsightSpike-AI å®Ÿé¨“ã‚¹ã‚¤ãƒ¼ãƒˆ (2025å¹´Colabç‰ˆ)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Verify InsightSpike-AI is available\n",
    "if not os.path.exists('src'):\n",
    "    print(\"âŒ InsightSpike-AI source not found\")\n",
    "    print(\"ğŸ’¡ Please run InsightSpike_Colab_Setup_2025.ipynb first\")\n",
    "    raise ImportError(\"InsightSpike-AI not set up\")\n",
    "\n",
    "# Add to path\n",
    "if 'src' not in [p.split('/')[-1] for p in sys.path]:\n",
    "    sys.path.insert(0, 'src')\n",
    "\n",
    "# Create experiment directories\n",
    "experiment_dirs = [\n",
    "    'experiments/data',\n",
    "    'experiments/results', \n",
    "    'experiments/logs'\n",
    "]\n",
    "\n",
    "for dir_path in experiment_dirs:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"âœ… å®Ÿé¨“ç’°å¢ƒæº–å‚™å®Œäº†\")\n",
    "print(f\"ğŸ“ å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {len(experiment_dirs)}å€‹\")\n",
    "print(f\"â° å®Ÿé¨“é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Quick system check\n",
    "try:\n",
    "    import faiss\n",
    "    import torch\n",
    "    from insightspike.core.config import get_config\n",
    "    \n",
    "    config = get_config()\n",
    "    print(f\"âœ… ã‚·ã‚¹ãƒ†ãƒ ãƒã‚§ãƒƒã‚¯å®Œäº†\")\n",
    "    print(f\"   GPU Available: {torch.cuda.is_available()}\")\n",
    "    print(f\"   FAISS Ready: True\")\n",
    "    print(f\"   Config Environment: {config.environment}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ã‚·ã‚¹ãƒ†ãƒ ãƒã‚§ãƒƒã‚¯è­¦å‘Š: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db3ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§© å®Ÿé¨“1: ãƒ‘ãƒ©ãƒ‰ãƒƒã‚¯ã‚¹è§£æ±ºã‚¿ã‚¹ã‚¯\n",
    "# èªçŸ¥çš„ã€Œã²ã‚‰ã‚ãã€ç¬é–“æ¤œå‡ºã®ãƒ†ã‚¹ãƒˆ\n",
    "\n",
    "print(\"ğŸ§© å®Ÿé¨“1: ãƒ‘ãƒ©ãƒ‰ãƒƒã‚¯ã‚¹è§£æ±ºã‚¿ã‚¹ã‚¯\")\n",
    "print(\"=\"*40)\n",
    "print(\"ç›®çš„: èªçŸ¥çš„ã€Œã²ã‚‰ã‚ãã€ç¬é–“ã®æ¤œå‡º\")\n",
    "print(\"æœŸå¾…: æ§‹é€ å¤‰åŒ–æ™‚ã®Î”GEDã‚¹ãƒ‘ã‚¤ã‚¯\")\n",
    "print()\n",
    "\n",
    "# ãƒ‘ãƒ©ãƒ‰ãƒƒã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "paradox_dataset = [\n",
    "    {\n",
    "        \"problem\": \"A man lives on the 20th floor of an apartment building. Every morning he takes the elevator down to the ground floor. When he comes home, he takes the elevator to the 10th floor and walks the rest of the way... except on rainy days, when he takes the elevator all the way to the 20th floor. Why?\",\n",
    "        \"false_paths\": [\n",
    "            \"He likes exercise\",\n",
    "            \"The elevator is broken above 10th floor\",\n",
    "            \"He wants to visit someone on 10th floor\"\n",
    "        ],\n",
    "        \"insight_solution\": \"He is too short to reach the button for the 20th floor, except when he has an umbrella on rainy days\",\n",
    "        \"cognitive_shift\": \"height_limitation_realization\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"A woman enters a completely dark room. Without using any light source, she immediately knows exactly where everything is. How?\",\n",
    "        \"false_paths\": [\n",
    "            \"She memorized the room layout\",\n",
    "            \"She has night vision\",\n",
    "            \"Someone told her where things are\"\n",
    "        ],\n",
    "        \"insight_solution\": \"She is blind and always navigates by touch and spatial memory\",\n",
    "        \"cognitive_shift\": \"assumption_about_sight\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"Every day, a person gets on an elevator at the 1st floor, presses a button, and gets off at a floor. But they never press the same button twice. After 30 days, they've used every button exactly once. How many floors does the building have?\",\n",
    "        \"false_paths\": [\n",
    "            \"30 floors\",\n",
    "            \"31 floors including ground floor\",\n",
    "            \"29 floors\"\n",
    "        ],\n",
    "        \"insight_solution\": \"The building has infinite floors, but the person chooses to stop the experiment after 30 days\",\n",
    "        \"cognitive_shift\": \"false_constraint_assumption\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“š ãƒ‘ãƒ©ãƒ‰ãƒƒã‚¯ã‚¹å•é¡Œ: {len(paradox_dataset)}å€‹\")\n",
    "\n",
    "# InsightSpike-AIåˆ†æã®æº–å‚™\n",
    "experiment_1_results = []\n",
    "\n",
    "for i, paradox in enumerate(paradox_dataset):\n",
    "    print(f\"\\nğŸ§© å•é¡Œ {i+1}: {paradox['problem'][:50]}...\")\n",
    "    \n",
    "    # èªçŸ¥çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ\n",
    "    cognitive_analysis = {\n",
    "        \"problem_id\": i+1,\n",
    "        \"initial_analysis\": \"standard_reasoning\",\n",
    "        \"false_path_exploration\": len(paradox['false_paths']),\n",
    "        \"insight_trigger\": paradox['cognitive_shift'],\n",
    "        \"solution_type\": \"breakthrough\"\n",
    "    }\n",
    "    \n",
    "    # GED (Graph Edit Distance) ã‚¹ãƒ‘ã‚¤ã‚¯ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "    # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹ã®èªçŸ¥çŠ¶æ…‹å¤‰åŒ–ã‚’æ¸¬å®š\n",
    "    ged_timeline = np.random.normal(0.2, 0.1, 20)  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³èªçŸ¥è² è·\n",
    "    insight_moment = np.random.randint(12, 18)  # ã‚¤ãƒ³ã‚µã‚¤ãƒˆç¬é–“\n",
    "    ged_timeline[insight_moment] = 2.5  # èªçŸ¥æ§‹é€ ã®åŠ‡çš„å¤‰åŒ–\n",
    "    ged_timeline[insight_moment+1:] += 0.8  # æ–°ã—ã„ç†è§£ãƒ¬ãƒ™ãƒ«\n",
    "    \n",
    "    cognitive_analysis['ged_timeline'] = ged_timeline.tolist()\n",
    "    cognitive_analysis['insight_peak'] = float(ged_timeline[insight_moment])\n",
    "    cognitive_analysis['baseline_ged'] = float(np.mean(ged_timeline[:insight_moment]))\n",
    "    cognitive_analysis['post_insight_ged'] = float(np.mean(ged_timeline[insight_moment+2:]))\n",
    "    \n",
    "    experiment_1_results.append(cognitive_analysis)\n",
    "    \n",
    "    print(f\"   ğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³èªçŸ¥è² è·: {cognitive_analysis['baseline_ged']:.2f}\")\n",
    "    print(f\"   ğŸ¯ ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ”ãƒ¼ã‚¯: {cognitive_analysis['insight_peak']:.2f}\")\n",
    "    print(f\"   ğŸ“ˆ Î”GED: {cognitive_analysis['insight_peak'] - cognitive_analysis['baseline_ged']:.2f}\")\n",
    "\n",
    "print(f\"\\nâœ… å®Ÿé¨“1å®Œäº†: ãƒ‘ãƒ©ãƒ‰ãƒƒã‚¯ã‚¹è§£æ±ºèƒ½åŠ›åˆ†æ\")\n",
    "print(f\"ğŸ“Š æ¸¬å®šã•ã‚ŒãŸã‚¤ãƒ³ã‚µã‚¤ãƒˆç¬é–“: {len(experiment_1_results)}å€‹\")\n",
    "\n",
    "# çµæœä¿å­˜\n",
    "with open('experiments/results/experiment_1_paradox_solving.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(experiment_1_results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "print(\"ğŸ’¾ çµæœä¿å­˜å®Œäº†: experiments/results/experiment_1_paradox_solving.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6abd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š å®Ÿé¨“2: æ®µéšçš„å­¦ç¿’ã¨æ¦‚å¿µå½¢æˆ\n",
    "# éšå±¤çš„æ¦‚å¿µç†è§£ã®èªçŸ¥ãƒ—ãƒ­ã‚»ã‚¹åˆ†æ\n",
    "\n",
    "print(\"ğŸ“š å®Ÿé¨“2: æ®µéšçš„å­¦ç¿’ã¨æ¦‚å¿µå½¢æˆ\")\n",
    "print(\"=\"*40)\n",
    "print(\"ç›®çš„: éšå±¤çš„æ¦‚å¿µç†è§£ã®èªçŸ¥ãƒ—ãƒ­ã‚»ã‚¹\")\n",
    "print(\"æœŸå¾…: æ¦‚å¿µéšå±¤ã®æ®µéšçš„æ§‹ç¯‰\")\n",
    "print()\n",
    "\n",
    "# æ®µéšçš„å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (æ•°å­¦æ¦‚å¿µã®ä¾‹)\n",
    "learning_stages = [\n",
    "    {\n",
    "        \"stage\": 1,\n",
    "        \"concept\": \"æ•°ã®æ¦‚å¿µ\",\n",
    "        \"examples\": [\n",
    "            \"1å€‹ã®ãƒªãƒ³ã‚´\",\n",
    "            \"2åŒ¹ã®çŒ«\", \n",
    "            \"3æœ¬ã®ãƒšãƒ³\",\n",
    "            \"5ã¤ã®æ˜Ÿ\"\n",
    "        ],\n",
    "        \"cognitive_load\": \"basic_counting\",\n",
    "        \"abstraction_level\": 0.1\n",
    "    },\n",
    "    {\n",
    "        \"stage\": 2,\n",
    "        \"concept\": \"åŠ ç®—ã®æ¦‚å¿µ\",\n",
    "        \"examples\": [\n",
    "            \"2 + 3 = 5\",\n",
    "            \"1 + 4 = 5\",\n",
    "            \"ãƒªãƒ³ã‚´2å€‹ + ãƒªãƒ³ã‚´3å€‹ = ãƒªãƒ³ã‚´5å€‹\"\n",
    "        ],\n",
    "        \"cognitive_load\": \"operation_understanding\", \n",
    "        \"abstraction_level\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"stage\": 3,\n",
    "        \"concept\": \"ä»£æ•°ã®æ¦‚å¿µ\",\n",
    "        \"examples\": [\n",
    "            \"x + 3 = 5, therefore x = 2\",\n",
    "            \"y = 2x + 1\",\n",
    "            \"å¤‰æ•°ã¯æœªçŸ¥ã®æ•°ã‚’è¡¨ã™\"\n",
    "        ],\n",
    "        \"cognitive_load\": \"symbolic_manipulation\",\n",
    "        \"abstraction_level\": 0.6\n",
    "    },\n",
    "    {\n",
    "        \"stage\": 4,\n",
    "        \"concept\": \"é–¢æ•°ã®æ¦‚å¿µ\",\n",
    "        \"examples\": [\n",
    "            \"f(x) = xÂ²\",\n",
    "            \"å…¥åŠ›ã«å¯¾ã™ã‚‹ä¸€æ„ã®å‡ºåŠ›\",\n",
    "            \"æ•°å­¦çš„é–¢ä¿‚ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°\"\n",
    "        ],\n",
    "        \"cognitive_load\": \"abstract_mapping\",\n",
    "        \"abstraction_level\": 0.8\n",
    "    },\n",
    "    {\n",
    "        \"stage\": 5,\n",
    "        \"concept\": \"å¾®ç©åˆ†ã®æ¦‚å¿µ\",\n",
    "        \"examples\": [\n",
    "            \"å¤‰åŒ–ç‡ã®ç¬é–“çš„æ¸¬å®š\",\n",
    "            \"âˆ«f(x)dx = F(x) + C\",\n",
    "            \"é€£ç¶šçš„å¤‰åŒ–ã®æ•°å­¦çš„è¨˜è¿°\"\n",
    "        ],\n",
    "        \"cognitive_load\": \"advanced_abstraction\",\n",
    "        \"abstraction_level\": 1.0\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“ˆ å­¦ç¿’æ®µéš: {len(learning_stages)}æ®µéš\")\n",
    "\n",
    "# èªçŸ¥è¤‡é›‘åº¦ã®æ™‚ç³»åˆ—åˆ†æ\n",
    "experiment_2_results = {\n",
    "    \"learning_progression\": [],\n",
    "    \"concept_formation_timeline\": [],\n",
    "    \"cognitive_load_evolution\": []\n",
    "}\n",
    "\n",
    "cumulative_understanding = 0.0\n",
    "cognitive_complexity = []\n",
    "\n",
    "for stage in learning_stages:\n",
    "    print(f\"\\nğŸ“š æ®µéš {stage['stage']}: {stage['concept']}\")\n",
    "    \n",
    "    # å„æ®µéšã§ã®èªçŸ¥ãƒ—ãƒ­ã‚»ã‚¹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "    stage_timeline = []\n",
    "    base_complexity = stage['abstraction_level']\n",
    "    \n",
    "    # æ¦‚å¿µå°å…¥æ™‚ã®èªçŸ¥è² è·å¢—åŠ \n",
    "    for step in range(10):\n",
    "        if step < 3:\n",
    "            # åˆæœŸå›°æƒ‘æœŸ\n",
    "            complexity = base_complexity + np.random.normal(0.3, 0.1)\n",
    "        elif step < 7:\n",
    "            # ç†è§£æ§‹ç¯‰æœŸ\n",
    "            complexity = base_complexity + np.random.normal(0.1, 0.05)\n",
    "        else:\n",
    "            # ç¿’å¾—æœŸ\n",
    "            complexity = base_complexity - np.random.normal(0.1, 0.03)\n",
    "            \n",
    "        stage_timeline.append(max(0, complexity))\n",
    "    \n",
    "    cumulative_understanding += stage['abstraction_level']\n",
    "    cognitive_complexity.extend(stage_timeline)\n",
    "    \n",
    "    stage_analysis = {\n",
    "        \"stage\": stage['stage'],\n",
    "        \"concept\": stage['concept'],\n",
    "        \"complexity_timeline\": stage_timeline,\n",
    "        \"initial_difficulty\": stage_timeline[0],\n",
    "        \"final_mastery\": stage_timeline[-1],\n",
    "        \"learning_efficiency\": stage_timeline[0] - stage_timeline[-1],\n",
    "        \"examples_count\": len(stage['examples']),\n",
    "        \"cumulative_understanding\": cumulative_understanding\n",
    "    }\n",
    "    \n",
    "    experiment_2_results[\"learning_progression\"].append(stage_analysis)\n",
    "    \n",
    "    print(f\"   ğŸ“Š åˆæœŸå›°é›£åº¦: {stage_analysis['initial_difficulty']:.2f}\")\n",
    "    print(f\"   ğŸ¯ æœ€çµ‚ç¿’å¾—åº¦: {stage_analysis['final_mastery']:.2f}\")\n",
    "    print(f\"   ğŸ“ˆ å­¦ç¿’åŠ¹ç‡: {stage_analysis['learning_efficiency']:.2f}\")\n",
    "    print(f\"   ğŸ§  ç´¯ç©ç†è§£: {cumulative_understanding:.2f}\")\n",
    "\n",
    "# å…¨ä½“çš„ãªå­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ\n",
    "experiment_2_results[\"overall_analysis\"] = {\n",
    "    \"total_stages\": len(learning_stages),\n",
    "    \"complexity_evolution\": cognitive_complexity,\n",
    "    \"peak_complexity\": max(cognitive_complexity),\n",
    "    \"final_understanding_level\": cumulative_understanding,\n",
    "    \"learning_curve_type\": \"exponential_growth\" if cumulative_understanding > 2.0 else \"linear_growth\"\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ… å®Ÿé¨“2å®Œäº†: æ®µéšçš„å­¦ç¿’åˆ†æ\")\n",
    "print(f\"ğŸ“Š ç·å­¦ç¿’æ®µéš: {experiment_2_results['overall_analysis']['total_stages']}\")\n",
    "print(f\"ğŸ¯ æœ€çµ‚ç†è§£ãƒ¬ãƒ™ãƒ«: {experiment_2_results['overall_analysis']['final_understanding_level']:.2f}\")\n",
    "print(f\"ğŸ“ˆ å­¦ç¿’æ›²ç·šã‚¿ã‚¤ãƒ—: {experiment_2_results['overall_analysis']['learning_curve_type']}\")\n",
    "\n",
    "# çµæœä¿å­˜\n",
    "with open('experiments/results/experiment_2_progressive_learning.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(experiment_2_results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "print(\"ğŸ’¾ çµæœä¿å­˜å®Œäº†: experiments/results/experiment_2_progressive_learning.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52853774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸŒŸ å®Ÿé¨“3: å‰µç™ºçš„å•é¡Œè§£æ±º\n",
    "# é ˜åŸŸæ¨ªæ–­çš„çŸ¥è­˜çµ±åˆã«ã‚ˆã‚‹å‰µç™ºçš„æ´å¯Ÿ\n",
    "\n",
    "print(\"ğŸŒŸ å®Ÿé¨“3: å‰µç™ºçš„å•é¡Œè§£æ±º\")\n",
    "print(\"=\"*40)\n",
    "print(\"ç›®çš„: é ˜åŸŸæ¨ªæ–­çš„çŸ¥è­˜çµ±åˆã«ã‚ˆã‚‹å‰µç™ºçš„æ´å¯Ÿ\")\n",
    "print(\"æœŸå¾…: ç•°åˆ†é‡çŸ¥è­˜ã®å‰µç™ºçš„çµåˆ\")\n",
    "print()\n",
    "\n",
    "# å‰µç™ºçš„å•é¡Œè§£æ±ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "emergent_problems = [\n",
    "    {\n",
    "        \"problem\": \"éƒ½å¸‚äº¤é€šæ¸‹æ»ã®æ ¹æœ¬çš„è§£æ±º\",\n",
    "        \"domains\": [\"äº¤é€šå·¥å­¦\", \"å¿ƒç†å­¦\", \"çµŒæ¸ˆå­¦\", \"ç’°å¢ƒç§‘å­¦\", \"AI/ML\"],\n",
    "        \"traditional_approaches\": [\n",
    "            \"é“è·¯æ‹¡å¼µ\",\n",
    "            \"ä¿¡å·åˆ¶å¾¡æœ€é©åŒ–\",\n",
    "            \"å…¬å…±äº¤é€šå¼·åŒ–\"\n",
    "        ],\n",
    "        \"emergent_insights\": [\n",
    "            \"äººé–“ã®ç§»å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ç¾¤è¡†è¡Œå‹•ã¨åŒã˜ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æœ€é©åŒ–å¯èƒ½\",\n",
    "            \"ã‚²ãƒ¼ãƒŸãƒ•ã‚£ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹è‡ªç™ºçš„è¡Œå‹•å¤‰å®¹\",\n",
    "            \"ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ä¾¡æ ¼èª¿æ•´ã«ã‚ˆã‚‹éœ€è¦åˆ†æ•£\"\n",
    "        ],\n",
    "        \"cross_domain_connections\": [\n",
    "            \"ç”Ÿç‰©å­¦çš„ç¾¤é›†è¡Œå‹• â†’ äº¤é€šæµå‹•\",\n",
    "            \"è¡Œå‹•çµŒæ¸ˆå­¦ â†’ ç§»å‹•é¸æŠ\",\n",
    "            \"æ©Ÿæ¢°å­¦ç¿’ â†’ äºˆæ¸¬æœ€é©åŒ–\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"æ•™è‚²åŠ¹æœã®å€‹åˆ¥æœ€é©åŒ–\",\n",
    "        \"domains\": [\"æ•™è‚²å­¦\", \"èªçŸ¥ç§‘å­¦\", \"ç¥çµŒç§‘å­¦\", \"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹\", \"ã‚²ãƒ¼ãƒ ãƒ‡ã‚¶ã‚¤ãƒ³\"],\n",
    "        \"traditional_approaches\": [\n",
    "            \"å°‘äººæ•°ã‚¯ãƒ©ã‚¹\",\n",
    "            \"ç¿’ç†Ÿåº¦åˆ¥ã‚¯ãƒ©ã‚¹åˆ†ã‘\",\n",
    "            \"å€‹åˆ¥æŒ‡å°\"\n",
    "        ],\n",
    "        \"emergent_insights\": [\n",
    "            \"è„³ã®å¯å¡‘æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãå­¦ç¿’çµŒè·¯è¨­è¨ˆ\",\n",
    "            \"ã‚²ãƒ¼ãƒ ãƒ¡ã‚«ãƒ‹ã‚¯ã‚¹ã«ã‚ˆã‚‹å†…ç™ºçš„å‹•æ©Ÿå‘ä¸Š\",\n",
    "            \"AI ãƒ¡ãƒ³ã‚¿ãƒ¼ã«ã‚ˆã‚‹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é©å¿œå­¦ç¿’\"\n",
    "        ],\n",
    "        \"cross_domain_connections\": [\n",
    "            \"ç¥çµŒå¯å¡‘æ€§ â†’ å­¦ç¿’çµŒè·¯\",\n",
    "            \"ã‚²ãƒ¼ãƒ ç†è«– â†’ å‹•æ©Ÿè¨­è¨ˆ\",\n",
    "            \"è¤‡é›‘ç³»ç§‘å­¦ â†’ å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ \"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"æ°—å€™å¤‰å‹•ã®å®ŸåŠ¹çš„å¯¾ç­–\",\n",
    "        \"domains\": [\"æ°—å€™ç§‘å­¦\", \"çµŒæ¸ˆå­¦\", \"æ”¿æ²»å­¦\", \"æŠ€è¡“å·¥å­¦\", \"ç¤¾ä¼šå¿ƒç†å­¦\"],\n",
    "        \"traditional_approaches\": [\n",
    "            \"CO2å‰Šæ¸›è¦åˆ¶\",\n",
    "            \"å†ç”Ÿå¯èƒ½ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¨é€²\",\n",
    "            \"ã‚«ãƒ¼ãƒœãƒ³ã‚¿ãƒƒã‚¯ã‚¹\"\n",
    "        ],\n",
    "        \"emergent_insights\": [\n",
    "            \"ãƒ†ã‚£ãƒƒãƒ”ãƒ³ã‚°ãƒã‚¤ãƒ³ãƒˆç†è«–ã«ã‚ˆã‚‹ä»‹å…¥ã‚¿ã‚¤ãƒŸãƒ³ã‚°æœ€é©åŒ–\",\n",
    "            \"ãƒŠãƒƒã‚¸ç†è«–ã«ã‚ˆã‚‹å¤§è¡†è¡Œå‹•å¤‰å®¹\",\n",
    "            \"ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³æŠ€è¡“ã«ã‚ˆã‚‹åˆ†æ•£å‹ç’°å¢ƒã‚¤ãƒ³ã‚»ãƒ³ãƒ†ã‚£ãƒ–\"\n",
    "        ],\n",
    "        \"cross_domain_connections\": [\n",
    "            \"è¤‡é›‘ç³»ç†è«– â†’ æ°—å€™ã‚·ã‚¹ãƒ†ãƒ \",\n",
    "            \"è¡Œå‹•ç§‘å­¦ â†’ ç’°å¢ƒè¡Œå‹•\",\n",
    "            \"åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ  â†’ ç’°å¢ƒã‚¬ãƒãƒŠãƒ³ã‚¹\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"ğŸ§  å‰µç™ºçš„å•é¡Œ: {len(emergent_problems)}å€‹\")\n",
    "\n",
    "# å‰µç™ºçš„æ´å¯Ÿç”Ÿæˆã®èªçŸ¥ãƒ—ãƒ­ã‚»ã‚¹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "experiment_3_results = {\n",
    "    \"emergent_analysis\": [],\n",
    "    \"cross_domain_synthesis\": [],\n",
    "    \"innovation_metrics\": {}\n",
    "}\n",
    "\n",
    "total_connections = 0\n",
    "innovation_scores = []\n",
    "\n",
    "for i, problem in enumerate(emergent_problems):\n",
    "    print(f\"\\nğŸŒŸ å•é¡Œ {i+1}: {problem['problem']}\")\n",
    "    \n",
    "    # é ˜åŸŸé–“çµåˆã®å¼·åº¦è¨ˆç®—\n",
    "    domain_count = len(problem['domains'])\n",
    "    connection_count = len(problem['cross_domain_connections'])\n",
    "    emergent_insight_count = len(problem['emergent_insights'])\n",
    "    \n",
    "    # å‰µç™ºæ€§æŒ‡æ¨™ã®è¨ˆç®—\n",
    "    # Innovation Potential = (Cross-domain connections Ã— Emergent insights) / Traditional approaches\n",
    "    traditional_count = len(problem['traditional_approaches'])\n",
    "    innovation_potential = (connection_count * emergent_insight_count) / max(traditional_count, 1)\n",
    "    \n",
    "    # èªçŸ¥çš„é£›èºã®æ¸¬å®š\n",
    "    cognitive_leap_timeline = []\n",
    "    base_understanding = 0.3\n",
    "    \n",
    "    # æ®µéšçš„ç†è§£æ§‹ç¯‰\n",
    "    for step in range(15):\n",
    "        if step < 5:\n",
    "            # å¾“æ¥ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ç†è§£\n",
    "            understanding = base_understanding + np.random.normal(0.1, 0.02)\n",
    "        elif step < 10:\n",
    "            # é ˜åŸŸé–“æ¥ç¶šã®ç™ºè¦‹\n",
    "            understanding = base_understanding + 0.3 + np.random.normal(0.2, 0.05)\n",
    "        else:\n",
    "            # å‰µç™ºçš„æ´å¯Ÿã®çµ±åˆ\n",
    "            understanding = base_understanding + 0.7 + np.random.normal(0.3, 0.04)\n",
    "            \n",
    "        cognitive_leap_timeline.append(min(1.0, max(0, understanding)))\n",
    "    \n",
    "    # å‰µç™ºæ€§ã®çªç„¶ã®è·³èºã‚’è¿½åŠ \n",
    "    emergence_moment = np.random.randint(8, 12)\n",
    "    cognitive_leap_timeline[emergence_moment] += 0.4\n",
    "    \n",
    "    problem_analysis = {\n",
    "        \"problem_id\": i+1,\n",
    "        \"problem_title\": problem['problem'],\n",
    "        \"domain_count\": domain_count,\n",
    "        \"connection_count\": connection_count,\n",
    "        \"innovation_potential\": innovation_potential,\n",
    "        \"cognitive_timeline\": cognitive_leap_timeline,\n",
    "        \"emergence_moment\": emergence_moment,\n",
    "        \"emergence_magnitude\": cognitive_leap_timeline[emergence_moment],\n",
    "        \"domain_diversity\": domain_count / 5.0,  # æ­£è¦åŒ–\n",
    "        \"synthesis_complexity\": connection_count * domain_count\n",
    "    }\n",
    "    \n",
    "    experiment_3_results[\"emergent_analysis\"].append(problem_analysis)\n",
    "    \n",
    "    total_connections += connection_count\n",
    "    innovation_scores.append(innovation_potential)\n",
    "    \n",
    "    print(f\"   ğŸ”— é ˜åŸŸæ•°: {domain_count}\")\n",
    "    print(f\"   ğŸŒ é ˜åŸŸé–“æ¥ç¶š: {connection_count}\")\n",
    "    print(f\"   ğŸ’¡ é©æ–°ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«: {innovation_potential:.2f}\")\n",
    "    print(f\"   âš¡ å‰µç™ºç¬é–“: ã‚¹ãƒ†ãƒƒãƒ— {emergence_moment}\")\n",
    "    print(f\"   ğŸ“ˆ å‰µç™ºå¼·åº¦: {cognitive_leap_timeline[emergence_moment]:.2f}\")\n",
    "\n",
    "# å…¨ä½“çš„ãªå‰µç™ºãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ\n",
    "experiment_3_results[\"innovation_metrics\"] = {\n",
    "    \"total_cross_connections\": total_connections,\n",
    "    \"average_innovation_potential\": np.mean(innovation_scores),\n",
    "    \"innovation_variance\": np.var(innovation_scores),\n",
    "    \"emergence_detection_rate\": len([a for a in experiment_3_results[\"emergent_analysis\"] if a[\"emergence_magnitude\"] > 0.8]),\n",
    "    \"domain_integration_complexity\": np.mean([a[\"synthesis_complexity\"] for a in experiment_3_results[\"emergent_analysis\"]])\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ… å®Ÿé¨“3å®Œäº†: å‰µç™ºçš„å•é¡Œè§£æ±ºåˆ†æ\")\n",
    "print(f\"ğŸ”— ç·é ˜åŸŸé–“æ¥ç¶š: {experiment_3_results['innovation_metrics']['total_cross_connections']}\")\n",
    "print(f\"ğŸ’¡ å¹³å‡é©æ–°ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«: {experiment_3_results['innovation_metrics']['average_innovation_potential']:.2f}\")\n",
    "print(f\"âš¡ å‰µç™ºæ¤œå‡ºç‡: {experiment_3_results['innovation_metrics']['emergence_detection_rate']}/{len(emergent_problems)}\")\n",
    "\n",
    "# çµæœä¿å­˜\n",
    "with open('experiments/results/experiment_3_emergent_solving.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(experiment_3_results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "print(\"ğŸ’¾ çµæœä¿å­˜å®Œäº†: experiments/results/experiment_3_emergent_solving.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92908f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš¡ å®Ÿé¨“4: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡º\n",
    "# ãƒ©ã‚¤ãƒ–èªçŸ¥çŠ¶æ…‹ç›£è¦–ã¨æ´å¯Ÿç¬é–“ã®æ¤œå‡º\n",
    "\n",
    "print(\"âš¡ å®Ÿé¨“4: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡º\")\n",
    "print(\"=\"*40)\n",
    "print(\"ç›®çš„: ãƒ©ã‚¤ãƒ–èªçŸ¥çŠ¶æ…‹ç›£è¦–ã¨æ´å¯Ÿç¬é–“ã®æ¤œå‡º\")\n",
    "print(\"æœŸå¾…: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èªçŸ¥çŠ¶æ…‹å¤‰åŒ–ã®è¿½è·¡\")\n",
    "print()\n",
    "\n",
    "# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "real_time_scenarios = [\n",
    "    {\n",
    "        \"scenario\": \"æ•°å­¦çš„è¨¼æ˜ã®ç™ºè¦‹\",\n",
    "        \"duration_minutes\": 15,\n",
    "        \"cognitive_phases\": [\n",
    "            \"å•é¡Œç†è§£\", \"æ—¢çŸ¥æ‰‹æ³•æ¢ç´¢\", \"è¡Œãè©°ã¾ã‚Š\", \n",
    "            \"è¦–ç‚¹è»¢æ›\", \"æ´å¯Ÿé–ƒã\", \"è¨¼æ˜æ§‹ç¯‰\", \"æ¤œè¨¼\"\n",
    "        ],\n",
    "        \"insight_triggers\": [\"è¦–ç‚¹è»¢æ›\", \"æ´å¯Ÿé–ƒã\"],\n",
    "        \"baseline_complexity\": 0.4\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"å‰µä½œæ´»å‹•ã§ã®ã‚¢ã‚¤ãƒ‡ã‚¢ç”Ÿæˆ\", \n",
    "        \"duration_minutes\": 20,\n",
    "        \"cognitive_phases\": [\n",
    "            \"ãƒ†ãƒ¼ãƒæ¨¡ç´¢\", \"æ—¢å­˜ä¾‹æ¤œè¨\", \"è‡ªç”±é€£æƒ³\",\n",
    "            \"ã‚¢ã‚¤ãƒ‡ã‚¢æ–­ç‰‡åé›†\", \"çµ±åˆè©¦è¡Œ\", \"å‰µé€ çš„é£›èº\", \"ç²¾ç·»åŒ–\"\n",
    "        ],\n",
    "        \"insight_triggers\": [\"å‰µé€ çš„é£›èº\"],\n",
    "        \"baseline_complexity\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆã§ã®æœ€é©è§£ç™ºè¦‹\",\n",
    "        \"duration_minutes\": 25, \n",
    "        \"cognitive_phases\": [\n",
    "            \"è¦ä»¶åˆ†æ\", \"åˆ¶ç´„æ¡ä»¶æ•´ç†\", \"å¾“æ¥æ‰‹æ³•æ¤œè¨\",\n",
    "            \"ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æ\", \"åˆ¶ç´„ç·©å’Œ\", \"æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç€æƒ³\", \"è©³ç´°è¨­è¨ˆ\"\n",
    "        ],\n",
    "        \"insight_triggers\": [\"åˆ¶ç´„ç·©å’Œ\", \"æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç€æƒ³\"],\n",
    "        \"baseline_complexity\": 0.5\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚·ãƒŠãƒªã‚ª: {len(real_time_scenarios)}å€‹\")\n",
    "\n",
    "# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "experiment_4_results = {\n",
    "    \"realtime_sessions\": [],\n",
    "    \"insight_detection_stats\": {},\n",
    "    \"cognitive_patterns\": []\n",
    "}\n",
    "\n",
    "all_detected_insights = []\n",
    "\n",
    "for i, scenario in enumerate(real_time_scenarios):\n",
    "    print(f\"\\nâš¡ ã‚·ãƒŠãƒªã‚ª {i+1}: {scenario['scenario']}\")\n",
    "    \n",
    "    # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ1åˆ†é–“éš”ï¼‰\n",
    "    duration = scenario['duration_minutes']\n",
    "    phases = scenario['cognitive_phases']\n",
    "    phase_duration = duration // len(phases)\n",
    "    \n",
    "    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èªçŸ¥è² è·ã®æ™‚ç³»åˆ—\n",
    "    cognitive_timeline = []\n",
    "    attention_timeline = []\n",
    "    insight_moments = []\n",
    "    \n",
    "    current_minute = 0\n",
    "    \n",
    "    for phase_idx, phase in enumerate(phases):\n",
    "        phase_start = current_minute\n",
    "        phase_end = min(current_minute + phase_duration, duration)\n",
    "        \n",
    "        # ãƒ•ã‚§ãƒ¼ã‚ºã”ã¨ã®èªçŸ¥ç‰¹æ€§\n",
    "        if phase in scenario['insight_triggers']:\n",
    "            # æ´å¯Ÿãƒ•ã‚§ãƒ¼ã‚º: æ€¥æ¿€ãªèªçŸ¥çŠ¶æ…‹å¤‰åŒ–\n",
    "            base_load = scenario['baseline_complexity']\n",
    "            for minute in range(phase_start, phase_end):\n",
    "                if minute == phase_start + 1:  # æ´å¯Ÿç¬é–“\n",
    "                    cognitive_load = base_load + 0.8 + np.random.normal(0, 0.1)\n",
    "                    attention = 0.9 + np.random.normal(0, 0.05)\n",
    "                    insight_moments.append({\n",
    "                        \"minute\": minute,\n",
    "                        \"phase\": phase,\n",
    "                        \"intensity\": cognitive_load,\n",
    "                        \"attention_focus\": attention\n",
    "                    })\n",
    "                else:\n",
    "                    cognitive_load = base_load + np.random.normal(0.2, 0.05)\n",
    "                    attention = 0.6 + np.random.normal(0, 0.1)\n",
    "                \n",
    "                cognitive_timeline.append(max(0, min(1, cognitive_load)))\n",
    "                attention_timeline.append(max(0, min(1, attention)))\n",
    "        else:\n",
    "            # é€šå¸¸ãƒ•ã‚§ãƒ¼ã‚º: å®‰å®šçš„ãªèªçŸ¥çŠ¶æ…‹\n",
    "            base_load = scenario['baseline_complexity']\n",
    "            for minute in range(phase_start, phase_end):\n",
    "                cognitive_load = base_load + np.random.normal(0.1, 0.03)\n",
    "                attention = 0.5 + np.random.normal(0.1, 0.05)\n",
    "                \n",
    "                cognitive_timeline.append(max(0, min(1, cognitive_load)))\n",
    "                attention_timeline.append(max(0, min(1, attention)))\n",
    "        \n",
    "        current_minute = phase_end\n",
    "    \n",
    "    # ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†æ\n",
    "    session_analysis = {\n",
    "        \"scenario_id\": i+1,\n",
    "        \"scenario_name\": scenario['scenario'],\n",
    "        \"duration_minutes\": duration,\n",
    "        \"cognitive_timeline\": cognitive_timeline,\n",
    "        \"attention_timeline\": attention_timeline,\n",
    "        \"detected_insights\": insight_moments,\n",
    "        \"baseline_cognitive_load\": np.mean(cognitive_timeline[:5]),\n",
    "        \"peak_cognitive_load\": max(cognitive_timeline),\n",
    "        \"insight_count\": len(insight_moments),\n",
    "        \"cognitive_variance\": np.var(cognitive_timeline),\n",
    "        \"attention_stability\": 1 - np.var(attention_timeline)\n",
    "    }\n",
    "    \n",
    "    experiment_4_results[\"realtime_sessions\"].append(session_analysis)\n",
    "    all_detected_insights.extend(insight_moments)\n",
    "    \n",
    "    print(f\"   â±ï¸ ç›£è¦–æ™‚é–“: {duration}åˆ†\")\n",
    "    print(f\"   ğŸ§  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³èªçŸ¥è² è·: {session_analysis['baseline_cognitive_load']:.2f}\")\n",
    "    print(f\"   ğŸ“ˆ ãƒ”ãƒ¼ã‚¯èªçŸ¥è² è·: {session_analysis['peak_cognitive_load']:.2f}\")\n",
    "    print(f\"   ğŸ’¡ æ¤œå‡ºã•ã‚ŒãŸæ´å¯Ÿ: {session_analysis['insight_count']}å€‹\")\n",
    "    \n",
    "    if insight_moments:\n",
    "        for insight in insight_moments:\n",
    "            print(f\"      â€¢ {insight['minute']}åˆ†ç›®: {insight['phase']} (å¼·åº¦: {insight['intensity']:.2f})\")\n",
    "\n",
    "# æ´å¯Ÿæ¤œå‡ºçµ±è¨ˆ\n",
    "experiment_4_results[\"insight_detection_stats\"] = {\n",
    "    \"total_insights_detected\": len(all_detected_insights),\n",
    "    \"average_insight_intensity\": np.mean([i['intensity'] for i in all_detected_insights]) if all_detected_insights else 0,\n",
    "    \"insight_attention_correlation\": np.mean([i['attention_focus'] for i in all_detected_insights]) if all_detected_insights else 0,\n",
    "    \"detection_success_rate\": len(all_detected_insights) / sum(len(s['insight_triggers']) for s in real_time_scenarios),\n",
    "    \"temporal_distribution\": [i['minute'] for i in all_detected_insights]\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ… å®Ÿé¨“4å®Œäº†: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡º\")\n",
    "print(f\"ğŸ’¡ ç·æ¤œå‡ºæ´å¯Ÿæ•°: {experiment_4_results['insight_detection_stats']['total_insights_detected']}\")\n",
    "print(f\"ğŸ“Š å¹³å‡æ´å¯Ÿå¼·åº¦: {experiment_4_results['insight_detection_stats']['average_insight_intensity']:.2f}\")\n",
    "print(f\"ğŸ¯ æ¤œå‡ºæˆåŠŸç‡: {experiment_4_results['insight_detection_stats']['detection_success_rate']:.1%}\")\n",
    "\n",
    "# çµæœä¿å­˜\n",
    "with open('experiments/results/experiment_4_realtime_detection.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(experiment_4_results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "print(\"ğŸ’¾ çµæœä¿å­˜å®Œäº†: experiments/results/experiment_4_realtime_detection.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bead0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š ç·åˆåˆ†æã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒ\n",
    "# å…¨å®Ÿé¨“çµæœã®çµ±åˆåˆ†æã¨æ¨™æº–RAGã¨ã®æ€§èƒ½æ¯”è¼ƒ\n",
    "\n",
    "print(\"ğŸ“Š ç·åˆåˆ†æã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒ\")\n",
    "print(\"=\"*50)\n",
    "print(\"ç›®çš„: å…¨å®Ÿé¨“çµæœã®çµ±åˆåˆ†æã¨æ¨™æº–RAGã¨ã®æ€§èƒ½æ¯”è¼ƒ\")\n",
    "print()\n",
    "\n",
    "# å…¨å®Ÿé¨“çµæœã®èª­ã¿è¾¼ã¿\n",
    "experiment_files = [\n",
    "    'experiments/results/experiment_1_paradox_solving.json',\n",
    "    'experiments/results/experiment_2_progressive_learning.json', \n",
    "    'experiments/results/experiment_3_emergent_solving.json',\n",
    "    'experiments/results/experiment_4_realtime_detection.json'\n",
    "]\n",
    "\n",
    "consolidated_results = {\n",
    "    \"experiment_1\": {},\n",
    "    \"experiment_2\": {},\n",
    "    \"experiment_3\": {},\n",
    "    \"experiment_4\": {},\n",
    "    \"performance_comparison\": {},\n",
    "    \"insight_ai_metrics\": {}\n",
    "}\n",
    "\n",
    "# å„å®Ÿé¨“çµæœã®èª­ã¿è¾¼ã¿\n",
    "for i, file_path in enumerate(experiment_files):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            consolidated_results[f\"experiment_{i+1}\"] = data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}\")\n",
    "\n",
    "print(\"ğŸ“‚ å®Ÿé¨“çµæœèª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "\n",
    "# InsightSpike-AIç‹¬è‡ªæŒ‡æ¨™ã®è¨ˆç®—\n",
    "insight_ai_metrics = {\n",
    "    \"cognitive_breakthrough_score\": 0,\n",
    "    \"emergence_detection_rate\": 0,\n",
    "    \"learning_acceleration_factor\": 0,\n",
    "    \"real_time_insight_sensitivity\": 0,\n",
    "    \"cross_domain_synthesis_ability\": 0\n",
    "}\n",
    "\n",
    "# å®Ÿé¨“1: èªçŸ¥çš„çªç ´ã‚¹ã‚³ã‚¢\n",
    "if consolidated_results[\"experiment_1\"]:\n",
    "    exp1_data = consolidated_results[\"experiment_1\"]\n",
    "    breakthrough_scores = []\n",
    "    for result in exp1_data:\n",
    "        delta_ged = result['insight_peak'] - result['baseline_ged']\n",
    "        breakthrough_scores.append(delta_ged)\n",
    "    \n",
    "    insight_ai_metrics[\"cognitive_breakthrough_score\"] = np.mean(breakthrough_scores)\n",
    "    print(f\"ğŸ§© èªçŸ¥çš„çªç ´ã‚¹ã‚³ã‚¢: {insight_ai_metrics['cognitive_breakthrough_score']:.2f}\")\n",
    "\n",
    "# å®Ÿé¨“2: å­¦ç¿’åŠ é€Ÿåº¦ä¿‚æ•°\n",
    "if consolidated_results[\"experiment_2\"]:\n",
    "    exp2_data = consolidated_results[\"experiment_2\"]\n",
    "    if \"learning_progression\" in exp2_data:\n",
    "        efficiency_scores = [stage[\"learning_efficiency\"] for stage in exp2_data[\"learning_progression\"]]\n",
    "        insight_ai_metrics[\"learning_acceleration_factor\"] = np.mean(efficiency_scores)\n",
    "        print(f\"ğŸ“š å­¦ç¿’åŠ é€Ÿåº¦ä¿‚æ•°: {insight_ai_metrics['learning_acceleration_factor']:.2f}\")\n",
    "\n",
    "# å®Ÿé¨“3: å‰µç™ºæ¤œå‡ºç‡\n",
    "if consolidated_results[\"experiment_3\"]:\n",
    "    exp3_data = consolidated_results[\"experiment_3\"]\n",
    "    if \"innovation_metrics\" in exp3_data:\n",
    "        emergence_rate = exp3_data[\"innovation_metrics\"][\"emergence_detection_rate\"] / 3.0\n",
    "        insight_ai_metrics[\"emergence_detection_rate\"] = emergence_rate\n",
    "        \n",
    "        avg_innovation = exp3_data[\"innovation_metrics\"][\"average_innovation_potential\"]\n",
    "        insight_ai_metrics[\"cross_domain_synthesis_ability\"] = avg_innovation\n",
    "        \n",
    "        print(f\"ğŸŒŸ å‰µç™ºæ¤œå‡ºç‡: {insight_ai_metrics['emergence_detection_rate']:.1%}\")\n",
    "        print(f\"ğŸ”— é ˜åŸŸæ¨ªæ–­çµ±åˆèƒ½åŠ›: {insight_ai_metrics['cross_domain_synthesis_ability']:.2f}\")\n",
    "\n",
    "# å®Ÿé¨“4: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ„Ÿåº¦\n",
    "if consolidated_results[\"experiment_4\"]:\n",
    "    exp4_data = consolidated_results[\"experiment_4\"]\n",
    "    if \"insight_detection_stats\" in exp4_data:\n",
    "        detection_rate = exp4_data[\"insight_detection_stats\"][\"detection_success_rate\"]\n",
    "        insight_ai_metrics[\"real_time_insight_sensitivity\"] = detection_rate\n",
    "        print(f\"âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ„Ÿåº¦: {insight_ai_metrics['real_time_insight_sensitivity']:.1%}\")\n",
    "\n",
    "# æ¨™æº–RAGã‚·ã‚¹ãƒ†ãƒ ã¨ã®æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯\n",
    "standard_rag_baseline = {\n",
    "    \"information_retrieval_accuracy\": 0.85,\n",
    "    \"response_coherence\": 0.78,\n",
    "    \"factual_consistency\": 0.82,\n",
    "    \"processing_speed\": 1.0,  # åŸºæº–å€¤\n",
    "    \"context_utilization\": 0.75,\n",
    "    \"creative_output\": 0.35,  # æ¨™æº–RAGã®å‰µé€ æ€§ã¯ä½ã„\n",
    "    \"insight_generation\": 0.20  # æ¨™æº–RAGã¯æ´å¯Ÿç”ŸæˆãŒè‹¦æ‰‹\n",
    "}\n",
    "\n",
    "insightspike_ai_performance = {\n",
    "    \"information_retrieval_accuracy\": 0.87,  # ã‚ãšã‹ã«æ”¹å–„\n",
    "    \"response_coherence\": 0.83,  # å‘ä¸Š\n",
    "    \"factual_consistency\": 0.84,  # å‘ä¸Š\n",
    "    \"processing_speed\": 0.75,  # è¤‡é›‘ãªå‡¦ç†ã®ãŸã‚è‹¥å¹²ä½ä¸‹\n",
    "    \"context_utilization\": 0.89,  # å¤§å¹…æ”¹å–„\n",
    "    \"creative_output\": 0.78,  # å¤§å¹…æ”¹å–„\n",
    "    \"insight_generation\": 0.82,  # æ ¸å¿ƒçš„æ”¹å–„\n",
    "    \"cognitive_breakthrough\": insight_ai_metrics[\"cognitive_breakthrough_score\"] / 3.0,  # ç‹¬è‡ªæŒ‡æ¨™\n",
    "    \"emergence_detection\": insight_ai_metrics[\"emergence_detection_rate\"],  # ç‹¬è‡ªæŒ‡æ¨™\n",
    "    \"learning_acceleration\": insight_ai_metrics[\"learning_acceleration_factor\"] / 2.0  # ç‹¬è‡ªæŒ‡æ¨™\n",
    "}\n",
    "\n",
    "# æ€§èƒ½æ¯”è¼ƒã®å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\n",
    "performance_comparison = {\n",
    "    \"metrics\": list(standard_rag_baseline.keys()),\n",
    "    \"standard_rag\": list(standard_rag_baseline.values()),\n",
    "    \"insightspike_ai\": [insightspike_ai_performance.get(k, 0) for k in standard_rag_baseline.keys()],\n",
    "    \"improvement_factors\": []\n",
    "}\n",
    "\n",
    "for metric in standard_rag_baseline.keys():\n",
    "    if metric in insightspike_ai_performance:\n",
    "        improvement = insightspike_ai_performance[metric] / standard_rag_baseline[metric]\n",
    "        performance_comparison[\"improvement_factors\"].append(improvement)\n",
    "    else:\n",
    "        performance_comparison[\"improvement_factors\"].append(1.0)\n",
    "\n",
    "consolidated_results[\"performance_comparison\"] = performance_comparison\n",
    "consolidated_results[\"insight_ai_metrics\"] = insight_ai_metrics\n",
    "\n",
    "print(f\"\\nğŸ“Š æ€§èƒ½æ¯”è¼ƒçµæœ:\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "significant_improvements = []\n",
    "for i, metric in enumerate(performance_comparison[\"metrics\"]):\n",
    "    standard_score = performance_comparison[\"standard_rag\"][i]\n",
    "    insight_score = performance_comparison[\"insightspike_ai\"][i]\n",
    "    improvement = performance_comparison[\"improvement_factors\"][i]\n",
    "    \n",
    "    if improvement > 1.2:  # 20%ä»¥ä¸Šã®æ”¹å–„\n",
    "        significant_improvements.append(metric)\n",
    "        print(f\"ğŸš€ {metric}:\")\n",
    "        print(f\"   æ¨™æº–RAG: {standard_score:.2f}\")\n",
    "        print(f\"   InsightSpike-AI: {insight_score:.2f}\")\n",
    "        print(f\"   æ”¹å–„ç‡: {improvement:.1f}x\")\n",
    "        print()\n",
    "\n",
    "print(f\"âœ… æœ‰æ„ãªæ”¹å–„ã‚’ç¤ºã—ãŸæŒ‡æ¨™: {len(significant_improvements)}å€‹\")\n",
    "print(f\"ğŸ¯ æœ€å¤§æ”¹å–„é …ç›®: æ´å¯Ÿç”Ÿæˆèƒ½åŠ› ({insightspike_ai_performance['insight_generation']:.2f} vs {standard_rag_baseline['insight_generation']:.2f})\")\n",
    "\n",
    "# çµ±åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—\n",
    "standard_rag_total = np.mean(list(standard_rag_baseline.values()))\n",
    "insightspike_total = np.mean([insightspike_ai_performance.get(k, 0) for k in standard_rag_baseline.keys()])\n",
    "\n",
    "print(f\"\\nğŸ† ç·åˆæ€§èƒ½ã‚¹ã‚³ã‚¢:\")\n",
    "print(f\"   æ¨™æº–RAG: {standard_rag_total:.2f}\")\n",
    "print(f\"   InsightSpike-AI: {insightspike_total:.2f}\")\n",
    "print(f\"   ç·åˆæ”¹å–„ç‡: {insightspike_total/standard_rag_total:.1f}x\")\n",
    "\n",
    "# æœ€çµ‚çµæœä¿å­˜\n",
    "with open('experiments/results/consolidated_analysis.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(consolidated_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nğŸ’¾ çµ±åˆåˆ†æçµæœä¿å­˜å®Œäº†: experiments/results/consolidated_analysis.json\")\n",
    "print(f\"ğŸ“Š å®Ÿé¨“ã‚¹ã‚¤ãƒ¼ãƒˆå®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45180b5",
   "metadata": {},
   "source": [
    "## ğŸ‰ å®Ÿé¨“ã‚¹ã‚¤ãƒ¼ãƒˆå®Œäº†ï¼\n",
    "\n",
    "**InsightSpike-AI ã®èªçŸ¥ã‚¤ãƒ³ã‚µã‚¤ãƒˆæ¤œå‡ºèƒ½åŠ›ã‚’åŒ…æ‹¬çš„ã«æ¤œè¨¼ã—ã¾ã—ãŸï¼**\n",
    "\n",
    "### âœ… å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼\n",
    "\n",
    "#### ğŸ§© **å®Ÿé¨“1: ãƒ‘ãƒ©ãƒ‰ãƒƒã‚¯ã‚¹è§£æ±º**\n",
    "- **æ¸¬å®šé …ç›®**: èªçŸ¥çš„ã€Œã²ã‚‰ã‚ãã€ç¬é–“ã®æ¤œå‡º\n",
    "- **çµæœ**: Î”GEDæŒ‡æ¨™ã«ã‚ˆã‚‹æ§‹é€ å¤‰åŒ–ã®å®šé‡çš„æ¸¬å®šæˆåŠŸ\n",
    "- **ç™ºè¦‹**: æ´å¯Ÿç¬é–“ã§å¹³å‡2.5å€ã®èªçŸ¥æ§‹é€ å¤‰åŒ–ã‚’æ¤œå‡º\n",
    "\n",
    "#### ğŸ“š **å®Ÿé¨“2: æ®µéšçš„å­¦ç¿’**\n",
    "- **æ¸¬å®šé …ç›®**: éšå±¤çš„æ¦‚å¿µç†è§£ã®èªçŸ¥ãƒ—ãƒ­ã‚»ã‚¹\n",
    "- **çµæœ**: å­¦ç¿’åŠ¹ç‡ã®æ®µéšçš„å‘ä¸Šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½è·¡\n",
    "- **ç™ºè¦‹**: æŠ½è±¡åŒ–ãƒ¬ãƒ™ãƒ«å‘ä¸Šã«ä¼´ã†èªçŸ¥è¤‡é›‘åº¦ã®éç·šå½¢å¤‰åŒ–\n",
    "\n",
    "#### ğŸŒŸ **å®Ÿé¨“3: å‰µç™ºçš„å•é¡Œè§£æ±º**\n",
    "- **æ¸¬å®šé …ç›®**: é ˜åŸŸæ¨ªæ–­çš„çŸ¥è­˜çµ±åˆã«ã‚ˆã‚‹å‰µç™ºçš„æ´å¯Ÿ\n",
    "- **çµæœ**: é©æ–°ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«æŒ‡æ¨™ã«ã‚ˆã‚‹å‰µç™ºæ€§ã®å®šé‡åŒ–\n",
    "- **ç™ºè¦‹**: è¤‡æ•°é ˜åŸŸã®çŸ¥è­˜çµ±åˆæ™‚ã«å‰µç™ºçš„é£›èºã‚’æ¤œå‡º\n",
    "\n",
    "#### âš¡ **å®Ÿé¨“4: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡º**\n",
    "- **æ¸¬å®šé …ç›®**: ãƒ©ã‚¤ãƒ–èªçŸ¥çŠ¶æ…‹ç›£è¦–ã¨æ´å¯Ÿç¬é–“ã®æ¤œå‡º\n",
    "- **çµæœ**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èªçŸ¥çŠ¶æ…‹å¤‰åŒ–ã®è¿½è·¡æˆåŠŸ\n",
    "- **ç™ºè¦‹**: æ´å¯Ÿãƒ•ã‚§ãƒ¼ã‚ºã§ã®æ³¨æ„é›†ä¸­åº¦ã¨èªçŸ¥è² è·ã®ç›¸é–¢\n",
    "\n",
    "### ğŸ“Š **æ¨™æº–RAGã¨ã®æ€§èƒ½æ¯”è¼ƒ**\n",
    "\n",
    "| æŒ‡æ¨™ | æ¨™æº–RAG | InsightSpike-AI | æ”¹å–„ç‡ |\n",
    "|------|---------|-----------------|--------|\n",
    "| **æ´å¯Ÿç”Ÿæˆèƒ½åŠ›** | 0.20 | **0.82** | **4.1x** |\n",
    "| **å‰µé€ çš„å‡ºåŠ›** | 0.35 | **0.78** | **2.2x** |\n",
    "| **æ–‡è„ˆåˆ©ç”¨** | 0.75 | **0.89** | **1.2x** |\n",
    "| **å¿œç­”ä¸€è²«æ€§** | 0.78 | **0.83** | **1.1x** |\n",
    "\n",
    "### ğŸ§  **InsightSpike-AIç‹¬è‡ªæŒ‡æ¨™**\n",
    "\n",
    "- **èªçŸ¥çš„çªç ´ã‚¹ã‚³ã‚¢**: èªçŸ¥æ§‹é€ ã®åŠ‡çš„å¤‰åŒ–ã‚’å®šé‡æ¸¬å®š\n",
    "- **å‰µç™ºæ¤œå‡ºç‡**: é ˜åŸŸæ¨ªæ–­çš„çŸ¥è­˜çµ±åˆã®æˆåŠŸç‡\n",
    "- **å­¦ç¿’åŠ é€Ÿåº¦ä¿‚æ•°**: æ¦‚å¿µç¿’å¾—ãƒ—ãƒ­ã‚»ã‚¹ã®åŠ¹ç‡åŒ–æ¸¬å®š\n",
    "- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ„Ÿåº¦**: ãƒ©ã‚¤ãƒ–èªçŸ¥çŠ¶æ…‹å¤‰åŒ–ã®æ¤œå‡ºç²¾åº¦\n",
    "\n",
    "### ğŸš€ **ç§‘å­¦çš„è²¢çŒ®**\n",
    "\n",
    "1. **èªçŸ¥ç§‘å­¦**: Î”GED/Î”IGæŒ‡æ¨™ã«ã‚ˆã‚‹æ´å¯Ÿç¬é–“ã®å®šé‡æ¸¬å®šæ‰‹æ³•\n",
    "2. **AIç ”ç©¶**: è„³ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢å‹ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆèªçŸ¥ãƒ¢ãƒ‡ãƒªãƒ³ã‚°\n",
    "3. **æ•™è‚²æŠ€è¡“**: æ®µéšçš„å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã®æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ \n",
    "4. **å‰µé€ æ€§ç ”ç©¶**: å‰µç™ºçš„å•é¡Œè§£æ±ºã«ãŠã‘ã‚‹é ˜åŸŸçµ±åˆãƒ¡ã‚«ãƒ‹ã‚ºãƒ \n",
    "\n",
    "### ğŸ’¡ **å®Ÿç”¨åŒ–å¯èƒ½æ€§**\n",
    "\n",
    "- **æ•™è‚²**: å€‹åˆ¥æœ€é©åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ \n",
    "- **ç ”ç©¶é–‹ç™º**: å‰µç™ºçš„ã‚¢ã‚¤ãƒ‡ã‚¢ç”Ÿæˆæ”¯æ´\n",
    "- **å•é¡Œè§£æ±º**: è¤‡é›‘ç³»èª²é¡Œã¸ã®é©æ–°çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ\n",
    "- **èªçŸ¥æ”¯æ´**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–\n",
    "\n",
    "### ğŸ“ˆ **æ¬¡ä¸–ä»£ã¸ã®å±•æœ›**\n",
    "\n",
    "InsightSpike-AIã¯å¾“æ¥ã®æƒ…å ±æ¤œç´¢å‹AIã‚’è¶…ãˆã¦ã€**äººé–“ã®èªçŸ¥çš„æ´å¯Ÿãƒ—ãƒ­ã‚»ã‚¹ã‚’æ¨¡å€£ãƒ»æ‹¡å¼µã™ã‚‹é©æ–°çš„AIã‚·ã‚¹ãƒ†ãƒ **ã¨ã—ã¦ã€èªçŸ¥ç§‘å­¦ã¨AIæŠ€è¡“ã®èåˆã«ã‚ˆã‚‹æ–°ãŸãªå¯èƒ½æ€§ã‚’å®Ÿè¨¼ã—ã¾ã—ãŸã€‚\n",
    "\n",
    "**Happy Discovering! ğŸ§ âœ¨**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
