Knowledge_ID,Text,Domain,Phase
K1,Entropy is a measure of disorder or randomness in a system.,fundamentals,1
K2,Energy is the capacity to do work and cannot be created or destroyed.,fundamentals,1
K3,Information represents the reduction of uncertainty about a system's state.,fundamentals,1
K4,Shannon entropy H(X) = -Î£ p(x) log p(x) quantifies information content.,mathematics,2
K5,Graph theory studies relationships between objects using nodes and edges.,mathematics,2
K6,Probability distributions describe the likelihood of different outcomes.,mathematics,2
K7,The second law of thermodynamics states that entropy always increases in isolated systems.,physics,3
K8,Maxwell's demon thought experiment links information processing to thermodynamics.,physics,3
K9,Landauer's principle: erasing one bit of information releases kT ln(2) of heat.,physics,3
K10,Living organisms maintain low internal entropy by consuming free energy.,biology,4
K11,DNA encodes hereditary information in a four-letter molecular alphabet.,biology,4
K12,Metabolism allows organisms to export entropy to their environment.,biology,4
K13,Information processing requires energy expenditure according to thermodynamic limits.,information,5
K14,Error correction codes add redundancy to protect information from corruption.,information,5
K15,Compression algorithms reduce data size by removing statistical redundancy.,information,5
