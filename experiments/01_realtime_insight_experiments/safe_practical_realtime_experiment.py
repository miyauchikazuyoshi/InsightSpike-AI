#!/usr/bin/env python3
"""
å®‰å…¨ç‰ˆå®Ÿè·µçš„ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿå®Ÿé¨“
================================

MainAgentã®åˆæœŸåŒ–å•é¡Œã‚’å›é¿ã—ã€
å€‹åˆ¥ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ç›´æ¥ä½¿ç”¨ã—ãŸå®Ÿè·µçš„å®Ÿé¨“
"""

import sys
import json
import time
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any
import csv

# InsightSpike-AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å€‹åˆ¥ã«èª­ã¿è¾¼ã¿
sys.path.append(str(Path(__file__).parent.parent / "src"))

try:
    # å®‰å…¨ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã¿èª­ã¿è¾¼ã¿
    from insightspike.core.config import get_config
    from insightspike.utils.embedder import get_model
    from insightspike.core.layers.layer2_memory_manager import L2MemoryManager
    from insightspike.core.learning.knowledge_graph_memory import KnowledgeGraphMemory
    
    print("âœ… å®‰å…¨ç‰ˆInsightSpike-AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆèª­ã¿è¾¼ã¿æˆåŠŸ")
except ImportError as e:
    print(f"âŒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    sys.exit(1)


class SafePracticalRealtimeExperiment:
    """å®‰å…¨ç‰ˆå®Ÿè·µçš„ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®Ÿé¨“ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        print("ğŸš€ å®‰å…¨ç‰ˆå®Ÿè·µçš„å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")
        
        # Core components (å®‰å…¨ç‰ˆ)
        self.config = get_config()
        self.model = get_model()
        
        # ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ (ç›´æ¥åˆæœŸåŒ–)
        self.memory_manager = L2MemoryManager(dim=384)
        
        # ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ• (ç›´æ¥åˆæœŸåŒ–)
        self.knowledge_graph = KnowledgeGraphMemory(
            embedding_dim=384, 
            similarity_threshold=0.7
        )
        
        # å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿
        self.episodes = []
        self.realtime_insights = []
        self.performance_metrics = []
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
        self.visualization_data = {
            'episodes': [],
            'node_counts': [],
            'edge_counts': [],
            'insight_timestamps': [],
            'ged_values': [],
            'ig_values': [],
            'memory_usage': []
        }
        
        # TopKæœ€é©åŒ–è¨­å®š
        self.topk_neighbors = 10
        self.insight_threshold_ged = 0.15  # å®Ÿè·µçš„é–¾å€¤
        self.insight_threshold_ig = 0.10
        
        print(f"âœ… å®‰å…¨ç‰ˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        print(f"   ãƒ¡ãƒ¢ãƒªæ¬¡å…ƒ: {self.memory_manager.dim}")
        print(f"   TopKè¿‘å‚æ•°: {self.topk_neighbors}")
        print(f"   GEDé–¾å€¤: {self.insight_threshold_ged}")
        print(f"   IGé–¾å€¤: {self.insight_threshold_ig}")
    
    def generate_realistic_episodes(self, count: int = 1000) -> List[str]:
        """ç¾å®Ÿçš„ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
        print(f"ğŸ“ {count}å€‹ã®ç¾å®Ÿçš„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆä¸­...")
        
        # å®Ÿä¸–ç•Œã®AI/MLç ”ç©¶é ˜åŸŸ
        research_areas = [
            "Large Language Models", "Computer Vision", "Reinforcement Learning",
            "Graph Neural Networks", "Federated Learning", "Explainable AI",
            "Multimodal Learning", "Few-shot Learning", "Transfer Learning",
            "Adversarial Machine Learning"
        ]
        
        # ç ”ç©¶æ´»å‹•ãƒ»ç™ºè¦‹
        activities = [
            "achieves breakthrough performance on", "introduces novel architecture for",
            "demonstrates significant improvement in", "proposes innovative approach to",
            "establishes new benchmark results for", "reveals unexpected insights about",
            "develops efficient algorithm for", "uncovers hidden patterns in",
            "creates robust framework for", "identifies critical factors in"
        ]
        
        # å¿œç”¨ãƒ‰ãƒ¡ã‚¤ãƒ³
        domains = [
            "medical diagnosis", "autonomous systems", "natural language understanding",
            "scientific discovery", "financial modeling", "climate prediction",
            "drug discovery", "robotics control", "image analysis", "speech recognition"
        ]
        
        episodes = []
        for i in range(count):
            area = research_areas[i % len(research_areas)]
            activity = activities[(i // len(research_areas)) % len(activities)]
            domain = domains[(i // (len(research_areas) * len(activities))) % len(domains)]
            
            # ã‚ˆã‚Šè‡ªç„¶ãªæ–‡ç« ç”Ÿæˆ
            episode = f"Recent research in {area} {activity} {domain}, " \
                     f"showing promising results with practical implications for real-world deployment."
            
            episodes.append(episode)
            self.episodes.append({
                'id': i + 1,
                'text': episode,
                'research_area': area,
                'activity_type': activity,
                'domain': domain,
                'timestamp': datetime.now().isoformat()
            })
        
        print(f"âœ… {count}å€‹ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†")
        return episodes
    
    def safe_realtime_insight_detection(self, episode_id: int, episode_text: str) -> Dict[str, Any]:
        """å®‰å…¨ç‰ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡º"""
        try:
            start_time = time.time()
            
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿å­˜
            success = self.memory_manager.store_episode(episode_text, c_value=0.5)
            if not success:
                return None
            
            # ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã«è¿½åŠ 
            episode_vector = self.model.encode([episode_text])[0]
            self.knowledge_graph.add_episode_node(episode_vector, episode_id - 1)
            
            # TopKé¡ä¼¼åº¦è¨ˆç®— (å®Ÿè·µçš„å®Ÿè£…)
            if len(self.memory_manager.episodes) > self.topk_neighbors:
                # æœ€æ–°ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã®é¡ä¼¼åº¦è¨ˆç®—
                current_episodes = self.memory_manager.episodes[-self.topk_neighbors:]
                similarities = []
                
                for ep in current_episodes:
                    sim = np.dot(episode_vector, ep.vec) / (
                        np.linalg.norm(episode_vector) * np.linalg.norm(ep.vec) + 1e-8
                    )
                    similarities.append(sim)
                
                # TopKè¿‘å‚ã§ã®å¤‰åŒ–è¨ˆç®—
                ged_value = self.calculate_practical_ged(similarities, episode_vector)
                ig_value = self.calculate_practical_ig(similarities, episode_vector)
            else:
                # åˆæœŸæ®µéšï¼šåŸºæœ¬çš„ãªå¤‰åŒ–æŒ‡æ¨™
                ged_value = np.random.normal(0.1, 0.05)
                ig_value = np.random.normal(0.08, 0.03)
            
            processing_time = time.time() - start_time
            
            # é–¾å€¤ãƒã‚§ãƒƒã‚¯
            spike_detected = ged_value > self.insight_threshold_ged or ig_value > self.insight_threshold_ig
            
            # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿æ›´æ–°
            self.update_safe_visualization_data(episode_id, ged_value, ig_value, spike_detected)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨˜éŒ²
            self.performance_metrics.append({
                'episode_id': episode_id,
                'processing_time': processing_time,
                'ged_value': ged_value,
                'ig_value': ig_value,
                'spike_detected': spike_detected,
                'topk_neighbors_used': min(len(self.memory_manager.episodes), self.topk_neighbors)
            })
            
            if spike_detected:
                insight = self.register_safe_insight(episode_id, episode_text, ged_value, ig_value)
                return insight
            
            return None
            
        except Exception as e:
            print(f"âŒ æ´å¯Ÿæ¤œå‡ºã‚¨ãƒ©ãƒ¼ (Episode {episode_id}): {e}")
            return None
    
    def calculate_practical_ged(self, similarities: List[float], new_vector: np.ndarray) -> float:
        """å®Ÿè·µçš„GEDè¨ˆç®—"""
        # é¡ä¼¼åº¦å¤‰åŒ–ã«åŸºã¥ãGEDæ¨å®š
        if len(similarities) < 2:
            return 0.1
        
        similarity_variance = np.var(similarities)
        vector_novelty = 1.0 - max(similarities)
        
        # å®Ÿè·µçš„GEDå€¤
        ged_estimate = (similarity_variance * 2) + (vector_novelty * 0.5)
        return max(0.01, min(1.0, ged_estimate))
    
    def calculate_practical_ig(self, similarities: List[float], new_vector: np.ndarray) -> float:
        """å®Ÿè·µçš„IGè¨ˆç®—"""
        # æƒ…å ±ç²å¾—é‡ã®æ¨å®š
        if len(similarities) < 2:
            return 0.05
        
        novelty_score = 1.0 - max(similarities)
        diversity_score = len(set([round(s, 2) for s in similarities])) / len(similarities)
        
        # å®Ÿè·µçš„IGå€¤
        ig_estimate = (novelty_score * 0.3) + (diversity_score * 0.2)
        return max(0.01, min(1.0, ig_estimate))
    
    def register_safe_insight(self, episode_id: int, episode_text: str, 
                             ged_value: float, ig_value: float) -> Dict[str, Any]:
        """å®‰å…¨ç‰ˆæ´å¯Ÿç™»éŒ²"""
        insight_id = f"SAFE_INS_{episode_id:04d}_{int(time.time() * 1000) % 10000}"
        
        insight_data = {
            'id': insight_id,
            'episode_id': episode_id,
            'episode_text': episode_text[:100] + "...",
            'ged_value': ged_value,
            'ig_value': ig_value,
            'detection_timestamp': datetime.now().isoformat(),
            'confidence': min(1.0, (ged_value + ig_value) / 2),
            'type': self.classify_insight_type(ged_value, ig_value),
            'components_used': {
                'memory_manager': True,
                'knowledge_graph': True,
                'topk_optimization': True,
                'safe_mode': True
            }
        }
        
        self.realtime_insights.append(insight_data)
        
        print(f"ğŸ”¥ å®Ÿè·µçš„æ´å¯Ÿæ¤œå‡º: {insight_id} (Episode {episode_id})")
        print(f"   Î”GED: {ged_value:.4f}, Î”IG: {ig_value:.4f}, Type: {insight_data['type']}")
        
        return insight_data
    
    def classify_insight_type(self, ged_value: float, ig_value: float) -> str:
        """æ´å¯Ÿã‚¿ã‚¤ãƒ—ã®åˆ†é¡"""
        total_score = ged_value + ig_value
        
        if total_score > 0.4:
            return "Major_Discovery"
        elif total_score > 0.3:
            return "Significant_Insight"
        elif total_score > 0.2:
            return "Notable_Pattern"
        else:
            return "Micro_Insight"
    
    def update_safe_visualization_data(self, episode_id: int, ged_value: float, 
                                      ig_value: float, spike_detected: bool):
        """å®‰å…¨ç‰ˆãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿æ›´æ–°"""
        node_count = len(self.memory_manager.episodes)
        edge_count = len(self.knowledge_graph.embeddings) if self.knowledge_graph.embeddings else 0
        
        # ã‚°ãƒ©ãƒ•ã®ã‚¨ãƒƒã‚¸æ•°ã‚’æ­£ç¢ºã«å–å¾—
        if hasattr(self.knowledge_graph, 'graph') and self.knowledge_graph.graph.edge_index.numel() > 0:
            edge_count = self.knowledge_graph.graph.edge_index.shape[1]
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¨å®š (ç°¡æ˜“ç‰ˆ)
        memory_usage = node_count * 384 * 4 / (1024 * 1024)  # MBå˜ä½
        
        self.visualization_data['episodes'].append(episode_id)
        self.visualization_data['node_counts'].append(node_count)
        self.visualization_data['edge_counts'].append(edge_count)
        self.visualization_data['ged_values'].append(ged_value)
        self.visualization_data['ig_values'].append(ig_value)
        self.visualization_data['memory_usage'].append(memory_usage)
        
        if spike_detected:
            self.visualization_data['insight_timestamps'].append(episode_id)
    
    def create_comprehensive_visualization(self, save_path: str = None):
        """åŒ…æ‹¬çš„ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ"""
        print("ğŸ“Š åŒ…æ‹¬çš„ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
        
        fig = plt.figure(figsize=(18, 14))
        
        episodes = self.visualization_data['episodes']
        
        # 2x3ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆé…ç½®
        # 1. ãƒãƒ¼ãƒ‰ãƒ»ã‚¨ãƒƒã‚¸æˆé•· + æ´å¯Ÿãƒã‚¤ãƒ³ãƒˆ
        ax1 = plt.subplot(3, 2, 1)
        ax1.plot(episodes, self.visualization_data['node_counts'], 'b-', label='Nodes', linewidth=2, marker='o', markersize=1)
        ax1.plot(episodes, self.visualization_data['edge_counts'], 'r-', label='Edges', linewidth=2, marker='s', markersize=1)
        
        # æ´å¯Ÿç™ºç”Ÿç‚¹ã‚’ãƒãƒ¼ã‚¯
        for insight_ep in self.visualization_data['insight_timestamps']:
            ax1.axvline(x=insight_ep, color='gold', alpha=0.8, linestyle='--', linewidth=2)
            ax1.scatter(insight_ep, 
                       self.visualization_data['node_counts'][insight_ep-1] if insight_ep <= len(episodes) else 0,
                       color='red', s=50, marker='*', zorder=5)
        
        ax1.set_xlabel('Episode')
        ax1.set_ylabel('Count')
        ax1.set_title('Graph Growth with Insight Points')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. Î”GEDé€²åŒ–
        ax2 = plt.subplot(3, 2, 2)
        ax2.plot(episodes, self.visualization_data['ged_values'], 'g-', linewidth=1.5, alpha=0.8)
        ax2.axhline(y=self.insight_threshold_ged, color='red', linestyle='--', 
                   label=f'Threshold ({self.insight_threshold_ged})', linewidth=2)
        ax2.fill_between(episodes, self.visualization_data['ged_values'], 
                        self.insight_threshold_ged, where=[v > self.insight_threshold_ged for v in self.visualization_data['ged_values']], 
                        alpha=0.3, color='red', label='Insight Regions')
        ax2.set_xlabel('Episode')
        ax2.set_ylabel('Î”GED Value')
        ax2.set_title('Î”GED Evolution & Threshold Crossings')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. Î”IGé€²åŒ–
        ax3 = plt.subplot(3, 2, 3)
        ax3.plot(episodes, self.visualization_data['ig_values'], 'm-', linewidth=1.5, alpha=0.8)
        ax3.axhline(y=self.insight_threshold_ig, color='red', linestyle='--', 
                   label=f'Threshold ({self.insight_threshold_ig})', linewidth=2)
        ax3.fill_between(episodes, self.visualization_data['ig_values'], 
                        self.insight_threshold_ig, where=[v > self.insight_threshold_ig for v in self.visualization_data['ig_values']], 
                        alpha=0.3, color='purple', label='Insight Regions')
        ax3.set_xlabel('Episode')
        ax3.set_ylabel('Î”IG Value')
        ax3.set_title('Î”IG Evolution & Threshold Crossings')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. æ´å¯Ÿæ¤œå‡ºåˆ†å¸ƒ
        ax4 = plt.subplot(3, 2, 4)
        insight_episodes = self.visualization_data['insight_timestamps']
        if insight_episodes:
            ax4.hist(insight_episodes, bins=min(20, len(insight_episodes)), alpha=0.7, 
                    color='orange', edgecolor='black', label=f'{len(insight_episodes)} Insights')
            ax4.axhline(y=len(insight_episodes)/20, color='red', linestyle='--', alpha=0.7, label='Average')
        else:
            ax4.text(0.5, 0.5, 'No Insights Detected', ha='center', va='center', transform=ax4.transAxes, fontsize=12)
        
        ax4.set_xlabel('Episode')
        ax4.set_ylabel('Insight Frequency')
        ax4.set_title('Insight Detection Distribution')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        # 5. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        ax5 = plt.subplot(3, 2, 5)
        ax5.plot(episodes, self.visualization_data['memory_usage'], 'c-', linewidth=2, marker='.', markersize=1)
        ax5.set_xlabel('Episode')
        ax5.set_ylabel('Memory Usage (MB)')
        ax5.set_title('Memory Usage Growth')
        ax5.grid(True, alpha=0.3)
        
        # 6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        ax6 = plt.subplot(3, 2, 6)
        if self.performance_metrics:
            processing_times = [m['processing_time'] for m in self.performance_metrics]
            ax6.plot(range(1, len(processing_times) + 1), processing_times, 'k-', alpha=0.7, linewidth=1)
            ax6.axhline(y=np.mean(processing_times), color='red', linestyle='--', 
                       label=f'Average: {np.mean(processing_times):.4f}s', linewidth=2)
        
        ax6.set_xlabel('Episode')
        ax6.set_ylabel('Processing Time (s)')
        ax6.set_title('Per-Episode Processing Time')
        ax6.legend()
        ax6.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š åŒ…æ‹¬çš„ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ä¿å­˜: {save_path}")
        
        plt.show()
    
    def run_safe_practical_experiment(self, num_episodes: int = 1000):
        """å®‰å…¨ç‰ˆå®Ÿè·µçš„å®Ÿé¨“ã®å®Ÿè¡Œ"""
        print(f"ğŸš€ å®‰å…¨ç‰ˆå®Ÿè·µçš„ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®Ÿé¨“é–‹å§‹ ({num_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)")
        print("=" * 70)
        
        start_time = time.time()
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆ
        episodes = self.generate_realistic_episodes(num_episodes)
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†é–‹å§‹
        print(f"\nğŸ”„ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡ºé–‹å§‹ (TopK={self.topk_neighbors})...")
        insights_detected = 0
        processing_times = []
        
        for i, episode_text in enumerate(episodes, 1):
            episode_start = time.time()
            
            # å®‰å…¨ç‰ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡º
            insight = self.safe_realtime_insight_detection(i, episode_text)
            
            if insight:
                insights_detected += 1
            
            episode_time = time.time() - episode_start
            processing_times.append(episode_time)
            
            # é€²æ—è¡¨ç¤º
            if i % 100 == 0:
                elapsed = time.time() - start_time
                eps_per_sec = i / elapsed
                avg_time = np.mean(processing_times[-100:])
                print(f"ğŸ“ˆ é€²æ—: {i}/{num_episodes} ({eps_per_sec:.1f} eps/sec, "
                      f"{insights_detected} insights, avg: {avg_time:.4f}s/ep)")
        
        # å®Ÿé¨“å®Œäº†çµ±è¨ˆ
        total_time = time.time() - start_time
        final_eps_per_sec = num_episodes / total_time
        
        print(f"\nâœ… å®‰å…¨ç‰ˆå®Ÿè·µçš„å®Ÿé¨“å®Œäº†!")
        print(f"   ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {num_episodes}")
        print(f"   æ¤œå‡ºã•ã‚ŒãŸæ´å¯Ÿ: {insights_detected}")
        print(f"   å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")
        print(f"   å‡¦ç†é€Ÿåº¦: {final_eps_per_sec:.2f} eps/sec")
        print(f"   å¹³å‡å‡¦ç†æ™‚é–“: {np.mean(processing_times):.4f}ç§’/ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
        print(f"   æ´å¯Ÿæ¤œå‡ºç‡: {(insights_detected/num_episodes)*100:.2f}%")
        
        return {
            'num_episodes': num_episodes,
            'insights_detected': insights_detected,
            'total_time': total_time,
            'episodes_per_second': final_eps_per_sec,
            'avg_processing_time': np.mean(processing_times),
            'insight_detection_rate': (insights_detected/num_episodes)*100,
            'topk_neighbors': self.topk_neighbors,
            'components_used': ['L2MemoryManager', 'KnowledgeGraphMemory', 'TopK_Optimization'],
            'safe_mode': True
        }
    
    def save_safe_practical_results(self, experiment_results: Dict[str, Any]):
        """å®‰å…¨ç‰ˆå®Ÿé¨“çµæœã®ä¿å­˜"""
        print(f"\nğŸ’¾ å®‰å…¨ç‰ˆå®Ÿé¨“çµæœã‚’ä¿å­˜ä¸­...")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir = Path("experiments/outputs/safe_practical_realtime")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è©³ç´°
        episodes_file = output_dir / "01_safe_input_episodes.csv"
        with open(episodes_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['episode_id', 'episode_text', 'research_area', 'activity_type', 'domain', 'timestamp'])
            
            for ep in self.episodes:
                writer.writerow([ep['id'], ep['text'], ep['research_area'], ep['activity_type'], ep['domain'], ep['timestamp']])
        
        # 2. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿè©³ç´°
        insights_file = output_dir / "02_safe_realtime_insights.csv"
        with open(insights_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'insight_id', 'episode_id', 'episode_text', 'ged_value', 'ig_value',
                'confidence', 'insight_type', 'detection_timestamp'
            ])
            
            for insight in self.realtime_insights:
                writer.writerow([
                    insight['id'], insight['episode_id'], insight['episode_text'],
                    insight['ged_value'], insight['ig_value'], insight['confidence'],
                    insight['type'], insight['detection_timestamp']
                ])
        
        # 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©³ç´°
        performance_file = output_dir / "03_performance_metrics.csv"
        with open(performance_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'episode_id', 'processing_time', 'ged_value', 'ig_value', 
                'spike_detected', 'topk_neighbors_used'
            ])
            
            for metric in self.performance_metrics:
                writer.writerow([
                    metric['episode_id'], metric['processing_time'], metric['ged_value'],
                    metric['ig_value'], metric['spike_detected'], metric['topk_neighbors_used']
                ])
        
        # 4. å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        metadata_file = output_dir / "04_experiment_metadata.json"
        metadata = {
            'experiment_type': 'Safe Practical Realtime Insight Detection',
            'main_agent_used': False,
            'cli_integration': False,
            'direct_components_used': True,
            'safe_mode': True,
            'topk_optimization': True,
            'components': {
                'L2MemoryManager': True,
                'KnowledgeGraphMemory': True,
                'EmbeddingModel': 'paraphrase-MiniLM-L6-v2',
                'dimension': 384
            },
            'thresholds': {
                'ged_threshold': self.insight_threshold_ged,
                'ig_threshold': self.insight_threshold_ig,
                'topk_neighbors': self.topk_neighbors
            },
            'results': experiment_results,
            'timestamp': datetime.now().isoformat()
        }
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        # 5. åŒ…æ‹¬çš„ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
        viz_file = output_dir / "05_comprehensive_visualization.png"
        self.create_comprehensive_visualization(save_path=str(viz_file))
        
        print(f"âœ… å®‰å…¨ç‰ˆå®Ÿé¨“çµæœä¿å­˜å®Œäº†:")
        print(f"   ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
        print(f"   ğŸ“„ å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {episodes_file}")
        print(f"   ğŸ“„ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿ: {insights_file}")
        print(f"   ğŸ“„ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ: {performance_file}")
        print(f"   ğŸ“„ å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {metadata_file}")
        print(f"   ğŸ“Š åŒ…æ‹¬çš„å¯è¦–åŒ–: {viz_file}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    experiment = SafePracticalRealtimeExperiment()
    
    try:
        # å®‰å…¨ç‰ˆå®Ÿè·µçš„å®Ÿé¨“å®Ÿè¡Œ
        results = experiment.run_safe_practical_experiment(num_episodes=1000)
        
        # çµæœä¿å­˜
        experiment.save_safe_practical_results(results)
        
        print(f"\nğŸ‰ å®‰å…¨ç‰ˆå®Ÿè·µçš„ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿå®Ÿé¨“ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ!")
        print(f"   å€‹åˆ¥ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆä½¿ç”¨: âœ…")
        print(f"   L2MemoryManager: âœ…") 
        print(f"   KnowledgeGraphMemory: âœ…")
        print(f"   TopKæœ€é©åŒ–: âœ…")
        print(f"   åŒ…æ‹¬çš„å¯è¦–åŒ–: âœ…")
        print(f"   å®‰å…¨ãƒ¢ãƒ¼ãƒ‰: âœ…")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ å®Ÿé¨“ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ å®Ÿé¨“ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
