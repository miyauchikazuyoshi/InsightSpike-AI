#!/usr/bin/env python3
"""
å®Ÿè·µçš„ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡ºå®Ÿé¨“ - ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚©ãƒ«ãƒˆè§£æ±ºç‰ˆ
=======================================================

SafeMainAgentã‚’ä½¿ç”¨ã—ã¦ã€æ¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ´å¯Ÿæ¤œå‡ºã¨ã‚°ãƒ©ãƒ•ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè£…
"""

import sys
import os
import json
import csv
import time
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional

# InsightSpike-AIã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent / "src"))

import logging
from safe_main_agent_test import SafeMainAgent

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RealTimeInsightDetector:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡ºå™¨"""
    
    def __init__(self):
        self.agent = SafeMainAgent()
        self.insight_events = []
        self.episode_history = []
        self.similarity_timeline = []
        self.graph_growth_data = []
        
        # æ´å¯Ÿæ¤œå‡ºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.similarity_threshold = 0.7  # é«˜é¡ä¼¼åº¦é–¾å€¤
        self.novelty_threshold = 0.3     # æ–°è¦æ€§é–¾å€¤
        self.insight_window = 5          # ç›´å‰5ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§è©•ä¾¡
        
    def initialize(self) -> bool:
        """æ¤œå‡ºå™¨ã‚’åˆæœŸåŒ–"""
        logger.info("ğŸš€ Initializing RealTimeInsightDetector...")
        return self.agent.initialize()
    
    def calculate_episode_novelty(self, new_episode: str) -> float:
        """æ–°ã—ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®æ–°è¦æ€§ã‚’è¨ˆç®—"""
        if len(self.episode_history) == 0:
            return 1.0  # æœ€åˆã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¯å®Œå…¨ã«æ–°è¦
        
        # ç›´è¿‘ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
        recent_episodes = self.episode_history[-self.insight_window:]
        
        max_similarity = 0.0
        for past_episode in recent_episodes:
            # ç°¡æ˜“é¡ä¼¼åº¦è¨ˆç®—ï¼ˆèªå½™é‡è¤‡ãƒ™ãƒ¼ã‚¹ï¼‰
            new_words = set(new_episode.lower().split())
            past_words = set(past_episode['text'].lower().split())
            
            if len(new_words | past_words) > 0:
                similarity = len(new_words & past_words) / len(new_words | past_words)
                max_similarity = max(max_similarity, similarity)
        
        return 1.0 - max_similarity  # æ–°è¦æ€§ = 1 - æœ€å¤§é¡ä¼¼åº¦
    
    def detect_insight_spike(self, episode_id: int, novelty: float, memory_stats: Dict) -> Optional[Dict]:
        """æ´å¯Ÿã‚¹ãƒ‘ã‚¤ã‚¯ã‚’æ¤œå‡º"""
        try:
            # æ´å¯Ÿæ¤œå‡ºæ¡ä»¶
            high_novelty = novelty > self.novelty_threshold
            memory_growth = memory_stats.get('total_episodes', 0) >= episode_id
            
            # ã‚¹ãƒ‘ã‚¤ã‚¯å¼·åº¦ã‚’è¨ˆç®—
            spike_strength = novelty
            if len(self.similarity_timeline) > 0:
                avg_novelty = np.mean([s['novelty'] for s in self.similarity_timeline[-10:]])
                spike_strength = novelty / (avg_novelty + 0.1)
            
            # æ´å¯Ÿæ¤œå‡º
            insight_detected = high_novelty and spike_strength > 1.5
            
            if insight_detected:
                insight_event = {
                    'insight_id': f"RT_INS_{episode_id:04d}",
                    'episode_id': episode_id,
                    'novelty_score': novelty,
                    'spike_strength': spike_strength,
                    'detection_timestamp': datetime.now().isoformat(),
                    'insight_type': self._classify_insight_type(novelty, spike_strength),
                    'memory_state': memory_stats.copy()
                }
                
                logger.info(f"ğŸ”¥ Insight detected: {insight_event['insight_id']} (novelty: {novelty:.3f})")
                return insight_event
            
            return None
            
        except Exception as e:
            logger.error(f"Insight detection failed: {e}")
            return None
    
    def _classify_insight_type(self, novelty: float, spike_strength: float) -> str:
        """æ´å¯Ÿã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡"""
        if novelty > 0.8 and spike_strength > 3.0:
            return "Breakthrough_Insight"
        elif novelty > 0.6 and spike_strength > 2.5:
            return "Major_Discovery"
        elif novelty > 0.4 and spike_strength > 2.0:
            return "Conceptual_Shift"
        else:
            return "Minor_Innovation"
    
    def process_episode(self, episode_text: str, episode_id: int) -> Dict[str, Any]:
        """å˜ä¸€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å‡¦ç†"""
        try:
            start_time = time.time()
            
            # 1. æ–°è¦æ€§è¨ˆç®—
            novelty = self.calculate_episode_novelty(episode_text)
            
            # 2. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¿å­˜
            success = self.agent.store_episode(episode_text, c_value=0.5)
            
            # 3. ãƒ¡ãƒ¢ãƒªçµ±è¨ˆå–å¾—
            memory_stats = self.agent.get_memory_stats()
            
            # 4. æ´å¯Ÿæ¤œå‡º
            insight_event = self.detect_insight_spike(episode_id, novelty, memory_stats)
            if insight_event:
                self.insight_events.append(insight_event)
            
            # 5. å‡¦ç†æ™‚é–“è¨ˆç®—
            processing_time = time.time() - start_time
            
            # 6. ãƒ‡ãƒ¼ã‚¿è¨˜éŒ²
            episode_data = {
                'episode_id': episode_id,
                'text': episode_text,
                'novelty': novelty,
                'processing_time': processing_time,
                'storage_success': success,
                'timestamp': datetime.now().isoformat()
            }
            
            self.episode_history.append(episode_data)
            
            # 7. é¡ä¼¼åº¦ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³æ›´æ–°
            self.similarity_timeline.append({
                'episode_id': episode_id,
                'novelty': novelty,
                'memory_size': memory_stats.get('total_episodes', 0),
                'timestamp': datetime.now().timestamp()
            })
            
            # 8. ã‚°ãƒ©ãƒ•æˆé•·ãƒ‡ãƒ¼ã‚¿æ›´æ–°
            self.graph_growth_data.append({
                'episode_id': episode_id,
                'total_episodes': memory_stats.get('total_episodes', 0),
                'dimension': memory_stats.get('dimension', 384),
                'index_trained': memory_stats.get('index_trained', False),
                'insight_detected': insight_event is not None
            })
            
            return {
                'success': success,
                'novelty': novelty,
                'processing_time': processing_time,
                'insight_detected': insight_event is not None,
                'memory_stats': memory_stats
            }
            
        except Exception as e:
            logger.error(f"Episode processing failed: {e}")
            return {'success': False, 'error': str(e)}
    
    def run_experiment(self, num_episodes: int = 1000):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡ºå®Ÿé¨“ã‚’å®Ÿè¡Œ"""
        logger.info(f"ğŸš€ Starting real-time insight detection experiment ({num_episodes} episodes)")
        
        start_time = time.time()
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆã¨ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†
        for i in range(1, num_episodes + 1):
            episode_text = self._generate_episode(i)
            result = self.process_episode(episode_text, i)
            
            # é€²æ—è¡¨ç¤º
            if i % 100 == 0:
                elapsed = time.time() - start_time
                eps_per_sec = i / elapsed
                logger.info(f"ğŸ“ˆ Progress: {i}/{num_episodes} ({eps_per_sec:.1f} eps/sec, {len(self.insight_events)} insights)")
        
        # å®Ÿé¨“å®Œäº†
        total_time = time.time() - start_time
        logger.info(f"âœ… Experiment completed in {total_time:.2f}s")
        logger.info(f"   Total insights detected: {len(self.insight_events)}")
        logger.info(f"   Average processing speed: {num_episodes/total_time:.2f} eps/sec")
        
        return {
            'total_episodes': num_episodes,
            'total_insights': len(self.insight_events),
            'total_time': total_time,
            'avg_eps_per_sec': num_episodes / total_time
        }
    
    def _generate_episode(self, episode_id: int) -> str:
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
        # 10ã®ãƒ™ãƒ¼ã‚¹ãƒˆãƒ”ãƒƒã‚¯
        topics = [
            "AI healthcare", "ML training", "Deep learning", "NLP interaction",
            "Computer vision", "Predictive analytics", "Data science", 
            "Neural networks", "Automation", "Personalized medicine"
        ]
        
        # ä¿®æ­£ã¨ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        modifications = [
            "advanced algorithms", "large datasets", "real-time processing",
            "improved accuracy", "cost reduction", "enhanced security",
            "cloud integration", "mobile optimization", "user experience",
            "scalability", "performance", "innovation", "automation",
            "intelligence", "efficiency", "reliability", "flexibility"
        ]
        
        # ãƒ©ãƒ³ãƒ€ãƒ ãªçµ„ã¿åˆã‚ã›ã§æ–°è¦æ€§ã‚’æ³¨å…¥
        topic = topics[episode_id % len(topics)]
        mod = modifications[(episode_id * 3) % len(modifications)]
        
        # æ™‚ã€…å®Œå…¨ã«æ–°ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ³¨å…¥ï¼ˆæ´å¯Ÿèª˜ç™ºï¼‰
        if episode_id % 137 == 0:  # ç´ æ•°é–“éš”ã§æ–°ãƒ‘ã‚¿ãƒ¼ãƒ³
            return f"Revolutionary breakthrough in {topic}: {mod} enables unprecedented capabilities through quantum-enhanced methodologies."
        elif episode_id % 73 == 0:
            return f"Paradigm shift discovered: {topic} integration with {mod} creates emergent properties beyond traditional approaches."
        else:
            return f"{topic.title()} systems can leverage {mod} for enhanced performance and user satisfaction."
    
    def visualize_results(self):
        """çµæœã‚’ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³"""
        logger.info("ğŸ“Š Generating visualizations...")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir = Path("experiments/outputs/realtime_insights")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. æ–°è¦æ€§ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³
        plt.figure(figsize=(12, 6))
        episodes = [s['episode_id'] for s in self.similarity_timeline]
        novelties = [s['novelty'] for s in self.similarity_timeline]
        
        plt.subplot(2, 1, 1)
        plt.plot(episodes, novelties, 'b-', alpha=0.7, linewidth=1)
        plt.axhline(y=self.novelty_threshold, color='r', linestyle='--', label='Novelty Threshold')
        
        # æ´å¯Ÿã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
        for insight in self.insight_events:
            plt.axvline(x=insight['episode_id'], color='red', alpha=0.8, linewidth=2)
        
        plt.title('Episode Novelty Timeline')
        plt.xlabel('Episode ID')
        plt.ylabel('Novelty Score')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 2. ãƒ¡ãƒ¢ãƒªæˆé•·
        plt.subplot(2, 1, 2)
        episodes = [g['episode_id'] for g in self.graph_growth_data]
        memory_sizes = [g['total_episodes'] for g in self.graph_growth_data]
        
        plt.plot(episodes, memory_sizes, 'g-', linewidth=2, label='Memory Size')
        
        # æ´å¯Ÿã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒãƒ¼ã‚¯
        insight_episodes = [i['episode_id'] for i in self.insight_events]
        insight_memory_sizes = [g['total_episodes'] for g in self.graph_growth_data 
                               if g['episode_id'] in insight_episodes]
        
        plt.scatter(insight_episodes, insight_memory_sizes, 
                   color='red', s=100, marker='*', label='Insights Detected', zorder=5)
        
        plt.title('Memory Growth & Insight Detection')
        plt.xlabel('Episode ID')
        plt.ylabel('Total Episodes in Memory')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_dir / "realtime_insights_timeline.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. æ´å¯Ÿåˆ†å¸ƒãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        if len(self.insight_events) > 0:
            plt.figure(figsize=(10, 6))
            
            # æ´å¯Ÿã‚¿ã‚¤ãƒ—ã®åˆ†å¸ƒ
            insight_types = [i['insight_type'] for i in self.insight_events]
            unique_types = list(set(insight_types))
            type_counts = [insight_types.count(t) for t in unique_types]
            
            plt.bar(unique_types, type_counts, color='skyblue')
            plt.title('Distribution of Insight Types')
            plt.xlabel('Insight Type')
            plt.ylabel('Count')
            plt.xticks(rotation=45)
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(output_dir / "insight_type_distribution.png", dpi=300, bbox_inches='tight')
            plt.close()
        
        logger.info(f"ğŸ“Š Visualizations saved to: {output_dir}")
    
    def save_detailed_summary(self):
        """è©³ç´°ã‚µãƒãƒªã‚’ä¿å­˜"""
        logger.info("ğŸ’¾ Saving detailed summary...")
        
        output_dir = Path("experiments/outputs/realtime_insights")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è©³ç´°
        episodes_file = output_dir / "01_input_episodes_realtime.csv"
        with open(episodes_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['episode_id', 'episode_text', 'novelty_score', 'processing_time', 'timestamp'])
            
            for ep in self.episode_history:
                writer.writerow([
                    ep['episode_id'], ep['text'], ep['novelty'], 
                    ep['processing_time'], ep['timestamp']
                ])
        
        # 2. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿã‚¤ãƒ™ãƒ³ãƒˆ
        insights_file = output_dir / "02_realtime_insights_detailed.csv"
        with open(insights_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'insight_id', 'episode_id', 'insight_type', 'novelty_score', 
                'spike_strength', 'detection_timestamp', 'memory_size'
            ])
            
            for insight in self.insight_events:
                writer.writerow([
                    insight['insight_id'], insight['episode_id'], insight['insight_type'],
                    insight['novelty_score'], insight['spike_strength'], 
                    insight['detection_timestamp'], insight['memory_state'].get('total_episodes', 0)
                ])
        
        # 3. å®Œå…¨è©³ç´°JSON
        full_details_file = output_dir / "03_realtime_experiment_full_details.json"
        full_data = {
            'experiment_metadata': {
                'experiment_type': 'realtime_insight_detection',
                'total_episodes': len(self.episode_history),
                'total_insights': len(self.insight_events),
                'novelty_threshold': self.novelty_threshold,
                'insight_window': self.insight_window,
                'generation_timestamp': datetime.now().isoformat()
            },
            'episode_history': self.episode_history,
            'insight_events': self.insight_events,
            'similarity_timeline': self.similarity_timeline,
            'graph_growth_data': self.graph_growth_data
        }
        
        with open(full_details_file, 'w', encoding='utf-8') as f:
            json.dump(full_data, f, indent=2, ensure_ascii=False)
        
        # 4. ã‚µãƒãƒªçµ±è¨ˆ
        summary_file = output_dir / "04_experiment_summary.json"
        summary = {
            'total_episodes_processed': len(self.episode_history),
            'total_insights_detected': len(self.insight_events),
            'insight_detection_rate': len(self.insight_events) / len(self.episode_history) if self.episode_history else 0,
            'average_novelty': np.mean([ep['novelty'] for ep in self.episode_history]) if self.episode_history else 0,
            'average_processing_time': np.mean([ep['processing_time'] for ep in self.episode_history]) if self.episode_history else 0,
            'insight_types_detected': list(set([i['insight_type'] for i in self.insight_events])),
            'peak_novelty_episode': max(self.episode_history, key=lambda x: x['novelty'])['episode_id'] if self.episode_history else None
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“„ Detailed summary saved:")
        logger.info(f"   ğŸ“„ Episodes: {episodes_file}")
        logger.info(f"   ğŸ“„ Insights: {insights_file}")  
        logger.info(f"   ğŸ“„ Full details: {full_details_file}")
        logger.info(f"   ğŸ“„ Summary: {summary_file}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    detector = RealTimeInsightDetector()
    
    try:
        # åˆæœŸåŒ–
        if not detector.initialize():
            logger.error("Failed to initialize detector")
            return
        
        # å®Ÿé¨“å®Ÿè¡Œ
        results = detector.run_experiment(num_episodes=1000)
        
        # çµæœä¿å­˜ã¨ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
        detector.save_detailed_summary()
        detector.visualize_results()
        
        logger.info("ğŸ‰ Real-time insight detection experiment completed successfully!")
        
    except Exception as e:
        logger.error(f"Experiment failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
