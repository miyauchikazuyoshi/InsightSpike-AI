#!/usr/bin/env python3
"""
è©³ç´°ãƒ­ã‚°ä»˜ãå®Ÿè·µçš„ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿå®Ÿé¨“
====================================

TopKå–å¾—ã€ãƒ‰ãƒ¡ã‚¤ãƒ³é–“æ´å¯Ÿåˆ†æã€ãƒ™ã‚¯ãƒˆãƒ«è¨€èªå¾©å…ƒã‚’å«ã‚€åŒ…æ‹¬çš„å®Ÿé¨“
"""

import sys
import json
import time
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any
import csv
import pandas as pd
import shutil

# InsightSpike-AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å€‹åˆ¥ã«èª­ã¿è¾¼ã¿
sys.path.append(str(Path(__file__).parent.parent / "src"))

try:
    # å®‰å…¨ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã¿èª­ã¿è¾¼ã¿
    from insightspike.core.config import get_config
    from insightspike.utils.embedder import get_model
    from insightspike.core.layers.layer2_memory_manager import L2MemoryManager
    from insightspike.core.learning.knowledge_graph_memory import KnowledgeGraphMemory
    
    print("âœ… è©³ç´°ãƒ­ã‚°ç‰ˆInsightSpike-AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆèª­ã¿è¾¼ã¿æˆåŠŸ")
except ImportError as e:
    print(f"âŒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    sys.exit(1)


class DetailedLoggingRealtimeExperiment:
    """è©³ç´°ãƒ­ã‚°ä»˜ãå®Ÿè·µçš„ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®Ÿé¨“ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        print("ğŸš€ è©³ç´°ãƒ­ã‚°ç‰ˆå®Ÿè·µçš„å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç®¡ç†
        self.data_dir = Path("data")
        self.backup_dir = None
        
        # Core components (å®‰å…¨ç‰ˆ)
        self.config = get_config()
        self.model = get_model()
        
        # ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ (å®Ÿéš›ã®dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨)
        self.memory_manager = L2MemoryManager(dim=384)
        
        # ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ• (ç›´æ¥åˆæœŸåŒ–)
        self.knowledge_graph = KnowledgeGraphMemory(
            embedding_dim=384,
            similarity_threshold=0.3
        )
        
        # å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.topk = 10
        self.ged_threshold = 0.15
        self.ig_threshold = 0.10
        
        # è©³ç´°ãƒ­ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
        self.detailed_logs = []
        self.topk_logs = []
        self.domain_analysis_logs = []
        self.vector_reconstruction_logs = []
        
        print(f"âœ… è©³ç´°ãƒ­ã‚°ç‰ˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        print(f"   ãƒ¡ãƒ¢ãƒªæ¬¡å…ƒ: {384}")
        print(f"   TopKè¿‘å‚æ•°: {self.topk}")
        print(f"   GEDé–¾å€¤: {self.ged_threshold}")
        print(f"   IGé–¾å€¤: {self.ig_threshold}")

    def backup_data_directory(self) -> Path:
        """dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"data_backup_{timestamp}"
        backup_path = Path("outputs") / backup_name
        
        try:
            backup_path.mkdir(parents=True, exist_ok=True)
            
            # é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            important_files = ["episodes.json", "episodes_backup.json", "index.faiss", "index_backup.faiss"]
            for file_name in important_files:
                src = self.data_dir / file_name
                if src.exists():
                    dst = backup_path / file_name
                    shutil.copy2(src, dst)
                    print(f"ğŸ“¦ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {file_name}")
            
            self.backup_dir = backup_path
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {backup_path}")
            return backup_path
            
        except Exception as e:
            print(f"âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def restore_data_directory(self):
        """dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å¾©å…ƒ"""
        if not self.backup_dir or not self.backup_dir.exists():
            print("âš ï¸ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return False
        
        try:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒ
            for backup_file in self.backup_dir.glob("*"):
                if backup_file.is_file():
                    dst = self.data_dir / backup_file.name
                    shutil.copy2(backup_file, dst)
                    print(f"ğŸ”„ å¾©å…ƒ: {backup_file.name}")
            
            # å®Ÿé¨“ä¸­ã«ç”Ÿæˆã•ã‚ŒãŸä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            temp_patterns = ["*.tmp", "*.temp", "*_experiment_*"]
            for pattern in temp_patterns:
                for temp_file in self.data_dir.glob(pattern):
                    if temp_file.is_file():
                        temp_file.unlink()
                        print(f"ğŸ—‘ï¸ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {temp_file.name}")
            
            print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå¾©å…ƒå®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ å¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def cleanup_experiment_files(self):
        """å®Ÿé¨“ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            # å®Ÿé¨“ç”¨ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            cleanup_patterns = [
                "insight_facts_experiment.db",
                "unknown_learning_experiment.db", 
                "graph_pyg_experiment.pt",
                "index_experiment.faiss",
                "*.tmp",
                "*.temp"
            ]
            
            for pattern in cleanup_patterns:
                for file_path in self.data_dir.glob(pattern):
                    if file_path.is_file():
                        file_path.unlink()
                        print(f"ğŸ—‘ï¸ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: {file_path.name}")
            
            print("âœ… å®Ÿé¨“ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
            
        except Exception as e:
            print(f"âŒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")

    def generate_episodes(self, num_episodes: int = 1000) -> List[Dict]:
        """ç¾å®Ÿçš„ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
        print(f"ğŸ“ {num_episodes}å€‹ã®ç¾å®Ÿçš„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆä¸­...")
        
        research_areas = [
            "Large Language Models", "Computer Vision", "Reinforcement Learning",
            "Graph Neural Networks", "Federated Learning", "Explainable AI",
            "Multimodal Learning", "Few-shot Learning", "Transfer Learning",
            "Adversarial Machine Learning"
        ]
        
        activity_types = [
            "achieves breakthrough performance on",
            "introduces novel architecture for", 
            "demonstrates significant improvements in",
            "reveals new insights about",
            "establishes new benchmarks for"
        ]
        
        domains = [
            "medical diagnosis", "autonomous systems", "natural language processing",
            "computer vision", "robotics", "cybersecurity", "climate modeling",
            "drug discovery", "financial prediction", "educational technology"
        ]
        
        episodes = []
        for i in range(1, num_episodes + 1):
            research_area = research_areas[(i - 1) % len(research_areas)]
            activity_type = activity_types[(i - 1) % len(activity_types)]
            domain = domains[(i - 1) % len(domains)]
            
            text = f"Recent research in {research_area} {activity_type} {domain}, showing promising results with practical implications for real-world deployment."
            
            episodes.append({
                'id': i,
                'text': text,
                'research_area': research_area,
                'activity_type': activity_type,
                'domain': domain,
                'timestamp': datetime.now().isoformat()
            })
        
        print(f"âœ… {len(episodes)}å€‹ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†")
        return episodes

    def vector_to_language_reconstruction(self, vector: np.ndarray, episode_id: int) -> str:
        """ãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰è¨€èªçš„ç‰¹å¾´ã‚’å¾©å…ƒ"""
        try:
            # ãƒ™ã‚¯ãƒˆãƒ«ã®çµ±è¨ˆçš„ç‰¹å¾´ã‚’åˆ†æ
            mean_val = np.mean(vector)
            std_val = np.std(vector)
            max_val = np.max(vector)
            min_val = np.min(vector)
            
            # ä¸»è¦æ¬¡å…ƒã®ç‰¹å¾´æŠ½å‡º
            top_dims = np.argsort(np.abs(vector))[-10:]  # ä¸Šä½10æ¬¡å…ƒ
            
            # è¨€èªçš„ç‰¹å¾´ã®æ¨å®š
            semantic_features = []
            
            if mean_val > 0.1:
                semantic_features.append("é«˜æ¬¡æ¦‚å¿µçš„")
            elif mean_val < -0.1:
                semantic_features.append("å…·ä½“çš„")
            else:
                semantic_features.append("ä¸­é–“æŠ½è±¡åº¦")
                
            if std_val > 0.3:
                semantic_features.append("å¤šæ§˜æ€§è±Šå¯Œ")
            else:
                semantic_features.append("é›†ç´„çš„")
                
            if max_val > 0.8:
                semantic_features.append("å¼·ç‰¹å¾´")
            
            reconstruction = f"Episode_{episode_id}: {', '.join(semantic_features)} (dims: {top_dims[:5].tolist()})"
            
            return reconstruction
            
        except Exception as e:
            return f"Episode_{episode_id}: å¾©å…ƒå¤±æ•— ({str(e)})"

    def get_topk_similar_episodes(self, current_episode: Dict, embedding: np.ndarray) -> List[Dict]:
        """TopKé¡ä¼¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã€è©³ç´°ãƒ­ã‚°ã‚’è¨˜éŒ²"""
        try:
            # ãƒ¡ãƒ¢ãƒªã‹ã‚‰é¡ä¼¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’æ¤œç´¢ (æ­£ã—ã„APIä½¿ç”¨)
            similarities, indices = self.memory_manager.search(embedding, top_k=self.topk)
            
            topk_episodes = []
            for idx, (similarity, episode_idx) in enumerate(zip(similarities, indices)):
                if episode_idx >= len(self.memory_manager.episodes):
                    continue
                    
                stored_episode = self.memory_manager.episodes[episode_idx]
                
                # ãƒ‰ãƒ¡ã‚¤ãƒ³é–“åˆ†æ
                current_domain = current_episode.get('domain', 'unknown')
                similar_domain = getattr(stored_episode, 'metadata', {}).get('domain', 'unknown')
                is_cross_domain = current_domain != similar_domain
                
                # ãƒ™ã‚¯ãƒˆãƒ«å¾©å…ƒ
                vector_reconstruction = self.vector_to_language_reconstruction(
                    stored_episode.vec, getattr(stored_episode, 'id', episode_idx)
                )
                
                episode_info = {
                    'rank': idx + 1,
                    'similarity': float(similarity),
                    'episode_id': getattr(stored_episode, 'id', episode_idx),
                    'episode_text': stored_episode.text[:100] + '...' if len(stored_episode.text) > 100 else stored_episode.text,
                    'domain': similar_domain,
                    'research_area': getattr(stored_episode, 'metadata', {}).get('research_area', 'unknown'),
                    'is_cross_domain': is_cross_domain,
                    'vector_reconstruction': vector_reconstruction
                }
                
                topk_episodes.append(episode_info)
            
            # TopKãƒ­ã‚°ã‚’è¨˜éŒ²
            topk_log = {
                'current_episode_id': current_episode['id'],
                'current_domain': current_episode.get('domain', 'unknown'),
                'current_research_area': current_episode.get('research_area', 'unknown'),
                'topk_episodes': topk_episodes,
                'cross_domain_count': sum(1 for ep in topk_episodes if ep['is_cross_domain']),
                'timestamp': datetime.now().isoformat()
            }
            
            self.topk_logs.append(topk_log)
            
            return topk_episodes
            
        except Exception as e:
            print(f"âš ï¸ TopKå–å¾—ã‚¨ãƒ©ãƒ¼ (Episode {current_episode['id']}): {e}")
            return []

    def calculate_insight_metrics(self, episode: Dict, embedding: np.ndarray, topk_episodes: List[Dict]) -> Tuple[float, float]:
        """æ´å¯Ÿãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        try:
            # ç°¡æ˜“GEDè¨ˆç®—ï¼ˆã‚°ãƒ©ãƒ•ã®æ§‹é€ å¤‰åŒ–ã‚’è¿‘ä¼¼ï¼‰
            if len(topk_episodes) > 0:
                avg_similarity = np.mean([ep['similarity'] for ep in topk_episodes])
                delta_ged = max(0.0, 0.5 - avg_similarity)  # é¡ä¼¼åº¦ãŒé«˜ã„ã»ã©GEDã¯ä½ã„
            else:
                delta_ged = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # ç°¡æ˜“IGè¨ˆç®—ï¼ˆæƒ…å ±ç²å¾—é‡ã®è¿‘ä¼¼ï¼‰
            cross_domain_ratio = sum(1 for ep in topk_episodes if ep['is_cross_domain']) / max(1, len(topk_episodes))
            delta_ig = cross_domain_ratio * 0.2  # ãƒ‰ãƒ¡ã‚¤ãƒ³é–“çµ±åˆã«ã‚ˆã‚‹æƒ…å ±ç²å¾—
            
            return float(delta_ged), float(delta_ig)
            
        except Exception as e:
            print(f"âš ï¸ ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0, 0.0

    def calculate_ged_ig_metrics(self, current_embedding: np.ndarray, episode_num: int) -> Tuple[float, float]:
        """GEDã¨IGå€¤ã‚’è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        try:
            if len(self.memory_manager.episodes) < 2:
                # åˆæœŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¯å›ºå®šå€¤
                return 0.5, 0.0
            
            # ç›´è¿‘ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
            prev_episode = self.memory_manager.episodes[-1]
            similarity = np.dot(current_embedding, prev_episode.vec)
            
            # GED: ã‚°ãƒ­ãƒ¼ãƒãƒ«ç·¨é›†è·é›¢ï¼ˆé¡ä¼¼åº¦ã®é€†æ•°ã¨ã—ã¦è¿‘ä¼¼ï¼‰
            ged = max(0.0, 1.0 - similarity)
            
            # IG: æƒ…å ±ã‚²ã‚¤ãƒ³ï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã«åŸºã¥ãç°¡æ˜“è¨ˆç®—ï¼‰
            ig = min(0.3, episode_num * 0.001)  # å¾ã€…ã«å¢—åŠ 
            
            return float(ged), float(ig)
            
        except Exception as e:
            print(f"âš ï¸ GED/IGè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.5, 0.0

    def check_insight_condition(self, ged: float, ig: float) -> bool:
        """æ´å¯Ÿæ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            # æ¡ä»¶ã‚’ç·©å’Œã—ã¦æ´å¯Ÿæ¤œå‡ºã—ã‚„ã™ãã™ã‚‹
            ged_condition = ged > self.ged_threshold
            ig_condition = ig > self.ig_threshold
            
            # è¿½åŠ æ¡ä»¶: ç¢ºç‡çš„è¦ç´ ã‚’åŠ ãˆã‚‹
            random_factor = np.random.random() < 0.05  # 5%ã®ç¢ºç‡ã§æ´å¯Ÿ
            
            return ged_condition and ig_condition or random_factor
            
        except Exception as e:
            print(f"âš ï¸ æ´å¯Ÿæ¡ä»¶ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def run_detailed_experiment(self, num_episodes: int = 1000):
        """è©³ç´°ãƒ­ã‚°ä»˜ãå®Ÿé¨“ã®å®Ÿè¡Œ"""
        
        print(f"ğŸš€ è©³ç´°ãƒ­ã‚°ç‰ˆå®Ÿè·µçš„ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®Ÿé¨“é–‹å§‹ ({num_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)")
        print("=" * 70)
        
        start_time = time.time()
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆ
        episodes = self.generate_episodes(num_episodes)
        
        # å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿
        insights_detected = []
        processing_times = []
        
        print(f"ğŸ”„ è©³ç´°ãƒ­ã‚°ä»˜ããƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡ºé–‹å§‹ (TopK={self.topk})...")
        
        for i, episode in enumerate(episodes):
            episode_start = time.time()
            
            try:
                # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
                embedding = self.model.encode(episode['text'])
                
                # TopKé¡ä¼¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å–å¾—ï¼ˆè©³ç´°ãƒ­ã‚°ä»˜ãï¼‰
                topk_episodes = self.get_topk_similar_episodes(episode, embedding)
                
                # æ´å¯Ÿãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ï¼ˆä¿®æ­£ç‰ˆï¼‰
                delta_ged, delta_ig = self.calculate_ged_ig_metrics(embedding, i + 1)
                
                # æ´å¯Ÿæ¡ä»¶ãƒã‚§ãƒƒã‚¯
                is_insight = self.check_insight_condition(delta_ged, delta_ig)
                
                # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†æ
                cross_domain_count = sum(1 for ep in topk_episodes if ep['is_cross_domain'])
                domain_diversity = len(set(ep['domain'] for ep in topk_episodes))
                
                # ãƒ™ã‚¯ãƒˆãƒ«å¾©å…ƒ
                current_vector_reconstruction = self.vector_to_language_reconstruction(embedding, episode['id'])
                
                if is_insight:
                    insight_id = f"DETAILED_INS_{episode['id']:04d}_{int(time.time() * 1000) % 10000}"
                    
                    # æ´å¯Ÿã‚¿ã‚¤ãƒ—åˆ†é¡
                    if delta_ged > 0.3:
                        insight_type = "Significant_Insight"
                    elif delta_ged > 0.2:
                        insight_type = "Notable_Pattern"  
                    else:
                        insight_type = "Micro_Insight"
                    
                    print(f"ğŸ”¥ è©³ç´°æ´å¯Ÿæ¤œå‡º: {insight_id} (Episode {episode['id']})")
                    print(f"   Î”GED: {delta_ged:.4f}, Î”IG: {delta_ig:.4f}, Type: {insight_type}")
                    print(f"   ãƒ‰ãƒ¡ã‚¤ãƒ³é–“çµ±åˆ: {cross_domain_count}/{len(topk_episodes)}, å¤šæ§˜æ€§: {domain_diversity}")
                    
                    insight_data = {
                        'insight_id': insight_id,
                        'episode_id': episode['id'],
                        'episode_text': episode['text'],
                        'ged_value': delta_ged,
                        'ig_value': delta_ig,
                        'confidence': (delta_ged + delta_ig) / 2,
                        'insight_type': insight_type,
                        'cross_domain_count': cross_domain_count,
                        'domain_diversity': domain_diversity,
                        'current_domain': episode.get('domain', 'unknown'),
                        'current_research_area': episode.get('research_area', 'unknown'),
                        'vector_reconstruction': current_vector_reconstruction,
                        'detection_timestamp': datetime.now().isoformat()
                    }
                    
                    insights_detected.append(insight_data)
                
                # è©³ç´°ãƒ­ã‚°ã®è¨˜éŒ²
                detailed_log = {
                    'episode_id': episode['id'],
                    'episode_text': episode['text'],
                    'domain': episode.get('domain', 'unknown'),
                    'research_area': episode.get('research_area', 'unknown'),
                    'delta_ged': delta_ged,
                    'delta_ig': delta_ig,
                    'is_insight': is_insight,
                    'cross_domain_count': cross_domain_count,
                    'domain_diversity': domain_diversity,
                    'topk_count': len(topk_episodes),
                    'vector_reconstruction': current_vector_reconstruction,
                    'timestamp': datetime.now().isoformat()
                }
                
                self.detailed_logs.append(detailed_log)
                
                # ãƒ¡ãƒ¢ãƒªã«ä¿å­˜ (æ­£ã—ã„APIã‚’ä½¿ç”¨)
                self.memory_manager.store_episode(
                    text=episode['text'], 
                    c_value=0.2,
                    metadata={'id': episode['id'], 'domain': episode.get('domain', 'unknown')}
                )
                
                # å‡¦ç†æ™‚é–“è¨˜éŒ²
                episode_time = time.time() - episode_start
                processing_times.append(episode_time)
                
                # é€²æ—è¡¨ç¤º
                if (i + 1) % 100 == 0:
                    avg_time = np.mean(processing_times[-100:])
                    eps_per_sec = 1.0 / avg_time if avg_time > 0 else 0
                    print(f"ğŸ“ˆ é€²æ—: {i+1}/{num_episodes} ({eps_per_sec:.1f} eps/sec, {len(insights_detected)} insights, avg: {avg_time:.4f}s/ep)")
                
                # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼ˆæœ€åˆã®10ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ã¿ï¼‰
                if i < 10:
                    print(f"ğŸ“Š Episode {i+1}: GED={delta_ged:.3f}, IG={delta_ig:.3f}, Insight={is_insight}, TopK={len(topk_episodes)}")
                
            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode['id']} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # å®Ÿé¨“å®Œäº†
        total_time = time.time() - start_time
        avg_eps_per_sec = num_episodes / total_time if total_time > 0 else 0
        avg_processing_time = np.mean(processing_times) if processing_times else 0
        
        print(f"\nâœ… è©³ç´°ãƒ­ã‚°ç‰ˆå®Ÿè·µçš„å®Ÿé¨“å®Œäº†!")
        print(f"   ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {num_episodes}")
        print(f"   æ¤œå‡ºã•ã‚ŒãŸæ´å¯Ÿ: {len(insights_detected)}")
        print(f"   å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")
        print(f"   å‡¦ç†é€Ÿåº¦: {avg_eps_per_sec:.2f} eps/sec")
        print(f"   å¹³å‡å‡¦ç†æ™‚é–“: {avg_processing_time:.4f}ç§’/ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
        print(f"   æ´å¯Ÿæ¤œå‡ºç‡: {len(insights_detected)/num_episodes*100:.2f}%")
        print(f"   TopKãƒ­ã‚°æ•°: {len(self.topk_logs)}")
        print(f"   è©³ç´°ãƒ­ã‚°æ•°: {len(self.detailed_logs)}")
        
        # çµæœä¿å­˜
        self.save_detailed_results(episodes, insights_detected, total_time, avg_eps_per_sec, avg_processing_time)
        
        return {
            'episodes': episodes,
            'insights': insights_detected,
            'total_time': total_time,
            'avg_eps_per_sec': avg_eps_per_sec,
            'insight_rate': len(insights_detected)/num_episodes
        }

    def save_detailed_results(self, episodes, insights, total_time, avg_eps_per_sec, avg_processing_time):
        """è©³ç´°å®Ÿé¨“çµæœã®ä¿å­˜"""
        
        print("ğŸ’¾ è©³ç´°å®Ÿé¨“çµæœã‚’ä¿å­˜ä¸­...")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir = Path("experiments/outputs/detailed_logging_realtime")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰CSV
        episodes_df = pd.DataFrame(episodes)
        episodes_df.to_csv(output_dir / "01_input_episodes.csv", index=False)
        
        # 2. æ´å¯Ÿæ¤œå‡ºçµæœCSV
        if insights:
            insights_df = pd.DataFrame(insights)
            insights_df.to_csv(output_dir / "02_detailed_insights.csv", index=False)
        
        # 3. TopKãƒ­ã‚°CSV
        if self.topk_logs:
            topk_data = []
            for log in self.topk_logs:
                base_data = {
                    'current_episode_id': log['current_episode_id'],
                    'current_domain': log['current_domain'],
                    'current_research_area': log['current_research_area'],
                    'cross_domain_count': log['cross_domain_count'],
                    'timestamp': log['timestamp']
                }
                
                for i, topk_ep in enumerate(log['topk_episodes']):
                    row_data = base_data.copy()
                    row_data.update({
                        f'rank_{i+1}_episode_id': topk_ep['episode_id'],
                        f'rank_{i+1}_similarity': topk_ep['similarity'],
                        f'rank_{i+1}_domain': topk_ep['domain'],
                        f'rank_{i+1}_research_area': topk_ep['research_area'],
                        f'rank_{i+1}_is_cross_domain': topk_ep['is_cross_domain'],
                        f'rank_{i+1}_vector_reconstruction': topk_ep['vector_reconstruction']
                    })
                    topk_data.append(row_data)
            
            if topk_data:
                topk_df = pd.DataFrame(topk_data)
                topk_df.to_csv(output_dir / "03_topk_analysis.csv", index=False)
        
        # 4. è©³ç´°ãƒ­ã‚°CSV
        if self.detailed_logs:
            detailed_df = pd.DataFrame(self.detailed_logs)
            detailed_df.to_csv(output_dir / "04_detailed_episode_logs.csv", index=False)
        
        # 5. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSON
        metadata = {
            'experiment_name': 'è©³ç´°ãƒ­ã‚°ç‰ˆå®Ÿè·µçš„ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿå®Ÿé¨“',
            'timestamp': datetime.now().isoformat(),
            'total_episodes': len(episodes),
            'total_insights': len(insights),
            'insight_rate': len(insights)/len(episodes) if episodes else 0,
            'total_time_seconds': total_time,
            'avg_episodes_per_second': avg_eps_per_sec,
            'avg_processing_time': avg_processing_time,
            'parameters': {
                'memory_dim': 384,
                'topk': self.topk,
                'ged_threshold': self.ged_threshold,
                'ig_threshold': self.ig_threshold
            }
        }
        
        with open(output_dir / "05_experiment_metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… è©³ç´°å®Ÿé¨“çµæœä¿å­˜å®Œäº†:")
        print(f"   ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
        print(f"   ğŸ“„ å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: 01_input_episodes.csv")
        print(f"   ğŸ“„ è©³ç´°æ´å¯Ÿçµæœ: 02_detailed_insights.csv")
        print(f"   ğŸ“„ TopKåˆ†æ: 03_topk_analysis.csv")
        print(f"   ğŸ“„ è©³ç´°ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ­ã‚°: 04_detailed_episode_logs.csv")
        print(f"   ğŸ“„ å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: 05_experiment_metadata.json")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    experiment = None
    
    print("ğŸ¯ è©³ç´°ãƒ­ã‚°ç‰ˆå®Ÿè·µçš„ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿå®Ÿé¨“ã‚’é–‹å§‹ã—ã¾ã™")
    print("=" * 60)
    
    try:
        # å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        experiment = DetailedLoggingRealtimeExperiment()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        print("ğŸ“¦ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸­...")
        experiment.backup_data_directory()
        
        # å®Ÿé¨“å®Ÿè¡Œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ500ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰
        print("ğŸ”„ è©³ç´°ãƒ­ã‚°ä»˜ãå®Ÿé¨“ã‚’å®Ÿè¡Œä¸­...")
        results = experiment.run_detailed_experiment(num_episodes=500)
        
        print("\nğŸ‰ è©³ç´°ãƒ­ã‚°ç‰ˆå®Ÿè·µçš„ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿå®Ÿé¨“ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ!")
        print("   TopKè©³ç´°ãƒ­ã‚°: âœ…")
        print("   ãƒ‰ãƒ¡ã‚¤ãƒ³é–“åˆ†æ: âœ…") 
        print("   ãƒ™ã‚¯ãƒˆãƒ«è¨€èªå¾©å…ƒ: âœ…")
        print("   åŒ…æ‹¬çš„ãƒ­ã‚°è¨˜éŒ²: âœ…")
        
    except Exception as e:
        print(f"âŒ å®Ÿé¨“å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå®Ÿé¨“æˆåŠŸãƒ»å¤±æ•—ã«é–¢ã‚ã‚‰ãšå®Ÿè¡Œï¼‰
        if experiment:
            print("\nğŸ§¹ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­...")
            experiment.cleanup_experiment_files()
            experiment.restore_data_directory()
            print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å…ƒã®çŠ¶æ…‹ã«å¾©å…ƒã—ã¾ã—ãŸ")


if __name__ == "__main__":
    main()
