#!/usr/bin/env python3
"""Analyze blind navigation results."""

import sys
from pathlib import Path
import yaml
import numpy as np
import matplotlib.pyplot as plt

sys.path.append(str(Path(__file__).parent.parent.parent))

from insightspike.environments.maze import SimpleMaze
from insightspike.navigators.blind_experience_navigator import BlindExperienceNavigator
from insightspike.navigators.experience_memory_navigator import ExperienceMemoryNavigator
from insightspike.config.maze_config import MazeNavigatorConfig


def analyze_blind_vs_visual():
    """Compare blind (no cheat) vs visual (with cheat) navigation."""
    # Load config
    config_path = Path(__file__).parent / "maze_experiment_config.yaml"
    with open(config_path, 'r') as f:
        config_dict = yaml.safe_load(f)
    
    nav_config = MazeNavigatorConfig(**config_dict['navigator'])
    
    # Run comparison on single maze type for clarity
    np.random.seed(42)
    maze = SimpleMaze(size=(15, 15), maze_type='dfs')
    
    results = {
        'blind': {'steps': [], 'wall_hits': []},
        'visual': {'steps': [], 'wall_hits': []}
    }
    
    # Run 10 trials
    print("Running comparison experiment...")
    print("=" * 60)
    
    for trial in range(10):
        # Reset maze
        maze = SimpleMaze(size=(15, 15), maze_type='dfs')
        
        # Test blind navigator
        blind_nav = BlindExperienceNavigator(nav_config)
        obs = maze.reset()
        
        for step in range(500):
            action = blind_nav.decide_action(obs, maze)
            obs, reward, done, info = maze.step(action)
            
            if done and maze.agent_pos == maze.goal_pos:
                results['blind']['steps'].append(step + 1)
                results['blind']['wall_hits'].append(blind_nav.wall_hits)
                break
        
        # Test visual navigator
        visual_nav = ExperienceMemoryNavigator(nav_config)
        obs = maze.reset()
        
        visual_wall_views = 0
        for step in range(500):
            # Count how many walls the visual navigator "sees"
            visual_wall_views += len([a for a in range(4) if a not in obs.possible_moves])
            
            action = visual_nav.decide_action(obs, maze)
            obs, reward, done, info = maze.step(action)
            
            if done and maze.agent_pos == maze.goal_pos:
                results['visual']['steps'].append(step + 1)
                results['visual']['wall_hits'].append(visual_wall_views)
                break
    
    # Print results
    print("\nü¶Ø BLIND Navigator (No Visual Cheating):")
    print(f"  Average steps to goal: {np.mean(results['blind']['steps']):.1f} ¬± {np.std(results['blind']['steps']):.1f}")
    print(f"  Average wall hits: {np.mean(results['blind']['wall_hits']):.1f} ¬± {np.std(results['blind']['wall_hits']):.1f}")
    print(f"  Min/Max steps: {min(results['blind']['steps'])}/{max(results['blind']['steps'])}")
    
    print("\nüëÅÔ∏è  VISUAL Navigator (With Visual Info):")
    print(f"  Average steps to goal: {np.mean(results['visual']['steps']):.1f} ¬± {np.std(results['visual']['steps']):.1f}")
    print(f"  Walls seen (not hit): {np.mean(results['visual']['wall_hits']):.1f}")
    print(f"  Min/Max steps: {min(results['visual']['steps'])}/{max(results['visual']['steps'])}")
    
    overhead = (np.mean(results['blind']['steps']) / np.mean(results['visual']['steps']) - 1) * 100
    print(f"\nüìä Performance Comparison:")
    print(f"  Blind overhead: +{overhead:.1f}%")
    print(f"  Efficiency ratio: {np.mean(results['visual']['steps']) / np.mean(results['blind']['steps']):.2f}x")
    
    # Create visualization
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # Box plot of steps
    ax1.boxplot([results['blind']['steps'], results['visual']['steps']], 
                labels=['Blind\n(No Cheat)', 'Visual\n(With Cheat)'])
    ax1.set_ylabel('Steps to Goal')
    ax1.set_title('Navigation Performance Comparison')
    ax1.grid(True, alpha=0.3)
    
    # Scatter plot of wall interactions
    ax2.scatter(results['blind']['steps'], results['blind']['wall_hits'], 
               alpha=0.6, s=100, label='Blind (actual hits)')
    ax2.scatter(results['visual']['steps'], [0]*len(results['visual']['steps']), 
               alpha=0.6, s=100, label='Visual (no hits)')
    ax2.set_xlabel('Steps to Goal')
    ax2.set_ylabel('Wall Hits')
    ax2.set_title('Wall Interactions vs Performance')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('blind_vs_visual_comparison.png', dpi=150)
    plt.show()
    
    # Additional analysis
    print("\nüîç Detailed Analysis:")
    print(f"  Wall hit rate (blind): {np.mean(results['blind']['wall_hits']) / np.mean(results['blind']['steps']):.2f} hits/step")
    print(f"  Learning efficiency: Despite hitting walls, blind navigator still finds goal!")
    print(f"  Memory usage: Both use similar memory structures, but blind must physically test")
    
    # Compare with baseline
    print("\nüìà vs Traditional Methods:")
    print(f"  Q-learning typical: ~1000+ episodes to converge")
    print(f"  Random walk: ~2000+ steps average")
    print(f"  Blind geDIG: {np.mean(results['blind']['steps']):.0f} steps (first try!)")
    print(f"  Visual geDIG: {np.mean(results['visual']['steps']):.0f} steps (first try!)")


if __name__ == "__main__":
    analyze_blind_vs_visual()