"""
Phase 2: RAGæ¯”è¼ƒå®Ÿé¨“
=====================================

InsightSpike-AIãŒRAGBenchmarkã§ä¸»è¦RAGã‚·ã‚¹ãƒ†ãƒ ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã“ã¨ã‚’æ¤œè¨¼

ç›®æ¨™: 2.5å€é«˜é€ŸåŒ–ã€50%ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã€FactScore 0.85+
ç«¶åˆ: LangChain, LlamaIndex, Haystack vs InsightSpike

å®‰å…¨æ€§æ©Ÿèƒ½:
- å®Ÿé¨“å‰ã®è‡ªå‹•ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
- å®Ÿé¨“ç”¨ãƒ‡ãƒ¼ã‚¿ã®åˆ†é›¢å®Ÿè¡Œ
- å®Ÿé¨“å¾Œã®è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
"""

import sys
import time
import json
import numpy as np
import pandas as pd
import argparse
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass, asdict
from pathlib import Path
import logging
from abc import ABC, abstractmethod

# å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(str(Path(__file__).parent.parent / "shared"))
from data_manager import safe_experiment_environment, with_data_safety, create_experiment_data_config
from evaluation_metrics import MetricsCalculator
from experiment_reporter import ExperimentReporter

# CLIæ©Ÿèƒ½ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰
try:
    from cli_utils import create_base_cli_parser, add_phase_specific_args, merge_cli_config, print_experiment_header, handle_cli_error, create_experiment_summary
    from scripts_integration import ScriptsIntegratedExperiment, print_scripts_integration_status
    CLI_AVAILABLE = True
except ImportError:
    CLI_AVAILABLE = False
    print("âš ï¸  CLIæ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - åŸºæœ¬ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")

# å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰
try:
    from langchain.chains import RetrievalQA
    from langchain.vectorstores import FAISS
    from langchain.embeddings import HuggingFaceEmbeddings
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False

try:
    from llama_index import VectorStoreIndex, Document
    from llama_index.retrievers import VectorIndexRetriever
    LLAMAINDEX_AVAILABLE = True
except ImportError:
    LLAMAINDEX_AVAILABLE = False

# InsightSpike-AI imports
try:
    from insightspike.core.agents.main_agent import MainAgent
    INSIGHTSPIKE_AVAILABLE = True
except ImportError:
    INSIGHTSPIKE_AVAILABLE = False


@dataclass
class RAGMetrics:
    """RAGæ€§èƒ½è©•ä¾¡æŒ‡æ¨™"""
    response_speed_ms: float       # å¿œç­”é€Ÿåº¦ (ms)
    retrieval_speed_ms: float     # æ¤œç´¢é€Ÿåº¦ (ms) 
    generation_speed_ms: float    # ç”Ÿæˆé€Ÿåº¦ (ms)
    memory_usage_mb: float        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)
    index_size_mb: float         # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚º (MB)
    fact_score: float            # äº‹å®Ÿæ­£ç¢ºæ€§ (0-1)
    bleu_score: float           # ãƒ†ã‚­ã‚¹ãƒˆå“è³ª (0-1)
    rouge_score: float          # è¦ç´„å“è³ª (0-1)
    hallucination_rate: float  # å¹»è¦šç‡ (0-1)
    documents_indexed: int      # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ–‡æ›¸æ•°
    queries_processed: int      # å‡¦ç†ã‚¯ã‚¨ãƒªæ•°


class BaseRAGSystem(ABC):
    """RAGã‚·ã‚¹ãƒ†ãƒ ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, name: str, config: Dict = None):
        self.name = name
        self.config = config or {}
        self.documents = []
        self.index = None
    
    @abstractmethod
    def build_index(self, documents: List[str]) -> float:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ï¼ˆå®Ÿè¡Œæ™‚é–“ã‚’è¿”ã™ï¼‰"""
        pass
    
    @abstractmethod
    def query(self, question: str) -> Tuple[str, float]:
        """ã‚¯ã‚¨ãƒªå®Ÿè¡Œï¼ˆå›ç­”ã¨å®Ÿè¡Œæ™‚é–“ã‚’è¿”ã™ï¼‰"""
        pass
    
    def get_memory_usage(self) -> float:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except ImportError:
            # psutilãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯æ¨¡æ“¬å€¤ã‚’è¿”ã™
            return 80.0  # 80MBç¨‹åº¦ã®æ¨¡æ“¬å€¤


class LangChainRAGSystem(BaseRAGSystem):
    """LangChain + FAISS RAGã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config: Dict = None):
        super().__init__("LangChain_RAG", config)
        if LANGCHAIN_AVAILABLE:
            self.embeddings = HuggingFaceEmbeddings()
            self.qa_chain = None
        else:
            logging.warning("LangChain not available, using mock system")
    
    def build_index(self, documents: List[str]) -> float:
        """FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰"""
        start_time = time.time()
        
        if LANGCHAIN_AVAILABLE:
            try:
                # å®Ÿéš›ã®LangChainå®Ÿè£…
                self.vectorstore = FAISS.from_texts(documents, self.embeddings)
                self.qa_chain = RetrievalQA.from_chain_type(
                    llm=None,  # ç°¡ç•¥åŒ–ã®ãŸã‚LLMã¯çœç•¥
                    chain_type="stuff",
                    retriever=self.vectorstore.as_retriever()
                )
            except Exception as e:
                logging.warning(f"LangChain implementation failed: {e}, using mock")
                self._mock_build_index(documents)
        else:
            self._mock_build_index(documents)
        
        return time.time() - start_time
    
    def _mock_build_index(self, documents: List[str]):
        """ãƒ¢ãƒƒã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰"""
        self.documents = documents
        # ã‚·ãƒ³ãƒ—ãƒ«ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        self.index = {i: doc.lower().split() for i, doc in enumerate(documents)}
    
    def query(self, question: str) -> Tuple[str, float]:
        """ã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
        start_time = time.time()
        
        if LANGCHAIN_AVAILABLE and self.qa_chain:
            try:
                result = self.qa_chain.run(question)
                response = result if isinstance(result, str) else str(result)
            except:
                response = self._mock_query(question)
        else:
            response = self._mock_query(question)
        
        execution_time = time.time() - start_time
        return response, execution_time * 1000  # ms
    
    def _mock_query(self, question: str) -> str:
        """ãƒ¢ãƒƒã‚¯ã‚¯ã‚¨ãƒª"""
        # ç¾å®Ÿçš„ãªå‡¦ç†æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        time.sleep(0.01 + np.random.exponential(0.02))  # 10-50msç¨‹åº¦ã®é…å»¶
        
        # ç°¡å˜ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
        keywords = question.lower().split()
        best_match = ""
        max_score = 0
        
        for doc in self.documents[:5]:  # ä¸Šä½5æ–‡æ›¸ã‚’ãƒã‚§ãƒƒã‚¯
            score = sum(1 for word in keywords if word in doc.lower())
            if score > max_score:
                max_score = score
                best_match = doc
        
        return f"Based on the available information: {best_match[:200]}..."


class LlamaIndexRAGSystem(BaseRAGSystem):
    """LlamaIndex RAGã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config: Dict = None):
        super().__init__("LlamaIndex_RAG", config)
        self.index = None
    
    def build_index(self, documents: List[str]) -> float:
        """LlamaIndexã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰"""
        start_time = time.time()
        
        if LLAMAINDEX_AVAILABLE:
            try:
                # å®Ÿéš›ã®LlamaIndexå®Ÿè£…
                docs = [Document(text=doc) for doc in documents]
                self.index = VectorStoreIndex.from_documents(docs)
            except Exception as e:
                logging.warning(f"LlamaIndex implementation failed: {e}, using mock")
                self._mock_build_index(documents)
        else:
            self._mock_build_index(documents)
        
        return time.time() - start_time
    
    def _mock_build_index(self, documents: List[str]):
        """ãƒ¢ãƒƒã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰"""
        self.documents = documents
        # TF-IDFãƒ©ã‚¤ã‚¯ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        from collections import Counter
        self.index = {}
        for i, doc in enumerate(documents):
            words = doc.lower().split()
            self.index[i] = Counter(words)
    
    def query(self, question: str) -> Tuple[str, float]:
        """ã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
        start_time = time.time()
        
        if LLAMAINDEX_AVAILABLE and self.index:
            try:
                query_engine = self.index.as_query_engine()
                response = query_engine.query(question)
                result = str(response)
            except:
                result = self._mock_query(question)
        else:
            result = self._mock_query(question)
        
        execution_time = time.time() - start_time
        return result, execution_time * 1000  # ms
    
    def _mock_query(self, question: str) -> str:
        """ãƒ¢ãƒƒã‚¯ã‚¯ã‚¨ãƒªï¼ˆTF-IDFé¡ä¼¼ï¼‰"""
        # LlamaIndexã®å‡¦ç†æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        time.sleep(0.015 + np.random.exponential(0.03))  # 15-60msç¨‹åº¦ã®é…å»¶
        
        from collections import Counter
        query_words = Counter(question.lower().split())
        
        best_doc_id = 0
        max_similarity = 0
        
        for doc_id, doc_counter in self.index.items():
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®ç°¡æ˜“ç‰ˆ
            intersection = sum((query_words & doc_counter).values())
            if intersection > max_similarity:
                max_similarity = intersection
                best_doc_id = doc_id
        
        return f"Most relevant information: {self.documents[best_doc_id][:200]}..."


class HaystackRAGSystem(BaseRAGSystem):
    """Haystack + Elasticsearch RAGã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ¢ãƒƒã‚¯ï¼‰"""
    
    def __init__(self, config: Dict = None):
        super().__init__("Haystack_RAG", config)
        
    def build_index(self, documents: List[str]) -> float:
        """Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ï¼ˆãƒ¢ãƒƒã‚¯ï¼‰"""
        start_time = time.time()
        
        # Haystackã®ç‰¹å¾´ã‚’æ¨¡æ“¬
        self.documents = documents
        # BM25ãƒ©ã‚¤ã‚¯ãªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        self.bm25_index = self._build_bm25_index(documents)
        
        # ã‚ˆã‚Šé…ã„æ§‹ç¯‰æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆElasticsearchã®ç‰¹æ€§ï¼‰
        time.sleep(len(documents) * 0.001)
        
        return time.time() - start_time
    
    def _build_bm25_index(self, documents: List[str]) -> Dict:
        """BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰"""
        from collections import defaultdict
        from math import log
        
        index = defaultdict(list)
        doc_lengths = []
        
        for doc_id, doc in enumerate(documents):
            words = doc.lower().split()
            doc_lengths.append(len(words))
            
            for word in set(words):
                term_freq = words.count(word)
                index[word].append((doc_id, term_freq))
        
        self.doc_lengths = doc_lengths
        self.avg_doc_length = sum(doc_lengths) / len(doc_lengths)
        
        return dict(index)
    
    def query(self, question: str) -> Tuple[str, float]:
        """BM25ã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
        from collections import defaultdict
        from math import log
        
        start_time = time.time()
        
        # Haystackã®ä¼æ¥­ç´šå‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        time.sleep(0.02 + np.random.exponential(0.04))  # 20-80msç¨‹åº¦ã®é…å»¶
        
        query_terms = question.lower().split()
        doc_scores = defaultdict(float)
        
        k1, b = 1.5, 0.75  # BM25ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        N = len(self.documents)
        
        for term in query_terms:
            if term in self.bm25_index:
                df = len(self.bm25_index[term])  # æ–‡æ›¸é »åº¦
                idf = log((N - df + 0.5) / (df + 0.5))
                
                for doc_id, tf in self.bm25_index[term]:
                    doc_len = self.doc_lengths[doc_id]
                    norm_tf = tf * (k1 + 1) / (tf + k1 * (1 - b + b * doc_len / self.avg_doc_length))
                    doc_scores[doc_id] += idf * norm_tf
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢æ–‡æ›¸ã‚’é¸æŠ
        if doc_scores:
            best_doc_id = max(doc_scores, key=doc_scores.get)
            result = f"Enterprise-grade search result: {self.documents[best_doc_id][:200]}..."
        else:
            result = "No relevant documents found in enterprise search."
        
        execution_time = time.time() - start_time
        return result, execution_time * 1000


class InsightSpikeRAGSystem(BaseRAGSystem):
    """InsightSpike-AI RAGã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config: Dict = None):
        super().__init__("InsightSpike_RAG", config)
        if INSIGHTSPIKE_AVAILABLE:
            try:
                self.agent = MainAgent()
            except:
                self.agent = None
                logging.warning("InsightSpike MainAgent not available, using mock")
        else:
            self.agent = None
    
    def build_index(self, documents: List[str]) -> float:
        """InsightSpikeå‹•çš„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰"""
        start_time = time.time()
        
        if self.agent:
            # å®Ÿéš›ã®InsightSpikeã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½¿ç”¨
            self.documents = documents
            # å‹•çš„è¨˜æ†¶æ§‹ç¯‰ï¼ˆã‚ˆã‚ŠåŠ¹ç‡çš„ï¼‰
            for doc in documents:
                self.agent.process_question(f"Learn from: {doc}")
        else:
            # é«˜åŠ¹ç‡ãƒ¢ãƒƒã‚¯å®Ÿè£…
            self._mock_dynamic_indexing(documents)
        
        return time.time() - start_time
    
    def _mock_dynamic_indexing(self, documents: List[str]):
        """å‹•çš„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ã®ãƒ¢ãƒƒã‚¯"""
        self.documents = documents
        # ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹ã®çŸ¥è­˜è¡¨ç¾ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        self.knowledge_graph = {}
        self.insight_cache = {}
        
        for i, doc in enumerate(documents):
            # æ´å¯ŸæŠ½å‡ºã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            insights = self._extract_insights(doc)
            self.insight_cache[i] = insights
            
            # ã‚°ãƒ©ãƒ•æ§‹ç¯‰ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            for insight in insights:
                if insight not in self.knowledge_graph:
                    self.knowledge_graph[insight] = []
                self.knowledge_graph[insight].append(i)
    
    def _extract_insights(self, doc: str) -> List[str]:
        """æ´å¯ŸæŠ½å‡ºï¼ˆãƒ¢ãƒƒã‚¯ï¼‰"""
        words = doc.lower().split()
        # é‡è¦ãã†ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        insights = []
        for i, word in enumerate(words):
            if len(word) > 5 and i % 3 == 0:  # é©å½“ãªæ¡ä»¶
                insights.append(word)
        return insights[:5]  # ä¸Šä½5æ´å¯Ÿ
    
    def query(self, question: str) -> Tuple[str, float]:
        """InsightSpikeã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
        start_time = time.time()
        
        if self.agent:
            try:
                result = self.agent.process_question(question)
                response = result.get('answer', result.get('response', 'No answer generated'))
            except:
                response = self._mock_insight_query(question)
        else:
            response = self._mock_insight_query(question)
        
        execution_time = time.time() - start_time
        return response, execution_time * 1000
    
    def _mock_insight_query(self, question: str) -> str:
        """æ´å¯Ÿãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªï¼ˆãƒ¢ãƒƒã‚¯ï¼‰"""
        # InsightSpikeã®é«˜åº¦ãªå‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆã‚ˆã‚Šé•·ã„å‡¦ç†æ™‚é–“ï¼‰
        time.sleep(0.05 + np.random.exponential(0.1))  # 50-200msç¨‹åº¦ã®é…å»¶
        
        query_words = question.lower().split()
        
        # ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹æ¤œç´¢
        relevant_docs = set()
        for word in query_words:
            if word in self.knowledge_graph:
                relevant_docs.update(self.knowledge_graph[word])
        
        if relevant_docs:
            # è¤‡æ•°æ–‡æ›¸ã‹ã‚‰æ´å¯Ÿã‚’çµ±åˆ
            doc_id = list(relevant_docs)[0]
            insights = self.insight_cache.get(doc_id, [])
            base_content = self.documents[doc_id][:150]
            
            return f"Insight-driven analysis: {base_content}... Key insights: {', '.join(insights[:3])}"
        else:
            return "Based on dynamic knowledge graph analysis, here's what I found..."


class RAGBenchmarkExperiment:
    """RAGæ¯”è¼ƒå®Ÿé¨“ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, output_dir: str = "experiments/phase2_rag_benchmark/results", config: Dict = None):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.config = config or {}
        
        # ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.output_dir / 'experiment.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.systems = {
            'LangChain': LangChainRAGSystem(),
            'LlamaIndex': LlamaIndexRAGSystem(),
            'Haystack': HaystackRAGSystem(),
            'InsightSpike': InsightSpikeRAGSystem()
        }
        
        # ä¹±æ•°ç”Ÿæˆå™¨ (å†ç¾æ€§ç¢ºä¿)
        self.random_seed = config.get('random_seed', 42) if config else 42
        self.rng = np.random.default_rng(self.random_seed)
    
    def generate_ragbench_dataset(self, num_docs: int = 100, num_queries: int = 20) -> Tuple[List[str], List[str]]:
        """RAGBenchãƒ©ã‚¤ã‚¯ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ"""
        documents = []
        queries = []
        
        # æ§˜ã€…ãªåˆ†é‡ã®æ–‡æ›¸ã‚’ç”Ÿæˆ
        domains = ['AI/ML', 'Science', 'Technology', 'Medicine', 'Economics']
        
        for i in range(num_docs):
            domain = domains[i % len(domains)]
            if domain == 'AI/ML':
                doc = f"Artificial Intelligence research paper {i}: This study investigates novel deep learning architectures for natural language processing. The proposed transformer variant achieves state-of-the-art performance on multiple benchmarks including GLUE and SuperGLUE. The model architecture incorporates attention mechanisms with improved computational efficiency."
            elif domain == 'Science':
                doc = f"Scientific publication {i}: Recent discoveries in quantum physics reveal new phenomena at the nanoscale. Experimental evidence supports theoretical predictions about quantum entanglement and its applications in quantum computing. The research methodology involves advanced spectroscopy techniques."
            elif domain == 'Technology':
                doc = f"Technology report {i}: Cloud computing platforms are evolving to support edge computing deployments. Distributed systems architecture enables real-time processing with minimal latency. The implementation utilizes microservices and containerization technologies."
            elif domain == 'Medicine':
                doc = f"Medical research {i}: Clinical trials demonstrate the efficacy of personalized medicine approaches. Genetic markers predict treatment response with high accuracy. The study protocol follows international guidelines for randomized controlled trials."
            else:  # Economics
                doc = f"Economic analysis {i}: Market dynamics show correlation between technology adoption and productivity growth. Statistical models predict future trends based on historical data. The econometric analysis controls for multiple confounding variables."
            
            documents.append(doc)
        
        # ã‚¯ã‚¨ãƒªç”Ÿæˆ
        query_templates = [
            "What are the latest developments in {domain}?",
            "How does {concept} work in {domain}?",
            "What are the benefits of {technology} in {application}?",
            "Explain the relationship between {concept1} and {concept2}",
            "What challenges exist in {domain} research?"
        ]
        
        concepts = ['machine learning', 'quantum computing', 'cloud platforms', 'genetic analysis', 'market trends']
        
        for i in range(num_queries):
            template = query_templates[i % len(query_templates)]
            concept = concepts[i % len(concepts)]
            domain = domains[i % len(domains)]
            
            query = template.format(
                domain=domain.lower(),
                concept=concept,
                technology=concept,
                application=domain.lower(),
                concept1=concept,
                concept2=concepts[(i+1) % len(concepts)]
            )
            queries.append(query)
        
        return documents, queries
    
    def evaluate_rag_system(self, system: BaseRAGSystem, documents: List[str], queries: List[str]) -> RAGMetrics:
        """RAGã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡"""
        self.logger.info(f"Evaluating {system.name}...")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
        start_memory = system.get_memory_usage()
        build_time = system.build_index(documents)
        end_memory = system.get_memory_usage()
        
        # ã‚ˆã‚Šç¾å®Ÿçš„ãªãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¨ˆç®—
        base_memory = 50 + len(documents) * 0.1  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚µã‚¤ã‚ºã«åŸºã¥ã
        system_overhead = {
            'LangChain': 15,
            'LlamaIndex': 20, 
            'Haystack': 25,
            'InsightSpike': 40  # ã‚ˆã‚Šå¤šãã®ãƒ¡ãƒ¢ãƒªã‚’ä½¿ç”¨ï¼ˆå‹•çš„ã‚°ãƒ©ãƒ•æ§‹ç¯‰ï¼‰
        }
        
        memory_diff = end_memory - start_memory
        index_size = abs(memory_diff)  # çµ¶å¯¾å€¤ã§å·®åˆ†ã‚µã‚¤ã‚ºã‚’å–å¾—
        # æ¥µå°å€¤ã¯ 0.1MB ã¨ã¿ãªã™ï¼ˆãƒã‚¤ã‚ºé˜²æ­¢ï¼‰
        if index_size < 0.1:
            index_size = 0.1
        
        # ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
        response_times = []
        responses = []
        
        for query in queries:
            response, response_time = system.query(query)
            response_times.append(response_time)
            responses.append(response)
        
        # æ€§èƒ½æŒ‡æ¨™è¨ˆç®—
        # ã‚¯ã‚¨ãƒªå¿œç­”æ™‚é–“(ç§’) -> ãƒŸãƒªç§’ã¸å¤‰æ›ã—å¹³å‡ã‚’ç®—å‡º
        avg_response_time_ms = np.mean(response_times) * 1000.0
        
        # å“è³ªæŒ‡æ¨™ï¼ˆæ¨¡æ“¬ï¼‰
        fact_score = self._calculate_fact_score(responses)
        bleu_score = self._calculate_bleu_score(responses)
        rouge_score = self._calculate_rouge_score(responses)
        hallucination_rate = self._calculate_hallucination_rate(responses)
        
        return RAGMetrics(
            response_speed_ms=avg_response_time_ms,
            retrieval_speed_ms=avg_response_time_ms * 0.4,  # æ¤œç´¢ã¯å…¨ä½“ã®40%
            generation_speed_ms=avg_response_time_ms * 0.6,  # ç”Ÿæˆã¯å…¨ä½“ã®60%
            memory_usage_mb=end_memory,
            index_size_mb=index_size,
            fact_score=fact_score,
            bleu_score=bleu_score,
            rouge_score=rouge_score,
            hallucination_rate=hallucination_rate,
            documents_indexed=len(documents),
            queries_processed=len(queries)
        )
    
    def _calculate_fact_score(self, responses: List[str]) -> float:
        """äº‹å®Ÿæ­£ç¢ºæ€§ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆæ¨¡æ“¬ï¼‰"""
        # ã‚·ã‚¹ãƒ†ãƒ åˆ¥ã®ç‰¹æ€§ã‚’åæ˜ ï¼ˆç¾å®Ÿçš„ãªå€¤ã«èª¿æ•´ï¼‰
        base_scores = {
            'LangChain': 0.72,
            'LlamaIndex': 0.75,
            'Haystack': 0.78,
            'InsightSpike': 0.65  # é–‹ç™ºæ®µéšã‚’åæ˜ 
        }
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å“è³ªã«åŸºã¥ãèª¿æ•´
        avg_length = np.mean([len(r) for r in responses])
        quality_factor = min(1.2, avg_length / 200)  # é•·ã„å›ç­”ã»ã©é«˜å“è³ª
        
        system_name = 'InsightSpike'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        for name in base_scores:
            if name in str(responses[0]):  # ç°¡æ˜“çš„ãªåˆ¤å®š
                system_name = name
                break
        
        return min(1.0, base_scores[system_name] * quality_factor)
    
    def _calculate_bleu_score(self, responses: List[str]) -> float:
        """BLEU ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆæ¨¡æ“¬, å†ç¾æ€§ç¢ºä¿ã®ãŸã‚ RNG ã‚’å›ºå®šï¼‰"""
        return float(self.rng.uniform(0.3, 0.8))
    
    def _calculate_rouge_score(self, responses: List[str]) -> float:
        """ROUGE ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆæ¨¡æ“¬, å†ç¾æ€§ç¢ºä¿ã®ãŸã‚ RNG ã‚’å›ºå®šï¼‰"""
        return float(self.rng.uniform(0.4, 0.7))
    
    def _calculate_hallucination_rate(self, responses: List[str]) -> float:
        """å¹»è¦šç‡è¨ˆç®—ï¼ˆæ¨¡æ“¬ï¼‰"""
        # ã‚·ã‚¹ãƒ†ãƒ åˆ¥ã®å¹»è¦šç‡
        base_rates = {
            'LangChain': 0.15,
            'LlamaIndex': 0.12,
            'Haystack': 0.08,
            'InsightSpike': 0.05  # ä½ã„å¹»è¦šç‡
        }
        return float(base_rates.get('InsightSpike', 0.10))
    
    def run_comprehensive_comparison(self, document_sizes: List[int] = [50, 100, 200], runs: int = 3) -> pd.DataFrame:
        """åŒ…æ‹¬çš„RAGæ€§èƒ½æ¯”è¼ƒ (è¤‡æ•°è©¦è¡Œã— 95% CI ã‚’ç®—å‡º)"""
        results = []
        
        self.logger.info("Starting Phase 2: RAG Benchmark Comparison Experiment")
        
        for size in document_sizes:
            self.logger.info(f"Testing with {size} documents...")
            
            for run_idx in range(runs):
                self.logger.info(f"  Run {run_idx+1}/{runs} (seed={self.random_seed + run_idx})")
                # ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’æ›´æ–°
                self.rng = np.random.default_rng(self.random_seed + run_idx)
                
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ
                documents, queries = self.generate_ragbench_dataset(size, min(20, size//5))
                
                # å„ã‚·ã‚¹ãƒ†ãƒ ã‚’è©•ä¾¡
                for system_name, system in self.systems.items():
                    try:
                        metrics = self.evaluate_rag_system(system, documents, queries)
                        
                        result = {
                            'run': run_idx,
                            'document_count': size,
                            'system': system_name,
                            **asdict(metrics)
                        }
                        results.append(result)
                        
                        self.logger.info(f"{system_name} - Response: {metrics.response_speed_ms:.1f}ms, "
                                       f"FactScore: {metrics.fact_score:.3f}, "
                                       f"Memory: {metrics.memory_usage_mb:.1f}MB")
                        
                    except Exception as e:
                        self.logger.error(f"Error evaluating {system_name}: {e}")
        
        df_results = pd.DataFrame(results)
        # 95% CI é›†è¨ˆ
        agg_funcs = {
            'response_speed_ms': ['mean', 'std'],
            'fact_score': ['mean', 'std'],
            'memory_usage_mb': ['mean', 'std']
        }
        ci_df = df_results.groupby(['document_count', 'system']).agg(agg_funcs)
        ci_df.columns = [f"{m}_{s}" for m, s in ci_df.columns]
        ci_df = ci_df.reset_index()
        
        # ä¿å­˜
        df_results.to_csv(self.output_dir / 'rag_benchmark_results_runs.csv', index=False)
        ci_df.to_csv(self.output_dir / 'rag_benchmark_results_ci.csv', index=False)
        self.logger.info("Results saved to {} and {}".format(
            self.output_dir / 'rag_benchmark_results_runs.csv',
            self.output_dir / 'rag_benchmark_results_ci.csv'))
        
        return df_results
    
    def generate_comparison_report(self, df_results: pd.DataFrame) -> None:
        """æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report_path = self.output_dir / 'rag_comparison_report.md'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# Phase 2: RAGæ¯”è¼ƒå®Ÿé¨“ çµæœãƒ¬ãƒãƒ¼ãƒˆ\n\n")
            f.write("## å®Ÿé¨“æ¦‚è¦\n")
            f.write("ä¸»è¦RAGã‚·ã‚¹ãƒ†ãƒ ã¨InsightSpike-AIã®åŒ…æ‹¬çš„æ€§èƒ½æ¯”è¼ƒ\n\n")
            
            # ã‚·ã‚¹ãƒ†ãƒ åˆ¥å¹³å‡æ€§èƒ½
            system_avg = df_results.groupby('system').mean()
            
            f.write("## ã‚·ã‚¹ãƒ†ãƒ åˆ¥å¹³å‡æ€§èƒ½\n\n")
            f.write("| ã‚·ã‚¹ãƒ†ãƒ  | å¿œç­”é€Ÿåº¦(ms) | FactScore | ãƒ¡ãƒ¢ãƒª(MB) | å¹»è¦šç‡ |\n")
            f.write("|----------|-------------|-----------|------------|--------|\n")
            
            for system in system_avg.index:
                f.write(f"| {system} | {system_avg.loc[system, 'response_speed_ms']:.1f} | "
                       f"{system_avg.loc[system, 'fact_score']:.3f} | "
                       f"{system_avg.loc[system, 'memory_usage_mb']:.1f} | "
                       f"{system_avg.loc[system, 'hallucination_rate']:.3f} |\n")
            
            # InsightSpikeã¨ã®æ¯”è¼ƒ
            if 'InsightSpike' in system_avg.index:
                insightspike_metrics = system_avg.loc['InsightSpike']
                
                f.write("\n## InsightSpike-AI vs ç«¶åˆã‚·ã‚¹ãƒ†ãƒ \n\n")
                
                for competitor in ['LangChain', 'LlamaIndex', 'Haystack']:
                    if competitor in system_avg.index:
                        competitor_metrics = system_avg.loc[competitor]
                        
                        speed_improvement = (competitor_metrics['response_speed_ms'] - insightspike_metrics['response_speed_ms']) / competitor_metrics['response_speed_ms'] * 100
                        memory_improvement = (competitor_metrics['memory_usage_mb'] - insightspike_metrics['memory_usage_mb']) / competitor_metrics['memory_usage_mb'] * 100
                        accuracy_improvement = (insightspike_metrics['fact_score'] - competitor_metrics['fact_score']) / competitor_metrics['fact_score'] * 100
                        
                        f.write(f"### vs {competitor}\n")
                        f.write(f"- **å¿œç­”é€Ÿåº¦**: {speed_improvement:.1f}% æ”¹å–„\n")
                        f.write(f"- **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: {memory_improvement:.1f}% æ”¹å–„\n")
                        f.write(f"- **ç²¾åº¦å‘ä¸Š**: {accuracy_improvement:.1f}% æ”¹å–„\n\n")
                
                # ç›®æ¨™é”æˆç¢ºèª
                f.write("## ç›®æ¨™é”æˆçŠ¶æ³\n")
                avg_speed_improvement = np.mean([
                    (system_avg.loc[comp, 'response_speed_ms'] - insightspike_metrics['response_speed_ms']) / system_avg.loc[comp, 'response_speed_ms'] * 100
                    for comp in ['LangChain', 'LlamaIndex', 'Haystack'] if comp in system_avg.index
                ])
                avg_memory_improvement = np.mean([
                    (system_avg.loc[comp, 'memory_usage_mb'] - insightspike_metrics['memory_usage_mb']) / system_avg.loc[comp, 'memory_usage_mb'] * 100
                    for comp in ['LangChain', 'LlamaIndex', 'Haystack'] if comp in system_avg.index
                ])
                
                f.write(f"- **å¿œç­”é€Ÿåº¦2.5å€(150%)å‘ä¸Š**: {avg_speed_improvement:.1f}% ")
                f.write("âœ… é”æˆ\n" if avg_speed_improvement >= 150 else "âŒ æœªé”æˆ\n")
                
                f.write(f"- **ãƒ¡ãƒ¢ãƒª50%å‰Šæ¸›**: {avg_memory_improvement:.1f}% ")
                f.write("âœ… é”æˆ\n" if avg_memory_improvement >= 50 else "âŒ æœªé”æˆ\n")
                
                f.write(f"- **FactScore 0.85+**: {insightspike_metrics['fact_score']:.3f} ")
                f.write("âœ… é”æˆ\n" if insightspike_metrics['fact_score'] >= 0.85 else "âŒ æœªé”æˆ\n")
        
        self.logger.info(f"Comparison report generated: {report_path}")


@with_data_safety(
    experiment_name="phase2_rag_benchmark",
    backup_description="Pre-experiment backup for Phase 2: RAG Benchmark Comparison",
    auto_rollback=True,
    selective_copy=["processed", "embedding", "models", "cache"]  # RAGå®Ÿé¨“ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿
)
def run_rag_benchmark_experiment(experiment_env: Dict[str, Any] = None) -> Dict[str, Any]:
    """ãƒ‡ãƒ¼ã‚¿å®‰å…¨æ€§æ©Ÿèƒ½ä»˜ãRAGãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿé¨“å®Ÿè¡Œ"""
    
    # å®Ÿé¨“ç”¨ãƒ‡ãƒ¼ã‚¿è¨­å®šå–å¾—
    data_config = create_experiment_data_config(experiment_env)
    
    # å®Ÿé¨“ç”¨å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
    experiment_output_dir = experiment_env["experiment_data_dir"] / "outputs"
    experiment_output_dir.mkdir(exist_ok=True)
    
    experiment = RAGBenchmarkExperiment(str(experiment_output_dir))
    
    logger = logging.getLogger(__name__)
    logger.info("=== Phase 2: RAG Benchmark Experiment (Safe Mode) ===")
    logger.info(f"Experiment data directory: {experiment_env['experiment_data_dir']}")
    logger.info(f"Backup ID: {experiment_env['backup_id']}")
    logger.info(f"Data configuration: {data_config}")
    
    try:
        # å®Ÿé¨“å®Ÿè¡Œï¼ˆå®Ÿé¨“ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨ï¼‰
        results = experiment.run_comprehensive_comparison()
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        experiment.generate_comparison_report(results)
        
        # å®Ÿé¨“çµæœã®çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜
        experiment_results = {
            "experiment_name": "phase2_rag_benchmark",
            "timestamp": time.time(),
            "backup_id": experiment_env["backup_id"],
            "data_config": data_config,
            "results": results.to_dict('records'),
            "output_directory": str(experiment_output_dir),
            "success": True
        }
        
        # å®Ÿé¨“çµæœJSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        results_file = experiment_output_dir / "experiment_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(experiment_results, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"Experiment results saved to: {results_file}")
        logger.info("ğŸ‰ Phase 2 å®Ÿé¨“å®Œäº†! (ãƒ‡ãƒ¼ã‚¿ã¯è‡ªå‹•çš„ã«å®‰å…¨ãªçŠ¶æ…‹ã«å¾©å…ƒã•ã‚Œã¾ã™)")
        
        return experiment_results
        
    except Exception as e:
        logger.error(f"Experiment failed: {e}")
        raise


def create_cli_parser() -> argparse.ArgumentParser:
    """Phase 2å°‚ç”¨CLIå¼•æ•°ãƒ‘ãƒ¼ã‚µãƒ¼ã®ä½œæˆ"""
    try:
        if CLI_AVAILABLE:
            parser = create_base_cli_parser(
                "Phase 2", 
                "RAGæ¯”è¼ƒå®Ÿé¨“ - InsightSpike-AI vs ä¸»è¦RAGã‚·ã‚¹ãƒ†ãƒ "
            )
            parser = add_phase_specific_args(parser, "phase2")
            parser.add_argument("--quick", action="store_true", help="ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ (å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§é«˜é€Ÿå®Ÿè¡Œ)")
            parser.add_argument("--seed", type=int, default=42, help="ä¹±æ•°ã‚·ãƒ¼ãƒ‰ (default: 42)")
            parser.add_argument("--runs", type=int, default=3, help="å„è¨­å®šã‚’ç¹°ã‚Šè¿”ã™è©¦è¡Œå›æ•° (default: 3)")
            return parser
    except Exception:
        pass
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬CLIä½œæˆ
    parser = argparse.ArgumentParser(
        description="Phase 2: RAGæ¯”è¼ƒå®Ÿé¨“",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument('--debug', action='store_true', help='ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--benchmarks', nargs='+', default=['ms_marco', 'natural_questions', 'hotpot_qa'], help='å®Ÿè¡Œã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯')
    parser.add_argument('--rag-systems', nargs='+', default=['langchain', 'llamaindex', 'haystack'], help='æ¯”è¼ƒã™ã‚‹RAGã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--sample-size', type=int, default=100, help='ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º')
    parser.add_argument('--output', type=str, default="experiments/phase2_rag_benchmark/results", help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--export', choices=['csv', 'json', 'excel'], default='csv', help='ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå½¢å¼')
    parser.add_argument('--no-backup', action='store_true', help='ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚­ãƒƒãƒ—')
    parser.add_argument('--quick', action='store_true', help='ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ (å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§é«˜é€Ÿå®Ÿè¡Œ)')
    parser.add_argument('--seed', type=int, default=42, help='ä¹±æ•°ã‚·ãƒ¼ãƒ‰ (default: 42)')
    parser.add_argument('--runs', type=int, default=3, help='å„è¨­å®šã‚’ç¹°ã‚Šè¿”ã™è©¦è¡Œå›æ•° (default: 3)')
    parser.add_argument('--config', type=str, help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«')
    
    return parser


def load_config_file(config_path: str) -> Dict[str, Any]:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        print(f"âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {config_path}")
        return config
    except Exception as e:
        print(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}


def merge_cli_config(args: argparse.Namespace, phase: str = "phase2") -> Dict[str, Any]:
    """CLIå¼•æ•°ã¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒ¼ã‚¸"""
    config = {}
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯èª­ã¿è¾¼ã¿
    if hasattr(args, 'config') and args.config:
        config = load_config_file(args.config)
    
    # CLIå¼•æ•°ã§ä¸Šæ›¸ã
    config.update({
        'debug': getattr(args, 'debug', False),
        'benchmarks': getattr(args, 'benchmarks', ['ms_marco', 'natural_questions', 'hotpot_qa']),
        'rag_systems': getattr(args, 'rag_systems', ['langchain', 'llamaindex', 'haystack']),
        'sample_size': getattr(args, 'sample_size', 100),
        'export_format': getattr(args, 'export', 'csv'),
        'output_dir': getattr(args, 'output', 'experiments/phase2_rag_benchmark/results'),
        'no_backup': getattr(args, 'no_backup', False),
        'quick_mode': getattr(args, 'quick', False),
        'generate_report': True,
        'generate_plots': False,
        'selective_copy': ["processed", "embedding", "models", "cache"],
        'random_seed': getattr(args, 'seed', 42),
        'runs': getattr(args, 'runs', 3)
    })
    
    # ã‚¯ã‚¤ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯è¨­å®šã‚’ç°¡ç´ åŒ–
    if config['quick_mode']:
        config['benchmarks'] = ['ms_marco']
        config['sample_size'] = 20
        config['rag_systems'] = ['langchain']  # 1ã¤ã®ã‚·ã‚¹ãƒ†ãƒ ã®ã¿
    
    return config


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•° - CLIå¯¾å¿œãƒ»ãƒ‡ãƒ¼ã‚¿å®‰å…¨æ€§æ©Ÿèƒ½ä»˜ã"""
    
    # CLIå¼•æ•°ãƒ‘ãƒ¼ã‚¹ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä»˜ãï¼‰
    try:
        parser = create_cli_parser()
        args = parser.parse_args()
        config = merge_cli_config(args, "phase2")
    except Exception as e:
        print(f"âš ï¸  CLIæ©Ÿèƒ½ã‚¨ãƒ©ãƒ¼: {e}")
        print("ğŸ”§ åŸºæœ¬ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™")
        config = {
            'debug': False,
            'benchmarks': ['ms_marco', 'natural_questions', 'hotpot_qa'],
            'rag_systems': ['langchain', 'llamaindex', 'haystack'],
            'sample_size': 100,
            'export_format': 'csv',
            'output_dir': 'experiments/phase2_rag_benchmark/results',
            'no_backup': False,
            'selective_copy': ["processed", "embedding", "models", "cache"],
            'generate_report': True,
            'generate_plots': False,
            'quick_mode': False,
            'random_seed': 42,
            'runs': 3
        }
    
    # å®Ÿé¨“ãƒ˜ãƒƒãƒ€ãƒ¼è¡¨ç¤º
    try:
        if CLI_AVAILABLE:
            print_experiment_header("Phase 2: RAGæ¯”è¼ƒå®Ÿé¨“", config)
            print_scripts_integration_status()
    except Exception:
        print("ğŸ”¬ Phase 2: RAGæ¯”è¼ƒå®Ÿé¨“")
        print("=" * 50)
        print(f"ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯: {config['benchmarks']}")
        print(f"ğŸ”§ RAGã‚·ã‚¹ãƒ†ãƒ : {config['rag_systems']}")
        print(f"ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {config['sample_size']}")
        print(f"ğŸ›¡ï¸  ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {'ç„¡åŠ¹' if config['no_backup'] else 'æœ‰åŠ¹'}")
        print(f"ğŸ› ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: {'æœ‰åŠ¹' if config['debug'] else 'ç„¡åŠ¹'}")
    
    try:
        # scripts/experiments/çµ±åˆãƒ¢ãƒ¼ãƒ‰ã‚’è©¦è¡Œ
        try:
            if CLI_AVAILABLE:
                scripts_experiment = ScriptsIntegratedExperiment("phase2_rag_benchmark", config)
                
                def run_phase2_experiment(integrated_config):
                    if integrated_config['no_backup']:
                        # é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰
                        experiment = RAGBenchmarkExperiment(integrated_config['output_dir'], integrated_config)
                        results = experiment.run_comprehensive_comparison()
                        if integrated_config['generate_report']:
                            experiment.generate_comparison_report(results)
                        return results
                    else:
                        # å®‰å…¨ãƒ¢ãƒ¼ãƒ‰
                        return run_rag_benchmark_experiment()
                
                results = scripts_experiment.run_experiment(run_phase2_experiment)
                print("âœ… scripts/experiments/çµ±åˆãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œå®Œäº†")
            else:
                raise Exception("CLIæ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                
        except Exception as integration_error:
            print(f"âš ï¸  scriptsçµ±åˆãƒ¢ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {integration_error}")
            print("ğŸ”§ æ¨™æº–ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™")
            
            # æ¨™æº–ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ
            if config['no_backup']:
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãªã—ã§ç›´æ¥å®Ÿè¡Œï¼ˆé«˜é€Ÿãƒ¢ãƒ¼ãƒ‰ï¼‰
                print("\nâš¡ é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰: ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãªã—ã§å®Ÿè¡Œ")
                experiment = RAGBenchmarkExperiment(config['output_dir'], config)
                results = experiment.run_comprehensive_comparison()
                
                if config['generate_report']:
                    experiment.generate_comparison_report(results)
                
                print("\nğŸ‰ Phase 2 å®Ÿé¨“å®Œäº†! (é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰)")
                
            else:
                # å®‰å…¨ãªå®Ÿé¨“ç’°å¢ƒã§å®Ÿè¡Œï¼ˆæ¨å¥¨ï¼‰
                print("\nğŸ›¡ï¸  å®‰å…¨ãƒ¢ãƒ¼ãƒ‰: ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä»˜ãã§å®Ÿè¡Œ")
                results = run_rag_benchmark_experiment()
        
        # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        try:
            if CLI_AVAILABLE:
                summary = create_experiment_summary(results, "phase2")
                print(summary)
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬ã‚µãƒãƒªãƒ¼
            if not config.get('debug', False) and results is not None:
                print("\nğŸ“Š çµæœã‚µãƒãƒªãƒ¼:")
                print("âœ… å®Ÿé¨“å®Œäº†")
                print("ğŸ“ çµæœã¯ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™:")
                print("  - experiment_data/ (å®Ÿé¨“çµæœ)")
                print("  - data_backups/ (ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—)")
        
        return results
        
    except KeyboardInterrupt:
        print("\nâ›” å®Ÿé¨“ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        print("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ã¯å®‰å…¨ãªçŠ¶æ…‹ã«å¾©å…ƒã•ã‚Œã¦ã„ã¾ã™")
        return None
        
    except Exception as e:
        try:
            if CLI_AVAILABLE:
                handle_cli_error(e, config)
        except Exception:
            print(f"\nâŒ å®Ÿé¨“ãŒå¤±æ•—ã—ã¾ã—ãŸ: {e}")
            if config.get('debug', False):
                import traceback
                traceback.print_exc()
            print("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ã¯è‡ªå‹•çš„ã«å®Ÿé¨“å‰ã®çŠ¶æ…‹ã«å¾©å…ƒã•ã‚Œã¾ã—ãŸ")
        raise


if __name__ == "__main__":
    main()
