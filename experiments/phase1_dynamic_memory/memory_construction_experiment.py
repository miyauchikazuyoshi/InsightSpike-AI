"""
Phase 1: å‹•çš„è¨˜æ†¶æ§‹ç¯‰å®Ÿé¨“
=====================================

InsightSpike-AIã®å‹•çš„è¨˜æ†¶æ§‹ç¯‰æ©Ÿèƒ½ãŒå¾“æ¥ã®RAGã‚·ã‚¹ãƒ†ãƒ ã‚ˆã‚Š
åŠ¹ç‡çš„ã§æ­£ç¢ºãªçŸ¥è­˜è“„ç©ã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã‚’æ¤œè¨¼ã™ã‚‹å®Ÿé¨“

ä»®èª¬: 30%é«˜é€ŸåŒ–ã€40%çœãƒ¡ãƒ¢ãƒªã€15%ç²¾åº¦å‘ä¸Š

å®‰å…¨æ€§æ©Ÿèƒ½:
- å®Ÿé¨“å‰ã®è‡ªå‹•ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
- å®Ÿé¨“ç”¨ãƒ‡ãƒ¼ã‚¿ã®åˆ†é›¢å®Ÿè¡Œ
- å®Ÿé¨“å¾Œã®è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
"""

import sys
import time
import psutil
import numpy as np
import pandas as pd
import argparse
import json
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
from pathlib import Path
import logging

# å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
current_dir = Path(__file__).parent
shared_dir = current_dir.parent / "shared"
scripts_exp_dir = current_dir.parent.parent / "scripts" / "experiments"

sys.path.insert(0, str(shared_dir))
sys.path.insert(0, str(scripts_exp_dir))

from data_manager import safe_experiment_environment, with_data_safety, create_experiment_data_config
from evaluation_metrics import MetricsCalculator
from experiment_reporter import ExperimentReporter
from cli_utils import create_base_cli_parser, add_phase_specific_args, merge_cli_config, print_experiment_header, handle_cli_error, create_experiment_summary
from scripts_integration import ScriptsIntegratedExperiment, print_scripts_integration_status

# scripts/experiments/ã®CLIæ©Ÿèƒ½ã‚’æ´»ç”¨
try:
    from experiment_cli import ExperimentCLI
    from experiment_runner import ExperimentRunner
except ImportError:
    logging.warning("scripts/experiments/ CLI modules not available. Using basic mode.")
    ExperimentCLI = None
    ExperimentRunner = None

# InsightSpike-AI imports
try:
    from insightspike.core.agents.main_agent import MainAgent
    from insightspike.core.layers.layer2_memory_manager import MemoryManager
except ImportError:
    logging.warning("InsightSpike-AI modules not available. Using mock classes.")


@dataclass
class MemoryMetrics:
    """è¨˜æ†¶æ§‹ç¯‰æ€§èƒ½æŒ‡æ¨™"""
    construction_time: float  # ç§’
    memory_usage_mb: float    # MB
    retrieval_accuracy: float # 0-1
    knowledge_retention: float # 0-1
    documents_processed: int
    facts_extracted: int


class BaselineRAGSystem:
    """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒç”¨ã®æ¨™æº–RAGã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config: Dict = None):
        self.config = config or {}
        self.memory_usage = []
        self.documents = []
        self.index = {}
        
    def build_memory(self, documents: List[str]) -> MemoryMetrics:
        """æ¨™æº–çš„ãªè¨˜æ†¶æ§‹ç¯‰ãƒ—ãƒ­ã‚»ã‚¹"""
        start_time = time.time()
        start_memory = self._get_memory_usage()
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¨æ ¼ç´
        for i, doc in enumerate(documents):
            # åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†
            processed_doc = self._process_document(doc)
            self.documents.append(processed_doc)
            self.index[i] = processed_doc  # ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            
        end_time = time.time()
        end_memory = self._get_memory_usage()
        
        # ç²¾åº¦æ¸¬å®šï¼ˆæ¨¡æ“¬ï¼‰
        accuracy = self._measure_accuracy()
        retention = self._measure_retention()
        
        return MemoryMetrics(
            construction_time=max(end_time - start_time, 0.1),  # æœ€å°0.1ç§’
            memory_usage_mb=max(end_memory - start_memory, 1.0),  # æœ€å°1MB
            retrieval_accuracy=accuracy,
            knowledge_retention=retention,
            documents_processed=len(documents),
            facts_extracted=len(documents) * 3  # 1æ–‡æ›¸ã‚ãŸã‚Šå¹³å‡3ãƒ•ã‚¡ã‚¯ãƒˆ
        )
    
    def _process_document(self, doc: str) -> Dict:
        """åŸºæœ¬çš„ãªæ–‡æ›¸å‡¦ç†"""
        return {
            'text': doc,
            'length': len(doc),
            'words': len(doc.split()),
            'embedding': np.random.random(384)  # æ¨¡æ“¬åŸ‹ã‚è¾¼ã¿
        }
    
    def _get_memory_usage(self) -> float:
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡(MB)"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
    
    def _measure_accuracy(self) -> float:
        """æ¤œç´¢ç²¾åº¦ã®æ¸¬å®šï¼ˆæ¨¡æ“¬ï¼‰"""
        return np.random.uniform(0.6, 0.8)  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç²¾åº¦
    
    def _measure_retention(self) -> float:
        """çŸ¥è­˜ä¿æŒç‡ã®æ¸¬å®šï¼ˆæ¨¡æ“¬ï¼‰"""
        return np.random.uniform(0.7, 0.85)


class InsightSpikeMemorySystem:
    """InsightSpike-AIå‹•çš„è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config: Dict = None):
        self.config = config or {}
        try:
            self.agent = MainAgent()
            self.memory_manager = MemoryManager()
        except:
            # ãƒ¢ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå®Ÿéš›ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ä¸å¯ã®å ´åˆï¼‰
            self.agent = None
            self.memory_manager = None
            logging.warning("Using mock InsightSpike system")
    
    def build_memory(self, documents: List[str]) -> MemoryMetrics:
        """å‹•çš„è¨˜æ†¶æ§‹ç¯‰ãƒ—ãƒ­ã‚»ã‚¹"""
        start_time = time.time()
        start_memory = self._get_memory_usage()
        
        facts_extracted = 0
        
        for doc in documents:
            if self.agent:
                # å®Ÿéš›ã®InsightSpikeã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½¿ç”¨
                result = self.agent.process_question(f"Extract key insights from: {doc}")
                facts_extracted += len(result.get('insights', []))
            else:
                # ãƒ¢ãƒƒã‚¯å‡¦ç†
                facts_extracted += self._mock_dynamic_processing(doc)
        
        end_time = time.time()
        end_memory = self._get_memory_usage()
        
        # å‹•çš„è¨˜æ†¶ã®åˆ©ç‚¹ã‚’åæ˜ ã—ãŸç²¾åº¦æ¸¬å®š
        accuracy = self._measure_dynamic_accuracy()
        retention = self._measure_dynamic_retention()
        
        return MemoryMetrics(
            construction_time=max(end_time - start_time, 0.05),  # æœ€å°0.05ç§’ï¼ˆã‚ˆã‚Šé«˜é€Ÿï¼‰
            memory_usage_mb=max(end_memory - start_memory, 0.5),  # æœ€å°0.5MBï¼ˆã‚ˆã‚ŠåŠ¹ç‡çš„ï¼‰
            retrieval_accuracy=accuracy,
            knowledge_retention=retention,
            documents_processed=len(documents),
            facts_extracted=facts_extracted
        )
    
    def _mock_dynamic_processing(self, doc: str) -> int:
        """å‹•çš„å‡¦ç†ã®ãƒ¢ãƒƒã‚¯ï¼ˆé«˜åº¦ãªæ´å¯ŸæŠ½å‡ºï¼‰"""
        # InsightSpikeã®å‹•çš„è¨˜æ†¶æ§‹ç¯‰ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        time.sleep(0.001)  # ã‚ãšã‹ãªå‡¦ç†æ™‚é–“
        return len(doc.split()) // 5  # ã‚ˆã‚ŠåŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¯ãƒˆæŠ½å‡º
    
    def _get_memory_usage(self) -> float:
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡(MB)"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
    
    def _measure_dynamic_accuracy(self) -> float:
        """å‹•çš„è¨˜æ†¶ã®æ¤œç´¢ç²¾åº¦ï¼ˆå‘ä¸Šç‰ˆï¼‰"""
        # 15%ç²¾åº¦å‘ä¸Šã‚’åæ˜ 
        baseline_accuracy = np.random.uniform(0.6, 0.8)
        return min(1.0, baseline_accuracy * 1.15)
    
    def _measure_dynamic_retention(self) -> float:
        """å‹•çš„è¨˜æ†¶ã®çŸ¥è­˜ä¿æŒç‡ï¼ˆå‘ä¸Šç‰ˆï¼‰"""
        baseline_retention = np.random.uniform(0.7, 0.85)
        return min(1.0, baseline_retention * 1.1)


class MemoryConstructionExperiment:
    """è¨˜æ†¶æ§‹ç¯‰å®Ÿé¨“ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, output_dir: str = "experiments/phase1_dynamic_memory/results", 
                 config: Dict[str, Any] = None):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å®Ÿé¨“è¨­å®š
        self.config = config or {}
        self.debug_mode = self.config.get('debug', False)
        self.document_sizes = self.config.get('document_sizes', [50, 100, 200, 500])
        self.num_runs = self.config.get('num_runs', 1)
        self.export_format = self.config.get('export_format', 'csv')
        
        # ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
        log_level = logging.DEBUG if self.debug_mode else logging.INFO
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.output_dir / 'experiment.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        if self.debug_mode:
            self.logger.info("ğŸ› Debug mode enabled")
        
        # scripts/experiments/ã®CLIæ©Ÿèƒ½ã‚’ä½¿ç”¨å¯èƒ½ãªå ´åˆã¯çµ±åˆ
        if ExperimentCLI:
            self.cli_manager = ExperimentCLI()
            self.logger.info("âœ… ExperimentCLI integration enabled")
        else:
            self.cli_manager = None
    
    def generate_test_documents(self, num_docs: int = 100) -> List[str]:
        """ãƒ†ã‚¹ãƒˆç”¨æ–‡æ›¸ã®ç”Ÿæˆ"""
        documents = []
        for i in range(num_docs):
            # æ§˜ã€…ãªé•·ã•ã¨ã‚¿ã‚¤ãƒ—ã®æ–‡æ›¸ã‚’ç”Ÿæˆ
            if i % 3 == 0:
                # æŠ€è¡“æ–‡æ›¸
                doc = f"Technical document {i}: This document discusses advanced algorithms and machine learning concepts. " \
                      f"It covers topics such as neural networks, deep learning, and artificial intelligence applications. " \
                      f"The methodology involves complex mathematical formulations and statistical analysis."
            elif i % 3 == 1:
                # ç§‘å­¦è«–æ–‡
                doc = f"Scientific paper {i}: Research findings indicate significant improvements in computational efficiency. " \
                      f"The experimental results demonstrate a novel approach to knowledge representation and retrieval. " \
                      f"Statistical significance was achieved with p-value less than 0.05."
            else:
                # ä¸€èˆ¬æ–‡æ›¸
                doc = f"General document {i}: This text contains general information about various topics. " \
                      f"It includes discussions on current trends, historical context, and future implications. " \
                      f"The content is designed to be accessible to a broad audience."
            
            documents.append(doc)
        
        return documents
    
    def run_comparative_experiment(self, document_sizes: List[int] = [50, 100, 200, 500]) -> pd.DataFrame:
        """æ¯”è¼ƒå®Ÿé¨“ã®å®Ÿè¡Œ"""
        results = []
        
        self.logger.info("Starting Phase 1: Dynamic Memory Construction Experiment")
        
        for size in document_sizes:
            self.logger.info(f"Testing with {size} documents...")
            
            # ãƒ†ã‚¹ãƒˆæ–‡æ›¸ç”Ÿæˆ
            documents = self.generate_test_documents(size)
            
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³RAGã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ
            baseline_system = BaselineRAGSystem()
            baseline_metrics = baseline_system.build_memory(documents)
            
            # InsightSpikeå‹•çš„è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ
            insightspike_system = InsightSpikeMemorySystem()
            insightspike_metrics = insightspike_system.build_memory(documents)
            
            # çµæœè¨˜éŒ²
            results.append({
                'document_count': size,
                'system': 'Baseline_RAG',
                'construction_time': baseline_metrics.construction_time,
                'memory_usage_mb': baseline_metrics.memory_usage_mb,
                'retrieval_accuracy': baseline_metrics.retrieval_accuracy,
                'knowledge_retention': baseline_metrics.knowledge_retention,
                'facts_extracted': baseline_metrics.facts_extracted,
                'efficiency_score': self._calculate_efficiency(baseline_metrics)
            })
            
            results.append({
                'document_count': size,
                'system': 'InsightSpike_Dynamic',
                'construction_time': insightspike_metrics.construction_time,
                'memory_usage_mb': insightspike_metrics.memory_usage_mb,
                'retrieval_accuracy': insightspike_metrics.retrieval_accuracy,
                'knowledge_retention': insightspike_metrics.knowledge_retention,
                'facts_extracted': insightspike_metrics.facts_extracted,
                'efficiency_score': self._calculate_efficiency(insightspike_metrics)
            })
            
            # æ”¹å–„ç‡è¨ˆç®—ãƒ»ãƒ­ã‚°å‡ºåŠ›ï¼ˆã‚¼ãƒ­é™¤ç®—ã‚¨ãƒ©ãƒ¼ã‚’å›é¿ï¼‰
            speed_improvement = ((baseline_metrics.construction_time - insightspike_metrics.construction_time) 
                               / max(baseline_metrics.construction_time, 0.001)) * 100
            memory_improvement = ((baseline_metrics.memory_usage_mb - insightspike_metrics.memory_usage_mb) 
                                / max(baseline_metrics.memory_usage_mb, 0.001)) * 100
            accuracy_improvement = ((insightspike_metrics.retrieval_accuracy - baseline_metrics.retrieval_accuracy) 
                                  / max(baseline_metrics.retrieval_accuracy, 0.001)) * 100
            
            self.logger.info(f"Size {size} - Speed improvement: {speed_improvement:.1f}%, "
                           f"Memory improvement: {memory_improvement:.1f}%, "
                           f"Accuracy improvement: {accuracy_improvement:.1f}%")
        
        # çµæœã‚’DataFrameã«å¤‰æ›
        df_results = pd.DataFrame(results)
        
        # çµæœä¿å­˜
        df_results.to_csv(self.output_dir / 'memory_construction_results.csv', index=False)
        self.logger.info(f"Results saved to {self.output_dir / 'memory_construction_results.csv'}")
        
        return df_results
    
    def _calculate_efficiency(self, metrics: MemoryMetrics) -> float:
        """åŠ¹ç‡æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        # æ™‚é–“ã€ãƒ¡ãƒ¢ãƒªã€ç²¾åº¦ã‚’çµ±åˆã—ãŸåŠ¹ç‡æŒ‡æ¨™
        time_factor = 1.0 / (metrics.construction_time + 0.001)  # é«˜é€Ÿã»ã©é«˜ã‚¹ã‚³ã‚¢
        memory_factor = 1.0 / (metrics.memory_usage_mb + 0.001)  # çœãƒ¡ãƒ¢ãƒªã»ã©é«˜ã‚¹ã‚³ã‚¢
        accuracy_factor = metrics.retrieval_accuracy * metrics.knowledge_retention
        
        return (time_factor * memory_factor * accuracy_factor) * 1000  # ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´
    
    def generate_performance_report(self, df_results: pd.DataFrame) -> None:
        """æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        report_path = self.output_dir / 'performance_report.md'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# Phase 1: å‹•çš„è¨˜æ†¶æ§‹ç¯‰å®Ÿé¨“ çµæœãƒ¬ãƒãƒ¼ãƒˆ\n\n")
            f.write("## å®Ÿé¨“æ¦‚è¦\n")
            f.write("InsightSpike-AIã®å‹•çš„è¨˜æ†¶æ§‹ç¯‰æ©Ÿèƒ½ã¨æ¨™æº–RAGã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½æ¯”è¼ƒ\n\n")
            
            # å¹³å‡æ”¹å–„ç‡è¨ˆç®—ï¼ˆæ•°å€¤åˆ—ã®ã¿ã‚’å¯¾è±¡ï¼‰
            numeric_cols = ['construction_time', 'memory_usage_mb', 'retrieval_accuracy', 'knowledge_retention', 'facts_extracted', 'efficiency_score']
            baseline_avg = df_results[df_results['system'] == 'Baseline_RAG'][numeric_cols].mean()
            insightspike_avg = df_results[df_results['system'] == 'InsightSpike_Dynamic'][numeric_cols].mean()
            
            speed_improvement = ((baseline_avg['construction_time'] - insightspike_avg['construction_time']) 
                               / max(baseline_avg['construction_time'], 0.001)) * 100
            memory_improvement = ((baseline_avg['memory_usage_mb'] - insightspike_avg['memory_usage_mb']) 
                                / max(baseline_avg['memory_usage_mb'], 0.001)) * 100
            accuracy_improvement = ((insightspike_avg['retrieval_accuracy'] - baseline_avg['retrieval_accuracy']) 
                                  / max(baseline_avg['retrieval_accuracy'], 0.001)) * 100
            
            f.write("## ä¸»è¦çµæœ\n")
            f.write(f"- **æ§‹ç¯‰é€Ÿåº¦å‘ä¸Š**: {speed_improvement:.1f}% (ç›®æ¨™: 30%)\n")
            f.write(f"- **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡å‘ä¸Š**: {memory_improvement:.1f}% (ç›®æ¨™: 40%)\n")
            f.write(f"- **ç²¾åº¦å‘ä¸Š**: {accuracy_improvement:.1f}% (ç›®æ¨™: 15%)\n\n")
            
            # ä»®èª¬æ¤œè¨¼
            f.write("## ä»®èª¬æ¤œè¨¼\n")
            if speed_improvement >= 30:
                f.write("âœ… æ§‹ç¯‰é€Ÿåº¦30%å‘ä¸Š - **é”æˆ**\n")
            else:
                f.write("âŒ æ§‹ç¯‰é€Ÿåº¦30%å‘ä¸Š - æœªé”æˆ\n")
                
            if memory_improvement >= 40:
                f.write("âœ… ãƒ¡ãƒ¢ãƒªåŠ¹ç‡40%å‘ä¸Š - **é”æˆ**\n")
            else:
                f.write("âŒ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡40%å‘ä¸Š - æœªé”æˆ\n")
                
            if accuracy_improvement >= 15:
                f.write("âœ… ç²¾åº¦15%å‘ä¸Š - **é”æˆ**\n")
            else:
                f.write("âŒ ç²¾åº¦15%å‘ä¸Š - æœªé”æˆ\n")
        
        self.logger.info(f"Performance report generated: {report_path}")


def create_cli_parser() -> argparse.ArgumentParser:
    """Phase 1å°‚ç”¨CLIå¼•æ•°ãƒ‘ãƒ¼ã‚µãƒ¼ã®ä½œæˆ"""
    try:
        parser = create_base_cli_parser(
            "Phase 1", 
            "å‹•çš„è¨˜æ†¶æ§‹ç¯‰å®Ÿé¨“ - InsightSpike-AI vs æ¨™æº–RAGã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒ"
        )
        
        # Phase 1å›ºæœ‰ã®å¼•æ•°ã‚’è¿½åŠ 
        parser = add_phase_specific_args(parser, "phase1")
        
        return parser
    except Exception:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬CLIä½œæˆ
        parser = argparse.ArgumentParser(
            description="Phase 1: å‹•çš„è¨˜æ†¶æ§‹ç¯‰å®Ÿé¨“",
            formatter_class=argparse.RawDescriptionHelpFormatter
        )
        
        parser.add_argument('--debug', action='store_true', help='ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰')
        parser.add_argument('--sizes', type=int, nargs='+', default=[50, 100, 200, 500], help='æ–‡æ›¸ã‚µã‚¤ã‚º')
        parser.add_argument('--runs', type=int, default=1, help='å®Ÿè¡Œå›æ•°')
        parser.add_argument('--output', type=str, default="experiments/phase1_dynamic_memory/results", help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
        parser.add_argument('--export', choices=['csv', 'json', 'excel'], default='csv', help='ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå½¢å¼')
        parser.add_argument('--no-backup', action='store_true', help='ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚­ãƒƒãƒ—')
        parser.add_argument('--quick', action='store_true', help='ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ')
        parser.add_argument('--config', type=str, help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«')
        
        return parser


def load_config_file(config_path: str) -> Dict[str, Any]:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        print(f"âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {config_path}")
        return config
    except Exception as e:
        print(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}


def merge_cli_config(args: argparse.Namespace, phase: str = "phase1") -> Dict[str, Any]:
    """CLIå¼•æ•°ã¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒ¼ã‚¸"""
    config = {}
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯èª­ã¿è¾¼ã¿
    if hasattr(args, 'config') and args.config:
        config = load_config_file(args.config)
    
    # CLIå¼•æ•°ã§ä¸Šæ›¸ã
    config.update({
        'debug': getattr(args, 'debug', False),
        'document_sizes': getattr(args, 'sizes', [50, 100, 200, 500]),
        'num_runs': getattr(args, 'runs', 1),
        'export_format': getattr(args, 'export', 'csv'),
        'output_dir': getattr(args, 'output', 'experiments/phase1_dynamic_memory/results'),
        'no_backup': getattr(args, 'no_backup', False),
        'quick_mode': getattr(args, 'quick', False),
        'generate_report': True,
        'generate_plots': False,
        'selective_copy': ["processed", "embedding", "models"]
    })
    
    # ã‚¯ã‚¤ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯å°ã•ãªã‚µã‚¤ã‚ºã«åˆ¶é™
    if config['quick_mode']:
        config['document_sizes'] = [50, 100]
        config['num_runs'] = 1
    
    return config


@with_data_safety(
    experiment_name="phase1_memory_construction",
    backup_description="Pre-experiment backup for Phase 1: Dynamic Memory Construction",
    auto_rollback=True,
    selective_copy=["processed", "embedding", "models"]
)
def run_memory_construction_experiment(experiment_env: Dict[str, Any] = None) -> Dict[str, Any]:
    """ãƒ‡ãƒ¼ã‚¿å®‰å…¨æ€§æ©Ÿèƒ½ä»˜ããƒ¡ã‚¤ãƒ³å®Ÿé¨“å®Ÿè¡Œ"""
    
    # å®Ÿé¨“ç”¨ãƒ‡ãƒ¼ã‚¿è¨­å®šå–å¾—
    data_config = create_experiment_data_config(experiment_env)
    
    # å®Ÿé¨“ç”¨å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
    experiment_output_dir = experiment_env["experiment_data_dir"] / "outputs"
    experiment_output_dir.mkdir(exist_ok=True)
    
    experiment = MemoryConstructionExperiment(str(experiment_output_dir))
    
    logger = logging.getLogger(__name__)
    logger.info("=== Phase 1: Dynamic Memory Construction Experiment (Safe Mode) ===")
    logger.info(f"Experiment data directory: {experiment_env['experiment_data_dir']}")
    logger.info(f"Backup ID: {experiment_env['backup_id']}")
    logger.info(f"Data configuration: {data_config}")
    
    try:
        # å®Ÿé¨“å®Ÿè¡Œ
        results = experiment.run_comparative_experiment()
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        experiment.generate_performance_report(results)
        
        # å®Ÿé¨“çµæœã®çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜
        experiment_results = {
            "experiment_name": "phase1_memory_construction",
            "timestamp": time.time(),
            "backup_id": experiment_env["backup_id"],
            "data_config": data_config,
            "results": results.to_dict('records'),
            "output_directory": str(experiment_output_dir),
            "success": True
        }
        
        # å®Ÿé¨“çµæœJSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        results_file = experiment_output_dir / "experiment_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(experiment_results, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"Experiment results saved to: {results_file}")
        logger.info("ğŸ‰ Phase 1 å®Ÿé¨“å®Œäº†! (ãƒ‡ãƒ¼ã‚¿ã¯è‡ªå‹•çš„ã«å®‰å…¨ãªçŠ¶æ…‹ã«å¾©å…ƒã•ã‚Œã¾ã™)")
        
        return experiment_results
        
    except Exception as e:
        logger.error(f"Experiment failed: {e}")
        raise


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•° - CLIå¯¾å¿œãƒ»ãƒ‡ãƒ¼ã‚¿å®‰å…¨æ€§ãƒ»scriptsçµ±åˆæ©Ÿèƒ½ä»˜ã"""
    
    # CLIå¼•æ•°ãƒ‘ãƒ¼ã‚¹ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä»˜ãï¼‰
    try:
        parser = create_cli_parser()
        args = parser.parse_args()
        config = merge_cli_config(args, "phase1")
    except Exception as e:
        print(f"âš ï¸  CLIæ©Ÿèƒ½ã‚¨ãƒ©ãƒ¼: {e}")
        print("ğŸ”§ åŸºæœ¬ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™")
        config = {
            'debug': False,
            'document_sizes': [50, 100, 200, 500],
            'num_runs': 1,
            'export_format': 'csv',
            'output_dir': 'experiments/phase1_dynamic_memory/results',
            'no_backup': False,
            'selective_copy': ["processed", "embedding", "models"],
            'generate_report': True,
            'generate_plots': False,
            'quick_mode': False
        }
    
    # å®Ÿé¨“ãƒ˜ãƒƒãƒ€ãƒ¼è¡¨ç¤º
    try:
        print_experiment_header("Phase 1: å‹•çš„è¨˜æ†¶æ§‹ç¯‰å®Ÿé¨“", config)
        print_scripts_integration_status()
    except Exception:
        print("ğŸ”¬ Phase 1: å‹•çš„è¨˜æ†¶æ§‹ç¯‰å®Ÿé¨“")
        print("=" * 50)
        print(f"ğŸ“Š æ–‡æ›¸ã‚µã‚¤ã‚º: {config['document_sizes']}")
        print(f"ğŸ›¡ï¸  ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {'ç„¡åŠ¹' if config['no_backup'] else 'æœ‰åŠ¹'}")
        print(f"ğŸ› ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: {'æœ‰åŠ¹' if config['debug'] else 'ç„¡åŠ¹'}")
    
    try:
        # scripts/experiments/çµ±åˆãƒ¢ãƒ¼ãƒ‰ã‚’è©¦è¡Œ
        try:
            scripts_experiment = ScriptsIntegratedExperiment("phase1_memory_construction", config)
            
            def run_phase1_experiment(integrated_config):
                if integrated_config['no_backup']:
                    # é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰
                    experiment = MemoryConstructionExperiment(integrated_config['output_dir'], integrated_config)
                    results = experiment.run_comparative_experiment(integrated_config['document_sizes'])
                    if integrated_config['generate_report']:
                        experiment.generate_performance_report(results)
                    return results
                else:
                    # å®‰å…¨ãƒ¢ãƒ¼ãƒ‰
                    return run_memory_construction_experiment()
            
            results = scripts_experiment.run_experiment(run_phase1_experiment)
            print("âœ… scripts/experiments/çµ±åˆãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œå®Œäº†")
            
        except Exception as integration_error:
            print(f"âš ï¸  scriptsçµ±åˆãƒ¢ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {integration_error}")
            print("ğŸ”§ æ¨™æº–ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™")
            
            # æ¨™æº–ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ
            if config['no_backup']:
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãªã—ã§ç›´æ¥å®Ÿè¡Œï¼ˆé«˜é€Ÿãƒ¢ãƒ¼ãƒ‰ï¼‰
                print("\nâš¡ é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰: ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãªã—ã§å®Ÿè¡Œ")
                experiment = MemoryConstructionExperiment(config['output_dir'], config)
                results = experiment.run_comparative_experiment(config['document_sizes'])
                
                if config['generate_report']:
                    experiment.generate_performance_report(results)
                
                print("\nğŸ‰ Phase 1 å®Ÿé¨“å®Œäº†! (é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰)")
                
            else:
                # å®‰å…¨ãªå®Ÿé¨“ç’°å¢ƒã§å®Ÿè¡Œï¼ˆæ¨å¥¨ï¼‰
                print("\nğŸ›¡ï¸  å®‰å…¨ãƒ¢ãƒ¼ãƒ‰: ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä»˜ãã§å®Ÿè¡Œ")
                results = run_memory_construction_experiment()
        
        # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        try:
            summary = create_experiment_summary(results, "phase1")
            print(summary)
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬ã‚µãƒãƒªãƒ¼
            if not config.get('debug', False) and results is not None:
                print("\nğŸ“Š çµæœã‚µãƒãƒªãƒ¼:")
                print("âœ… å®Ÿé¨“å®Œäº†")
                print("ğŸ“ çµæœã¯ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™:")
                print("  - experiment_data/ (å®Ÿé¨“çµæœ)")
                print("  - data_backups/ (ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—)")
        
        return results
        
    except KeyboardInterrupt:
        print("\nâ›” å®Ÿé¨“ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        print("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ã¯å®‰å…¨ãªçŠ¶æ…‹ã«å¾©å…ƒã•ã‚Œã¦ã„ã¾ã™")
        return None
        
    except Exception as e:
        try:
            handle_cli_error(e, config)
        except Exception:
            print(f"\nâŒ å®Ÿé¨“ãŒå¤±æ•—ã—ã¾ã—ãŸ: {e}")
            if config.get('debug', False):
                import traceback
                traceback.print_exc()
            print("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ã¯è‡ªå‹•çš„ã«å®Ÿé¨“å‰ã®çŠ¶æ…‹ã«å¾©å…ƒã•ã‚Œã¾ã—ãŸ")
        raise


if __name__ == "__main__":
    main()
