{
  "results": {
    "BM25": {
      "mrr": 0.8184990662931838,
      "recall_at_k": {
        "1": 0.6176470588235294,
        "3": 0.6764705882352942,
        "5": 0.7352941176470589,
        "10": 0.8529411764705882
      },
      "precision_at_k": {
        "1": 0.7647058823529411,
        "3": 0.2745098039215686,
        "5": 0.17647058823529407,
        "10": 0.09999999999999998
      },
      "exact_match": 0.47058823529411764,
      "f1_score": 0.0,
      "avg_latency_ms": 0.14725385927686505,
      "per_question_type": {
        "factual": {
          "recall_at_k": 0.8,
          "precision_at_k": 0.28833333333333333,
          "exact_match": 0.6444444444444445,
          "f1": 0.0
        },
        "multi-hop": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.4083333333333333,
          "exact_match": 0.0,
          "f1": 0.0
        },
        "reasoning": {
          "recall_at_k": 0.8333333333333334,
          "precision_at_k": 0.30833333333333346,
          "exact_match": 0.7037037037037037,
          "f1": 0.0
        }
      }
    },
    "InsightSpike-RAG": {
      "mrr": 1.0,
      "recall_at_k": {
        "1": 0.8529411764705882,
        "3": 0.9117647058823529,
        "5": 0.9509803921568627,
        "10": 0.9509803921568627
      },
      "precision_at_k": {
        "1": 1.0,
        "3": 0.3725490196078432,
        "5": 0.2392156862745098,
        "10": 0.1196078431372549
      },
      "exact_match": 0.7058823529411765,
      "f1_score": 0.025009775990168147,
      "avg_latency_ms": 33.88373992022346,
      "per_question_type": {
        "factual": {
          "recall_at_k": 1.0,
          "precision_at_k": 0.40833333333333344,
          "exact_match": 1.0,
          "f1": 0.0
        },
        "multi-hop": {
          "recall_at_k": 0.7166666666666667,
          "precision_at_k": 0.49166666666666664,
          "exact_match": 0.0,
          "f1": 0.0850332383665717
        },
        "reasoning": {
          "recall_at_k": 1.0,
          "precision_at_k": 0.40833333333333344,
          "exact_match": 1.0,
          "f1": 0.0
        }
      }
    },
    "InsightSpike-RAG-Default": {
      "mrr": 1.0,
      "recall_at_k": {
        "1": 0.8529411764705882,
        "3": 0.8529411764705882,
        "5": 0.8529411764705882,
        "10": 0.8529411764705882
      },
      "precision_at_k": {
        "1": 1.0,
        "3": 0.3333333333333335,
        "5": 0.19999999999999996,
        "10": 0.09999999999999998
      },
      "exact_match": 0.7058823529411765,
      "f1_score": 0.0,
      "avg_latency_ms": 33.520600375007184,
      "per_question_type": {
        "factual": {
          "recall_at_k": 1.0,
          "precision_at_k": 0.40833333333333344,
          "exact_match": 1.0,
          "f1": 0.0
        },
        "multi-hop": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.4083333333333333,
          "exact_match": 0.0,
          "f1": 0.0
        },
        "reasoning": {
          "recall_at_k": 1.0,
          "precision_at_k": 0.40833333333333344,
          "exact_match": 1.0,
          "f1": 0.0
        }
      }
    },
    "TF-IDF": {
      "mrr": 1.0,
      "recall_at_k": {
        "1": 0.8529411764705882,
        "3": 0.8529411764705882,
        "5": 0.8529411764705882,
        "10": 0.8529411764705882
      },
      "precision_at_k": {
        "1": 1.0,
        "3": 0.3333333333333335,
        "5": 0.19999999999999996,
        "10": 0.09999999999999998
      },
      "exact_match": 0.7058823529411765,
      "f1_score": 0.0,
      "avg_latency_ms": 1.0135454290053423,
      "per_question_type": {
        "factual": {
          "recall_at_k": 1.0,
          "precision_at_k": 0.40833333333333344,
          "exact_match": 1.0,
          "f1": 0.0
        },
        "multi-hop": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.4083333333333333,
          "exact_match": 0.0,
          "f1": 0.0
        },
        "reasoning": {
          "recall_at_k": 1.0,
          "precision_at_k": 0.40833333333333344,
          "exact_match": 1.0,
          "f1": 0.0
        }
      }
    }
  },
  "statistical_tests": {
    "pairwise_ttests": {
      "BM25_vs_InsightSpike-RAG_mrr": {
        "t_statistic": -5.483806572095595,
        "p_value": 3.0842951791310793e-07,
        "significant": 