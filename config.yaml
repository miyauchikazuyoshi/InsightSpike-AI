# InsightSpike Configuration
# This file sets the default configuration for the CLI

# LLM Provider Settings
llm:
  # Use "clean" provider for fast initialization (no external dependencies)
  # Options: openai, anthropic, ollama, local, clean, mock
  provider: clean
  
  # Model name (used only for local provider)
  model: distilgpt2
  
  # For experiments requiring actual LLM responses, uncomment:
  # provider: local
  # model: distilgpt2

# Memory Settings
memory:
  # Batch size for memory operations
  batch_size: 32
  max_retrieved_docs: 10
  # Use simple flat index for small datasets
  index_type: flat

# Reasoning Settings
reasoning:
  # Maximum cycles for convergence
  max_cycles: 10
  
  # Quality thresholds
  convergence_threshold: 0.8
  spike_threshold: 0.7

# Advanced Settings
# Note: These are for future use
# performance:
#   enable_cache: true
#   parallel_workers: 4