#!/usr/bin/env python3
"""
TinyLlamaè¨­å®šã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ
================================

TinyLlamaã®å‹•ä½œã«å¿…è¦ãªä¾å­˜é–¢ä¿‚ã¨è¨­å®šã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚
"""

import sys
from pathlib import Path

# ãƒ‘ã‚¹è¨­å®š
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_dependencies():
    """å¿…è¦ãªä¾å­˜é–¢ä¿‚ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ” ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯...")
    
    # 1. transformers
    try:
        import transformers
        print(f"âœ… transformers: {transformers.__version__}")
    except ImportError as e:
        print(f"âŒ transformers: {e}")
        return False
    
    # 2. torch
    try:
        import torch
        print(f"âœ… torch: {torch.__version__}")
    except ImportError as e:
        print(f"âŒ torch: {e}")
        return False
    
    return True

def test_config():
    """TinyLlamaè¨­å®šã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\nâš™ï¸ è¨­å®šãƒã‚§ãƒƒã‚¯...")
    
    try:
        from insightspike.core.config import get_config
        config = get_config()
        
        print(f"âœ… Provider: {config.llm.provider}")
        print(f"âœ… Model: {config.llm.model_name}")
        print(f"âœ… Temperature: {config.llm.temperature}")
        print(f"âœ… Max Tokens: {config.llm.max_tokens}")
        
        # TinyLlamaãƒ¢ãƒ‡ãƒ«ã‹ãƒã‚§ãƒƒã‚¯
        if "TinyLlama" in config.llm.model_name:
            print("ğŸ¯ TinyLlamaè¨­å®šç¢ºèªæ¸ˆã¿")
            return True
        else:
            print("âš ï¸ TinyLlamaä»¥å¤–ã®ãƒ¢ãƒ‡ãƒ«è¨­å®š")
            return False
            
    except Exception as e:
        print(f"âŒ è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
        return False

def test_model_loading():
    """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆï¼ˆè»½é‡ï¼‰"""
    print("\nğŸ¤– ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ...")
    
    try:
        # Mockãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ãƒ†ã‚¹ãƒˆ
        from insightspike.core.layers.layer4_llm_provider import get_llm_provider
        from insightspike.core.config import get_config
        
        config = get_config()
        # ãƒ†ã‚¹ãƒˆç”¨ã«mockãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
        config.llm.provider = "mock"
        
        provider = get_llm_provider(config)
        result = provider.generate_response(
            context={"documents": []}, 
            question="Test question"
        )
        
        print("âœ… LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å‹•ä½œç¢ºèª")
        print(f"âœ… Response: {result['response'][:50]}...")
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ"""
    print("=" * 50)
    print("ğŸ§ª TinyLlama ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    deps_ok = test_dependencies()
    config_ok = test_config()
    model_ok = test_model_loading()
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 50)
    print("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("=" * 50)
    
    if deps_ok and config_ok and model_ok:
        print("ğŸ‰ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸï¼")
        print("âœ… TinyLlamaã®è¨­å®šå¤‰æ›´ã§pyproject.tomlã‚„ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã®è¿½åŠ å¤‰æ›´ã¯ä¸è¦ã§ã™")
        print("ğŸš€ æ—¢å­˜ã®ä¾å­˜é–¢ä¿‚ã§TinyLlamaãŒå‹•ä½œã—ã¾ã™")
        return True
    else:
        print("âŒ ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—")
        print("ğŸ“ è¿½åŠ ã®è¨­å®šå¤‰æ›´ãŒå¿…è¦ãªå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
