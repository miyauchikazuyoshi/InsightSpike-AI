#!/usr/bin/env python3
"""
Experiment 1: çµ±åˆCLIæ´»ç”¨ã«ã‚ˆã‚‹åŒ…æ‹¬çš„æ´å¯Ÿå®Ÿé¨“
===============================================

ä»¥ä¸‹ã®è¦ç´ ã‚’çµ±åˆã—ãŸå®Ÿé¨“:
- æ”¹å–„ã•ã‚ŒãŸCLIï¼ˆclean-tempã‚³ãƒãƒ³ãƒ‰ï¼‰ã‚’æœ€å¤§é™æ´»ç”¨
- graph_pyg.ptã‚’ç¢ºå®Ÿã«ä¿æŒã™ã‚‹å®‰å…¨ãªãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
- åŸºæœ¬ã‚°ãƒ©ãƒ•ä½œæˆ
- æŠ•å…¥ç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆ  
- 1æ–‡ãšã¤ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¿½åŠ 
- å†…ç™ºå ±é…¬è©•ä¾¡
- TopKå–å¾—ã¨ãƒãƒ¼ãƒ‰çµåˆ
- æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ™ã‚¯ãƒˆãƒ«ã®è¨€èªå¤‰æ›
- ã‚°ãƒ©ãƒ•æˆé•·ã®ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
- å®Ÿé¨“ã®å†ç¾æ€§ç¢ºä¿
"""

import sys
import json
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any
import subprocess
import sqlite3
import csv
import traceback

# InsightSpike-AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
sys.path.append(str(Path(__file__).parent.parent / "src"))

try:
    from insightspike.core.config import get_config
    from insightspike.utils.embedder import get_model
    from insightspike.core.layers.layer2_memory_manager import L2MemoryManager
    from insightspike.core.learning.knowledge_graph_memory import KnowledgeGraphMemory
    print("âœ… InsightSpike-AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆèª­ã¿è¾¼ã¿æˆåŠŸ")
except ImportError as e:
    print(f"âŒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    sys.exit(1)


class ComprehensiveExperiment1:
    """çµ±åˆCLIæ´»ç”¨ã«ã‚ˆã‚‹åŒ…æ‹¬çš„å®Ÿé¨“ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        print("ğŸš€ Experiment 1: çµ±åˆCLIæ´»ç”¨å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã¨ãƒ‘ã‚¹è¨­å®š
        self.project_root = Path(__file__).parent.parent
        self.exp_dir = Path(__file__).parent
        self.scripts_dir = self.project_root / "scripts" / "experiments"
        
        # ãƒ¡ã‚¤ãƒ³ãƒªãƒã‚¸ãƒˆãƒªã®dataãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½¿ç”¨ï¼ˆå®Ÿé¨“ã¯ã“ã¡ã‚‰ã§å®Ÿè¡Œï¼‰
        self.data_dir = self.project_root / "data"
        
        # å®Ÿé¨“å°‚ç”¨å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.output_dir = self.exp_dir / "outputs"
        self.logs_dir = self.exp_dir / "logs"
        self.exp_data_dir = self.exp_dir / "data"  # å®Ÿé¨“ç”¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆå‚è€ƒç”¨ï¼‰
        self.visualizations_dir = self.exp_dir / "visualizations"
        
        # å®Ÿé¨“å°‚ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        for dir_path in [self.output_dir, self.logs_dir, self.exp_data_dir, self.visualizations_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±
        self.session_id = f"experiment_1_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.experiment_name = "comprehensive_insight_experiment"
        
        # å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚ˆã‚Šç¾å®Ÿçš„ãªé–¾å€¤ï¼‰
        self.topk = 10
        self.ged_threshold = 0.20  # GEDé–¾å€¤ã‚’å°‘ã—ä¸Šã’ã‚‹
        self.ig_threshold = 0.05   # IGé–¾å€¤ã‚’å¤§å¹…ã«ä¸‹ã’ã‚‹
        self.episodes_per_batch = 50  # ãƒãƒƒãƒå‡¦ç†ç”¨
        
        # ãƒ‡ãƒ¼ã‚¿è¨˜éŒ²ç”¨
        self.episode_logs = []
        self.insight_logs = []
        self.topk_logs = []
        self.graph_evolution_logs = []
        self.intrinsic_reward_logs = []
        
        # ã‚°ãƒ©ãƒ•å¯è¦–åŒ–ç”¨
        self.graph_snapshots = []
        
        print(f"âœ… Experiment 1 åˆæœŸåŒ–å®Œäº†")
        print(f"   ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {self.session_id}")
        print(f"   å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        print(f"   TopKè¿‘å‚æ•°: {self.topk}")
        
    def run_cli_command(self, command: List[str], capture_output: bool = True) -> subprocess.CompletedProcess:
        """CLIã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ï¼‰"""
        try:
            print(f"ğŸ“‹ CLIå®Ÿè¡Œ: {' '.join(command)}")
            result = subprocess.run(
                command, 
                capture_output=capture_output, 
                text=True, 
                cwd=self.project_root,
                timeout=300  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            )
            
            if result.returncode != 0:
                print(f"âš ï¸ CLIè­¦å‘Š (code {result.returncode}): {result.stderr}")
            
            return result
        except subprocess.TimeoutExpired:
            print(f"â° CLIã‚³ãƒãƒ³ãƒ‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {' '.join(command)}")
            raise
        except Exception as e:
            print(f"âŒ CLIå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def setup_experiment_environment(self) -> bool:
        """å®Ÿé¨“ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        print("ğŸ› ï¸ å®Ÿé¨“ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹...")
        print(f"   ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.data_dir}")
        print(f"   ğŸ”§ æ–°ã—ã„CLIã‚³ãƒãƒ³ãƒ‰ï¼ˆclean-tempï¼‰ã‚’ä½¿ç”¨ã—ã¦graph_pyg.ptã‚’ä¿æŒã—ã¾ã™")
        
        try:
            # 1. ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹ç¢ºèª
            print("1ï¸âƒ£ ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹ç¢ºèª...")
            result = self.run_cli_command([
                "python", str(self.scripts_dir / "experiment_cli.py"), "status"
            ])
            
            # 2. å®Ÿé¨“ç”¨ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆgraph_pyg.ptã¯ä¿æŒï¼‰
           # print("2ï¸âƒ£ å®Ÿé¨“ç”¨ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—...")
           # result = self.run_cli_command([
           #     "python", str(self.scripts_dir / "experiment_cli.py"), "clean-temp"
           # ])
           # if result.returncode != 0:
           #     print(f"âš ï¸ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è­¦å‘Š: {result.stderr}")
           #else:
           #     print("âœ… ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
            
            # 3. å®Ÿé¨“ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            print("3ï¸âƒ£ å®Ÿé¨“ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ...")
            result = self.run_cli_command([
                "python", str(self.scripts_dir / "experiment_cli.py"), 
                "create-session", self.session_id
            ])
            if result.returncode != 0:
                print(f"âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå¤±æ•—")
                return False
            
            # 4. ã‚°ãƒ©ãƒ•ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ…‹ç¢ºèªã¨æº–å‚™
            graph_path = self.data_dir / "graph_pyg.pt"
            if not graph_path.exists():
                print("4ï¸âƒ£ ã‚°ãƒ©ãƒ•ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ - åˆæœŸãƒ¡ãƒ¢ãƒªæ§‹ç¯‰...")
                result = self.run_cli_command([
                    "python", str(self.scripts_dir / "experiment_cli.py"), 
                    "build-memory", "--episodes", "50", "--seed", "42"
                ])
                if result.returncode != 0:
                    print(f"âŒ åˆæœŸãƒ¡ãƒ¢ãƒªæ§‹ç¯‰å¤±æ•—: {result.stderr}")
                    return False
                print("âœ… åˆæœŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰å®Œäº†")
                print("ğŸ“‹ graph_pyg.ptã¯å®Ÿé¨“å®Ÿè¡Œæ™‚ã«è‡ªå‹•ä½œæˆã•ã‚Œã¾ã™")
            elif graph_path.stat().st_size == 0:
                print("4ï¸âƒ£ ç©ºã®ã‚°ãƒ©ãƒ•ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º - å‰Šé™¤ã—ã¦å†æ§‹ç¯‰æº–å‚™...")
                graph_path.unlink()  # ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                result = self.run_cli_command([
                    "python", str(self.scripts_dir / "experiment_cli.py"), 
                    "build-memory", "--episodes", "50", "--seed", "42"
                ])
                if result.returncode != 0:
                    print(f"âŒ åˆæœŸãƒ¡ãƒ¢ãƒªæ§‹ç¯‰å¤±æ•—: {result.stderr}")
                    return False
                print("âœ… åˆæœŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰å®Œäº†")
                print("ğŸ“‹ graph_pyg.ptã¯å®Ÿé¨“å®Ÿè¡Œæ™‚ã«è‡ªå‹•ä½œæˆã•ã‚Œã¾ã™")
            else:
                print(f"4ï¸âƒ£ æ—¢å­˜ã®ã‚°ãƒ©ãƒ•ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ (ã‚µã‚¤ã‚º: {graph_path.stat().st_size} bytes)")
                print("ğŸ”’ clean-tempã«ã‚ˆã£ã¦graph_pyg.ptãŒä¿æŒã•ã‚Œã¾ã—ãŸ")
            
            print("âœ… å®Ÿé¨“ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def generate_experimental_data(self, num_episodes: int = 500) -> List[Dict]:
        """æŠ•å…¥ç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆæ”¹å–„ç‰ˆï¼šã‚ˆã‚Šå¤šæ§˜ã§ç¾å®Ÿçš„ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰"""
        print(f"ğŸ“ æŠ•å…¥ç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆä¸­ ({num_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)...")
        
        # ã‚ˆã‚Šå¤šæ§˜ãªç ”ç©¶é ˜åŸŸ
        research_areas = [
            "Large Language Models", "Computer Vision", "Natural Language Processing",
            "Reinforcement Learning", "Graph Neural Networks", "Federated Learning",
            "Explainable AI", "Multimodal Learning", "Few-shot Learning", 
            "Transfer Learning", "Meta-Learning", "Adversarial Learning",
            "Quantum Computing", "Neuromorphic Computing", "Edge AI",
            "Causal Inference", "Continual Learning", "Self-Supervised Learning"
        ]
        
        # ã‚ˆã‚Šç¾å®Ÿçš„ãªã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£è¡¨ç¾
        activity_types = [
            "achieves breakthrough performance on",
            "introduces novel architecture for",
            "demonstrates significant improvements in", 
            "reveals new insights about",
            "establishes new benchmarks for",
            "proposes innovative approach to",
            "discovers unexpected connections in",
            "enables practical applications of",
            "solves long-standing challenges in",
            "creates new paradigm for",
            "bridges the gap between theory and practice in",
            "unveils hidden patterns within",
            "revolutionizes understanding of",
            "provides robust solution for"
        ]
        
        # ã‚ˆã‚Šå¤šæ§˜ãªãƒ‰ãƒ¡ã‚¤ãƒ³
        domains = [
            "medical diagnosis", "autonomous systems", "natural language understanding",
            "computer vision", "robotics", "cybersecurity", "climate modeling",
            "drug discovery", "financial prediction", "educational technology",
            "smart cities", "industrial automation", "healthcare AI",
            "agricultural technology", "space exploration", "biomedical research",
            "environmental monitoring", "social network analysis", "genome analysis",
            "material science", "energy optimization"
        ]
        
        # è¤‡é›‘æ€§ã‚’å¢—ã™ç¾å®Ÿçš„ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆ
        episodes = []
        for i in range(1, num_episodes + 1):
            research_area_idx = (i - 1) % len(research_areas)
            activity_idx = (i - 1) % len(activity_types)
            domain_idx = (i - 1) % len(domains)
            
            research_area = research_areas[research_area_idx]
            activity_type = activity_types[activity_idx]
            domain = domains[domain_idx]
            
            # ã‚ˆã‚Šè‡ªç„¶ã§å¤šæ§˜ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆå‰ã®å®Ÿé¨“ã‚¹ã‚¿ã‚¤ãƒ«ã‚’å‚è€ƒï¼‰
            if i <= 100:
                complexity = "basic"
                # åŸºæœ¬çš„ãªç ”ç©¶å ±å‘Šã‚¹ã‚¿ã‚¤ãƒ«
                text = f"Recent research in {research_area} {activity_type} {domain}, showing promising results with practical implications for real-world deployment."
            elif i <= 300:
                complexity = "intermediate"
                # ã‚ˆã‚Šè©³ç´°ãªç ”ç©¶çµæœã‚¹ã‚¿ã‚¤ãƒ«
                variations = [
                    f"Advanced research in {research_area} {activity_type} {domain}, demonstrating significant performance gains and revealing novel cross-domain applications.",
                    f"Cutting-edge work in {research_area} {activity_type} {domain}, with experimental results showing substantial improvements over existing methods.",
                    f"Innovative approaches in {research_area} {activity_type} {domain}, leading to breakthrough discoveries with broad implications for the field."
                ]
                text = variations[i % len(variations)]
            else:
                complexity = "advanced"
                # é«˜åº¦ãªç ”ç©¶çµ±åˆã‚¹ã‚¿ã‚¤ãƒ«
                variations = [
                    f"Groundbreaking research in {research_area} {activity_type} {domain}, revealing deep theoretical connections and practical breakthroughs with significant implications for multiple interdisciplinary fields and future technological development.",
                    f"Pioneering work in {research_area} {activity_type} {domain}, establishing new theoretical frameworks and demonstrating exceptional practical performance across diverse application scenarios.",
                    f"Revolutionary advances in {research_area} {activity_type} {domain}, integrating complex methodologies and achieving unprecedented results that reshape our understanding of fundamental principles."
                ]
                text = variations[i % len(variations)]
            
            episodes.append({
                'id': i,
                'text': text,
                'research_area': research_area,
                'activity_type': activity_type,
                'domain': domain,
                'complexity': complexity,
                'timestamp': datetime.now().isoformat()
            })
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        episodes_df = pd.DataFrame(episodes)
        episodes_df.to_csv(self.exp_data_dir / "experimental_episodes.csv", index=False)
        
        print(f"âœ… {len(episodes)}å€‹ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†")
        print(f"   ä¿å­˜å…ˆ: {self.exp_data_dir / 'experimental_episodes.csv'}")
        print(f"   è¤‡é›‘æ€§ãƒ¬ãƒ™ãƒ«: basic={sum(1 for ep in episodes if ep['complexity'] == 'basic')}, " +
              f"intermediate={sum(1 for ep in episodes if ep['complexity'] == 'intermediate')}, " +
              f"advanced={sum(1 for ep in episodes if ep['complexity'] == 'advanced')}")
        
        return episodes
    
    def initialize_core_components(self):
        """ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–"""
        print("ğŸ§  ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–ä¸­...")
        
        try:
            self.config = get_config()
            self.model = get_model()
            self.memory_manager = L2MemoryManager(dim=384)
            self.knowledge_graph = KnowledgeGraphMemory(
                embedding_dim=384,
                similarity_threshold=0.3
            )
            
            # graph_pyg.ptãŒå­˜åœ¨ã—ãªã„å ´åˆã®å‡¦ç†
            graph_path = self.data_dir / "graph_pyg.pt"
            if not graph_path.exists():
                print("ğŸ“‹ graph_pyg.ptãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€å®Ÿé¨“å®Ÿè¡Œä¸­ã«æ–°è¦ä½œæˆã•ã‚Œã¾ã™")
                # ç©ºã®ã‚°ãƒ©ãƒ•ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦å¾Œã®å‡¦ç†ã‚’ã‚¹ãƒ ãƒ¼ã‚ºã«
                import torch
                dummy_graph = torch.empty(0)
                torch.save(dummy_graph, graph_path)
                print("âœ… åˆæœŸã‚°ãƒ©ãƒ•ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†")
            
            print("âœ… ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
            
        except Exception as e:
            print(f"âŒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def calculate_intrinsic_reward(self, embedding: np.ndarray, episode_id: int) -> Tuple[float, Dict]:
        """å†…ç™ºå ±é…¬è¨ˆç®—ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        try:
            if len(self.memory_manager.episodes) < 2:
                return 0.1, {"type": "initial", "ged": 0.1, "ig": 0.0}
            
            # ç›´è¿‘ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã®é¡ä¼¼åº¦
            prev_episode = self.memory_manager.episodes[-1]
            similarity = np.dot(embedding, prev_episode.vec)
            
            # è¤‡æ•°ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã®å¹³å‡é¡ä¼¼åº¦ï¼ˆã‚ˆã‚Šæ­£ç¢ºãªGEDè¨ˆç®—ï¼‰
            if len(self.memory_manager.episodes) >= 5:
                recent_episodes = self.memory_manager.episodes[-5:]
                similarities = [np.dot(embedding, ep.vec) for ep in recent_episodes]
                avg_similarity = np.mean(similarities)
            else:
                avg_similarity = similarity
            
            # GED (Global Edit Distance) - æ­£è¦åŒ–ã•ã‚ŒãŸé¡ä¼¼åº¦ã®é€†æ•°
            ged = max(0.0, (1.0 - avg_similarity)) * 0.5  # 0.5å€ã§èª¿æ•´
            
            # IG (Information Gain) - èª¿æ•´ã•ã‚ŒãŸè¨ˆç®—
            ig = min(0.15, episode_id * 0.001)  # 0.001ã«å¢—åŠ ã§ã€100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§æœ€å¤§0.1
            
            # å†…ç™ºå ±é…¬è¨ˆç®—ï¼ˆã‚ˆã‚Šä¿å®ˆçš„ã«ï¼‰
            intrinsic_reward = (ged + ig) / 2
            
            # å ±é…¬ã‚¿ã‚¤ãƒ—åˆ†é¡ï¼ˆå³æ ¼åŒ–ï¼‰
            if intrinsic_reward > 0.2:
                reward_type = "high"
            elif intrinsic_reward > 0.1:
                reward_type = "medium"
            else:
                reward_type = "low"
            
            reward_info = {
                "type": reward_type,
                "ged": float(ged),
                "ig": float(ig),
                "similarity": float(similarity),
                "avg_similarity": float(avg_similarity),
                "intrinsic_reward": float(intrinsic_reward)
            }
            
            return float(intrinsic_reward), reward_info
            
        except Exception as e:
            print(f"âš ï¸ å†…ç™ºå ±é…¬è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0, {"type": "error", "ged": 0.0, "ig": 0.0}
    
    def get_topk_connected_episodes(self, current_episode: Dict, embedding: np.ndarray) -> List[Dict]:
        """TopKå–å¾—ã¨ãƒãƒ¼ãƒ‰çµåˆæƒ…å ±å–å¾—"""
        try:
            # TopKé¡ä¼¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ¤œç´¢
            similarities, indices = self.memory_manager.search(embedding, top_k=self.topk)
            
            connected_episodes = []
            for idx, (similarity, episode_idx) in enumerate(zip(similarities, indices)):
                if episode_idx >= len(self.memory_manager.episodes):
                    continue
                    
                stored_episode = self.memory_manager.episodes[episode_idx]
                
                # ãƒãƒ¼ãƒ‰æ¥ç¶šæƒ…å ±
                current_domain = current_episode.get('domain', 'unknown')
                connected_domain = getattr(stored_episode, 'metadata', {}).get('domain', 'unknown')
                is_cross_domain = current_domain != connected_domain
                
                # ã‚¨ãƒƒã‚¸é‡ã¿ï¼ˆé¡ä¼¼åº¦ï¼‰
                edge_weight = float(similarity)
                
                episode_info = {
                    'rank': idx + 1,
                    'connected_episode_id': getattr(stored_episode, 'id', episode_idx),
                    'similarity': edge_weight,
                    'connected_text': stored_episode.text[:100] + '...' if len(stored_episode.text) > 100 else stored_episode.text,
                    'connected_domain': connected_domain,
                    'connected_research_area': getattr(stored_episode, 'metadata', {}).get('research_area', 'unknown'),
                    'is_cross_domain': is_cross_domain,
                    'edge_weight': edge_weight,
                    'connection_type': 'cross_domain' if is_cross_domain else 'same_domain'
                }
                
                connected_episodes.append(episode_info)
            
            return connected_episodes
            
        except Exception as e:
            print(f"âš ï¸ TopKæ¥ç¶šå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def vector_to_language_conversion(self, vector: np.ndarray, episode_id: int, episode_text: str) -> Dict:
        """æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ™ã‚¯ãƒˆãƒ«ã®è¨€èªå¤‰æ›"""
        try:
            # ãƒ™ã‚¯ãƒˆãƒ«çµ±è¨ˆåˆ†æ
            mean_val = float(np.mean(vector))
            std_val = float(np.std(vector))
            max_val = float(np.max(vector))
            min_val = float(np.min(vector))
            
            # ä¸»è¦æ¬¡å…ƒæŠ½å‡º
            top_dims = np.argsort(np.abs(vector))[-10:].tolist()
            top_values = vector[top_dims].tolist()
            
            # è¨€èªçš„ç‰¹å¾´æ¨å®š
            semantic_features = []
            
            # æŠ½è±¡åº¦åˆ¤å®š
            if mean_val > 0.1:
                semantic_features.append("é«˜æ¬¡æ¦‚å¿µçš„")
                abstraction_level = "high"
            elif mean_val < -0.1:
                semantic_features.append("å…·ä½“çš„")
                abstraction_level = "low"
            else:
                semantic_features.append("ä¸­é–“æŠ½è±¡åº¦")
                abstraction_level = "medium"
                
            # å¤šæ§˜æ€§åˆ¤å®š
            if std_val > 0.3:
                semantic_features.append("å¤šæ§˜æ€§è±Šå¯Œ")
                diversity_level = "high"
            elif std_val > 0.2:
                semantic_features.append("ä¸­ç¨‹åº¦å¤šæ§˜æ€§")
                diversity_level = "medium"
            else:
                semantic_features.append("é›†ç´„çš„")
                diversity_level = "low"
                
            # å¼·åº¦åˆ¤å®š
            if max_val > 0.8:
                semantic_features.append("å¼·ç‰¹å¾´")
                intensity_level = "high"
            elif max_val > 0.5:
                semantic_features.append("ä¸­ç¨‹åº¦ç‰¹å¾´")
                intensity_level = "medium"
            else:
                semantic_features.append("å¼±ç‰¹å¾´")
                intensity_level = "low"
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
            text_words = episode_text.lower().split()
            keywords = [word for word in text_words if len(word) > 4][:5]
            
            language_conversion = {
                'episode_id': episode_id,
                'semantic_features': semantic_features,
                'abstraction_level': abstraction_level,
                'diversity_level': diversity_level,
                'intensity_level': intensity_level,
                'vector_stats': {
                    'mean': mean_val,
                    'std': std_val,
                    'max': max_val,
                    'min': min_val
                },
                'top_dimensions': top_dims,
                'top_values': top_values,
                'extracted_keywords': keywords,
                'language_description': f"Episode_{episode_id}: {', '.join(semantic_features)} (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords)})"
            }
            
            return language_conversion
            
        except Exception as e:
            print(f"âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«è¨€èªå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'episode_id': episode_id,
                'error': str(e),
                'language_description': f"Episode_{episode_id}: å¤‰æ›å¤±æ•—"
            }
    
    def capture_graph_snapshot(self, episode_num: int) -> Dict:
        """ã‚°ãƒ©ãƒ•çŠ¶æ…‹ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå–å¾—"""
        try:
            # ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹
            total_episodes = len(self.memory_manager.episodes)
            
            # ã‚°ãƒ©ãƒ•æ§‹é€ æ¨å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
            # å®Ÿéš›ã®ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„å ´åˆã®è¿‘ä¼¼
            estimated_nodes = total_episodes
            estimated_edges = min(total_episodes * self.topk, total_episodes * (total_episodes - 1) // 2)
            
            # ã‚°ãƒ©ãƒ•å¯†åº¦è¨ˆç®—
            max_possible_edges = total_episodes * (total_episodes - 1) // 2 if total_episodes > 1 else 0
            graph_density = estimated_edges / max_possible_edges if max_possible_edges > 0 else 0.0
            
            snapshot = {
                'episode_number': episode_num,
                'total_episodes': total_episodes,
                'nodes_count': estimated_nodes,
                'edges_count': estimated_edges,
                'graph_density': float(graph_density),
                'avg_degree': float(estimated_edges * 2 / estimated_nodes) if estimated_nodes > 0 else 0.0,
                'timestamp': datetime.now().isoformat()
            }
            
            self.graph_evolution_logs.append(snapshot)
            return snapshot
            
        except Exception as e:
            print(f"âš ï¸ ã‚°ãƒ©ãƒ•ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'episode_number': episode_num,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def generate_insight_episode(self, current_episode: Dict, connected_episodes: List[Dict], vector_language: Dict) -> str:
        """æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ç”Ÿæˆï¼ˆTopKã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‹ã‚‰ã®çµ±åˆï¼‰"""
        try:
            # ç¾åœ¨ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®åŸºæœ¬æƒ…å ±
            current_domain = current_episode.get('domain', 'unknown')
            current_research_area = current_episode.get('research_area', 'unknown')
            
            # æ¥ç¶šã•ã‚ŒãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
            domains = [ep.get('domain', 'unknown') for ep in connected_episodes]
            research_areas = [ep.get('research_area', 'unknown') for ep in connected_episodes]
            
            # ãƒ‰ãƒ¡ã‚¤ãƒ³é–“çµ±åˆã®åˆ†æ
            unique_domains = list(set(domains))
            unique_research_areas = list(set(research_areas))
            
            # çµ±åˆã‚¿ã‚¤ãƒ—ã®æ±ºå®š
            if len(unique_domains) >= 3:
                integration_type = "å¤šé ˜åŸŸçµ±åˆæ´å¯Ÿ"
                integration_scope = f"{len(unique_domains)}å€‹ã®é ˜åŸŸ"
            elif len(unique_domains) == 2:
                integration_type = "é ˜åŸŸæ¨ªæ–­æ´å¯Ÿ"
                integration_scope = f"{unique_domains[0]}ã¨{unique_domains[1]}"
            else:
                integration_type = "é ˜åŸŸå†…æ·±åŒ–æ´å¯Ÿ"
                integration_scope = current_domain
            
            # ãƒ™ã‚¯ãƒˆãƒ«è¨€èªçš„ç‰¹å¾´ã®çµ„ã¿è¾¼ã¿
            semantic_features = vector_language.get('semantic_features', ['ä¸­é–“ç‰¹å¾´'])
            abstraction = vector_language.get('abstraction_level', 'medium')
            
            # æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ†ã‚­ã‚¹ãƒˆã®ç”Ÿæˆï¼ˆçµ±åˆçš„å†…å®¹ï¼‰
            if abstraction == 'high':
                insight_text = f"{integration_type}: {current_research_area}ã«ãŠã‘ã‚‹é©æ–°çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒ{integration_scope}ã§ã®ç†è«–çš„çµ±åˆã‚’å®Ÿç¾ã—ã€{', '.join(semantic_features[:2])}ã‚’é€šã˜ã¦æ–°ãŸãªèªçŸ¥ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’ç¢ºç«‹ã™ã‚‹ã€‚TopKåˆ†æã«ã‚ˆã‚Š{len(connected_episodes)}ã®é–¢é€£ç ”ç©¶ã¨ã®æ·±å±¤çš„æ¥ç¶šæ€§ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã€å¾“æ¥ã®é ˜åŸŸå¢ƒç•Œã‚’è¶…è¶Šã—ãŸçµ±åˆçš„ç†è§£ãŒå‰µç™ºã•ã‚Œã‚‹ã€‚"
            elif abstraction == 'medium':
                insight_text = f"{integration_type}: {current_research_area}ã®æœ€æ–°ç ”ç©¶ãŒ{integration_scope}ã«ãŠã„ã¦{', '.join(semantic_features[:2])}ãªç‰¹æ€§ã‚’ç¤ºã—ã€{len(connected_episodes)}ã®é–¢é€£ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã®çµ±åˆã«ã‚ˆã‚Šæ–°ãŸãªç ”ç©¶ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’æç¤ºã™ã‚‹ã€‚ã“ã®çµ±åˆçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯å¾“æ¥ã®å˜ä¸€é ˜åŸŸç ”ç©¶ã‚’è¶…è¶Šã—ã€è¤‡åˆçš„ãªå•é¡Œè§£æ±ºèƒ½åŠ›ã‚’å®Ÿç¾ã™ã‚‹ã€‚"
            else:
                insight_text = f"{integration_type}: {current_research_area}ã«ãŠã„ã¦{integration_scope}ã§ã®å®Ÿè·µçš„å¿œç”¨ãŒç¢ºèªã•ã‚Œã€{len(connected_episodes)}ã®é–¢é€£ç ”ç©¶ã¨ã®çµ±åˆã«ã‚ˆã‚ŠåŠ¹æœçš„ãªè§£æ±ºç­–ãŒå°å‡ºã•ã‚Œã‚‹ã€‚ã“ã®çµ±åˆã«ã‚ˆã‚Š{', '.join(semantic_features[:2])}ãªæ”¹å–„åŠ¹æœãŒå®Ÿè¨¼ã•ã‚Œã¦ã„ã‚‹ã€‚"
            
            return insight_text
            
        except Exception as e:
            print(f"âš ï¸ æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªæ´å¯Ÿãƒ†ã‚­ã‚¹ãƒˆ
            return f"æ´å¯Ÿçµ±åˆ: {current_episode.get('research_area', 'unknown')}ã«ãŠã‘ã‚‹{current_episode.get('domain', 'unknown')}ã§ã®çµ±åˆçš„ç™ºè¦‹ (TopK: {len(connected_episodes)})"

    def process_single_episode(self, episode: Dict) -> Dict:
        """1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å‡¦ç†"""
        try:
            episode_start_time = time.time()
            
            # ã‚°ãƒ©ãƒ•ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            graph_path = self.data_dir / "graph_pyg.pt"
            if episode['id'] <= 5:  # æœ€åˆã®5ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ã¿
                if graph_path.exists():
                    print(f"ğŸ” Debug Episode {episode['id']}: graph_pyg.pt exists (size: {graph_path.stat().st_size} bytes)")
                else:
                    print(f"ğŸš¨ Debug Episode {episode['id']}: graph_pyg.pt MISSING!")
            
            # ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            embedding = self.model.encode(episode['text'])
            
            # å†…ç™ºå ±é…¬è¨ˆç®—
            intrinsic_reward, reward_info = self.calculate_intrinsic_reward(embedding, episode['id'])
            
            # TopKæ¥ç¶šå–å¾—
            connected_episodes = self.get_topk_connected_episodes(episode, embedding)
            
            # æ´å¯Ÿæ¡ä»¶ãƒã‚§ãƒƒã‚¯ï¼ˆå³æ ¼åŒ–ï¼‰
            is_insight = (reward_info['ged'] > self.ged_threshold and 
                         reward_info['ig'] > self.ig_threshold)
            
            # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼ˆæœ€åˆã®10ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ã¿ï¼‰
            if episode['id'] <= 10:
                print(f"ğŸ“Š Episode {episode['id']}: GED={reward_info['ged']:.4f} (é–¾å€¤>{self.ged_threshold}), IG={reward_info['ig']:.4f} (é–¾å€¤>{self.ig_threshold}), æ´å¯Ÿ={is_insight}")
            
            # æ´å¯Ÿã®å ´åˆã€ãƒ™ã‚¯ãƒˆãƒ«è¨€èªå¤‰æ›å®Ÿè¡Œ
            vector_language = None
            insight_episode_text = episode['text']  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
            
            if is_insight:
                vector_language = self.vector_to_language_conversion(embedding, episode['id'], episode['text'])
                
                # çµ±åˆçš„æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
                insight_episode_text = self.generate_insight_episode(episode, connected_episodes, vector_language)
                
                insight_data = {
                    'insight_id': f"EXP1_INS_{episode['id']:04d}",
                    'episode_id': episode['id'],
                    'original_episode_text': episode['text'],  # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
                    'insight_episode_text': insight_episode_text,  # çµ±åˆç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ
                    'intrinsic_reward': intrinsic_reward,
                    'ged_value': reward_info['ged'],
                    'ig_value': reward_info['ig'],
                    'vector_language': vector_language,
                    'connected_episodes_count': len(connected_episodes),
                    'cross_domain_connections': sum(1 for ep in connected_episodes if ep['is_cross_domain']),
                    'detection_timestamp': datetime.now().isoformat()
                }
                
                self.insight_logs.append(insight_data)
                print(f"ğŸ”¥ æ´å¯Ÿæ¤œå‡º: {insight_data['insight_id']} (å ±é…¬: {intrinsic_reward:.4f})")
                print(f"   ğŸ’¡ çµ±åˆæ´å¯Ÿ: {insight_episode_text[:100]}...")
            
            
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ­ã‚°è¨˜éŒ²
            episode_log = {
                'episode_id': episode['id'],
                'episode_text': episode['text'],
                'domain': episode.get('domain', 'unknown'),
                'research_area': episode.get('research_area', 'unknown'),
                'complexity': episode.get('complexity', 'unknown'),
                'intrinsic_reward': intrinsic_reward,
                'ged_value': reward_info['ged'],
                'ig_value': reward_info['ig'],
                'is_insight': is_insight,
                'connected_episodes_count': len(connected_episodes),
                'cross_domain_connections': sum(1 for ep in connected_episodes if ep['is_cross_domain']),
                'processing_time': time.time() - episode_start_time,
                'timestamp': datetime.now().isoformat()
            }
            
            self.episode_logs.append(episode_log)
            
            # TopKãƒ­ã‚°è¨˜éŒ²
            if connected_episodes:
                topk_log = {
                    'current_episode_id': episode['id'],
                    'current_domain': episode.get('domain', 'unknown'),
                    'connected_episodes': connected_episodes,
                    'timestamp': datetime.now().isoformat()
                }
                self.topk_logs.append(topk_log)
            
            # å†…ç™ºå ±é…¬ãƒ­ã‚°è¨˜éŒ²
            reward_log = {
                'episode_id': episode['id'],
                'intrinsic_reward': intrinsic_reward,
                **reward_info,
                'timestamp': datetime.now().isoformat()
            }
            self.intrinsic_reward_logs.append(reward_log)
            
            # ãƒ¡ãƒ¢ãƒªã«ä¿å­˜
            self.memory_manager.store_episode(
                text=episode['text'],
                c_value=intrinsic_reward,
                metadata={
                    'id': episode['id'],
                    'domain': episode.get('domain', 'unknown'),
                    'research_area': episode.get('research_area', 'unknown'),
                    'complexity': episode.get('complexity', 'unknown')
                }
            )
            
            # ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã«ã‚‚ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¿½åŠ 
            try:
                self.knowledge_graph.add_experience(embedding, episode['text'])
                if episode['id'] <= 5:  # æœ€åˆã®5ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ã¿
                    print(f"ğŸ§  Episode {episode['id']}: Added to knowledge graph")
            except Exception as kg_error:
                print(f"âš ï¸ Knowledge graph update error for episode {episode['id']}: {kg_error}")
            
            # ã‚°ãƒ©ãƒ•ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜çŠ¶æ…‹ã‚’å®šæœŸçš„ã«ãƒã‚§ãƒƒã‚¯
            if episode['id'] % 10 == 0:
                graph_path = self.data_dir / "graph_pyg.pt"
                if graph_path.exists():
                    print(f"ğŸ” Checkpoint Episode {episode['id']}: graph_pyg.pt size: {graph_path.stat().st_size} bytes")
                else:
                    print(f"ğŸš¨ Checkpoint Episode {episode['id']}: graph_pyg.pt MISSING!")
            
            return episode_log
            
        except Exception as e:
            print(f"âš ï¸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode['id']} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {'episode_id': episode['id'], 'error': str(e)}
    
    def run_comprehensive_experiment(self, episodes: List[Dict]) -> Dict:
        """åŒ…æ‹¬çš„å®Ÿé¨“å®Ÿè¡Œ"""
        print(f"ğŸš€ åŒ…æ‹¬çš„å®Ÿé¨“é–‹å§‹ ({len(episodes)}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)")
        print("=" * 70)
        
        start_time = time.time()
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.initialize_core_components()
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¿½è·¡
        processed_episodes = 0
        
        for i, episode in enumerate(episodes):
            try:
                # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å‡¦ç†
                self.process_single_episode(episode)
                processed_episodes += 1
                
                # å®šæœŸçš„ãªã‚°ãƒ©ãƒ•ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
                if (i + 1) % 25 == 0:
                    self.capture_graph_snapshot(i + 1)
                
                # é€²æ—è¡¨ç¤º
                if (i + 1) % 50 == 0:
                    insights_count = len(self.insight_logs)
                    avg_reward = np.mean([log['intrinsic_reward'] for log in self.intrinsic_reward_logs[-50:]])
                    print(f"ğŸ“ˆ é€²æ—: {i+1}/{len(episodes)} ({insights_count} insights, avg_reward: {avg_reward:.4f})")
                
            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode['id']} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # æœ€çµ‚ã‚°ãƒ©ãƒ•ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
        self.capture_graph_snapshot(len(episodes))
        
        # å®Ÿé¨“å®Œäº†
        total_time = time.time() - start_time
        
        results = {
            'total_episodes': len(episodes),
            'processed_episodes': processed_episodes,
            'insights_detected': len(self.insight_logs),
            'insight_rate': len(self.insight_logs) / processed_episodes if processed_episodes > 0 else 0,
            'avg_intrinsic_reward': np.mean([log['intrinsic_reward'] for log in self.intrinsic_reward_logs]) if self.intrinsic_reward_logs else 0,
            'total_time': total_time,
            'episodes_per_second': processed_episodes / total_time if total_time > 0 else 0
        }
        
        print(f"\nâœ… åŒ…æ‹¬çš„å®Ÿé¨“å®Œäº†!")
        print(f"   å‡¦ç†ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {processed_episodes}/{len(episodes)}")
        print(f"   æ¤œå‡ºã•ã‚ŒãŸæ´å¯Ÿ: {len(self.insight_logs)}")
        print(f"   æ´å¯Ÿæ¤œå‡ºç‡: {results['insight_rate']*100:.2f}%")
        print(f"   å¹³å‡å†…ç™ºå ±é…¬: {results['avg_intrinsic_reward']:.4f}")
        print(f"   å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")
        
        return results
    
    def save_comprehensive_logs(self) -> None:
        """åŒ…æ‹¬çš„ãƒ­ã‚°ä¿å­˜"""
        print("ğŸ’¾ åŒ…æ‹¬çš„ãƒ­ã‚°ä¿å­˜ä¸­...")
        
        try:
            # 1. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ­ã‚°CSV
            if self.episode_logs:
                episodes_df = pd.DataFrame(self.episode_logs)
                episodes_df.to_csv(self.logs_dir / "01_episode_logs.csv", index=False)
                print(f"   âœ… ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ­ã‚°: {len(self.episode_logs)}ä»¶")
            
            # 2. æ´å¯Ÿãƒ­ã‚°CSVï¼ˆãƒ™ã‚¯ãƒˆãƒ«è¨€èªå¤‰æ›å«ã‚€ï¼‰ - å¸¸ã«ä½œæˆ
            insight_data = []
            if self.insight_logs:
                # ãƒ™ã‚¯ãƒˆãƒ«è¨€èªå¤‰æ›ãƒ‡ãƒ¼ã‚¿ã‚’å±•é–‹
                for insight in self.insight_logs:
                    row = {
                        'insight_id': insight['insight_id'],
                        'episode_id': insight['episode_id'],
                        'episode_text': insight['episode_text'],
                        'intrinsic_reward': insight['intrinsic_reward'],
                        'ged_value': insight['ged_value'],
                        'ig_value': insight['ig_value'],
                        'connected_episodes_count': insight['connected_episodes_count'],
                        'cross_domain_connections': insight['cross_domain_connections'],
                        'detection_timestamp': insight['detection_timestamp']
                    }
                    
                    # ãƒ™ã‚¯ãƒˆãƒ«è¨€èªå¤‰æ›ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
                    if 'vector_language' in insight and insight['vector_language']:
                        vl = insight['vector_language']
                        row.update({
                            'language_description': vl.get('language_description', ''),
                            'abstraction_level': vl.get('abstraction_level', ''),
                            'diversity_level': vl.get('diversity_level', ''),
                            'intensity_level': vl.get('intensity_level', ''),
                            'semantic_features': ', '.join(vl.get('semantic_features', [])),
                            'extracted_keywords': ', '.join(vl.get('extracted_keywords', [])),
                            'vector_mean': vl.get('vector_stats', {}).get('mean', 0),
                            'vector_std': vl.get('vector_stats', {}).get('std', 0),
                            'vector_max': vl.get('vector_stats', {}).get('max', 0),
                            'vector_min': vl.get('vector_stats', {}).get('min', 0)
                        })
                    
                    insight_data.append(row)
            
            # å¸¸ã«æ´å¯Ÿãƒ­ã‚°ã‚’ä½œæˆï¼ˆç©ºã§ã‚‚ï¼‰
            insights_df = pd.DataFrame(insight_data)
            insights_df.to_csv(self.logs_dir / "02_insight_logs_with_vector_language.csv", index=False)
            print(f"   âœ… æ´å¯Ÿãƒ­ã‚°ï¼ˆãƒ™ã‚¯ãƒˆãƒ«è¨€èªå¤‰æ›å«ã‚€ï¼‰: {len(self.insight_logs)}ä»¶")
            
            # 3. TopKæ¥ç¶šãƒ­ã‚°CSV - å¸¸ã«ä½œæˆ
            topk_data = []
            if self.topk_logs:
                for log in self.topk_logs:
                    base_data = {
                        'current_episode_id': log['current_episode_id'],
                        'current_domain': log['current_domain'],
                        'timestamp': log['timestamp']
                    }
                    
                    for i, connected in enumerate(log['connected_episodes']):
                        row_data = base_data.copy()
                        row_data.update({
                            f'rank_{i+1}_episode_id': connected['connected_episode_id'],
                            f'rank_{i+1}_similarity': connected['similarity'],
                            f'rank_{i+1}_domain': connected['connected_domain'],
                            f'rank_{i+1}_research_area': connected['connected_research_area'],
                            f'rank_{i+1}_is_cross_domain': connected['is_cross_domain'],
                            f'rank_{i+1}_edge_weight': connected['edge_weight'],
                            f'rank_{i+1}_connection_type': connected['connection_type']
                        })
                        topk_data.append(row_data)
            
            # å¸¸ã«TopKãƒ­ã‚°ã‚’ä½œæˆï¼ˆç©ºã§ã‚‚ï¼‰
            if topk_data:
                topk_df = pd.DataFrame(topk_data)
                topk_df.to_csv(self.logs_dir / "03_topk_connections.csv", index=False)
                print(f"   âœ… TopKæ¥ç¶šãƒ­ã‚°: {len(topk_data)}ä»¶")
            else:
                # ç©ºã®DataFrameã§ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã¿ä½œæˆ
                empty_df = pd.DataFrame(columns=['current_episode_id', 'current_domain', 'timestamp'])
                empty_df.to_csv(self.logs_dir / "03_topk_connections.csv", index=False)
                print(f"   âœ… TopKæ¥ç¶šãƒ­ã‚°: 0ä»¶ï¼ˆç©ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼‰")
            
            # 4. å†…ç™ºå ±é…¬ãƒ­ã‚°CSV
            if self.intrinsic_reward_logs:
                rewards_df = pd.DataFrame(self.intrinsic_reward_logs)
                rewards_df.to_csv(self.logs_dir / "04_intrinsic_rewards.csv", index=False)
                print(f"   âœ… å†…ç™ºå ±é…¬ãƒ­ã‚°: {len(self.intrinsic_reward_logs)}ä»¶")
            
            # 5. ã‚°ãƒ©ãƒ•æˆé•·ãƒ­ã‚°CSV
            if self.graph_evolution_logs:
                graph_df = pd.DataFrame(self.graph_evolution_logs)
                graph_df.to_csv(self.logs_dir / "05_graph_evolution.csv", index=False)
                print(f"   âœ… ã‚°ãƒ©ãƒ•æˆé•·ãƒ­ã‚°: {len(self.graph_evolution_logs)}ä»¶")
            
            print("âœ… åŒ…æ‹¬çš„ãƒ­ã‚°ä¿å­˜å®Œäº†")
            
        except Exception as e:
            print(f"âŒ ãƒ­ã‚°ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def create_graph_visualizations(self) -> None:
        """ã‚°ãƒ©ãƒ•æˆé•·ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ"""
        print("ğŸ“Š ã‚°ãƒ©ãƒ•æˆé•·ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
        
        try:
            if not self.graph_evolution_logs:
                print("âš ï¸ ã‚°ãƒ©ãƒ•æˆé•·ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return
            
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            graph_df = pd.DataFrame(self.graph_evolution_logs)
            
            # å›³1: ãƒãƒ¼ãƒ‰æ•°ã¨ã‚¨ãƒƒã‚¸æ•°ã®æˆé•·
            plt.figure(figsize=(15, 10))
            
            # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ1: ãƒãƒ¼ãƒ‰æ•°æˆé•·
            plt.subplot(2, 3, 1)
            plt.plot(graph_df['episode_number'], graph_df['nodes_count'], 'b-', linewidth=2, marker='o', markersize=4)
            plt.title('ãƒãƒ¼ãƒ‰æ•°ã®æˆé•·', fontsize=12)
            plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°')
            plt.ylabel('ãƒãƒ¼ãƒ‰æ•°')
            plt.grid(True, alpha=0.3)
            
            # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ2: ã‚¨ãƒƒã‚¸æ•°æˆé•·
            plt.subplot(2, 3, 2)
            plt.plot(graph_df['episode_number'], graph_df['edges_count'], 'r-', linewidth=2, marker='s', markersize=4)
            plt.title('ã‚¨ãƒƒã‚¸æ•°ã®æˆé•·', fontsize=12)
            plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°')
            plt.ylabel('ã‚¨ãƒƒã‚¸æ•°')
            plt.grid(True, alpha=0.3)
            
            # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ3: ã‚°ãƒ©ãƒ•å¯†åº¦
            plt.subplot(2, 3, 3)
            plt.plot(graph_df['episode_number'], graph_df['graph_density'], 'g-', linewidth=2, marker='^', markersize=4)
            plt.title('ã‚°ãƒ©ãƒ•å¯†åº¦ã®å¤‰åŒ–', fontsize=12)
            plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°')
            plt.ylabel('ã‚°ãƒ©ãƒ•å¯†åº¦')
            plt.grid(True, alpha=0.3)
            
            # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ4: å¹³å‡æ¬¡æ•°
            plt.subplot(2, 3, 4)
            plt.plot(graph_df['episode_number'], graph_df['avg_degree'], 'm-', linewidth=2, marker='d', markersize=4)
            plt.title('å¹³å‡æ¬¡æ•°ã®å¤‰åŒ–', fontsize=12)
            plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°')
            plt.ylabel('å¹³å‡æ¬¡æ•°')
            plt.grid(True, alpha=0.3)
            
            # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ5: ãƒãƒ¼ãƒ‰ãƒ»ã‚¨ãƒƒã‚¸æ•°æ¯”è¼ƒ
            plt.subplot(2, 3, 5)
            plt.plot(graph_df['episode_number'], graph_df['nodes_count'], 'b-', label='ãƒãƒ¼ãƒ‰æ•°', linewidth=2)
            plt.plot(graph_df['episode_number'], graph_df['edges_count']/10, 'r-', label='ã‚¨ãƒƒã‚¸æ•°/10', linewidth=2)
            plt.title('ãƒãƒ¼ãƒ‰ãƒ»ã‚¨ãƒƒã‚¸æ•°æ¯”è¼ƒ', fontsize=12)
            plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°')
            plt.ylabel('æ•°é‡')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ6: æˆé•·ç‡
            if len(graph_df) > 1:
                node_growth_rate = graph_df['nodes_count'].pct_change().fillna(0)
                edge_growth_rate = graph_df['edges_count'].pct_change().fillna(0)
                
                plt.subplot(2, 3, 6)
                plt.plot(graph_df['episode_number'], node_growth_rate, 'b-', label='ãƒãƒ¼ãƒ‰æˆé•·ç‡', linewidth=2)
                plt.plot(graph_df['episode_number'], edge_growth_rate, 'r-', label='ã‚¨ãƒƒã‚¸æˆé•·ç‡', linewidth=2)
                plt.title('æˆé•·ç‡ã®å¤‰åŒ–', fontsize=12)
                plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°')
                plt.ylabel('æˆé•·ç‡')
                plt.legend()
                plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(self.visualizations_dir / "graph_evolution_analysis.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            # å›³2: å†…ç™ºå ±é…¬ã¨ã‚°ãƒ©ãƒ•æˆé•·ã®é–¢ä¿‚
            if self.intrinsic_reward_logs:
                plt.figure(figsize=(12, 8))
                
                # å†…ç™ºå ±é…¬ã®ç§»å‹•å¹³å‡
                rewards_df = pd.DataFrame(self.intrinsic_reward_logs)
                rewards_df['moving_avg'] = rewards_df['intrinsic_reward'].rolling(window=20, min_periods=1).mean()
                
                plt.subplot(2, 2, 1)
                plt.plot(rewards_df['episode_id'], rewards_df['intrinsic_reward'], 'lightblue', alpha=0.5, label='å†…ç™ºå ±é…¬')
                plt.plot(rewards_df['episode_id'], rewards_df['moving_avg'], 'blue', linewidth=2, label='ç§»å‹•å¹³å‡(20)')
                plt.title('å†…ç™ºå ±é…¬ã®å¤‰åŒ–', fontsize=12)
                plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°')
                plt.ylabel('å†…ç™ºå ±é…¬')
                plt.legend()
                plt.grid(True, alpha=0.3)
                
                plt.subplot(2, 2, 2)
                plt.plot(rewards_df['episode_id'], rewards_df['ged'], 'red', alpha=0.7, label='GEDå€¤')
                plt.plot(rewards_df['episode_id'], rewards_df['ig'], 'orange', alpha=0.7, label='IGå€¤')
                plt.title('GEDãƒ»IGå€¤ã®å¤‰åŒ–', fontsize=12)
                plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°')
                plt.ylabel('å€¤')
                plt.legend()
                plt.grid(True, alpha=0.3)
                
                # æ´å¯Ÿæ¤œå‡ºã®åˆ†å¸ƒ
                if self.insight_logs:
                    insight_episodes = [insight['episode_id'] for insight in self.insight_logs]
                    plt.subplot(2, 2, 3)
                    plt.hist(insight_episodes, bins=20, alpha=0.7, color='green')
                    plt.title('æ´å¯Ÿæ¤œå‡ºã®åˆ†å¸ƒ', fontsize=12)
                    plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°')
                    plt.ylabel('æ´å¯Ÿæ¤œå‡ºæ•°')
                    plt.grid(True, alpha=0.3)
                
                # å ±é…¬ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ
                plt.subplot(2, 2, 4)
                reward_types = [log['type'] for log in self.intrinsic_reward_logs]
                type_counts = pd.Series(reward_types).value_counts()
                plt.pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%')
                plt.title('å†…ç™ºå ±é…¬ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ', fontsize=12)
                
                plt.tight_layout()
                plt.savefig(self.visualizations_dir / "reward_and_insights_analysis.png", dpi=300, bbox_inches='tight')
                plt.close()
            
            print("âœ… ã‚°ãƒ©ãƒ•ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆå®Œäº†")
            print(f"   ğŸ“Š ã‚°ãƒ©ãƒ•æˆé•·åˆ†æ: graph_evolution_analysis.png")
            print(f"   ğŸ“Š å ±é…¬ãƒ»æ´å¯Ÿåˆ†æ: reward_and_insights_analysis.png")
            
        except Exception as e:
            print(f"âŒ ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def backup_experiment_data(self) -> bool:
        """å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆè§£æç”¨ï¼‰"""
        print("ğŸ“¦ å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆä¸­...")
        
        try:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            backup_dir = self.output_dir / f"data_backup_{self.session_id}"
            backup_dir.mkdir(parents=True, exist_ok=True)
            
            # dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå…¨ä½“ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            import shutil
            
            # é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            important_files = [
                "graph_pyg.pt",           # ã‚°ãƒ©ãƒ•æ§‹é€ 
                "index.faiss",            # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                "index.json",             # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                "episodes.json",          # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿
                "episodes_backup.json",   # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                "insight_facts.db",       # æ´å¯Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
                "unknown_learning.db"     # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
            ]
            
            backed_up_files = []
            for file_name in important_files:
                src_file = self.data_dir / file_name
                if src_file.exists():
                    dst_file = backup_dir / file_name
                    try:
                        shutil.copy2(src_file, dst_file)
                        backed_up_files.append(file_name)
                        print(f"   âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {file_name}")
                    except Exception as e:
                        print(f"   âš ï¸ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—: {file_name} - {e}")
                else:
                    print(f"   âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«æœªç™ºè¦‹: {file_name}")
            
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            important_dirs = ["processed", "embedding", "cache", "logs", "raw", "samples"]
            for dir_name in important_dirs:
                src_dir = self.data_dir / dir_name
                if src_dir.exists() and src_dir.is_dir():
                    dst_dir = backup_dir / dir_name
                    try:
                        shutil.copytree(src_dir, dst_dir, dirs_exist_ok=True)
                        print(f"   âœ… ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {dir_name}")
                    except Exception as e:
                        print(f"   âš ï¸ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—: {dir_name} - {e}")
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚µãƒãƒªãƒ¼ä½œæˆ
            backup_summary = {
                'backup_timestamp': datetime.now().isoformat(),
                'session_id': self.session_id,
                'backed_up_files': backed_up_files,
                'backup_directory': str(backup_dir),
                'total_insights': len(self.insight_logs),
                'total_episodes': len(self.episode_logs)
            }
            
            with open(backup_dir / "backup_summary.json", 'w', encoding='utf-8') as f:
                json.dump(backup_summary, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†")
            print(f"   ğŸ“ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å…ˆ: {backup_dir}")
            print(f"   ğŸ“„ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(backed_up_files)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def reset_data_to_initial_state(self) -> bool:
        """å®Ÿé¨“å¾Œã«dataãƒ•ã‚©ãƒ«ãƒ€ã‚’åˆæœŸçŠ¶æ…‹ã«æˆ»ã™ï¼ˆæ”¹å–„ç‰ˆCLIã‚’ä½¿ç”¨ï¼‰"""
        print("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ã‚’åˆæœŸçŠ¶æ…‹ã«å¾©å…ƒä¸­...")
        print("   ğŸ”’ æ”¹å–„ã•ã‚ŒãŸCLIã‚’ä½¿ç”¨ã—ã¦graph_pyg.ptã‚’ä¿æŒã—ã¾ã™")
        
        try:
            # æ”¹å–„ã•ã‚ŒãŸCLIã®clean-tempã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨
            result = self.run_cli_command([
                "python", str(self.scripts_dir / "experiment_cli.py"), "clean-temp"
            ])
            
            if result.returncode == 0:
                print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€åˆæœŸçŠ¶æ…‹å¾©å…ƒå®Œäº†ï¼ˆgraph_pyg.ptä¿æŒï¼‰")
                return True
            else:
                print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€åˆæœŸçŠ¶æ…‹å¾©å…ƒå¤±æ•—: {result.stderr}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®åˆæœŸåŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
                reset_script = self.scripts_dir / "reset_data_to_initial_state.py"
                if reset_script.exists():
                    print("   ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®åˆæœŸåŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨")
                    fallback_result = self.run_cli_command([
                        "python", str(reset_script)
                    ])
                    return fallback_result.returncode == 0
                return False
                
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def save_graph_data(self) -> None:
        """ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
        print("ğŸ’¾ ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­...")
        
        try:
            # ãƒ¡ã‚¤ãƒ³dataãƒ•ã‚©ãƒ«ãƒ€ã®graph_pyg.ptã®å­˜åœ¨ç¢ºèª
            main_graph_file = self.data_dir / "graph_pyg.pt"
            if main_graph_file.exists():
                print(f"   ğŸ” ãƒ¡ã‚¤ãƒ³graph_pyg.ptç¢ºèª: {main_graph_file.stat().st_size} bytes")
                
                # å®Ÿé¨“ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç”¨ã«ã‚³ãƒ”ãƒ¼
                backup_graph_file = self.exp_data_dir / f"graph_pyg_backup_{self.session_id}.pt"
                import shutil
                shutil.copy2(main_graph_file, backup_graph_file)
                print(f"   ğŸ’¾ ã‚°ãƒ©ãƒ•ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_graph_file.name}")
            else:
                print("   âš ï¸ ãƒ¡ã‚¤ãƒ³graph_pyg.ptãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                
                # ç©ºã®ã‚°ãƒ©ãƒ•ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
                import torch
                dummy_graph = torch.empty(0)
                torch.save(dummy_graph, main_graph_file)
                print("   ğŸ”§ ç©ºã®graph_pyg.ptã‚’ä½œæˆã—ã¾ã—ãŸ")
            
            # ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ä¿å­˜å‡¦ç†
            if hasattr(self, 'knowledge_graph') and self.knowledge_graph is not None:
                # æ‰‹å‹•ã§PyTorchã‚°ãƒ©ãƒ•ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
                try:
                    # ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã®çŠ¶æ…‹ã‚’ç¢ºèª
                    if hasattr(self.knowledge_graph, 'save_graph') and callable(self.knowledge_graph.save_graph):
                        self.knowledge_graph.save_graph(str(main_graph_file))
                        print("   ğŸ’¾ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰æ˜ç¤ºçš„ã«graph_pyg.ptã‚’ä¿å­˜")
                    else:
                        print("   â„¹ï¸ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã«save_graphãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
                        
                except Exception as kg_save_error:
                    print(f"   âš ï¸ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ä¿å­˜ã‚¨ãƒ©ãƒ¼: {kg_save_error}")
                    
                graph_data = {
                    'session_id': self.session_id,
                    'total_episodes': len(self.memory_manager.episodes) if hasattr(self, 'memory_manager') else 0,
                    'embedding_dim': getattr(self.knowledge_graph, 'embedding_dim', 384),
                    'similarity_threshold': getattr(self.knowledge_graph, 'similarity_threshold', 0.3),
                    'graph_saved_timestamp': datetime.now().isoformat()
                }
                
                # ã‚°ãƒ©ãƒ•ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                with open(self.exp_data_dir / "graph_metadata.json", 'w', encoding='utf-8') as f:
                    json.dump(graph_data, f, indent=2, ensure_ascii=False)
                
                print(f"   âœ… ã‚°ãƒ©ãƒ•ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: graph_metadata.json")
                
            # ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®çŠ¶æ…‹ç¢ºèª
            if hasattr(self, 'memory_manager') and self.memory_manager is not None:
                print(f"   ğŸ“Š ãƒ¡ãƒ¢ãƒªå†…ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {len(self.memory_manager.episodes)}")
                
            print("âœ… ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")
            print("âœ… ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")
            
        except Exception as e:
            print(f"âŒ ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()

    def run_full_experiment(self) -> Dict:
        """ãƒ•ãƒ«å®Ÿé¨“å®Ÿè¡Œ"""
        print("ğŸ¯ Experiment 1: ãƒ•ãƒ«å®Ÿé¨“é–‹å§‹")
        print("=" * 80)
        
        try:
            # 1. å®Ÿé¨“ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            if not self.setup_experiment_environment():
                raise Exception("å®Ÿé¨“ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—")
            
            # 2. æŠ•å…¥ç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ã«å°‘æ•°ï¼‰
            episodes = self.generate_experimental_data(num_episodes=100)
            
            # 3. åŒ…æ‹¬çš„å®Ÿé¨“å®Ÿè¡Œ
            results = self.run_comprehensive_experiment(episodes)
            
            # 4. åŒ…æ‹¬çš„ãƒ­ã‚°ä¿å­˜
            self.save_comprehensive_logs()
            
            # 5. ã‚°ãƒ©ãƒ•ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ
            self.create_graph_visualizations()
            
            # 6. ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            self.save_graph_data()
            
            # 7. å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼ä¿å­˜
            final_results = {
                'experiment_name': 'Experiment 1: çµ±åˆCLIæ´»ç”¨åŒ…æ‹¬çš„æ´å¯Ÿå®Ÿé¨“',
                'session_id': self.session_id,
                'experiment_results': results,
                'files_generated': {
                    'logs': [
                        '01_episode_logs.csv',
                        '02_insight_logs_with_vector_language.csv', 
                        '03_topk_connections.csv',
                        '04_intrinsic_rewards.csv',
                        '05_graph_evolution.csv'
                    ],
                    'visualizations': [
                        'graph_evolution_analysis.png',
                        'reward_and_insights_analysis.png'
                    ],
                    'data': [
                        'experimental_episodes.csv'
                    ]
                },
                'timestamp': datetime.now().isoformat()
            }
            
            with open(self.output_dir / "experiment_1_results.json", 'w', encoding='utf-8') as f:
                json.dump(final_results, f, indent=2, ensure_ascii=False)
            
            print("\nğŸ‰ Experiment 1 å®Œå…¨å®Œäº†!")
            print(f"   ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {self.session_id}")
            print(f"   å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
            print(f"   ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.logs_dir}")
            print(f"   ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³: {self.visualizations_dir}")
            
            # 8. å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’è§£æç”¨ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            print(f"\n8ï¸âƒ£ å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆè§£æç”¨ï¼‰...")
            self.backup_experiment_data()
            
            # 9. ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ã‚’åˆæœŸçŠ¶æ…‹ã«å¾©å…ƒ
            print(f"\n9ï¸âƒ£ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€åˆæœŸçŠ¶æ…‹å¾©å…ƒ...")
            self.reset_data_to_initial_state()
            
            return final_results
            
        except Exception as e:
            print(f"âŒ ãƒ•ãƒ«å®Ÿé¨“ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return {'error': str(e)}


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ Experiment 1: çµ±åˆCLIæ´»ç”¨ã«ã‚ˆã‚‹åŒ…æ‹¬çš„æ´å¯Ÿå®Ÿé¨“")
    print("=" * 80)
    
    try:
        experiment = ComprehensiveExperiment1()
        results = experiment.run_full_experiment()
        
        if 'error' not in results:
            print("\nâœ… ã™ã¹ã¦ã®å®Ÿé¨“ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ!")
            print("\nğŸ“‹ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
            for category, files in results['files_generated'].items():
                print(f"   {category.upper()}:")
                for file in files:
                    print(f"     - {file}")
        else:
            print(f"\nâŒ å®Ÿé¨“ã‚¨ãƒ©ãƒ¼: {results['error']}")
            
    except Exception as e:
        print(f"âŒ ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
