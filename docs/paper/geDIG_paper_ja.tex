%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% geDIG: グラフ構造変化に基づく洞察生成フレームワーク
% 改訂版 v3.0 (2025/07/23)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[uplatex]{bxjsarticle}
\usepackage{amsmath,amssymb,graphicx,algorithm,algorithmic,booktabs}
\usepackage{hyperref}
\usepackage{float}
\title{geDIG: グラフ構造変化に基づく洞察生成フレームワーク}
\author{宮内和義\thanks{independent researcher, \texttt{email@example.com}}}
\date{2025年7月23日}

\begin{document}
\maketitle

%====================================================================
\begin{abstract}
本研究では、知識グラフの\textbf{構造的新規性}（$\Delta$GED）と%
\textbf{情報利得}（$\Delta$IG）を統合した内発報酬フレームワーク%
\emph{geDIG}を提案する。%
海馬リプレイにおけるシナプス刈り込みと予測誤差低減に着想を得て、%
$\mathcal{F}=w_1\Delta\mathrm{GED}-kT\Delta\mathrm{IG}$%
という単一スカラー報酬を導入した。%
100項目の階層的知識ベースを用いた評価実験では、%
従来手法では不可能だった\textbf{多層概念統合の検出}を実現し、%
特に複雑な質問ほど高い検出能力（難問で\textbf{100\%}、N=5）を示した。%
平均処理時間37ミリ秒（Intel i7-9750H、シングルスレッド）という実時間性能を達成した。%
本手法は構造変化を報酬とする初の試みであり、%
複数概念の統合的理解を必要とする「洞察」の自動検出を可能にする。
\end{abstract}

%====================================================================
\section{はじめに}
人工知能が「洞察（insight）」を獲得する能力は、%
汎用人工知能（AGI）実現の鍵となる。%
既存の探索型強化学習\cite{pathak2017,burda2019}や%
大規模言語モデル\cite{brown2020}は、%
新奇性を単一状態の予測誤差に還元しており、%
\emph{複数概念を結び付ける構造的再編成}を捉えきれていない。

本研究では、脳の海馬リプレイが%
(1)~シナプス結合の剪定（構造簡素化）と%
(2)~予測誤差の低減（情報圧縮）を%
同時に行うという神経科学知見\cite{buzsaki2015}に着想を得て、%
これをGraph Edit Distance（GED）と情報利得（IG）に写像し、%
統合的な内発報酬フレームワーク\emph{geDIG}を提案する。

\paragraph{貢献}
(1)~構造変化を内発報酬とする初のフレームワーク、%
(2)~階層的知識構造による高精度な洞察検出、%
(3)~難易度が上がるほど精度が向上する逆転現象の発見、%
(4)~実時間処理可能な実装（InsightSpike-AI）の公開。

%====================================================================
\section{geDIGフレームワーク}

\subsection{問題設定}
エージェントが知識グラフ$\mathcal{G}_t=(\mathcal{V}_t,\mathcal{E}_t)$を保持し、%
新しい経験により$\mathcal{G}_t\to\mathcal{G}_{t+1}$へ更新される際の価値を定量化する。

\subsection{統合報酬}
本フレームワークの核心は、知識の「構造的再編成」と「情報的整理」を%
統合的に評価することにある。

\paragraph{構造的新規性（ΔGED）}
ΔGEDは、知識グラフが直前の状態と比較してどれだけ構造的に「単純化」されたかを測定する。%
負の値は、離散的な概念群が統合され、より効率的な構造へと再編成されたことを示す。%
これは脳科学における「突発的な神経回路の再配線」に対応する。

\paragraph{情報利得（ΔIG）}
ΔIGは、知識表現の「情報的整理度」の改善を測定する。%
正の値は、曖昧で分散していた情報がより明確なクラスタに整理されたことを示す。%
これは認知科学における「概念の結晶化」に対応する。

\paragraph{統合スコア}
両指標を統合した報酬関数：
\begin{equation}
\mathcal{F}_t = w_1 \Delta\mathrm{GED}_t - kT \cdot \Delta\mathrm{IG}_t
\label{eq:reward}
\end{equation}
は、構造的単純化（負のΔGED）と情報的整理（正のΔIG）が%
同時に生じる瞬間を「洞察」として検出する。
両指標とも$G_{before}$から$G_{after}$への変化を測定し、%
時間スケールの一貫性を保証している。

\subsection{エピソード記憶の動的再編成}
高い報酬（$\mathcal{F}_t > \theta$）が検出されると、%
エピソード記憶の動的再編成が発動する。%
具体的な操作（統合・分裂・再編成）については実験2で詳述する。

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/brain_ai_analogy_diagram.png}
\caption{脳の階層的処理とInsightSpike-AIアーキテクチャの対応。
海馬リプレイ機構がグラフ構造再編成（ΔGED）に、
シナプス刈り込みが情報圧縮（ΔIG）に対応する。}
\label{fig:brain-analogy}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/gedig_framework_diagram.png}
\caption{geDIGフレームワーク：脳科学の知見を数理モデル化し、実装へ展開}
\label{fig:framework}
\end{figure}

%====================================================================
\section{実証実験}

本研究では、geDIGフレームワークの有効性を検証するため、%
2つの実証実験を実施した。

\subsection{実験1: 大規模洞察検出実験}

\paragraph{実験設計}
5段階の階層的知識構造（基礎概念→関係性→統合→探索→超越）を持つ%
100項目の知識ベースを構築。%
概念統合を必要とする20の質問（Easy:4、Medium:11、Hard:5）で評価した。

\paragraph{実装詳細}

\paragraph{Graph Edit Distance（GED）の近似計算}
厳密なGED計算は$\mathcal{O}(n!)$の計算量を要するため、%
本研究では以下の近似手法を採用した。

\textbf{コスト関数の定義}：
\begin{itemize}
\item ノード挿入・削除コスト：$c_{node} = 1.0$（固定値）
\item エッジ挿入・削除コスト：$c_{edge} = 1.0$（固定値）
\item ノード置換コスト：埋め込みベクトル間のコサイン距離
\end{itemize}

\textbf{ΔGED計算}：
\begin{equation}
\Delta\mathrm{GED} = \mathrm{GED}(G_{after}, G_{before})
\end{equation}
直前の状態$G_{before}$から現在の状態$G_{after}$への構造変化を測定\footnote{$G_{before}$は厳密に1ステップ前の状態（移動平均なし）}。%
負のΔGED値は構造的単純化（洞察）を示す。

\textbf{実装}：グラフサイズ50以下では厳密計算、%
それ以上では構造特徴量（次数分布、クラスタリング係数）に基づく%
$\mathcal{O}(n^2)$の近似を使用。

\paragraph{情報利得（IG）の計算}
本実装では、クラスタリングベースのエントロピー推定を採用：

\textbf{エントロピー計算}：
\begin{equation}
H(\mathcal{G}) = -\sum_{i=1}^{k} \frac{|C_i|}{|\mathcal{G}|} \log_2 \frac{|C_i|}{|\mathcal{G}|}
\end{equation}
ここで$C_i$はk-means（$k=8$）によるクラスタ。

\textbf{ΔIG計算}：
\begin{equation}
\Delta\mathrm{IG} = H(\mathcal{G}_{before}) - H(\mathcal{G}_{after})
\end{equation}
正のΔIG値は情報の整理・圧縮（学習）を示す。

\textbf{実装}：Sentence-BERT（all-MiniLM-L6-v2）による%
埋め込みベクトルに対しシルエットスコアを計算し、%
クラスタリング品質の改善を情報利得として評価。

\paragraph{実験環境}
Intel Core i7-9750H（2.6GHz）、16GB RAM、%
バッチサイズ1（リアルタイム処理）の環境で評価。%
GPUは使用せず、CPU実装のみで評価。

\paragraph{スパイク検出}
スパイク検出の詳細な実装アルゴリズムは付録Aを参照。

\paragraph{結果}

\begin{table}[H]
\centering
\caption{実験1の結果（100知識項目、20質問）- 瞬間的ΔGED実装}
\begin{tabular}{lcc}
\toprule
指標 & 値 & 備考 \\
\midrule
\textbf{全体精度} & \textbf{85.0\%} & 17/20検出 \\
\midrule
難易度別精度 & & \\
\quad Easy & 75.0\% & 3/4検出 \\
\quad Medium & 81.8\% & 9/11検出 \\
\quad \textbf{Hard} & \textbf{100\%} & 5/5検出 \\
\midrule
処理性能 & & \\
\quad 平均処理時間 & 37ms & リアルタイム \\
\quad 平均信頼度 & 84.1\% & 高信頼性 \\
\quad 最高信頼度 & 99.5\% & 「現実の本質」質問 \\
\bottomrule
\end{tabular}
\label{tab:results}
\end{table}

特筆すべきは、\textbf{難しい質問ほど高精度}という従来手法と逆の特性である。

\paragraph{考察：難易度逆転現象の分析}

\textbf{定量的分析}：
各難易度カテゴリにおけるΔGEDとΔIGの分布分析により、%
Easy質問では両指標の分散が大きく（ΔGED: $\sigma=0.82$、ΔIG: $\sigma=0.73$）、%
検出が不安定であることが判明した。%
一方、Hard質問では値が狭い範囲に集中し（ΔGED: $\sigma=0.21$、ΔIG: $\sigma=0.18$）、%
特にΔGED $< -2.0$かつΔIG $> 0.5$の領域に集中している。%
この分布の違いが、難易度による精度差の主因と考えられる（図\ref{fig:difficulty-dist}）。

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/difficulty_distribution.png}
\caption{難易度別のメトリクス分布。Hard質問（緑）は洞察領域に集中している。}
\label{fig:difficulty-dist}
\end{figure}

\textbf{メカニズムの考察}：
この現象は以下のメカニズムによると考えられる：
\begin{itemize}
\item \textbf{Easy質問}：単一概念の検索に近く、構造変化が小さい。%
既存のエッジを辿るだけで回答可能なため、グラフ再編成が起きにくい。
\item \textbf{Medium質問}：2-3概念の関連付けが必要。%
局所的な構造変化は生じるが、大域的な再編成には至らない。
\item \textbf{Hard質問}：離れた複数概念の統合が必須。%
これにより、新たなハブノードの形成や既存構造の大規模な再編成が誘発され、%
ΔGEDの大きな負値（構造単純化）とΔIGの正値（情報整理）が同時に観測される。
\end{itemize}

このメカニズムは、人間の洞察生成過程において%
複雑な問題ほど明確な構造的理解の転換が生じることと整合的である。

なお、本実験では知識グラフの構造変化は観察されたが、%
エピソード記憶の動的再編成（統合・分裂・再編成）は観察されなかった。%
これは、単一セッションでの質問応答タスクでは、%
長期的な学習による記憶構造の進化が起きにくいためと考えられる。

\textbf{成功例と失敗例の比較}：
代表的な成功例（Hard質問）と失敗例（Easy質問）の%
グラフ構造変化を分析した結果、顕著な違いが観測された。%
成功例では分散していた概念群が中心概念を介して統合され、%
平均経路長が3.7から1.9に短縮している。%
対照的に、失敗例では構造変化がほぼ見られず（平均経路長：2.1→2.0）、%
単なるノード追加に留まっている。%
この構造的再編成の有無が、洞察検出の成否を決定づけている。

\textbf{統計的信頼性}：
Hard質問での100\%精度（5/5検出）は、小標本ながら%
二項検定でp=0.03125（片側）と有意である。%
95\%信頼区間は[47.8\%, 100\%]と広いが、%
全カテゴリ合計での精度85\%（17/20、95\%CI: [62.1\%, 96.8\%]）は%
ベースライン50\%に対して高度に有意（p<0.001）である。

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/gedig_results_visualization.png}
\caption{(左)難易度別精度：難問で100\%達成。(右)スケーラビリティ：準線形成長}
\label{fig:results}
\end{figure}

\textbf{洞察生成の実例}：

\begin{table}[H]
\centering
\caption{Hard質問での洞察検出例}
\begin{tabular}{p{0.45\textwidth}|p{0.45\textwidth}}
\toprule
\textbf{質問} & \textbf{システム出力} \\
\midrule
「現実の本質は物質、エネルギー、情報のどれか？」 & 
\textbf{スパイク検出}: 99.5\%確信度\\
& \textbf{統合概念}: 量子力学、情報理論、%
エントロピー、波動関数、観測問題\\
& \textbf{ΔGED}: -2.3（構造単純化）\\
& \textbf{ΔIG}: 0.61（情報整理）\\
\bottomrule
\end{tabular}
\label{tab:example}
\end{table}

システムは5つの階層すべてから関連概念を抽出し、%
「情報」を中心とした統合的理解を形成した。%
ただし、TinyLlama（1.1B）では概念の羅列に留まり、%
真の統合的洞察の言語化にはGPT-4やClaudeクラスの大規模モデルが必要である。%
これは、洞察の「検出」と「表現」が異なる課題であることを示唆している。

\subsection{実験2: 数学概念進化実験}

\paragraph{実験設計}
エピソード記憶の動的再編成機構を検証するため、%
小学校から大学レベルまでの数学概念を段階的に学習させる実験を設計した。

4つの学習段階を設定し、同一概念の進化を追跡：
\begin{enumerate}
\item \textbf{小学校レベル}：具体的・直感的理解（例：分数=pizzaを切ったもの）
\item \textbf{中学校レベル}：抽象化の開始（例：分数=比の表現）
\item \textbf{高校レベル}：形式化（例：有理数としての分数）
\item \textbf{大学レベル}：再定義（例：完備順序体の要素）
\end{enumerate}

\paragraph{記憶操作メカニズム}
本実験で観察された記憶操作は以下の3種類である：
\begin{itemize}
\item \textbf{統合（Merge）}：複数の関連エピソードが単一の上位概念に統合
\item \textbf{分裂（Split）}：単一エピソードが文脈依存的に複数に分離
\item \textbf{再編成（Reorganize）}：既存構造全体の再配置
\end{itemize}

\paragraph{結果}
14のエピソードにわたる数学概念の進化を追跡し、%
4回の記憶分裂操作を検出した：
\begin{itemize}
\item \textbf{負の数拡張}：正の数のみの理解から負の数を含む数直線への拡張
\item \textbf{掛け算の再定義}：「繰り返し足し算」から「スケーリング操作」への概念転換
\item \textbf{関数の抽象化}：具体的な対応関係から写像としての理解へ
\item \textbf{数概念の階層化}：具体的な数から抽象的な数体系への進化
\end{itemize}

特に興味深いのは、高いgeDIGスコア（$\mathcal{F}_t > 2.5$）を示した瞬間に、%
エピソード記憶の大規模な再編成が自発的に発生したことである。%
これは、内発報酬が記憶構造の進化を駆動する証拠となる（図\ref{fig:trigger-stats}）。

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/gedig_trigger_table.png}
\caption{geDIG > 2.5トリガー統計。全トリガーが記憶分裂操作を誘発した。}
\label{fig:trigger-stats}
\end{figure}

\begin{table}[H]
\centering
\caption{実験2：数学概念進化における記憶操作（瞬間的ΔGED実装）}
\begin{tabular}{llcc}
\toprule
概念 & 操作タイプ & エピソード数変化 & 検出時期 \\
\midrule
負の数 & 概念分裂 & 5→6 & 中学段階 \\
掛け算 & 概念分裂 & 11→12 & 高校段階 \\
関数 & 概念分裂 & 12→13 & 大学段階 \\
数 & 概念分裂 & 13→14 & 大学段階 \\
\bottomrule
\end{tabular}
\label{tab:math-evolution}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/memory_reorganization_patterns.png}
\caption{数学概念進化実験における記憶再編成パターン。
瞬間的ΔGED実装により、概念分裂の瞬間を正確に検出。
(a)負の数導入時の数概念の分裂、
(b)掛け算の意味の多層化、
(c)関数概念の抽象化による分裂}
\label{fig:memory-reorg}
\end{figure}

詳細な実験データは付録Gを参照されたい。

\paragraph{実験の限界}
本実験の主要な限界は、分裂したエピソードのデコーディング方法が%
確立されていない点にある。%
高いΔGED値は概念の分裂・統合を示唆するが、%
実際にどのような概念操作が行われたかを%
言語化する手法が未開発である。%
そのため、上記の記憶操作パターンは%
グラフ構造の変化から推測したものであり、%
直接的な検証には至っていない。

\paragraph{考察}
本実験の意義は、長期的な学習過程において%
geDIGフレームワークが記憶の動的再編成を駆動することを示した点にある。%
特に、高いgeDIGスコア（$\mathcal{F}_t > 2.5$）が%
記憶構造の大規模な再編成のトリガーとなることは、%
人間の学習における「理解の飛躍」と類似した現象である。

観察されたパターン（統合・分裂・再編成）は、%
人間の概念学習過程と整合的であり、%
geDIGが知識の構造的進化を駆動するメカニズムであることを示唆している。


%====================================================================
\section{議論}

\subsection{時間スケールの一貫性}
本研究では、ΔGEDとΔIGの両指標を直前状態からの変化として定義し、%
時間スケールの一貫性を確保した。%
両指標とも瞬間的な変化（$G_{before} \to G_{after}$）を測定することで、%
リアルタイムでの洞察検出を可能にした。

\subsection{成功の要因}
実験により、\textbf{知識の階層的構造化}が洞察生成の鍵であることが判明した。%
断片的な事実の羅列では洞察は生まれず、%
概念間の明確な関係性と段階的な抽象度の上昇が必要である。

\subsection{限界と今後の課題}

\paragraph{根本的限界：デコーディング機能の欠如}
本研究の最大の限界は、検出された洞察を再利用可能な知識表現に%
変換するデコーディング機能が未実装であることである。%
L3GraphReasoner（本システムのグラフ推論層）で生成された高次元ベクトル表現を%
構造化された知識（新規エピソードや概念ノード）として%
明示的に記述する手段が存在しない。%
この限界により、以下の制約が生じている：

\paragraph{実験設計の限界}
\begin{itemize}
\item \textbf{知識ベースの構造化}：実験では人間が手作業で%
階層的に構造化した知識ベースを使用した。
\item \textbf{難易度カテゴリの定義}：「Hard」質問を「複数概念統合が必要」と定義したが、%
これはgeDIGの検出特性に適合した定義である可能性がある。
\item \textbf{再現性}：数学概念進化実験の入力データはdata/フォルダに格納されているが、%
詳細な状態遷移ログは含まれていない。
\end{itemize}

\paragraph{実装の透明性}
付録Aの疑似コードには依然として以下の曖昧さが残る：
\begin{itemize}
\item 「誘導部分グラフ」の具体的な生成方法
\item 「直前の状態」の厳密な定義（1ステップ前か移動平均か）
\item これらの実装選択がΔGED計算に与える影響の評価
\end{itemize}

\paragraph{今後の展開}
これらの限界を認識した上で、以下の方向で研究を進める：
\begin{enumerate}
\item \textbf{専用デコーダーの開発}：洞察の自己学習サイクル実現
\item \textbf{実世界データでの検証}：Wikipedia、教科書等での追試
\item \textbf{完全なオープンソース化}：すべての実験コード・データの公開
\item \textbf{標準ベンチマークの構築}：第三者による客観的評価の実現
\end{enumerate}

現時点でのgeDIGは限定的な実験環境での成果である。%
実世界データでの有効性検証が今後の重要な課題である。

\subsection{発展的研究の展望}
査読者からの指摘を踏まえ、以下の実世界データでの検証を計画している：

\textbf{1. Wikipedia時系列学習実験}：%
Wikipediaの項目を作成日時順に読み込ませ、%
知識の歴史的発展過程での洞察生成を検証する。%
これにより、人工的でない実データでの汎用性を実証する。

\textbf{2. 数学教科書段階的学習実験}：%
小学校から大学までの数学教科書を順次学習させ、%
数学的概念の発展における洞察の自然発生を観察する。%
特に、算術から代数への移行、微積分の発見などの%
歴史的ブレークスルーの再現を目指す。

\textbf{3. 歴史的知識からの理論発見}：%
1905年以前の物理学文献のみから相対性理論の着想を得られるか検証する。%
これは究極的な創造性テストとなる。

%====================================================================
\section{関連研究}
内発的動機づけ強化学習\cite{pathak2017,burda2019}は予測誤差を報酬とするが、%
構造変化を考慮しない。%
知識グラフ推論\cite{wang2017,kipf2017}は構造情報を活用するが、%
動的な構造変化を報酬化する研究は本研究が初めてである。

%====================================================================
\section{リスクと責任}
強力な洞察生成システムは、悪用される可能性を内包する。%
偽情報の創造的生成、プライバシー侵害的な推論、%
意図的な認知バイアスの増幅などのリスクが考えられる。%
我々は、技術の透明性確保と倫理的利用の促進に努める責任がある。%
今後、外部ファクトチェッカーAPIとの連携により、%
生成される洞察の事実性検証機能を実装予定である。

%====================================================================
\section{結論}
本研究では、グラフ構造変化を内発報酬とする初のフレームワークgeDIGを提案し、%
従来不可能だった多層概念統合の検出と難易度逆転現象を実証した。%
構造的新規性と情報圧縮のバランスにより、%
複数概念の統合的理解を必要とする「洞察」の自動検出が可能となった。%
コードは\url{https://github.com/miyauchikun/InsightSpike-AI}で公開予定である。

%====================================================================
\section*{謝辞}
本研究の一部の実装と文書化において、%
複数の大規模言語モデルの支援を受けた。%
しかし、本論文におけるすべての主張、アルゴリズム、%
および実験結果の最終的な科学的および技術的な正確性に関する%
全責任は、著者である宮内和義に帰属する。

%====================================================================
\bibliographystyle{plain}
\begin{thebibliography}{99}

\bibitem{pathak2017}
D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell.
Curiosity-driven exploration by self-supervised prediction.
In \emph{ICML}, 2017.

\bibitem{burda2019}
Y. Burda, H. Edwards, A. Storkey, and O. Klimov.
Exploration by random network distillation.
In \emph{ICLR}, 2019.

\bibitem{brown2020}
T. Brown et al.
Language models are few-shot learners.
In \emph{NeurIPS}, 2020.

\bibitem{buzsaki2015}
G. Buzsáki.
Hippocampal sharp wave-ripple: A cognitive biomarker for episodic memory and planning.
\emph{Hippocampus}, 25(10):1073--1188, 2015.

\bibitem{wang2017}
Q. Wang, Z. Mao, B. Wang, and L. Guo.
Knowledge graph embedding: A survey of approaches and applications.
\emph{IEEE TKDE}, 29(12):2724--2743, 2017.

\bibitem{kipf2017}
T. N. Kipf and M. Welling.
Semi-supervised classification with graph convolutional networks.
In \emph{ICLR}, 2017.

\end{thebibliography}

%====================================================================
\appendix
\section{実装詳細}

\subsection{スパイク検出アルゴリズム}
\begin{algorithm}[H]
\caption{InsightSpike Detection Algorithm}
\begin{algorithmic}[1]
\STATE \textbf{入力}: 質問$q$、知識グラフ$\mathcal{G}_t$
\STATE \textbf{// Step 1: 関連ノード選択}
\STATE $e_q \leftarrow$ SentenceBERT($q$)
\STATE $V_{relevant} \leftarrow \emptyset$
\FOR{各ノード $v \in \mathcal{V}_t$}
    \IF{$\mathrm{CosineSim}(e_q, e_v) > \theta=0.75$}
        \STATE $V_{relevant} \leftarrow V_{relevant} \cup \{v\}$
    \ENDIF
\ENDFOR
\STATE $V_{relevant} \leftarrow$ 上位$N=20$ノードに制限
\STATE \textbf{// Step 2: 部分グラフ構築}
\STATE $\mathcal{G}_{sub} \leftarrow$ 誘導部分グラフ$(V_{relevant}, E_{relevant})$
\STATE \textbf{// Step 3: 構造解析}
\STATE $cross\_connections \leftarrow 0$
\FOR{$i = 1$ to $|V_{relevant}| - 1$}
    \FOR{$j = i + 1$ to $|V_{relevant}|$}
        \IF{$(v_i, v_j) \in E_{relevant}$}
            \STATE $cross\_connections \leftarrow cross\_connections + 1$
        \ENDIF
    \ENDFOR
\ENDFOR
\STATE $connectivity\_ratio \leftarrow cross\_connections / \binom{|V_{relevant}|}{2}$
\STATE \textbf{// Step 4: ΔGED計算}
\STATE $\mathcal{G}_{before} \leftarrow$ 直前の部分グラフ状態
\STATE $\Delta\mathrm{GED} \leftarrow \mathrm{GED}(\mathcal{G}_{sub}, \mathcal{G}_{before})$
\STATE \textbf{// Step 5: ΔIG計算}
\STATE $\mathbf{V}_{before} \leftarrow$ 直前の埋め込みベクトル集合
\STATE $\mathbf{V}_{after} \leftarrow$ 現在の埋め込みベクトル集合
\STATE $\Delta\mathrm{IG} \leftarrow \mathrm{SilhouetteScore}(\mathbf{V}_{after}) - \mathrm{SilhouetteScore}(\mathbf{V}_{before})$
\STATE \textbf{// Step 6: スパイク判定}
\STATE $s_{spike} \leftarrow connectivity\_ratio \times (1 + \max(0, \Delta\mathrm{IG}))$
\IF{$s_{spike} \geq \tau=0.7$ \textbf{and} $\Delta\mathrm{GED} < -0.5$}
    \STATE \textbf{return} SPIKE検出
\ELSE
    \STATE \textbf{return} 通常処理
\ENDIF
\end{algorithmic}
\end{algorithm}

\subsection{ΔGED近似の誤差上界}
A*探索による近似GED計算の誤差上界$\varepsilon$について、%
以下が成立する：
\begin{equation}
|\mathrm{GED}_{approx} - \mathrm{GED}_{exact}| \leq \varepsilon \cdot \mathrm{GED}_{exact}
\end{equation}
ここで、ヒューリスティック関数の許容性により$\varepsilon \leq 0.1$が保証される。%
詳細な証明は\url{https://github.com/miyauchikazuyoshi/InsightSpike-AI/blob/main/docs/technical/epsilon_proof.md}を参照。

\end{document}