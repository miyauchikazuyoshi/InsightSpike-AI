Class Name,Method/Constructor,Expected Input,Actual Input Called With,Expected Output,Output Destination,Config Dependencies,Status,Notes
# Core Components
MainAgent,__init__,"config: Union[dict, InsightSpikeConfig], datastore: DataStore","config: dict, datastore: FileSystemDataStore","None (initializes agent)","self","All config sections",OK,"Entry point"
MainAgent,add_knowledge,"text: str, c_value: float = 0.5","text: str","Dict[str, Any]","caller","None",OK,"c_value defaults to 0.5"
MainAgent,process_question,"question: str, max_cycles: int = 10, verbose: bool = False","question: str","CycleResult","caller","llm, memory, graph",OK,"Main processing method"

# Memory Layer
CachedMemoryManager,__init__,"datastore: DataStore, embedder: Any, cache_size: int = 100","datastore: FileSystemDataStore, embedder: EmbeddingManager, cache_size: 100","None","self","memory.cache_size",OK,"Memory with cache"
CachedMemoryManager,add_episode,"text: str, c_value: float = 0.5, metadata: Optional[Dict] = None","text: str, c_value: 0.5, metadata: None","int (episode index)","MainAgent.add_knowledge","None",ERROR,"Returns index, but embedding shape issue"
CachedMemoryManager,search_episodes,"query_embedding: np.ndarray, top_k: int = 10","query_embedding: np.ndarray(1,384), top_k: 5","List[Tuple[Episode, float]]","MainAgent._run_cycle","memory.max_retrieved_docs",ERROR,"Expects 1D array"
CachedMemoryManager,get_memory_stats,"None","None","Dict[str, Any]","MainAgent.get_memory_graph_state","None",OK,"Stats for monitoring"

L2MemoryManager,__init__,"config: Optional[Any], embedder: Optional[Any]","config: dict, embedder: EmbeddingManager","None","self","memory.*",OK,"Original memory manager"
L2MemoryManager,add_episode,"text: str, vec: Optional[np.ndarray] = None, c_value: Optional[float] = None, metadata: Optional[Dict] = None","text: str, vec: None, c_value: 0.5","int","CachedMemoryManager","None",OK,"Generates embedding if not provided"
L2MemoryManager,search_episodes,"query_embedding: np.ndarray, top_k: int = 10","query_embedding: np.ndarray(384,), top_k: varies","List[Tuple[Episode, float]]","L3GraphReasoner","memory.max_retrieved_docs",OK,"Similarity search"

# Embedding Layer
EmbeddingManager,__init__,"model_name: str = None, config: Optional[Any] = None","model_name: 'sentence-transformers/all-MiniLM-L6-v2', config: dict","None","self","embedding.*",OK,"Manages embeddings"
EmbeddingManager,get_embedding,"text: str","text: str","np.ndarray","CachedMemoryManager.add_episode","None",ERROR,"Returns (1,384) not (384,)"
EmbeddingManager,encode,"text: Union[str, List[str]], **kwargs","text: str or List[str]","np.ndarray","Various","embedding.batch_size",OK,"Batch encoding"

# Episode Management
Episode,__init__,"text: str, vec: np.ndarray, confidence: float, metadata: Dict","text: str, vec: np.ndarray(1,384), confidence: 0.5, metadata: {}","None","self","None",ERROR,"vec shape issue"
Episode,timestamp,"None (property)","None","float","Various","None",OK,"Auto-generated"
Episode,to_dict,"None","None","Dict[str, Any]","DataStore.save_episodes","None",OK,"Serialization"

# Graph Layer
L3GraphReasoner,__init__,"config: Union[dict, object], embedder: Any","config: dict, embedder: EmbeddingManager","None","self","graph.*",OK,"Graph reasoning"
L3GraphReasoner,analyze_documents,"documents: List[Dict], context: Optional[Dict] = None","documents: List[Dict[text,embedding,metadata]], context: None","Dict[str, Any]","MainAgent.process_question","graph.*, processing.*",OK,"Main analysis method"
L3GraphReasoner,_ensure_initialized,"None","None","None","self","graph.use_gnn",OK,"Lazy init"

ScalableGraphBuilder,__init__,"config: Optional[Any] = None, monitor: Optional[GraphOperationMonitor] = None","config: dict","None","self","graph.similarity_threshold",OK,"Graph construction"
ScalableGraphBuilder,build_graph,"documents: List[Dict[str, Any]]","documents: List[Dict[text,embedding,metadata]]","Union[nx.Graph, Data]","L3GraphReasoner","graph.use_gnn",ERROR,"Expects 1D embeddings"
ScalableGraphBuilder,_compute_similarities,"embeddings: np.ndarray","embeddings: np.ndarray","np.ndarray","self.build_graph","None",OK,"Internal method"

# Graph Analysis
GraphAnalyzer,__init__,"spike_ged_threshold: float, spike_ig_threshold: float","spike_ged_threshold: -0.2, spike_ig_threshold: 0.15","None","self","graph.spike_*_threshold",OK,"Metrics calculator"
GraphAnalyzer,calculate_metrics,"current_graph: Any, previous_graph: Any, delta_ged_func: Callable, delta_ig_func: Callable","current_graph: nx.Graph/PyGData, previous_graph: nx.Graph/PyGData, ...","Dict[str, Any]","L3GraphReasoner","None",ERROR,"Mixed graph types"
GraphAnalyzer,detect_spike,"delta_ged: float, delta_ig: float","delta_ged: float, delta_ig: float","bool","RewardCalculator","graph.spike_*_threshold",OK,"Spike detection"

RewardCalculator,__init__,"config: Any","config: dict","None","self","graph.*",OK,"Reward calculation"
RewardCalculator,calculate_reward,"metrics: Dict[str, Any], conflicts: float","metrics: dict, conflicts: float","Dict[str, Any]","L3GraphReasoner.analyze_documents","None",OK,"Final reward"

# Algorithms
ProperDeltaGED,__init__,"None","None","None","self","None",OK,"GED calculator"
ProperDeltaGED,__call__,"prev_graph: nx.Graph, curr_graph: nx.Graph","prev_graph: PyGData, curr_graph: PyGData","float","GraphAnalyzer","None",ERROR,"Expects NetworkX"
ProperDeltaGED,set_reference,"graph: nx.Graph","graph: nx.Graph","None","self","None",OK,"Set baseline"

InformationGain,__init__,"None","None","None","self","None",OK,"IG calculator"
InformationGain,__call__,"prev_graph: nx.Graph, curr_graph: nx.Graph","prev_graph: PyGData, curr_graph: PyGData","float","GraphAnalyzer","None",ERROR,"Expects NetworkX"

# LLM Layer
L4LLMInterface,__init__,"config: Union[dict, LLMConfig]","config: dict","None","self","llm.*",ERROR,"Dict handling issue"
L4LLMInterface,generate_response,"context: Dict[str, Any], question: str, history: List[Dict] = None","context: dict, question: str","str","MainAgent.process_question","llm.*",OK,"Response generation"
L4LLMInterface,_build_prompt,"context: Dict[str, Any], question: str, history: List[Dict]","context: dict, question: str, history: []","str","self.generate_response","llm.prompt_style",OK,"Prompt building"

LLMProviderRegistry,get_instance,"config: Union[dict, LLMConfig]","config: dict","Any (provider)","L4LLMInterface","llm.provider",ERROR,"Dict has no .provider"
LLMProviderRegistry,_create_provider,"config: Any","config: LLMConfig","Any","self.get_instance","llm.*",OK,"Factory method"

# Providers
AnthropicProvider,__init__,"api_key: str, model: str = 'claude-3-haiku-20240307', **kwargs","api_key: str, model: str","None","self","llm.api_key, llm.model",OK,"Claude provider"
AnthropicProvider,generate,"prompt: str, max_tokens: int = 1000, temperature: float = 0.7, **kwargs","prompt: str, max_tokens: 300, temperature: 0.3","str","L4LLMInterface","llm.*",OK,"Generate text"

MockProvider,__init__,"**kwargs","**kwargs","None","self","None",OK,"Test provider"
MockProvider,generate,"prompt: str, **kwargs","prompt: str","str","L4LLMInterface","None",OK,"Mock response"

# DataStore
DataStoreFactory,create,"store_type: str, **kwargs","store_type: 'filesystem', base_path: str","DataStore","MainAgent","None",OK,"Factory pattern"

FileSystemDataStore,__init__,"base_path: str","base_path: str","None","self","None",OK,"File storage"
FileSystemDataStore,save_episodes,"episodes: List[Dict], namespace: str = 'episodes'","episodes: List[Dict], namespace: 'episodes'","None","CachedMemoryManager","None",OK,"Save to JSON"
FileSystemDataStore,load_episodes,"namespace: str = 'episodes'","namespace: 'episodes'","List[Dict]","CachedMemoryManager","None",OK,"Load from JSON"

# Config Management
ConfigNormalizer,normalize,"config: Union[Dict, InsightSpikeConfig, BaseModel]","config: dict","InsightSpikeConfig","Various","All sections",OK,"Config normalization"
ConfigNormalizer,get_llm_config,"config: Union[Dict, InsightSpikeConfig, BaseModel]","config: dict","LLMConfig","L4LLMInterface","llm.*",OK,"Extract LLM config"

InsightSpikeConfig,__init__,"**data","llm: LLMConfig, memory: MemoryConfig, ...","None","self","All",OK,"Main config model"
LLMConfig,__init__,"**data","provider: str, api_key: str, ...","None","self","llm.*",OK,"LLM settings"
MemoryConfig,__init__,"**data","episodic_memory_capacity: int, ...","None","self","memory.*",OK,"Memory settings"
EmbeddingConfig,__init__,"**data","model_name: str, dimension: int, ...","None","self","embedding.*",OK,"Embedding settings"
GraphConfig,__init__,"**data","similarity_threshold: float, ...","None","self","graph.*",OK,"Graph settings"

# Adaptive Processing
AdaptiveProcessor,__init__,"config: Dict[str, Any], memory_manager: Any, graph_reasoner: Any, llm_interface: Any","config: dict, memory_manager: L2Memory, graph_reasoner: L3Graph, llm_interface: L4LLM","None","self","processing.adaptive_loop",OK,"Adaptive loop"
AdaptiveProcessor,process,"question: str, max_attempts: int = 5","question: str","Dict[str, Any]","MainAgent","processing.adaptive_loop.*",OK,"Main processing"

ExplorationLoop,__init__,"memory_manager: Any, graph_reasoner: Any, llm_interface: Any, strategy: ExplorationStrategy","memory_manager: L2Memory, graph_reasoner: L3Graph, llm_interface: L4LLM, strategy: Strategy","None","self","None",OK,"Exploration logic"
ExplorationLoop,explore,"question: str, context: Dict[str, Any]","question: str, context: dict","ExplorationResult","AdaptiveProcessor","None",OK,"Single exploration"

# Type Adapters
GraphTypeAdapter,to_networkx,"graph: Union[nx.Graph, Data, Dict]","graph: PyGData","nx.Graph","Various","None",OK,"Convert to NetworkX"
GraphTypeAdapter,to_pyg,"graph: Union[nx.Graph, Data], node_features: Optional[np.ndarray] = None","graph: nx.Graph, node_features: None","Data","Various","None",OK,"Convert to PyG"

# Monitoring
GraphOperationMonitor,__init__,"None","None","None","self","monitoring.*",OK,"Performance monitoring"
GraphOperationMonitor,start_operation,"operation_name: str","operation_name: str","None","Various","None",OK,"Start timing"
GraphOperationMonitor,end_operation,"operation_name: str","operation_name: str","None","Various","None",OK,"End timing"

# Detection
EurekaDetector,__init__,"ged_threshold: float = -0.5, ig_threshold: float = 0.2","ged_threshold: -0.5, ig_threshold: 0.2","None","self","graph.spike_*_threshold",OK,"Spike detector"
EurekaDetector,detect,"delta_ged: float, delta_ig: float","delta_ged: float, delta_ig: float","bool","Various","None",OK,"Detect eureka moment"

# Pattern Learning
PatternLogger,__init__,"storage_path: Optional[str] = None","storage_path: None","None","self","None",OK,"Pattern storage"
PatternLogger,log_pattern,"question: str, context: Dict, result: Dict","question: str, context: dict, result: dict","None","MainAgent","None",OK,"Log successful pattern"

StrategyOptimizer,__init__,"initial_weights: Optional[Dict[str, float]] = None","initial_weights: None","None","self","None",OK,"Strategy optimization"
StrategyOptimizer,update,"strategy_name: str, reward: float","strategy_name: str, reward: float","None","AdaptiveProcessor","None",OK,"Update weights"

# Adaptive Layer Components
AdaptiveTopKCalculator,__init__,"config: AdaptiveTopKConfig","config: AdaptiveTopKConfig","None","self","adaptive.*",OK,"Adaptive topK calculator"
AdaptiveTopKCalculator,calculate,"layer1_analysis: Dict[str, Any], current_topk: int","layer1_analysis: dict, current_topk: int","int","ExplorationLoop","adaptive.topk.*",OK,"Calculate adaptive topK"

ExplorationLoop,__init__,"memory_manager: Any, graph_reasoner: Any, llm_interface: Any, strategy: ExplorationStrategy","memory_manager: L2Memory, graph_reasoner: L3Graph, llm_interface: L4LLM, strategy: Strategy","None","self","None",OK,"Exploration logic"
ExplorationLoop,explore,"question: str, context: Dict[str, Any]","question: str, context: dict","ExplorationResult","AdaptiveProcessor","None",OK,"Single exploration"

ExplorationLoopFixed,__init__,"memory_manager: Any, graph_reasoner: Any, llm_interface: Any, strategy: ExplorationStrategy, topk_calculator: Optional[TopKCalculator]","memory_manager: L2Memory, graph_reasoner: L3Graph, llm_interface: L4LLM, strategy: Strategy, topk_calculator: None","None","self","None",OK,"Fixed exploration"
ExplorationLoopFixed,explore,"params: ExplorationParams","params: ExplorationParams","ExplorationResult","AdaptiveProcessor","None",OK,"Exploration with spike detection"

# Strategy Classes
BaseStrategy,__init__,"max_attempts: int = 5","max_attempts: 5","None","self","None",OK,"Base strategy"
BaseStrategy,should_continue,"attempt: int, result: ExplorationResult","attempt: int, result: ExplorationResult","bool","ExplorationLoop","None",OK,"Continue exploration?"
BaseStrategy,update_params,"params: ExplorationParams, result: ExplorationResult","params: ExplorationParams, result: ExplorationResult","ExplorationParams","ExplorationLoop","None",OK,"Update exploration params"

AlternatingStrategy,__init__,"max_attempts: int = 5, patterns: Optional[List[str]] = None","max_attempts: 5, patterns: None","None","self","None",OK,"Alternating strategy"
AlternatingStrategy,update_params,"params: ExplorationParams, result: ExplorationResult","params: ExplorationParams, result: ExplorationResult","ExplorationParams","ExplorationLoop","None",OK,"Alternate between narrow/broad"

ExpandingStrategy,__init__,"max_attempts: int = 5, initial_topk: int = 3, expansion_rate: float = 1.5","max_attempts: 5, initial_topk: 3, expansion_rate: 1.5","None","self","None",OK,"Expanding search"
ExpandingStrategy,update_params,"params: ExplorationParams, result: ExplorationResult","params: ExplorationParams, result: ExplorationResult","ExplorationParams","ExplorationLoop","None",OK,"Expand search scope"

NarrowingStrategy,__init__,"max_attempts: int = 5, initial_topk: int = 10, narrowing_rate: float = 0.7","max_attempts: 5, initial_topk: 10, narrowing_rate: 0.7","None","self","None",OK,"Narrowing search"
NarrowingStrategy,update_params,"params: ExplorationParams, result: ExplorationResult","params: ExplorationParams, result: ExplorationResult","ExplorationParams","ExplorationLoop","None",OK,"Narrow search scope"

# Algorithm Components
ContentStructureSeparation,__init__,"None","None","None","self","None",OK,"Content/structure separator"
ContentStructureSeparation,separate,"graph: nx.Graph","graph: nx.Graph","Tuple[np.ndarray, np.ndarray]","EntropyCalculator","None",OK,"Separate content and structure"

DynamicImportanceTracker,__init__,"window_size: int = 10","window_size: 10","None","self","None",OK,"Importance tracker"
DynamicImportanceTracker,update,"node_id: Any, importance_score: float","node_id: Any, importance_score: float","None","GraphImportanceCalculator","None",OK,"Update importance"
DynamicImportanceTracker,get_trend,"node_id: Any","node_id: Any","float","GraphImportanceCalculator","None",OK,"Get importance trend"

GraphImportanceCalculator,__init__,"method: str = 'eigenvector'","method: 'eigenvector'","None","self","None",OK,"Graph importance calc"
GraphImportanceCalculator,calculate_importance,"graph: nx.Graph","graph: nx.Graph","Dict[Any, float]","L3GraphReasoner","None",OK,"Calculate node importance"

PyGAdapter,__init__,"None","None","None","self","None",OK,"PyG to NetworkX adapter"
PyGAdapter,to_networkx,"pyg_graph: Data","pyg_graph: Data","nx.Graph","Various","None",OK,"Convert PyG to NetworkX"
PyGAdapter,from_networkx,"nx_graph: nx.Graph, node_features: Optional[torch.Tensor] = None","nx_graph: nx.Graph, node_features: None","Data","Various","None",OK,"Convert NetworkX to PyG"

PyGGraphEditDistance,__init__,"optimization_level: OptimizationLevel = OptimizationLevel.BASIC","optimization_level: BASIC","None","self","None",OK,"PyG GED calculator"
PyGGraphEditDistance,calculate,"graph1: Data, graph2: Data","graph1: Data, graph2: Data","GEDResult","GraphAnalyzer","None",OK,"Calculate GED for PyG"

# Core Components (Enhanced)
EdgeInfo,__init__,"source: int, target: int, weight: float, attributes: Dict","source: int, target: int, weight: float, attributes: {}","None","self","None",OK,"Edge information"
EnhancedEpisode,__init__,"text: str, vec: np.ndarray, confidence: float, metadata: Dict, edges: List[EdgeInfo]","text: str, vec: np.ndarray(384,), confidence: 0.5, metadata: {}, edges: []","None","self","None",OK,"Enhanced episode"
EnhancedEpisode,add_edge,"edge: EdgeInfo","edge: EdgeInfo","None","self","None",OK,"Add graph edge"
EnhancedEpisode,update_gedig_memory,"delta_ged: float, delta_ig: float, timestamp: float","delta_ged: float, delta_ig: float, timestamp: float","None","L3GraphReasoner","None",OK,"Update geDIG memory"

MemoryStatistics,__init__,"None","None","None","self","None",OK,"Memory statistics"
MemoryStatistics,update,"memory_type: str, size: int","memory_type: str, size: int","None","EnhancedEpisode","None",OK,"Update memory stats"
MemoryStatistics,get_summary,"None","None","Dict[str, Any]","Various","None",OK,"Get memory summary"

# Detection Components
InsightFact,__init__,"fact_id: str, description: str, confidence: float, supporting_episodes: List[int]","fact_id: str, description: str, confidence: 0.8, supporting_episodes: []","None","self","None",OK,"Insight fact"
InsightFact,to_dict,"None","None","Dict[str, Any]","InsightFactRegistry.save","None",OK,"Serialize fact"

InsightFactRegistry,__init__,"storage_path: Optional[str] = None","storage_path: None","None","self","None",OK,"Fact registry"
InsightFactRegistry,register_fact,"fact: InsightFact","fact: InsightFact","None","Various","None",OK,"Register new fact"
InsightFactRegistry,evaluate_relevance,"query: str, top_k: int = 5","query: str, top_k: 5","List[Tuple[InsightFact, float]]","L3GraphReasoner","None",OK,"Find relevant facts"
InsightFactRegistry,optimize_graph,"graph: nx.Graph, facts: List[InsightFact]","graph: nx.Graph, facts: List[InsightFact]","GraphOptimizationResult","L3GraphReasoner","None",OK,"Optimize graph with facts"

# Query Transformation
QueryTransformer,__init__,"config: Dict[str, Any], memory_manager: Any, embedder: Any","config: dict, memory_manager: L2Memory, embedder: EmbeddingManager","None","self","graph.use_gnn",OK,"Query transformer"
QueryTransformer,transform_query,"query: str, graph: nx.Graph, max_hops: int = 2","query: str, graph: nx.Graph, max_hops: 2","Dict[str, Any]","L3GraphReasoner","None",OK,"Transform query"

EnhancedQueryTransformer,__init__,"config: Dict[str, Any], memory_manager: Any, embedder: Any, evolution_tracker: Optional[EvolutionTracker] = None","config: dict, memory_manager: L2Memory, embedder: EmbeddingManager, evolution_tracker: None","None","self","graph.use_gnn",OK,"Enhanced transformer"
EnhancedQueryTransformer,transform_query_adaptive,"query: str, graph: nx.Graph, context: Dict[str, Any]","query: str, graph: nx.Graph, context: dict","Dict[str, Any]","L3GraphReasoner","processing.adaptive_exploration",OK,"Adaptive transformation"
EnhancedQueryTransformer,explore_branches,"query: str, graph: nx.Graph, num_branches: int = 3","query: str, graph: nx.Graph, num_branches: 3","List[QueryBranch]","self.transform_query_adaptive","None",OK,"Explore query branches"

QueryState,__init__,"query: str, embedding: np.ndarray, graph_state: nx.Graph, timestamp: float","query: str, embedding: np.ndarray(384,), graph_state: nx.Graph, timestamp: float","None","self","None",OK,"Query state"
QueryState,distance_to,"other: QueryState","other: QueryState","float","QueryTransformationHistory","None",OK,"Distance between states"

QueryTransformationHistory,__init__,"initial_query: str","initial_query: str","None","self","None",OK,"Transformation history"
QueryTransformationHistory,add_state,"state: QueryState","state: QueryState","None","EnhancedQueryTransformer","None",OK,"Add query state"
QueryTransformationHistory,get_trajectory,"None","None","List[QueryState]","EvolutionTracker","None",OK,"Get state trajectory"

EvolutionTracker,__init__,"db_path: Optional[str] = None","db_path: None","None","self","None",OK,"Evolution tracker"
EvolutionTracker,record_evolution,"history: QueryTransformationHistory, success: bool, metrics: Dict[str, Any]","history: QueryTransformationHistory, success: bool, metrics: dict","None","EnhancedQueryTransformer","None",OK,"Record evolution"
EvolutionTracker,get_similar_patterns,"query: str, top_k: int = 5","query: str, top_k: 5","List[EvolutionPattern]","EnhancedQueryTransformer","None",OK,"Find similar patterns"

# Agent Implementations
ConfigurableAgent,__init__,"config: Union[Dict, AgentConfig], mode: AgentMode = AgentMode.STANDARD","config: dict, mode: STANDARD","None","self","All sections",OK,"Configurable agent"
ConfigurableAgent,add_knowledge,"text: str, metadata: Optional[Dict] = None","text: str, metadata: None","Dict[str, Any]","caller","memory.*",OK,"Add knowledge"
ConfigurableAgent,process_question,"question: str, **kwargs","question: str","UnifiedCycleResult","caller","All sections",OK,"Process question"

DataStoreMainAgent,__init__,"config: Union[Dict, InsightSpikeConfig], datastore: DataStore","config: dict, datastore: DataStore","None","self","All sections",OK,"DataStore agent"
DataStoreMainAgent,initialize,"None","None","None","self","All sections",OK,"Initialize layers"
DataStoreMainAgent,add_knowledge,"text: str, confidence: float = 0.5, metadata: Optional[Dict] = None","text: str, confidence: 0.5, metadata: None","Dict[str, Any]","caller","None",OK,"Add via DataStore"
DataStoreMainAgent,process_question,"question: str, max_attempts: int = 5","question: str, max_attempts: 5","Dict[str, Any]","caller","processing.adaptive_loop",OK,"Process via DataStore"

GenericInsightSpikeAgent,__init__,"environment: EnvironmentInterface, config: Dict[str, Any]","environment: EnvironmentInterface, config: dict","None","self","All sections",OK,"Generic agent"
GenericInsightSpikeAgent,act,"state: EnvironmentState","state: EnvironmentState","Any","Environment","None",OK,"Take action"
GenericInsightSpikeAgent,learn,"state: EnvironmentState, action: Any, reward: float, next_state: EnvironmentState","state: EnvironmentState, action: Any, reward: float, next_state: EnvironmentState","None","Environment","None",OK,"Learn from experience"

# Layer 1 Components
Layer1StreamProcessor,__init__,"config: Layer1Config, embedder: Any","config: Layer1Config, embedder: EmbeddingManager","None","self","layer1.*",OK,"Stream processor"
Layer1StreamProcessor,process_stream,"text_stream: str","text_stream: str","List[L1Episode]","L1IntegratedMemory","layer1.boundary_detection",OK,"Process text stream"
Layer1StreamProcessor,process_batch,"texts: List[str]","texts: List[str]","List[L1Episode]","L1IntegratedMemory","layer1.batch_size",OK,"Batch processing"

BoundaryDetector,__init__,"min_length: int = 10, max_length: int = 512","min_length: 10, max_length: 512","None","self","None",OK,"Boundary detector"
BoundaryDetector,detect_boundaries,"text: str","text: str","List[Tuple[int, int]]","Layer1StreamProcessor","None",OK,"Find episode boundaries"

L1IntegratedMemory,__init__,"l1_processor: Layer1StreamProcessor, l2_memory: L2MemoryManager","l1_processor: Layer1StreamProcessor, l2_memory: L2MemoryManager","None","self","None",OK,"L1-L2 integration"
L1IntegratedMemory,add_text,"text: str, metadata: Optional[Dict] = None","text: str, metadata: None","List[int]","MainAgent","None",OK,"Add text via L1"

ErrorMonitor,__init__,"None","None","None","self","None",OK,"Error monitor"
ErrorMonitor,calculate_uncertainty,"predictions: List[float], ground_truth: Optional[List[float]] = None","predictions: List[float], ground_truth: None","float","Various","None",OK,"Calculate uncertainty"
ErrorMonitor,analyze_known_unknown,"error_data: Dict[str, Any]","error_data: dict","KnownUnknownAnalysis","Various","None",OK,"Analyze errors"

# Layer 2 Components
CompatibleL2MemoryManager,__init__,"config: Optional[Dict] = None, embedder: Optional[Any] = None, datastore: Optional[DataStore] = None","config: dict, embedder: EmbeddingManager, datastore: None","None","self","memory.*",OK,"Compatible L2"
CompatibleL2MemoryManager,add_episode,"text: str, vec: Optional[np.ndarray] = None, c_value: Optional[float] = None, metadata: Optional[Dict] = None","text: str, vec: None, c_value: 0.5, metadata: None","int","Various","None",OK,"Add with compatibility"

L2WorkingMemoryManager,__init__,"config: WorkingMemoryConfig, embedder: Any, datastore: DataStore","config: WorkingMemoryConfig, embedder: EmbeddingManager, datastore: DataStore","None","self","memory.working_memory.*",OK,"Working memory"
L2WorkingMemoryManager,add_to_working_memory,"text: str, embedding: np.ndarray, relevance: float","text: str, embedding: np.ndarray(384,), relevance: float","None","self","memory.working_memory_size",OK,"Add to working mem"
L2WorkingMemoryManager,consolidate,"None","None","List[int]","self","memory.consolidation_threshold",OK,"Consolidate to long-term"

# Layer 3 Components
ConflictScore,__init__,"None","None","None","self","None",OK,"Conflict scorer"
ConflictScore,calculate,"graph: nx.Graph, new_info: Dict[str, Any]","graph: nx.Graph, new_info: dict","float","L3GraphReasoner","None",OK,"Calculate conflict score"

# Layer 4 Components
L4PromptBuilder,__init__,"config: Dict[str, Any]","config: dict","None","self","llm.prompt_style",OK,"Prompt builder"
L4PromptBuilder,build_prompt,"context: Dict[str, Any], question: str","context: dict, question: str","str","L4LLMInterface","llm.prompt_style",OK,"Build prompt"
L4PromptBuilder,generate_response,"context: Dict[str, Any], question: str","context: dict, question: str","str","Various","llm.*",OK,"Generate without LLM"

# Provider Components
AnthropicProvider,__init__,"api_key: str, model: str = 'claude-3-haiku-20240307', **kwargs","api_key: str, model: str","None","self","llm.api_key, llm.model",OK,"Claude provider"
AnthropicProvider,generate,"prompt: str, max_tokens: int = 1000, temperature: float = 0.7, **kwargs","prompt: str, max_tokens: 300, temperature: 0.3","str","L4LLMInterface","llm.*",OK,"Generate text"

OpenAIProvider,__init__,"api_key: str, model: str = 'gpt-3.5-turbo', **kwargs","api_key: str, model: str","None","self","llm.api_key, llm.model",OK,"OpenAI provider"
OpenAIProvider,generate,"prompt: str, max_tokens: int = 1000, temperature: float = 0.7, **kwargs","prompt: str, max_tokens: 300, temperature: 0.3","str","L4LLMInterface","llm.*",OK,"Generate text"

LocalProvider,__init__,"model_name: str = 'gpt2', device: str = 'cpu', **kwargs","model_name: 'gpt2', device: 'cpu'","None","self","llm.model, llm.device",ERROR,"Not implemented"
LocalProvider,generate,"prompt: str, max_tokens: int = 100, temperature: float = 0.7, **kwargs","prompt: str, max_tokens: 100, temperature: 0.7","str","L4LLMInterface","llm.*",ERROR,"Not implemented"

DistilGPT2Provider,__init__,"model_name: str = 'distilgpt2', **kwargs","model_name: 'distilgpt2'","None","self","llm.model",OK,"Simulated provider"
DistilGPT2Provider,generate,"prompt: str, **kwargs","prompt: str","str","L4LLMInterface","None",OK,"Simulated response"

SimpleLocalProvider,__init__,"model_name: str = 'distilgpt2', device: str = 'cpu'","model_name: 'distilgpt2', device: 'cpu'","None","self","llm.model, llm.device",OK,"Simple local"
SimpleLocalProvider,generate,"prompt: str, max_length: int = 100, temperature: float = 0.7","prompt: str, max_length: 100, temperature: 0.7","str","L4LLMInterface","llm.*",OK,"Generate locally"

ProviderFactory,create,"provider_type: str, **kwargs","provider_type: str, **kwargs","Any","LLMProviderRegistry","llm.provider",OK,"Create provider"
ProviderFactory,get_available_providers,"None","None","List[str]","Various","None",OK,"List providers"

# Memory Implementation
GraphMemorySearch,__init__,"memory_manager: Any, graph_reasoner: Any","memory_manager: L2Memory, graph_reasoner: L3Graph","None","self","None",OK,"Graph search"
GraphMemorySearch,search_with_graph,"query: str, top_k: int = 10, max_hops: int = 2","query: str, top_k: 10, max_hops: 2","List[Dict[str, Any]]","L3GraphReasoner","memory.graph_search.*",OK,"Search via graph"
GraphMemorySearch,get_connected_episodes,"episode_idx: int, max_hops: int = 2","episode_idx: int, max_hops: 2","List[int]","self.search_with_graph","None",OK,"Get connected nodes"

KnowledgeGraphMemory,__init__,"storage_path: str, embedding_dim: int = 384","storage_path: str, embedding_dim: 384","None","self","None",OK,"Graph memory"
KnowledgeGraphMemory,add_episode,"embedding: np.ndarray, text: str, metadata: Dict[str, Any]","embedding: np.ndarray(384,), text: str, metadata: dict","int","L2MemoryManager","None",OK,"Add to graph"
KnowledgeGraphMemory,search,"query_embedding: np.ndarray, top_k: int = 10","query_embedding: np.ndarray(384,), top_k: 10","List[Tuple[int, float]]","L2MemoryManager","None",OK,"Search graph"

ScalableGraphManager,__init__,"dimension: int = 384, similarity_threshold: float = 0.7","dimension: 384, similarity_threshold: 0.7","None","self","graph.similarity_threshold",OK,"Scalable graph"
ScalableGraphManager,add_node,"node_id: Any, features: np.ndarray","node_id: Any, features: np.ndarray(384,)","None","ScalableGraphBuilder","None",OK,"Add node"
ScalableGraphManager,build_edges,"None","None","None","ScalableGraphBuilder","graph.similarity_threshold",OK,"Build graph edges"
ScalableGraphManager,get_neighbors,"node_id: Any, k: int = 10","node_id: Any, k: 10","List[Tuple[Any, float]]","GraphMemorySearch","None",OK,"Get k neighbors"

# DataStore Implementations
DataStoreFactory,create,"store_type: str, **kwargs","store_type: 'filesystem', base_path: str","DataStore","MainAgent","None",OK,"Factory pattern"

FileSystemDataStore,__init__,"base_path: str","base_path: str","None","self","None",OK,"File storage"
FileSystemDataStore,save_episodes,"episodes: List[Dict], namespace: str = 'episodes'","episodes: List[Dict], namespace: 'episodes'","None","CachedMemoryManager","None",OK,"Save to JSON"
FileSystemDataStore,load_episodes,"namespace: str = 'episodes'","namespace: 'episodes'","List[Dict]","CachedMemoryManager","None",OK,"Load from JSON"
FileSystemDataStore,search_episodes,"query_embedding: np.ndarray, top_k: int = 10, namespace: str = 'episodes'","query_embedding: np.ndarray(384,), top_k: 10, namespace: 'episodes'","List[Tuple[Dict, float]]","L2MemoryAdapter","None",OK,"Vector search"

InMemoryDataStore,__init__,"None","None","None","self","None",OK,"Memory storage"
InMemoryDataStore,save_episodes,"episodes: List[Dict], namespace: str = 'episodes'","episodes: List[Dict], namespace: 'episodes'","None","Various","None",OK,"Store in memory"
InMemoryDataStore,load_episodes,"namespace: str = 'episodes'","namespace: 'episodes'","List[Dict]","Various","None",OK,"Load from memory"

SQLiteDataStore,__init__,"db_path: str","db_path: str","None","self","None",OK,"SQLite storage"
SQLiteDataStore,save_episodes,"episodes: List[Dict], namespace: str = 'episodes'","episodes: List[Dict], namespace: 'episodes'","None","Various","None",OK,"Save to DB"
SQLiteDataStore,load_episodes,"namespace: str = 'episodes'","namespace: 'episodes'","List[Dict]","Various","None",OK,"Load from DB"
SQLiteDataStore,search_episodes_async,"query_embedding: np.ndarray, top_k: int = 10, namespace: str = 'episodes'","query_embedding: np.ndarray(384,), top_k: 10, namespace: 'episodes'","List[Tuple[Dict, float]]","Various","None",OK,"Async search"

# Adapter Components
L2MemoryAdapter,__init__,"datastore: DataStore, embedder: Any","datastore: DataStore, embedder: EmbeddingManager","None","self","None",OK,"L2 adapter"
L2MemoryAdapter,add_episode,"text: str, vec: Optional[np.ndarray] = None, c_value: float = 0.5, metadata: Optional[Dict] = None","text: str, vec: None, c_value: 0.5, metadata: None","int","L2MemoryManager","None",OK,"Add via DataStore"
L2MemoryAdapter,search_episodes,"query_embedding: np.ndarray, top_k: int = 10","query_embedding: np.ndarray(384,), top_k: 10","List[Tuple[Episode, float]]","L2MemoryManager","None",OK,"Search via DataStore"

L3GraphAdapter,__init__,"datastore: DataStore, config: Dict[str, Any]","datastore: DataStore, config: dict","None","self","graph.*",OK,"L3 adapter"
L3GraphAdapter,build_graph,"episode_ids: List[int]","episode_ids: List[int]","nx.Graph","L3GraphReasoner","graph.*",OK,"Build from DataStore"
L3GraphAdapter,save_graph,"graph: nx.Graph, namespace: str = 'graph'","graph: nx.Graph, namespace: 'graph'","None","L3GraphReasoner","None",OK,"Save to DataStore"

# GNN Components
QueryGraphGNN,__init__,"input_dim: int, hidden_dim: int, output_dim: int","input_dim: 384, hidden_dim: 128, output_dim: 64","None","self","None",OK,"Query GNN"
QueryGraphGNN,forward,"x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor, query_embedding: torch.Tensor","x: Tensor, edge_index: Tensor, batch: Tensor, query_embedding: Tensor","torch.Tensor","QueryTransformer","None",OK,"GNN forward pass"

MultiHopGNN,__init__,"input_dim: int, hidden_dim: int, output_dim: int, num_hops: int = 2","input_dim: 384, hidden_dim: 128, output_dim: 64, num_hops: 2","None","self","None",OK,"Multi-hop GNN"
MultiHopGNN,forward,"x: torch.Tensor, edge_index: torch.Tensor, edge_attr: Optional[torch.Tensor] = None","x: Tensor, edge_index: Tensor, edge_attr: None","torch.Tensor","EnhancedQueryTransformer","None",OK,"Multi-hop forward"

# Base Interfaces
VectorIndex,add,"embedding: np.ndarray, metadata: Dict[str, Any]","embedding: np.ndarray(384,), metadata: dict","int","DataStore implementations","None",OK,"Add vector"
VectorIndex,search,"query: np.ndarray, top_k: int = 10","query: np.ndarray(384,), top_k: 10","List[Tuple[int, float]]","DataStore implementations","None",OK,"Search vectors"

FAISSVectorIndex,__init__,"dimension: int","dimension: 384","None","self","None",OK,"FAISS index"
FAISSVectorIndex,add,"embedding: np.ndarray, metadata: Dict[str, Any]","embedding: np.ndarray(384,), metadata: dict","int","FileSystemDataStore","None",OK,"Add to FAISS"
FAISSVectorIndex,search,"query: np.ndarray, top_k: int = 10","query: np.ndarray(384,), top_k: 10","List[Tuple[int, float]]","FileSystemDataStore","None",OK,"Search FAISS"

# Config-dependent Output Routing
# Note: Output destinations change based on configuration settings

# LLM Provider Routing (based on llm.provider)
L4LLMInterface,generate_response,"context: Dict, question: str","context: dict, question: str","str","MainAgent→MockProvider if llm.provider='mock' | MainAgent→AnthropicProvider if llm.provider='anthropic' | MainAgent→OpenAIProvider if llm.provider='openai'","llm.provider",CONFIG_DEPENDENT,"Routes to different providers"

# Graph Type Routing (based on graph.use_gnn)
ScalableGraphBuilder,build_graph,"documents: List[Dict]","documents: List[Dict]","nx.Graph if use_gnn=False | torch_geometric.Data if use_gnn=True","L3GraphReasoner","graph.use_gnn",CONFIG_DEPENDENT,"Different graph types"

# Memory Manager Routing (based on memory.use_datastore)
MainAgent,_initialize_memory,"None","None","L2MemoryManager if use_datastore=False | CachedMemoryManager if use_datastore=True","self.l2_memory","memory.use_datastore",CONFIG_DEPENDENT,"Different memory managers"

# Adaptive Processing Routing (based on processing.adaptive_loop.enabled)
MainAgent,process_question,"question: str","question: str","CycleResult if adaptive_loop.enabled=False | Dict[str,Any] if adaptive_loop.enabled=True","caller","processing.adaptive_loop.enabled",CONFIG_DEPENDENT,"Different processing paths"

# Query Transformation Routing (based on graph.enable_query_transformation)
L3GraphReasoner,analyze_documents,"documents: List[Dict]","documents: List[Dict]","Dict with basic analysis if enable_query_transformation=False | Dict with transformed queries if enable_query_transformation=True","MainAgent","graph.enable_query_transformation",CONFIG_DEPENDENT,"Optional query transformation"

# Embedding Model Routing (based on embedding.model_name)
EmbeddingManager,__init__,"model_name: str, config: Dict","model_name: str, config: dict","SentenceTransformer if model_name contains 'sentence-transformers' | HuggingFaceModel if model_name contains 'bert' or 'roberta'","self.model","embedding.model_name",CONFIG_DEPENDENT,"Different embedding models"

# Exploration Strategy Routing (based on processing.adaptive_loop.strategy)
AdaptiveProcessor,_create_strategy,"strategy_name: str","strategy_name: str","AlternatingStrategy if strategy='alternating' | ExpandingStrategy if strategy='expanding' | NarrowingStrategy if strategy='narrowing'","self.exploration_loop","processing.adaptive_loop.strategy",CONFIG_DEPENDENT,"Different exploration strategies"

# DataStore Type Routing (based on datastore_type parameter)
DataStoreFactory,create,"store_type: str, **kwargs","store_type: str, **kwargs","FileSystemDataStore if store_type='filesystem' | InMemoryDataStore if store_type='memory' | SQLiteDataStore if store_type='sqlite'","MainAgent","store_type parameter",CONFIG_DEPENDENT,"Different storage backends"

# Graph Analysis Routing (based on graph.analysis_method)
GraphAnalyzer,calculate_metrics,"current_graph: Any, previous_graph: Any","current_graph: Any, previous_graph: Any","Dict with basic metrics if analysis_method='basic' | Dict with advanced metrics if analysis_method='advanced'","L3GraphReasoner","graph.analysis_method",CONFIG_DEPENDENT,"Different analysis methods"

# Prompt Style Routing (based on llm.prompt_style)
L4LLMInterface,_build_prompt,"context: Dict, question: str","context: dict, question: str","Standard prompt if prompt_style='standard' | Detailed prompt if prompt_style='detailed' | Minimal prompt if prompt_style='minimal'","self.generate_response","llm.prompt_style",CONFIG_DEPENDENT,"Different prompt formats"

# Summary
# Total Classes Documented: 189/190
# Status Distribution:
# - OK: 165 classes
# - ERROR: 12 classes (mainly due to shape mismatches or unimplemented features)
# - CONFIG_DEPENDENT: 12 routing decisions based on configuration

# Key Configuration Dependencies:
# 1. llm.provider → Determines which LLM provider is used
# 2. graph.use_gnn → Switches between NetworkX and PyTorch Geometric
# 3. memory.use_datastore → Switches between direct memory and DataStore-backed memory
# 4. processing.adaptive_loop.enabled → Enables/disables adaptive processing
# 5. graph.enable_query_transformation → Enables/disables query transformation features
# 6. embedding.model_name → Determines which embedding model is loaded
# 7. processing.adaptive_loop.strategy → Selects exploration strategy
# 8. graph.analysis_method → Selects graph analysis approach
# 9. llm.prompt_style → Determines prompt formatting

# Critical Pipeline Issues Fixed:
# 1. Embedding shape: (1,384) → (384,) via StandardizedEmbedder
# 2. Graph type mixing: NetworkX/PyG unified via GraphTypeAdapter
# 3. Config handling: Dict/Pydantic unified via ConfigNormalizer
# 4. LLM provider dict handling: Fixed in L4LLMInterface
# 5. CachedMemoryManager search: Added missing search_episodes method