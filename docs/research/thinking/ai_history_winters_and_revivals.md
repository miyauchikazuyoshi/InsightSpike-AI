# AI史：挫折と復活の70年

## 第1期：黎明期（1950-1974）「楽観の時代」

### 始まり
- **1950**: チューリングテスト提案
- **1956**: ダートマス会議（"AI"命名）
  - 「思考する機械は10年以内に実現」

### 初期の成功
```
1956: Logic Theorist（定理証明）
1959: General Problem Solver
1966: ELIZA（対話システム）
1970: SHRDLU（自然言語理解）
```

### 楽観的予測
- 「1970年代には人間レベルのAI」
- 「機械翻訳は数年で完成」
- 研究資金が潤沢に流入

## 第1の冬（1974-1980）「幻滅」

### 挫折の原因
1. **組み合わせ爆発**
   ```
   チェス: 10^120の可能性
   囲碁: 10^360の可能性
   → 総当たりは不可能
   ```

2. **モラベックのパラドックス**
   - 難しいこと（数学）は簡単
   - 簡単なこと（歩く）は難しい

3. **ALPAC報告書（1966）**
   - 「機械翻訳は失敗」
   - 米国の研究資金カット

### 象徴的な失敗
```
入力: "The spirit is willing but the flesh is weak"
機械翻訳（英→露→英）
出力: "The vodka is good but the meat is rotten"
```

## 第2期：知識工学（1980-1987）「エキスパートシステム」

### 復活の兆し
- **前提**: 知能 = 知識の量
- **方法**: 専門家の知識をルール化

### 商業的成功
```
MYCIN: 医療診断（成功率69%、医師65%）
DENDRAL: 分子構造推定
R1/XCON: DECのコンピュータ構成（年間4000万ドル節約）
```

### 日本の第五世代コンピュータ
- 1982年：570億円の国家プロジェクト
- 「10年で人間を超える」

## 第2の冬（1987-1993）「知識の限界」

### 挫折の原因
1. **知識獲得ボトルネック**
   - 専門家から知識を引き出すのが困難
   - ルールの保守が爆発的に増加

2. **脆弱性**
   ```
   IF 症状A AND 症状B THEN 病気X
   → 少しでも想定外だと動作しない
   ```

3. **LISPマシンの崩壊**
   - 専用ハードウェアが汎用PCに敗北
   - Symbolics社倒産（1987）

### 第五世代の失敗
- Prologベースの並列推論マシン
- 実用化できず「失敗」の烙印

## 暗黒期（1993-2000）「AI冬の極寒」

### AI という言葉がタブーに
- 研究者は「AI」を避け始める
  - "機械学習"
  - "知的システム"
  - "計算知能"

### 統計的手法への転換
- 決定木、SVM、ベイズ推定
- 「理論より経験」

## 第3期：統計的機械学習（2000-2012）「静かな革命」

### データの時代
```
2001: データ量 > アルゴリズムの巧妙さ
"The Unreasonable Effectiveness of Data"
```

### 重要な転換点
1. **計算資源の向上**
   - CPU: ムーアの法則
   - 2007: CUDA（GPU計算）

2. **ビッグデータ**
   - Web: 無限のテキストデータ
   - ImageNet: 1400万枚の画像

3. **理論的breakthrough**
   ```
   2006: Deep Belief Networks（Hinton）
   → 深い層の学習が可能に
   ```

## 深層学習革命（2012-現在）「第3の春」

### 2012年：転換点
**ImageNet競技会**
```
従来手法: エラー率26%
AlexNet（深層学習）: エラー率16%
→ 圧倒的な差で優勝
```

### 急速な進展
```
2014: GAN（敵対的生成）
2015: ResNet（人間超え）
2017: Transformer（Attention）
2018: BERT（言語理解）
2020: GPT-3（1750億パラメータ）
2023: ChatGPT（世界を変える）
```

## なぜ深層学習は成功したか

### 1. 三位一体の条件が揃った
```
Big Data + GPU + アルゴリズム
   ↓        ↓         ↓
ImageNet  CUDA    Backprop改良
```

### 2. End-to-End学習
```
従来: 特徴抽出 → 分類（人間が設計）
深層: 生データ → 結果（自動で学習）
```

### 3. 表現学習
- 人間が特徴を設計しない
- データから自動的に表現を獲得

## 各時代の根本的な誤り

### 第1期の誤り
「知能は論理である」
→ 現実：知能の大部分は非論理的

### 第2期の誤り
「知能は知識量である」
→ 現実：知識の使い方が重要

### 第3期の課題
「知能はパターン認識である」
→ 疑問：理解や創造性は？

## 歴史から学ぶ教訓

### 1. 過度な楽観は禁物
- 「10年で人間超え」→ 70年経っても...
- でも諦めない

### 2. ブレークスルーは予想外
- Hinton（2006）: 学会で無視される
- 6年後：世界を変える

### 3. 古いアイデアの復活
```
1943: McCulloch-Pitts（ニューロン）
 ↓ 40年の冬
1986: Backpropagation
 ↓ 20年の停滞
2006: Deep Learning
```

## geDIGの位置づけ

### 歴史的文脈で見ると
1. **古典的AIの洞察**を継承（差分、構造、探索）
2. **深層学習の成果**を活用（ベクトル空間）
3. **新しい問題設定**（洞察生成）

### 第4の波？
```
第1波: 論理・記号
第2波: 知識・ルール
第3波: データ・パターン
第4波: 洞察・創造性？ ← geDIG
```

## 未来への示唆

### 次の冬は来るか？
- 深層学習の限界は見え始めている
  - 説明性の欠如
  - エネルギー消費
  - 創造性の壁

### geDIGが示す方向
- スケーリング競争からの脱却
- 人間的な思考プロセスへの回帰
- 小さくて賢いAI

歴史は繰り返すが、螺旋的に進化する。