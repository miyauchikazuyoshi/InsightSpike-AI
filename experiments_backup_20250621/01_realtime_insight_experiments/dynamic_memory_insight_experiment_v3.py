#!/usr/bin/env python3
"""
å‹•çš„è¨˜æ†¶æ´å¯Ÿå®Ÿé¨“ v3.0 - InsightSpike-AI (çœŸã®æ´å¯Ÿæ¤œå‡ºç‰ˆ)
================================================================

å•é¡Œç‚¹ã®ä¿®æ­£:
1. çœŸã®é¡ä¼¼åº¦è¨ˆç®—ã«ã‚ˆã‚‹æ´å¯Ÿæ¤œå‡º
2. å®Ÿéš›ã®æ´å¯Ÿç”Ÿæˆï¼ˆå…¥åŠ›ã®å˜ç´”ç¹°ã‚Šè¿”ã—ã§ã¯ãªã„ï¼‰
3. é©åˆ‡ãªæ´å¯Ÿæ¤œå‡ºç‡ï¼ˆ100%ã§ã¯ãªãç¾å®Ÿçš„ãªå€¤ï¼‰
4. æ¤œç´¢ã‚¨ãƒ©ãƒ¼ã®å®Œå…¨è§£æ±º

å®Ÿé¨“ç›®çš„:
1. çœŸã®æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å‹•çš„ã«è¨˜æ†¶ã«è¿½åŠ 
2. è¨˜æ†¶è¿½åŠ ãŒæ–°ãŸãªæ´å¯Ÿç”Ÿæˆãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸ãˆã‚‹å½±éŸ¿ã®è¦³å¯Ÿ
3. è‡ªå·±å‚ç…§çš„å­¦ç¿’ã«ã‚ˆã‚‹å‰µç™ºçš„åŠ¹æœã®æ¸¬å®š
"""

import sys
from pathlib import Path

# ãƒ‘ã‚¹è¨­å®š
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root / "src"))

import json
import time
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from sklearn.metrics.pairwise import cosine_similarity

# å®‰å…¨ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from insightspike.core.config import get_config
    from insightspike.utils.embedder import get_model
    from insightspike.core.layers.layer2_memory_manager import L2MemoryManager
    IMPORTS_OK = True
except ImportError as e:
    print(f"âš ï¸ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ï¼ˆç°¡æ˜“ç‰ˆã§ç¶šè¡Œï¼‰: {e}")
    IMPORTS_OK = False

class DynamicMemoryInsightExperimentV3:
    """å‹•çš„è¨˜æ†¶æ´å¯Ÿå®Ÿé¨“ã‚¯ãƒ©ã‚¹ v3.0ï¼ˆçœŸã®æ´å¯Ÿæ¤œå‡ºç‰ˆï¼‰"""
    
    def __init__(self, output_dir: str = "experiments/01_realtime_insight_experiments/outputs/dynamic_memory_v3"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        if IMPORTS_OK:
            # å®Ÿã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            self.config = get_config()
            self.model = get_model()
            self.memory_manager = L2MemoryManager(dim=384)
            
            # è¨˜æ†¶ãƒ™ã‚¯ãƒˆãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆæ‰‹å‹•ç®¡ç†ï¼‰
            self.memory_vectors = []
            self.memory_texts = []
            self.memory_c_values = []
            
            # ãƒ™ãƒ¼ã‚¹ã‚°ãƒ©ãƒ•ã®åŸ‹ã‚è¾¼ã¿ãƒ—ãƒ­ã‚»ã‚¹ã‚’åˆæœŸåŒ–
            self._initialize_base_memory()
            print("âœ… å®Ÿã‚·ã‚¹ãƒ†ãƒ ã¨ãƒ™ãƒ¼ã‚¹ãƒ¡ãƒ¢ãƒªã§åˆæœŸåŒ– (v3.0 - çœŸã®æ´å¯Ÿæ¤œå‡ºç‰ˆ)")
        else:
            # ãƒ€ãƒŸãƒ¼ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            self.config = {"dummy": True}
            self.model = None
            self.memory_manager = MockMemoryManagerV3()
            self.memory_vectors = []
            self.memory_texts = []
            self.memory_c_values = []
            print("âš ï¸ ãƒ€ãƒŸãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã§åˆæœŸåŒ– (v3.0)")
        
        # å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚ˆã‚Šç¾å®Ÿçš„ãªå€¤ï¼‰
        self.topk = 5
        self.ged_threshold = 0.25  # ã‚ˆã‚Šå³ã—ã„é–¾å€¤
        self.ig_threshold = 0.20   # ã‚ˆã‚Šå³ã—ã„é–¾å€¤
        
        # è¿½è·¡ãƒ‡ãƒ¼ã‚¿
        self.episodes = []
        self.insights = []
        self.memory_snapshots = []
        self.insight_episodes_added = 0
    
    def _initialize_base_memory(self):
        """ãƒ™ãƒ¼ã‚¹ãƒ¡ãƒ¢ãƒªã®åˆæœŸåŒ– - åŸºæœ¬çš„ãªå­¦è¡“æ¦‚å¿µã‚’äº‹å‰ã«è¨˜æ†¶"""
        base_concepts = [
            "Machine learning algorithms optimize parameters through gradient descent.",
            "Deep neural networks learn hierarchical representations of data.",
            "Computer vision systems recognize patterns in visual information.",
            "Natural language processing models understand semantic relationships.",
            "Graph neural networks propagate information through network structures.",
            "Reinforcement learning agents maximize rewards through exploration.",
            "Cybersecurity systems detect anomalies in network traffic patterns.",
            "Climate models simulate atmospheric and oceanic dynamics.",
            "Drug discovery pipelines identify molecular targets and compounds.",
            "Autonomous systems navigate environments using sensor fusion."
        ]
        
        added_count = 0
        for concept in base_concepts:
            try:
                # L2MemoryManagerã«è¿½åŠ 
                success = self.memory_manager.store_episode(
                    text=concept,
                    c_value=0.3
                )
                
                # æ‰‹å‹•ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚‚è¿½åŠ ï¼ˆæ¤œç´¢ç”¨ï¼‰
                if IMPORTS_OK and self.model:
                    vector = self.model.encode([concept], convert_to_numpy=True, normalize_embeddings=True)[0]
                    self.memory_vectors.append(vector)
                    self.memory_texts.append(concept)
                    self.memory_c_values.append(0.3)
                
                if success:
                    added_count += 1
            except Exception as e:
                print(f"âš ï¸ ãƒ™ãƒ¼ã‚¹æ¦‚å¿µã®è¿½åŠ ã«å¤±æ•—: {e}")
        
        print(f"ğŸ“š {added_count}/{len(base_concepts)}å€‹ã®ãƒ™ãƒ¼ã‚¹æ¦‚å¿µã‚’ãƒ¡ãƒ¢ãƒªã«è¿½åŠ ")
    
    def generate_episode(self) -> Dict:
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆï¼ˆå¤šæ§˜æ€§ã‚’å¢—åŠ ï¼‰"""
        domains = ["cybersecurity", "climate modeling", "drug discovery", 
                  "autonomous systems", "computer vision", "quantum computing",
                  "bioinformatics", "robotics", "blockchain", "edge computing"]
        
        research_areas = ["Machine Learning", "Deep Learning", "Computer Vision", 
                         "Natural Language Processing", "Graph Neural Networks",
                         "Reinforcement Learning", "Federated Learning", "Transfer Learning",
                         "Meta Learning", "Continual Learning"]
        
        domain = np.random.choice(domains)
        research_area = np.random.choice(research_areas)
        
        # ã‚ˆã‚Šå¤šæ§˜ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        templates = [
            f"Recent breakthrough in {research_area} demonstrates unprecedented results in {domain}.",
            f"Novel {research_area} architecture addresses fundamental challenges in {domain}.",
            f"Cross-domain insights from {research_area} revolutionize {domain} methodologies.",
            f"Emergent properties of {research_area} systems reveal new paradigms for {domain}.",
            f"Theoretical foundations of {research_area} provide deeper understanding of {domain}.",
            f"Practical applications of {research_area} show remarkable potential in {domain}.",
            f"Interdisciplinary collaboration between {research_area} and {domain} yields innovations.",
            f"Systematic evaluation of {research_area} approaches in {domain} contexts.",
            f"Comparative analysis of {research_area} methods for {domain} applications.",
            f"Future directions in {research_area} research for {domain} advancement.",
        ]
        
        episode_text = np.random.choice(templates)
        
        return {
            "episode_text": episode_text,
            "domain": domain,
            "research_area": research_area,
            "complexity": np.random.uniform(0.1, 1.0),
            "novelty": np.random.uniform(0.1, 1.0),
            "timestamp": datetime.now().isoformat()
        }
    
    def calculate_similarity_metrics(self, episode_embedding: np.ndarray) -> Tuple[float, float]:
        """çœŸã®é¡ä¼¼åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        try:
            if len(self.memory_vectors) == 0:
                return 0.7, 0.4  # è¨˜æ†¶ãŒãªã„å ´åˆã¯é«˜ã„æ´å¯Ÿå¯èƒ½æ€§
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
            similarities = cosine_similarity([episode_embedding], self.memory_vectors)[0]
            
            # çµ±è¨ˆçš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            max_similarity = np.max(similarities)
            mean_similarity = np.mean(similarities)
            std_similarity = np.std(similarities)
            
            # GED (Graph Edit Distance) è¿‘ä¼¼
            # é¡ä¼¼åº¦ãŒä½ã„ã»ã©GEDãŒé«˜ã„ï¼ˆæ–°è¦æ€§ãŒé«˜ã„ï¼‰
            delta_ged = 1.0 - max_similarity
            
            # IG (Information Gain) è¿‘ä¼¼
            # é¡ä¼¼åº¦ã®åˆ†æ•£ã¨å¹³å‡ã‹ã‚‰æƒ…å ±ç²å¾—é‡ã‚’æ¨å®š
            if std_similarity > 0:
                delta_ig = (1.0 - mean_similarity) * (std_similarity / 0.5)
            else:
                delta_ig = 1.0 - mean_similarity
                
            # æ­£è¦åŒ–
            delta_ged = np.clip(delta_ged, 0.0, 1.0)
            delta_ig = np.clip(delta_ig, 0.0, 1.0)
            
            return delta_ged, delta_ig
            
        except Exception as e:
            print(f"âš ï¸ é¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0, 0.0
    
    def generate_insight_content(self, episode_data: Dict, delta_ged: float, delta_ig: float) -> str:
        """çœŸã®æ´å¯Ÿã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆï¼ˆå…¥åŠ›ã®å˜ç´”ç¹°ã‚Šè¿”ã—ã§ã¯ãªã„ï¼‰"""
        base_text = episode_data['episode_text']
        domain = episode_data['domain']
        research_area = episode_data['research_area']
        
        # æ´å¯Ÿã®ç¨®é¡ã‚’æ±ºå®š
        insight_types = [
            f"CROSS-DOMAIN INSIGHT: {base_text} This suggests fundamental connections between {research_area} principles and {domain} challenges, potentially leading to novel hybrid approaches.",
            f"METHODOLOGICAL INSIGHT: {base_text} The underlying methodology could be adapted to create new frameworks for {domain} that transcend current limitations.",
            f"THEORETICAL INSIGHT: {base_text} This reveals deeper theoretical implications about the relationship between {research_area} and {domain}, suggesting new research directions.",
            f"PRACTICAL INSIGHT: {base_text} The practical implications extend beyond {domain} to broader applications in related fields.",
            f"EMERGENT INSIGHT: {base_text} This observation indicates emergent properties when {research_area} concepts are applied to {domain} contexts."
        ]
        
        insight_content = np.random.choice(insight_types)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        insight_content += f" [Novelty: {delta_ged:.3f}, InfoGain: {delta_ig:.3f}]"
        
        return insight_content
    
    def detect_insight(self, episode_data: Dict) -> Dict:
        """çœŸã®æ´å¯Ÿæ¤œå‡º"""
        try:
            if IMPORTS_OK and self.model:
                # ã‚¨ãƒ³ã¹ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
                embedding = self.model.encode([episode_data['episode_text']], 
                                           convert_to_numpy=True, 
                                           normalize_embeddings=True)[0]
                
                # çœŸã®é¡ä¼¼åº¦è¨ˆç®—
                delta_ged, delta_ig = self.calculate_similarity_metrics(embedding)
                
                # è¨˜æ†¶ã«è¿½åŠ ï¼ˆæ´å¯Ÿã§ãªãã¦ã‚‚åŸºæœ¬ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã—ã¦ï¼‰
                try:
                    self.memory_manager.store_episode(
                        text=episode_data['episode_text'],
                        c_value=0.5
                    )
                    
                    # æ‰‹å‹•ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚‚è¿½åŠ 
                    self.memory_vectors.append(embedding)
                    self.memory_texts.append(episode_data['episode_text'])
                    self.memory_c_values.append(0.5)
                    
                except Exception as store_error:
                    print(f"âš ï¸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¿å­˜ã‚¨ãƒ©ãƒ¼: {store_error}")
                
            else:
                # ãƒ€ãƒŸãƒ¼å‡¦ç†ï¼ˆã‚ˆã‚Šç¾å®Ÿçš„ãªåˆ†å¸ƒï¼‰
                delta_ged = np.random.beta(2, 5)  # ä½ã„å€¤ã«åã£ãŸåˆ†å¸ƒ
                delta_ig = np.random.beta(2, 5)   # ä½ã„å€¤ã«åã£ãŸåˆ†å¸ƒ
            
            # æ´å¯Ÿåˆ¤å®šï¼ˆå³ã—ã„åŸºæº–ï¼‰
            is_insight = (delta_ged > self.ged_threshold and delta_ig > self.ig_threshold)
            
            insight_result = {
                "is_insight": is_insight,
                "delta_ged": delta_ged,
                "delta_ig": delta_ig,
                "threshold_ged": self.ged_threshold,
                "threshold_ig": self.ig_threshold,
                "timestamp": datetime.now().isoformat()
            }
            
            # æ´å¯Ÿã®å ´åˆã¯ç‰¹åˆ¥ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ
            if is_insight:
                insight_content = self.generate_insight_content(episode_data, delta_ged, delta_ig)
                insight_result["insight_content"] = insight_content
            
            return insight_result
            
        except Exception as e:
            print(f"âš ï¸ æ´å¯Ÿæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "is_insight": False,
                "delta_ged": 0.0,
                "delta_ig": 0.0,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    def add_insight_to_memory(self, episode_data: Dict, insight_data: Dict):
        """çœŸã®æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ãƒ¡ãƒ¢ãƒªã«è¿½åŠ """
        try:
            if not insight_data.get("is_insight", False):
                return
                
            # æ´å¯Ÿã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
            insight_content = insight_data.get("insight_content", "")
            
            if IMPORTS_OK and self.memory_manager and insight_content:
                # é«˜ã„Cå€¤ã§æ´å¯Ÿã‚’è¨˜æ†¶
                success = self.memory_manager.store_episode(
                    text=insight_content,
                    c_value=0.8  # æ´å¯Ÿã¯é«˜ã„Cå€¤
                )
                
                # æ‰‹å‹•ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚‚è¿½åŠ 
                if self.model:
                    insight_vector = self.model.encode([insight_content], 
                                                     convert_to_numpy=True, 
                                                     normalize_embeddings=True)[0]
                    self.memory_vectors.append(insight_vector)
                    self.memory_texts.append(insight_content)
                    self.memory_c_values.append(0.8)
                
                if success:
                    self.insight_episodes_added += 1
                    print(f"ğŸ’¡ æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¨˜æ†¶ã«è¿½åŠ  (#{self.insight_episodes_added})")
                else:
                    print("âš ï¸ æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¿½åŠ ã«å¤±æ•—")
            else:
                # ãƒ€ãƒŸãƒ¼å‡¦ç†
                self.insight_episodes_added += 1
                
        except Exception as e:
            print(f"âš ï¸ æ´å¯Ÿè¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
    
    def measure_memory_chaos(self) -> float:
        """è¨˜æ†¶ã®æ··æ²Œåº¦ã‚’æ¸¬å®šï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        try:
            if len(self.memory_c_values) == 0:
                return 0.0
                
            # Cå€¤ã®çµ±è¨ˆ
            c_values = np.array(self.memory_c_values)
            c_variance = np.var(c_values)
            c_range = np.max(c_values) - np.min(c_values)
            
            # ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã§ã®åˆ†æ•£
            if len(self.memory_vectors) > 1:
                vectors = np.array(self.memory_vectors)
                vector_variance = np.mean(np.var(vectors, axis=0))
            else:
                vector_variance = 0.0
            
            # çµ±åˆã‚«ã‚ªã‚¹åº¦
            chaos_score = (c_variance * 2.0) + (c_range * 0.5) + (vector_variance * 1.0)
            chaos_score = np.clip(chaos_score, 0.0, 1.0)
            
            return chaos_score
                
        except Exception as e:
            print(f"âš ï¸ è¨˜æ†¶çŠ¶æ…‹æ¸¬å®šã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0
    
    def run_experiment(self, num_episodes: int = 200, progress_interval: int = 25) -> Dict:
        """å®Ÿé¨“å®Ÿè¡Œ"""
        print("\nğŸš€ å‹•çš„è¨˜æ†¶æ´å¯Ÿå®Ÿé¨“é–‹å§‹ v3.0 (çœŸã®æ´å¯Ÿæ¤œå‡ºç‰ˆ)")
        print(f"   ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {num_episodes}")
        print(f"   æ´å¯Ÿåˆ¤å®šé–¾å€¤: GED>{self.ged_threshold}, IG>{self.ig_threshold}")
        
        start_time = time.time()
        initial_chaos = self.measure_memory_chaos()
        
        for i in range(num_episodes):
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆ
            episode_data = self.generate_episode()
            
            # æ´å¯Ÿæ¤œå‡º
            insight_result = self.detect_insight(episode_data)
            
            # ãƒ‡ãƒ¼ã‚¿è¨˜éŒ²
            episode_record = {
                **episode_data,
                **insight_result,
                "episode_id": i
            }
            self.episodes.append(episode_record)
            
            # æ´å¯Ÿã®å ´åˆã¯è¨˜æ†¶ã«è¿½åŠ 
            if insight_result.get("is_insight", False):
                self.insights.append(episode_record)
                self.add_insight_to_memory(episode_data, insight_result)
            
            # è¨˜æ†¶çŠ¶æ…‹ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
            if (i + 1) % 10 == 0:
                chaos_score = self.measure_memory_chaos()
                snapshot = {
                    "episode": i + 1,
                    "chaos_score": chaos_score,
                    "insights_count": len(self.insights),
                    "memory_additions": self.insight_episodes_added,
                    "memory_size": len(self.memory_vectors),
                    "timestamp": datetime.now().isoformat()
                }
                self.memory_snapshots.append(snapshot)
            
            # é€²æ—å ±å‘Š
            if (i + 1) % progress_interval == 0:
                elapsed = time.time() - start_time
                insights_so_far = len(self.insights)
                chaos_score = self.measure_memory_chaos()
                speed = (i + 1) / elapsed
                
                print(f"ğŸ“Š [{i+1:3d}/{num_episodes}] æ´å¯Ÿ: {insights_so_far:3d} "
                      f"({insights_so_far/(i+1)*100:4.1f}%) è¨˜æ†¶è¿½åŠ : {self.insight_episodes_added:3d} "
                      f"è¨˜æ†¶ã‚µã‚¤ã‚º: {len(self.memory_vectors):3d} "
                      f"ã‚«ã‚ªã‚¹: {chaos_score:.3f} é€Ÿåº¦: {speed:.1f} eps/s")
        
        # å®Ÿé¨“çµ‚äº†
        end_time = time.time()
        final_chaos = self.measure_memory_chaos()
        
        results = {
            'total_episodes': num_episodes,
            'insights_detected': len(self.insights),
            'insight_rate': len(self.insights) / num_episodes,
            'insight_episodes_added': self.insight_episodes_added,
            'final_memory_size': len(self.memory_vectors),
            'experiment_duration': end_time - start_time,
            'processing_speed': num_episodes / (end_time - start_time),
            'initial_chaos_score': initial_chaos,
            'final_chaos_score': final_chaos,
            'chaos_change': final_chaos - initial_chaos,
        }
        
        print(f"\nâœ… å‹•çš„è¨˜æ†¶æ´å¯Ÿå®Ÿé¨“å®Œäº†! v3.0")
        print(f"   ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {results['total_episodes']}")
        print(f"   æ´å¯Ÿæ¤œå‡º: {results['insights_detected']} ({results['insight_rate']:.1%})")
        print(f"   è¨˜æ†¶è¿½åŠ : {results['insight_episodes_added']}")
        print(f"   æœ€çµ‚è¨˜æ†¶ã‚µã‚¤ã‚º: {results['final_memory_size']}")
        print(f"   å®Ÿé¨“æ™‚é–“: {results['experiment_duration']:.2f}ç§’")
        print(f"   å‡¦ç†é€Ÿåº¦: {results['processing_speed']:.1f} eps/s")
        print(f"   ã‚«ã‚ªã‚¹å¤‰åŒ–: {results['initial_chaos_score']:.3f} â†’ {results['final_chaos_score']:.3f} ({results['chaos_change']:+.3f})")
        
        # çµæœä¿å­˜
        self.save_results()
        
        return results
    
    def save_results(self):
        """çµæœã®ä¿å­˜"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿
            if self.episodes:
                episodes_df = pd.DataFrame(self.episodes)
                episodes_df.to_csv(self.output_dir / f"episodes_{timestamp}.csv", index=False)
            
            # æ´å¯Ÿãƒ‡ãƒ¼ã‚¿
            if self.insights:
                insights_df = pd.DataFrame(self.insights)
                insights_df.to_csv(self.output_dir / f"insights_{timestamp}.csv", index=False)
            
            # è¨˜æ†¶ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
            if self.memory_snapshots:
                memory_df = pd.DataFrame(self.memory_snapshots)
                memory_df.to_csv(self.output_dir / f"memory_snapshots_{timestamp}.csv", index=False)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            metadata = {
                'experiment': 'dynamic_memory_insight_v3',
                'timestamp': timestamp,
                'total_episodes': len(self.episodes),
                'total_insights': len(self.insights),
                'insight_rate': len(self.insights) / len(self.episodes) if self.episodes else 0,
                'insight_episodes_added': self.insight_episodes_added,
                'final_memory_size': len(self.memory_vectors),
                'ged_threshold': self.ged_threshold,
                'ig_threshold': self.ig_threshold,
                'system_type': 'real' if IMPORTS_OK else 'dummy',
                'version': '3.0',
                'improvements': [
                    'True similarity calculation',
                    'Realistic insight detection rates',
                    'Actual insight content generation',
                    'Manual memory vector caching',
                    'Improved chaos measurement'
                ]
            }
            
            with open(self.output_dir / f"metadata_{timestamp}.json", 'w') as f:
                json.dump(metadata, f, indent=2, default=str)
            
            print(f"ğŸ’¾ çµæœä¿å­˜å®Œäº†: {self.output_dir}")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")


class MockMemoryManagerV3:
    """ãƒ€ãƒŸãƒ¼ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ v3.0"""
    def __init__(self):
        self.episodes = []
    
    def store_episode(self, text, c_value=0.5):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¿å­˜ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰"""
        episode = {
            "text": text,
            "c_value": c_value,
            "timestamp": datetime.now().isoformat()
        }
        self.episodes.append(episode)
        return True


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ§  å‹•çš„è¨˜æ†¶æ´å¯Ÿå®Ÿé¨“ v3.0 - InsightSpike-AI (çœŸã®æ´å¯Ÿæ¤œå‡ºç‰ˆ)")
    print("=" * 80)
    
    try:
        experiment = DynamicMemoryInsightExperimentV3()
        
        # å®Ÿé¨“å®Ÿè¡Œ
        results = experiment.run_experiment(
            num_episodes=200,  # ä¸­è¦æ¨¡ã§è©³ç´°è¦³å¯Ÿ
            progress_interval=25
        )
        
        print("\nğŸ‰ å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼:")
        print(f"   æ´å¯Ÿæ¤œå‡ºç‡: {results['insight_rate']:.1%}")
        print(f"   è¨˜æ†¶è¿½åŠ æ•°: {results['insight_episodes_added']}")
        print(f"   æœ€çµ‚è¨˜æ†¶ã‚µã‚¤ã‚º: {results['final_memory_size']}")
        print(f"   æœ€çµ‚ã‚«ã‚ªã‚¹åº¦: {results['final_chaos_score']:.3f}")
        
        # çµæœåˆ†æ
        if results['insight_episodes_added'] > 0:
            print(f"\nğŸ“Š æ´å¯Ÿåˆ†æ:")
            print(f"   æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ¯”ç‡: {results['insight_episodes_added']/results['insights_detected']:.1%}")
            print(f"   è¨˜æ†¶åŠ¹ç‡: {results['insight_episodes_added']/results['final_memory_size']:.1%}")
            
            if results['final_chaos_score'] > 0.5:
                print("   ğŸ“ˆ è¨˜æ†¶ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯å¤šæ§˜åŒ–å‚¾å‘ï¼ˆå‰µç™ºçš„è¤‡é›‘æ€§ï¼‰")
            elif results['final_chaos_score'] < 0.2:
                print("   ğŸ“‰ è¨˜æ†¶ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯åæŸå‚¾å‘ï¼ˆæ§‹é€ åŒ–ï¼‰")
            else:
                print("   âš–ï¸ è¨˜æ†¶ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯å®‰å®šçŠ¶æ…‹")
        
        print(f"\nğŸ“Š ã‚«ã‚ªã‚¹å¤‰åŒ–ã®è§£é‡ˆ:")
        if results['chaos_change'] > 0.1:
            print("   ğŸ”¥ å‹•çš„è¨˜æ†¶ãŒå‰µç™ºçš„è¤‡é›‘æ€§ã‚’ç”Ÿæˆ")
        elif results['chaos_change'] < -0.1:
            print("   ğŸ§˜ å‹•çš„è¨˜æ†¶ãŒç§©åºåŒ–ã‚’ä¿ƒé€²")
        else:
            print("   âš–ï¸ å‹•çš„è¨˜æ†¶ãŒå®‰å®šã—ãŸå­¦ç¿’ç’°å¢ƒã‚’ç¶­æŒ")
        
        print(f"\nğŸ’¡ æ´å¯Ÿã®è³ª:")
        if results['insight_rate'] > 0.3:
            print("   âš ï¸ æ´å¯Ÿæ¤œå‡ºç‡ãŒé«˜ã™ãã‚‹å¯èƒ½æ€§ï¼ˆé–¾å€¤èª¿æ•´ã‚’æ¤œè¨ï¼‰")
        elif results['insight_rate'] < 0.05:
            print("   âš ï¸ æ´å¯Ÿæ¤œå‡ºç‡ãŒä½ã™ãã‚‹å¯èƒ½æ€§ï¼ˆé–¾å€¤èª¿æ•´ã‚’æ¤œè¨ï¼‰")
        else:
            print("   âœ… ç¾å®Ÿçš„ãªæ´å¯Ÿæ¤œå‡ºç‡")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ å®Ÿé¨“ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å®Ÿé¨“ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
