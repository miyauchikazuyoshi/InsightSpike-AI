#!/usr/bin/env python3
"""
å‹•çš„è¨˜æ†¶æ´å¯Ÿå®Ÿé¨“ - è©³ç´°ãƒ­ã‚°ç‰ˆãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒ™ãƒ¼ã‚¹
===========================================

å…ƒã®è©³ç´°ãƒ­ã‚°ç‰ˆå®Ÿè·µçš„ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿå®Ÿé¨“ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€
æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å‹•çš„ã«è¨˜æ†¶ã«è¿½åŠ ã™ã‚‹å®Ÿé¨“ã€‚

å®Ÿéš›ã®dataãƒ•ã‚©ãƒ«ãƒ€ã«æ›¸ãè¾¼ã¿ãªãŒã‚‰ã€æ´å¯Ÿã®è¨˜æ†¶è¿½åŠ ãŒ
æ¬¡ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®æ¤œå‡ºã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹ã‚’è¦³å¯Ÿã™ã‚‹ã€‚

ä¿®æ­£ç‚¹:
- å…ƒã®DetailedLoggingRealtimeExperimentã‚¯ãƒ©ã‚¹ã‚’ç¶™æ‰¿
- æ´å¯Ÿæ¤œå‡ºæ™‚ã«å¼·åŒ–ã•ã‚ŒãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¨˜æ†¶ã«è¿½åŠ 
- å®Ÿéš›ã®dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨ï¼ˆbackup/restoreä»˜ãï¼‰
- å‹•çš„è¨˜æ†¶ã®åŠ¹æœã‚’è©³ç´°ã«è¨˜éŒ²
"""

import sys
import json
import time
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any
import csv
import pandas as pd
import shutil

# å…ƒã®è©³ç´°ãƒ­ã‚°ç‰ˆå®Ÿé¨“ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(str(Path(__file__).parent))
from detailed_logging_realtime_experiment import DetailedLoggingRealtimeExperiment

class DynamicMemoryInsightExperiment(DetailedLoggingRealtimeExperiment):
    """å‹•çš„è¨˜æ†¶æ´å¯Ÿå®Ÿé¨“ã‚¯ãƒ©ã‚¹ï¼ˆè©³ç´°ãƒ­ã‚°ç‰ˆãƒ™ãƒ¼ã‚¹ï¼‰"""
    
    def __init__(self):
        # è¦ªã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        super().__init__()
        
        print("ğŸ§  å‹•çš„è¨˜æ†¶å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰ã«æ‹¡å¼µä¸­...")
        
        # å‹•çš„è¨˜æ†¶å®Ÿé¨“ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.insight_memory_boost = 0.8  # æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®Cå€¤
        self.normal_memory_c = 0.2       # é€šå¸¸ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®Cå€¤
        
        # å‹•çš„è¨˜æ†¶è¿½è·¡ç”¨ãƒ‡ãƒ¼ã‚¿
        self.dynamic_memory_logs = []
        self.insight_memory_additions = 0
        self.memory_state_snapshots = []
        
        # åˆæœŸã‚°ãƒ©ãƒ•æ§‹ç¯‰ãƒ—ãƒ­ãƒˆã‚³ãƒ«
        self._build_initial_knowledge_graph()
        
        print("âœ… å‹•çš„è¨˜æ†¶å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        print(f"   æ´å¯Ÿè¨˜æ†¶å¼·åŒ–å€¤: {self.insight_memory_boost}")
        print(f"   é€šå¸¸è¨˜æ†¶å€¤: {self.normal_memory_c}")
    
    def _build_initial_knowledge_graph(self):
        """åˆæœŸãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã¨ãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰"""
        print("ğŸ—ï¸ åˆæœŸã‚°ãƒ©ãƒ•æ§‹ç¯‰ãƒ—ãƒ­ãƒˆã‚³ãƒ«é–‹å§‹...")
        
        # åŸºç›¤æ¦‚å¿µã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç¾¤
        foundational_episodes = [
            {
                "text": "Machine learning algorithms learn patterns from data through iterative optimization processes.",
                "domain": "Machine Learning",
                "research_area": "Algorithm Design",
                "complexity": 0.6,
                "novelty": 0.4
            },
            {
                "text": "Deep neural networks use hierarchical feature learning to extract representations at multiple levels.",
                "domain": "Machine Learning", 
                "research_area": "Deep Learning",
                "complexity": 0.8,
                "novelty": 0.6
            },
            {
                "text": "Computer vision systems process visual information through convolutional and attention mechanisms.",
                "domain": "Computer Vision",
                "research_area": "Visual Processing",
                "complexity": 0.7,
                "novelty": 0.5
            },
            {
                "text": "Natural language processing leverages transformer architectures for semantic understanding.",
                "domain": "NLP",
                "research_area": "Language Models", 
                "complexity": 0.9,
                "novelty": 0.7
            },
            {
                "text": "Graph neural networks propagate and aggregate information across network structures.",
                "domain": "Graph Learning",
                "research_area": "Network Analysis",
                "complexity": 0.8,
                "novelty": 0.8
            },
            {
                "text": "Reinforcement learning agents optimize policies through reward-based exploration and exploitation.",
                "domain": "Reinforcement Learning",
                "research_area": "Decision Making",
                "complexity": 0.9,
                "novelty": 0.6
            },
            {
                "text": "Cybersecurity systems detect anomalies and threats through behavioral pattern analysis.",
                "domain": "Cybersecurity",
                "research_area": "Threat Detection",
                "complexity": 0.7,
                "novelty": 0.5
            },
            {
                "text": "Climate modeling simulates atmospheric dynamics using numerical methods and data assimilation.",
                "domain": "Climate Science",
                "research_area": "Environmental Modeling",
                "complexity": 0.8,
                "novelty": 0.6
            }
        ]
        
        # åŸºç›¤ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ãƒ¡ãƒ¢ãƒªã¨ã‚°ãƒ©ãƒ•ã«è¿½åŠ 
        initial_count = 0
        for episode_data in foundational_episodes:
            try:
                # L2ãƒ¡ãƒ¢ãƒªã«ä¿å­˜ï¼ˆé€šå¸¸ã®Cå€¤ï¼‰
                memory_success = self.memory_manager.store_episode(
                    text=episode_data["text"],
                    c_value=self.normal_memory_c
                )
                
                # ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã«è¿½åŠ 
                if hasattr(self.knowledge_graph, 'add_episode'):
                    graph_success = self.knowledge_graph.add_episode(
                        text=episode_data["text"],
                        metadata=episode_data
                    )
                else:
                    graph_success = True  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                
                if memory_success:
                    initial_count += 1
                    
            except Exception as e:
                print(f"âš ï¸ åˆæœŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
        
        print(f"ğŸ“š åˆæœŸã‚°ãƒ©ãƒ•æ§‹ç¯‰å®Œäº†: {initial_count}/{len(foundational_episodes)} ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¿½åŠ ")
        
        # åˆæœŸãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
        initial_snapshot = {
            "timestamp": datetime.now().isoformat(),
            "phase": "initial_graph_construction", 
            "episodes_added": initial_count,
            "memory_type": "foundational_knowledge",
            "avg_c_value": self.normal_memory_c
        }
        self.memory_state_snapshots.append(initial_snapshot)
    
    def create_insight_enhanced_episode(self, original_episode: Dict, insight_data: Dict) -> str:
        """æ´å¯Ÿæƒ…å ±ã§å¼·åŒ–ã•ã‚ŒãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ"""
        
        # æ´å¯Ÿã‚¿ã‚¤ãƒ—ã«åŸºã¥ããƒ†ã‚­ã‚¹ãƒˆå¼·åŒ–
        insight_type = insight_data.get('insight_type', 'Micro_Insight')
        ged_value = insight_data.get('ged_value', 0.0)
        ig_value = insight_data.get('ig_value', 0.0)
        cross_domain_count = insight_data.get('cross_domain_count', 0)
        
        # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã«æ´å¯Ÿãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        enhanced_text = f"""INSIGHT_EPISODE: {original_episode['text']}
        
INSIGHT_METADATA:
- Type: {insight_type}
- GED: {ged_value:.4f} (structural novelty)
- IG: {ig_value:.4f} (information gain)
- Cross-domain connections: {cross_domain_count}
- Domain: {original_episode.get('domain', 'unknown')}
- Research Area: {original_episode.get('research_area', 'unknown')}
- Detection Time: {insight_data.get('detection_timestamp', 'unknown')}

This episode represents a significant insight that demonstrates novel patterns 
and cross-domain knowledge integration capabilities."""
        
        return enhanced_text
    
    def add_insight_to_memory(self, episode: Dict, insight_data: Dict):
        """æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å¼·åŒ–ã—ã¦ãƒ¡ãƒ¢ãƒªã«è¿½åŠ """
        try:
            # å¼·åŒ–ã•ã‚ŒãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
            enhanced_text = self.create_insight_enhanced_episode(episode, insight_data)
            
            # é«˜ã„Cå€¤ã§è¨˜æ†¶ã«ä¿å­˜ï¼ˆå®Ÿéš›ã®dataãƒ•ã‚©ãƒ«ãƒ€ã«æ›¸ãè¾¼ã¿ï¼‰
            success = self.memory_manager.store_episode(
                text=enhanced_text,
                c_value=self.insight_memory_boost,
                metadata={
                    'type': 'insight_episode',
                    'original_id': episode['id'],
                    'insight_id': insight_data['insight_id'],
                    'insight_type': insight_data.get('insight_type', 'Unknown'),
                    'ged_value': insight_data.get('ged_value', 0.0),
                    'ig_value': insight_data.get('ig_value', 0.0),
                    'domain': episode.get('domain', 'unknown'),
                    'research_area': episode.get('research_area', 'unknown'),
                    'enhancement_timestamp': datetime.now().isoformat()
                }
            )
            
            if success:
                self.insight_memory_additions += 1
                print(f"ğŸ’¡ æ´å¯Ÿè¨˜æ†¶è¿½åŠ  #{self.insight_memory_additions}: {insight_data['insight_id']}")
                print(f"   Cå€¤: {self.insight_memory_boost}, ã‚¿ã‚¤ãƒ—: {insight_data.get('insight_type', 'Unknown')}")
                
                # å‹•çš„è¨˜æ†¶ãƒ­ã‚°ã«è¨˜éŒ²
                memory_log = {
                    'addition_number': self.insight_memory_additions,
                    'insight_id': insight_data['insight_id'],
                    'original_episode_id': episode['id'],
                    'enhanced_text_length': len(enhanced_text),
                    'c_value': self.insight_memory_boost,
                    'memory_size_before': len(self.memory_manager.episodes) - 1,
                    'memory_size_after': len(self.memory_manager.episodes),
                    'insight_type': insight_data.get('insight_type', 'Unknown'),
                    'ged_value': insight_data.get('ged_value', 0.0),
                    'ig_value': insight_data.get('ig_value', 0.0),
                    'timestamp': datetime.now().isoformat()
                }
                
                self.dynamic_memory_logs.append(memory_log)
                
            else:
                print(f"âš ï¸ æ´å¯Ÿè¨˜æ†¶è¿½åŠ å¤±æ•—: {insight_data['insight_id']}")
                
        except Exception as e:
            print(f"âŒ æ´å¯Ÿè¨˜æ†¶è¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
    
    def capture_memory_state_snapshot(self, episode_number: int):
        """è¨˜æ†¶çŠ¶æ…‹ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—"""
        try:
            # ãƒ¡ãƒ¢ãƒªçµ±è¨ˆã‚’å–å¾—ï¼ˆå®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
            if not hasattr(self.memory_manager, 'episodes'):
                print(f"âš ï¸ ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã«episodesã‚¢ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
                return None
            
            total_episodes = len(self.memory_manager.episodes)
            
            # Cå€¤ã®åˆ†å¸ƒã‚’åˆ†æ
            c_values = []
            insight_episodes = []
            domains = set()
            
            for ep in self.memory_manager.episodes:
                # Cå€¤ã®å–å¾—
                if hasattr(ep, 'c'):
                    c_values.append(ep.c)
                
                # æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®åˆ¤å®š
                metadata = getattr(ep, 'metadata', {}) or {}
                if metadata.get('type') == 'insight_episode':
                    insight_episodes.append(ep)
                
                # ãƒ‰ãƒ¡ã‚¤ãƒ³ã®å–å¾—
                domain = metadata.get('domain', 'unknown')
                domains.add(domain)
            
            c_mean = np.mean(c_values) if c_values else 0.0
            c_std = np.std(c_values) if c_values else 0.0
            insight_ratio = len(insight_episodes) / total_episodes if total_episodes > 0 else 0.0
            domain_diversity = len(domains)
            
            snapshot = {
                'episode_number': episode_number,
                'total_memory_size': total_episodes,
                'insight_episodes_count': len(insight_episodes),
                'insight_ratio': insight_ratio,
                'c_value_mean': c_mean,
                'c_value_std': c_std,
                'domain_diversity': domain_diversity,
                'dynamic_additions': self.insight_memory_additions,
                'timestamp': datetime.now().isoformat()
            }
            
            self.memory_state_snapshots.append(snapshot)
            
            return snapshot
            
        except Exception as e:
            print(f"âš ï¸ ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def run_dynamic_memory_experiment(self, num_episodes: int = 500):
        """å‹•çš„è¨˜æ†¶å®Ÿé¨“ã®å®Ÿè¡Œ"""
        
        print(f"ğŸš€ å‹•çš„è¨˜æ†¶æ´å¯Ÿå®Ÿé¨“é–‹å§‹ ({num_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)")
        print("=" * 70)
        print("å®Ÿé¨“å†…å®¹: æ´å¯Ÿæ¤œå‡ºâ†’è¨˜æ†¶å¼·åŒ–â†’æ¬¡å›æ¤œå‡ºã¸ã®å½±éŸ¿ã‚’è¦³å¯Ÿ")
        print()
        
        start_time = time.time()
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆ
        episodes = self.generate_episodes(num_episodes)
        
        # å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿
        insights_detected = []
        processing_times = []
        
        # åˆæœŸè¨˜æ†¶çŠ¶æ…‹ã‚’è¨˜éŒ²
        initial_snapshot = self.capture_memory_state_snapshot(0)
        print(f"ğŸ“Š åˆæœŸè¨˜æ†¶çŠ¶æ…‹: ã‚µã‚¤ã‚º={initial_snapshot['total_memory_size']}, å¤šæ§˜æ€§={initial_snapshot['domain_diversity']}")
        
        print(f"ğŸ”„ å‹•çš„è¨˜æ†¶ä»˜ããƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡ºé–‹å§‹ (TopK={self.topk})...")
        
        for i, episode in enumerate(episodes):
            episode_start = time.time()
            
            try:
                # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
                embedding = self.model.encode(episode['text'])
                
                # TopKé¡ä¼¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å–å¾—ï¼ˆå®Ÿéš›ã®ãƒ¡ãƒ¢ãƒªã‹ã‚‰ï¼‰
                topk_episodes = self.get_topk_similar_episodes(episode, embedding)
                
                # æ´å¯Ÿãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
                delta_ged, delta_ig = self.calculate_ged_ig_metrics(embedding, i + 1)
                
                # æ´å¯Ÿæ¡ä»¶ãƒã‚§ãƒƒã‚¯
                is_insight = self.check_insight_condition(delta_ged, delta_ig)
                
                # é€šå¸¸ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿å­˜ï¼ˆä½ã„Cå€¤ï¼‰
                self.memory_manager.store_episode(
                    text=episode['text'], 
                    c_value=self.normal_memory_c,
                    metadata={'id': episode['id'], 'domain': episode.get('domain', 'unknown')}
                )
                
                if is_insight:
                    insight_id = f"DYN_INS_{episode['id']:04d}_{int(time.time() * 1000) % 10000}"
                    
                    # æ´å¯Ÿã‚¿ã‚¤ãƒ—åˆ†é¡
                    if delta_ged > 0.3:
                        insight_type = "Significant_Insight"
                    elif delta_ged > 0.2:
                        insight_type = "Notable_Pattern"  
                    else:
                        insight_type = "Micro_Insight"
                    
                    # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†æ
                    cross_domain_count = sum(1 for ep in topk_episodes if ep.get('is_cross_domain', False))
                    domain_diversity = len(set(ep.get('domain', 'unknown') for ep in topk_episodes))
                    
                    print(f"ğŸ”¥ å‹•çš„è¨˜æ†¶æ´å¯Ÿæ¤œå‡º: {insight_id} (Episode {episode['id']})")
                    print(f"   Î”GED: {delta_ged:.4f}, Î”IG: {delta_ig:.4f}, Type: {insight_type}")
                    print(f"   ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º: {len(self.memory_manager.episodes)}")
                    
                    insight_data = {
                        'insight_id': insight_id,
                        'episode_id': episode['id'],
                        'episode_text': episode['text'],
                        'ged_value': delta_ged,
                        'ig_value': delta_ig,
                        'confidence': (delta_ged + delta_ig) / 2,
                        'insight_type': insight_type,
                        'cross_domain_count': cross_domain_count,
                        'domain_diversity': domain_diversity,
                        'current_domain': episode.get('domain', 'unknown'),
                        'current_research_area': episode.get('research_area', 'unknown'),
                        'memory_size_at_detection': len(self.memory_manager.episodes),
                        'dynamic_additions_so_far': self.insight_memory_additions,
                        'detection_timestamp': datetime.now().isoformat()
                    }
                    
                    insights_detected.append(insight_data)
                    
                    # ğŸ¯ æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å¼·åŒ–ã—ã¦ãƒ¡ãƒ¢ãƒªã«è¿½åŠ ï¼ˆå‹•çš„è¨˜æ†¶ã®æ ¸å¿ƒï¼‰
                    self.add_insight_to_memory(episode, insight_data)
                
                # è©³ç´°ãƒ­ã‚°ã®è¨˜éŒ²
                detailed_log = {
                    'episode_id': episode['id'],
                    'episode_text': episode['text'],
                    'domain': episode.get('domain', 'unknown'),
                    'research_area': episode.get('research_area', 'unknown'),
                    'delta_ged': delta_ged,
                    'delta_ig': delta_ig,
                    'is_insight': is_insight,
                    'memory_size': len(self.memory_manager.episodes),
                    'dynamic_additions': self.insight_memory_additions,
                    'topk_count': len(topk_episodes),
                    'timestamp': datetime.now().isoformat()
                }
                
                self.detailed_logs.append(detailed_log)
                
                # å‡¦ç†æ™‚é–“è¨˜éŒ²
                episode_time = time.time() - episode_start
                processing_times.append(episode_time)
                
                # è¨˜æ†¶çŠ¶æ…‹ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼ˆ50ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ï¼‰
                if (i + 1) % 50 == 0:
                    snapshot = self.capture_memory_state_snapshot(i + 1)
                    if snapshot:
                        print(f"ğŸ“Š è¨˜æ†¶çŠ¶æ…‹ (Episode {i+1}): ã‚µã‚¤ã‚º={snapshot['total_memory_size']}, "
                              f"æ´å¯Ÿç‡={snapshot['insight_ratio']:.1%}, å‹•çš„è¿½åŠ ={snapshot['dynamic_additions']}")
                
                # é€²æ—è¡¨ç¤º
                if (i + 1) % 100 == 0:
                    avg_time = np.mean(processing_times[-100:])
                    eps_per_sec = 1.0 / avg_time if avg_time > 0 else 0
                    memory_size = len(self.memory_manager.episodes)
                    print(f"ğŸ“ˆ é€²æ—: {i+1}/{num_episodes} ({eps_per_sec:.1f} eps/sec, "
                          f"{len(insights_detected)} insights, {self.insight_memory_additions} dynamic adds, "
                          f"memory: {memory_size})")
                
                # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼ˆæœ€åˆã®10ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ã¿ï¼‰
                if i < 10:
                    print(f"ğŸ“Š Episode {i+1}: GED={delta_ged:.3f}, IG={delta_ig:.3f}, "
                          f"Insight={is_insight}, Memory={len(self.memory_manager.episodes)}")
                
            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode['id']} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # å®Ÿé¨“å®Œäº†
        total_time = time.time() - start_time
        avg_eps_per_sec = num_episodes / total_time if total_time > 0 else 0
        avg_processing_time = np.mean(processing_times) if processing_times else 0
        final_memory_size = len(self.memory_manager.episodes)
        
        print(f"\nâœ… å‹•çš„è¨˜æ†¶æ´å¯Ÿå®Ÿé¨“å®Œäº†!")
        print(f"   ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {num_episodes}")
        print(f"   æ¤œå‡ºã•ã‚ŒãŸæ´å¯Ÿ: {len(insights_detected)}")
        print(f"   å‹•çš„è¨˜æ†¶è¿½åŠ : {self.insight_memory_additions}")
        print(f"   æœ€çµ‚ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º: {final_memory_size}")
        print(f"   å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")
        print(f"   å‡¦ç†é€Ÿåº¦: {avg_eps_per_sec:.2f} eps/sec")
        print(f"   æ´å¯Ÿæ¤œå‡ºç‡: {len(insights_detected)/num_episodes*100:.2f}%")
        print(f"   å‹•çš„è¨˜æ†¶åŠ¹æœ: {self.insight_memory_additions/len(insights_detected)*100:.1f}%ã®æ´å¯ŸãŒè¨˜æ†¶å¼·åŒ–")
        
        # è¨˜æ†¶çŠ¶æ…‹ã®å¤‰åŒ–ã‚’åˆ†æ
        if len(self.memory_state_snapshots) >= 2:
            initial = self.memory_state_snapshots[0]
            final = self.memory_state_snapshots[-1]
            
            print(f"\nğŸ“Š è¨˜æ†¶çŠ¶æ…‹ã®å¤‰åŒ–:")
            
            # å®‰å…¨ã«ã‚­ãƒ¼ã‚¢ã‚¯ã‚»ã‚¹
            initial_size = initial.get('total_memory_size', 0)
            final_size = final.get('total_memory_size', 0)
            initial_ratio = initial.get('insight_ratio', 0.0)
            final_ratio = final.get('insight_ratio', 0.0)
            initial_diversity = initial.get('domain_diversity', 0)
            final_diversity = final.get('domain_diversity', 0)
            initial_c_mean = initial.get('c_value_mean', 0.0)
            final_c_mean = final.get('c_value_mean', 0.0)
            
            print(f"   ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º: {initial_size} â†’ {final_size} "
                  f"({final_size - initial_size:+d})")
            print(f"   æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç‡: {initial_ratio:.1%} â†’ {final_ratio:.1%}")
            print(f"   ãƒ‰ãƒ¡ã‚¤ãƒ³å¤šæ§˜æ€§: {initial_diversity} â†’ {final_diversity}")
            print(f"   Cå€¤å¹³å‡: {initial_c_mean:.3f} â†’ {final_c_mean:.3f}")
        elif len(self.memory_state_snapshots) == 1:
            print(f"\nğŸ“Š è¨˜æ†¶çŠ¶æ…‹: æœ€çµ‚ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®ã¿åˆ©ç”¨å¯èƒ½")
            final = self.memory_state_snapshots[0]
            print(f"   æœ€çµ‚ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º: {final.get('total_memory_size', 0)}")
            print(f"   æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç‡: {final.get('insight_ratio', 0.0):.1%}")
        else:
            print(f"\nâš ï¸ è¨˜æ†¶çŠ¶æ…‹ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãŒä¸è¶³ï¼ˆ{len(self.memory_state_snapshots)}å€‹ï¼‰")
        
        # çµæœä¿å­˜
        self.save_dynamic_memory_results(episodes, insights_detected, total_time, avg_eps_per_sec, avg_processing_time, final_memory_size)
        
        return {
            'episodes': episodes,
            'insights': insights_detected,
            'dynamic_additions': self.insight_memory_additions,
            'final_memory_size': final_memory_size,
            'total_time': total_time,
            'avg_eps_per_sec': avg_eps_per_sec,
            'insight_rate': len(insights_detected)/num_episodes
        }
    
    def save_dynamic_memory_results(self, episodes, insights, total_time, avg_eps_per_sec, avg_processing_time, final_memory_size):
        """å‹•çš„è¨˜æ†¶å®Ÿé¨“çµæœã®ä¿å­˜"""
        
        print("ğŸ’¾ å‹•çš„è¨˜æ†¶å®Ÿé¨“çµæœã‚’ä¿å­˜ä¸­...")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir = Path("experiments/01_realtime_insight_experiments/outputs/dynamic_memory_detailed")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰CSV
        episodes_df = pd.DataFrame(episodes)
        episodes_df.to_csv(output_dir / "01_input_episodes.csv", index=False)
        
        # 2. æ´å¯Ÿæ¤œå‡ºçµæœCSV
        if insights:
            insights_df = pd.DataFrame(insights)
            insights_df.to_csv(output_dir / "02_dynamic_insights.csv", index=False)
        
        # 3. å‹•çš„è¨˜æ†¶ãƒ­ã‚°CSV
        if self.dynamic_memory_logs:
            dynamic_df = pd.DataFrame(self.dynamic_memory_logs)
            dynamic_df.to_csv(output_dir / "03_dynamic_memory_logs.csv", index=False)
        
        # 4. è¨˜æ†¶çŠ¶æ…‹ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆCSV
        if self.memory_state_snapshots:
            snapshots_df = pd.DataFrame(self.memory_state_snapshots)
            snapshots_df.to_csv(output_dir / "04_memory_state_snapshots.csv", index=False)
        
        # 5. TopKãƒ­ã‚°CSVï¼ˆç¶™æ‰¿ï¼‰
        if self.topk_logs:
            topk_data = []
            for log in self.topk_logs:
                base_data = {
                    'current_episode_id': log['current_episode_id'],
                    'current_domain': log['current_domain'],
                    'current_research_area': log['current_research_area'],
                    'cross_domain_count': log['cross_domain_count'],
                    'timestamp': log['timestamp']
                }
                
                for i, topk_ep in enumerate(log['topk_episodes']):
                    row_data = base_data.copy()
                    row_data.update({
                        f'rank_{i+1}_episode_id': topk_ep['episode_id'],
                        f'rank_{i+1}_similarity': topk_ep['similarity'],
                        f'rank_{i+1}_domain': topk_ep['domain'],
                        f'rank_{i+1}_research_area': topk_ep['research_area'],
                        f'rank_{i+1}_is_cross_domain': topk_ep['is_cross_domain']
                    })
                    topk_data.append(row_data)
            
            if topk_data:
                topk_df = pd.DataFrame(topk_data)
                topk_df.to_csv(output_dir / "05_topk_analysis.csv", index=False)
        
        # 6. è©³ç´°ãƒ­ã‚°CSVï¼ˆç¶™æ‰¿ï¼‰
        if self.detailed_logs:
            detailed_df = pd.DataFrame(self.detailed_logs)
            detailed_df.to_csv(output_dir / "06_detailed_episode_logs.csv", index=False)
        
        # 7. å‹•çš„è¨˜æ†¶å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSON
        metadata = {
            'experiment_name': 'å‹•çš„è¨˜æ†¶æ´å¯Ÿå®Ÿé¨“ (è©³ç´°ãƒ­ã‚°ç‰ˆãƒ™ãƒ¼ã‚¹)',
            'timestamp': datetime.now().isoformat(),
            'total_episodes': len(episodes),
            'total_insights': len(insights),
            'dynamic_memory_additions': self.insight_memory_additions,
            'insight_rate': len(insights)/len(episodes) if episodes else 0,
            'dynamic_addition_rate': self.insight_memory_additions/len(insights) if insights else 0,
            'final_memory_size': final_memory_size,
            'total_time_seconds': total_time,
            'avg_episodes_per_second': avg_eps_per_sec,
            'avg_processing_time': avg_processing_time,
            'parameters': {
                'memory_dim': 384,
                'topk': self.topk,
                'ged_threshold': self.ged_threshold,
                'ig_threshold': self.ig_threshold,
                'insight_memory_boost': self.insight_memory_boost,
                'normal_memory_c': self.normal_memory_c
            },
            'memory_state_changes': {
                'snapshots_captured': len(self.memory_state_snapshots),
                'initial_size': self._get_memory_size_from_snapshot(0) if self.memory_state_snapshots else 0,
                'final_size': self._get_memory_size_from_snapshot(-1) if self.memory_state_snapshots else 0
            }
        }
        
        with open(output_dir / "07_dynamic_memory_metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… å‹•çš„è¨˜æ†¶å®Ÿé¨“çµæœä¿å­˜å®Œäº†:")
        print(f"   ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
        print(f"   ğŸ“„ å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: 01_input_episodes.csv")
        print(f"   ğŸ“„ å‹•çš„æ´å¯Ÿçµæœ: 02_dynamic_insights.csv")
        print(f"   ğŸ“„ å‹•çš„è¨˜æ†¶ãƒ­ã‚°: 03_dynamic_memory_logs.csv")
        print(f"   ğŸ“„ è¨˜æ†¶çŠ¶æ…‹å¤‰åŒ–: 04_memory_state_snapshots.csv")
        print(f"   ğŸ“„ TopKåˆ†æ: 05_topk_analysis.csv")
        print(f"   ğŸ“„ è©³ç´°ãƒ­ã‚°: 06_detailed_episode_logs.csv")
        print(f"   ğŸ“„ å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: 07_dynamic_memory_metadata.json")
    
    def _get_memory_size_from_snapshot(self, index):
        """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‹ã‚‰ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚ºã‚’å®‰å…¨ã«å–å¾—"""
        if not self.memory_state_snapshots:
            return 0
        
        snapshot = self.memory_state_snapshots[index]
        
        # å¯èƒ½ãªã‚­ãƒ¼ã‚’é †ç•ªã«è©¦ã™
        for key in ['total_memory_size', 'memory_size', 'size']:
            if key in snapshot:
                return snapshot[key]
        
        # ã©ã®ã‚­ãƒ¼ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚¼ãƒ­ã‚’è¿”ã™
        print(f"âš ï¸  ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ[{index}]ã®ã‚­ãƒ¼: {list(snapshot.keys())}")
        return 0


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    experiment = None
    
    print("ğŸ§  å‹•çš„è¨˜æ†¶æ´å¯Ÿå®Ÿé¨“ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆè©³ç´°ãƒ­ã‚°ç‰ˆãƒ™ãƒ¼ã‚¹ï¼‰")
    print("=" * 70)
    print("å®Ÿé¨“å†…å®¹:")
    print("  1. é€šå¸¸ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å‡¦ç†ã¨æ´å¯Ÿæ¤œå‡º")
    print("  2. æ´å¯Ÿæ¤œå‡ºæ™‚ã«å¼·åŒ–ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å‹•çš„ã«è¨˜æ†¶è¿½åŠ ")
    print("  3. å‹•çš„è¨˜æ†¶ãŒæ¬¡ã®æ´å¯Ÿæ¤œå‡ºã«ä¸ãˆã‚‹å½±éŸ¿ã‚’è¦³å¯Ÿ")
    print("  4. å®Ÿéš›ã®dataãƒ•ã‚©ãƒ«ãƒ€ã«æ°¸ç¶šåŒ–ï¼ˆbackup/restoreä»˜ãï¼‰")
    print()
    
    try:
        # å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        experiment = DynamicMemoryInsightExperiment()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        print("ğŸ“¦ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸­...")
        experiment.backup_data_directory()
        
        # å‹•çš„è¨˜æ†¶å®Ÿé¨“å®Ÿè¡Œï¼ˆ300ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§åŠ¹æœã‚’è¦³å¯Ÿï¼‰
        print("ğŸ”„ å‹•çš„è¨˜æ†¶å®Ÿé¨“ã‚’å®Ÿè¡Œä¸­...")
        results = experiment.run_dynamic_memory_experiment(num_episodes=300)
        
        print("\nğŸ‰ å‹•çš„è¨˜æ†¶æ´å¯Ÿå®Ÿé¨“ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ!")
        print("   å‹•çš„è¨˜æ†¶è¿½åŠ : âœ…")
        print("   è¨˜æ†¶çŠ¶æ…‹è¿½è·¡: âœ…") 
        print("   å½±éŸ¿åŠ¹æœæ¸¬å®š: âœ…")
        print("   å®Ÿdataãƒ•ã‚©ãƒ«ãƒ€æ›¸ãè¾¼ã¿: âœ…")
        
        # å®Ÿé¨“åŠ¹æœã®ç°¡å˜ãªåˆ†æ
        if results['insights'] and results['dynamic_additions'] > 0:
            dynamic_rate = results['dynamic_additions'] / len(results['insights'])
            print(f"\nğŸ“Š å‹•çš„è¨˜æ†¶åŠ¹æœã‚µãƒãƒªãƒ¼:")
            print(f"   æ´å¯Ÿæ¤œå‡º: {len(results['insights'])}ä»¶")
            print(f"   å‹•çš„è¨˜æ†¶è¿½åŠ : {results['dynamic_additions']}ä»¶")
            print(f"   å‹•çš„è¨˜æ†¶ç‡: {dynamic_rate:.1%}")
            print(f"   æœ€çµ‚ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º: {results['final_memory_size']}")
        
    except Exception as e:
        print(f"âŒ å®Ÿé¨“å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå®Ÿé¨“æˆåŠŸãƒ»å¤±æ•—ã«é–¢ã‚ã‚‰ãšå®Ÿè¡Œï¼‰
        if experiment:
            print("\nğŸ§¹ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­...")
            experiment.cleanup_experiment_files()
            experiment.restore_data_directory()
            print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å…ƒã®çŠ¶æ…‹ã«å¾©å…ƒã—ã¾ã—ãŸ")


if __name__ == "__main__":
    main()
