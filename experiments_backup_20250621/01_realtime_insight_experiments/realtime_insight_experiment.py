#!/usr/bin/env python3
"""
æ¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ´å¯Ÿæ¤œå‡ºå®Ÿé¨“ + ã‚°ãƒ©ãƒ•æˆé•·ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
===========================================================

TopKæœ€é©åŒ–ã‚’ä½¿ç”¨ã—ã¦æ¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§æ´å¯Ÿã‚’æ¤œå‡ºã—ã€
ã‚°ãƒ©ãƒ•ã®æˆé•·éç¨‹ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚ºã—ã¾ã™ã€‚
"""

import sys
import json
import time
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import networkx as nx
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any
import csv
import torch
from collections import defaultdict, deque

# InsightSpike-AIã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent / "src"))

try:
    from insightspike.core.agents.main_agent import MainAgent
    from insightspike.utils.embedder import get_model
    from insightspike.core.config import get_config
    from insightspike.core.learning.knowledge_graph_memory import KnowledgeGraphMemory
    print("ğŸ“¦ InsightSpike components imported successfully")
except ImportError as e:
    print(f"âŒ Import error: {e}")
    sys.exit(1)


class TopKOptimizedGEDCalculator:
    """TopKæœ€é©åŒ–ã•ã‚ŒãŸGEDè¨ˆç®—å™¨"""
    
    def __init__(self, k: int = 10):
        self.k = k
        self.previous_embeddings = []
        self.similarity_threshold = 0.7
        
    def calculate_optimized_ged(self, new_embedding: np.ndarray, 
                              knowledge_graph: KnowledgeGraphMemory) -> float:
        """TopKè¿‘å‚ã§ã®GEDè¨ˆç®—"""
        try:
            if not self.previous_embeddings:
                return 0.0
            
            # Step 1: TopKé¡ä¼¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å–å¾—
            topk_indices, topk_similarities = self._get_topk_similar(
                new_embedding, min(self.k, len(self.previous_embeddings))
            )
            
            # Step 2: TopKè¿‘å‚ã§ã®ãƒ­ãƒ¼ã‚«ãƒ«GEDè¨ˆç®—
            local_ged = self._calculate_local_ged(topk_indices, topk_similarities)
            
            return local_ged
            
        except Exception as e:
            print(f"âŒ TopK GEDè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0
    
    def _get_topk_similar(self, new_embedding: np.ndarray, k: int) -> Tuple[List[int], List[float]]:
        """TopKé¡ä¼¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’é«˜é€Ÿå–å¾—"""
        if not self.previous_embeddings:
            return [], []
        
        # å…¨æ—¢å­˜ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã®é¡ä¼¼åº¦è¨ˆç®—
        similarities = []
        for i, prev_emb in enumerate(self.previous_embeddings):
            sim = np.dot(new_embedding, prev_emb) / (
                np.linalg.norm(new_embedding) * np.linalg.norm(prev_emb) + 1e-8
            )
            similarities.append((i, sim))
        
        # TopKå–å¾—
        similarities.sort(key=lambda x: x[1], reverse=True)
        topk = similarities[:k]
        
        indices = [idx for idx, _ in topk]
        scores = [score for _, score in topk]
        
        return indices, scores
    
    def _calculate_local_ged(self, topk_indices: List[int], 
                           topk_similarities: List[float]) -> float:
        """ãƒ­ãƒ¼ã‚«ãƒ«é ˜åŸŸã§ã®GEDè¨ˆç®—"""
        if not topk_indices:
            return 0.0
        
        # é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®GEDè¿‘ä¼¼
        avg_similarity = np.mean(topk_similarities)
        connectivity_change = len([s for s in topk_similarities if s > self.similarity_threshold])
        
        # GEDè¿‘ä¼¼å€¤ (é¡ä¼¼åº¦ãŒé«˜ã„ã»ã©æ§‹é€ å¤‰åŒ–ã¯å°ã•ã„)
        ged_value = max(0.1, 2.0 - avg_similarity * 1.5 + connectivity_change * 0.1)
        
        return ged_value
    
    def add_embedding(self, embedding: np.ndarray):
        """æ–°ã—ã„åŸ‹ã‚è¾¼ã¿ã‚’è¿½åŠ """
        self.previous_embeddings.append(embedding.copy())


class RealTimeInsightExperiment:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡ºå®Ÿé¨“"""
    
    def __init__(self):
        self.config = get_config()
        self.model = get_model()
        
        # ç°¡æ˜“ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ç›´æ¥ä½œæˆ
        self.embeddings = []
        self.episodes = []
        self.graph_snapshots = []
        
        # TopKæœ€é©åŒ–GEDè¨ˆç®—å™¨
        self.ged_calculator = TopKOptimizedGEDCalculator(k=10)
        
        # æ´å¯Ÿæ¤œå‡ºè¨­å®š
        self.ged_threshold = 0.3  # ã‚ˆã‚Šæ•æ„Ÿãªé–¾å€¤
        self.ig_threshold = 0.1   # ã‚ˆã‚Šæ•æ„Ÿãªé–¾å€¤
        
        # çµæœä¿å­˜
        self.insight_events = []
        self.performance_metrics = []
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ç”¨
        self.graph_evolution = []
        self.similarity_network = nx.Graph()
        
        # å‚ç…§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆãƒ™ã‚¯ãƒˆãƒ«â†’ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ç”¨ï¼‰
        self.reference_texts = []
        self.reference_vectors = None
        
    def setup_reference_database(self, episodes: List[str]):
        """å‚ç…§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰"""
        print("ğŸ“š å‚ç…§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ä¸­...")
        self.reference_texts = episodes[:50]  # æœ€åˆã®50ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å‚ç…§ã¨ã—ã¦ä½¿ç”¨
        self.reference_vectors = self.model.encode(self.reference_texts)
        print(f"âœ… å‚ç…§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰å®Œäº† ({len(self.reference_texts)}ä»¶)")
    
    def vector_to_text_approximation(self, vector: np.ndarray, top_k: int = 3) -> List[Tuple[str, float]]:
        """ãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰è¿‘ä¼¼ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        if self.reference_vectors is None:
            return []
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        similarities = self.reference_vectors @ vector
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            results.append((self.reference_texts[idx], similarities[idx]))
        
        return results
    
    def generate_episodes(self, count: int = 1000) -> List[str]:
        """å®Ÿé¨“ç”¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
        print(f"ğŸ“ {count}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆä¸­...")
        
        # åŸºæœ¬ãƒˆãƒ”ãƒƒã‚¯
        topics = [
            "AI healthcare", "ML training", "Deep learning", "NLP interaction",
            "Computer vision", "Predictive analytics", "Data science", 
            "Neural networks", "Automation", "Personalized medicine"
        ]
        
        # ä¿®æ­£ã‚¿ã‚¤ãƒ—
        modifications = [
            "advanced algorithms", "large datasets", "real-time processing",
            "improved accuracy", "cost reduction", "enhanced security",
            "cloud integration", "mobile optimization", "user experience",
            "scalability", "performance", "innovation", "automation",
            "intelligence", "efficiency", "reliability", "flexibility"
        ]
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns = [
            "through {mod} and continuous learning",
            "by leveraging {mod} and computational power", 
            "using {mod} and cutting-edge technology"
        ]
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        templates = {
            "AI healthcare": "AI can revolutionize healthcare diagnostics",
            "ML training": "Machine learning models require high-quality training data",
            "Deep learning": "Deep learning excels at pattern recognition tasks",
            "NLP interaction": "Natural language processing enables human-computer interaction",
            "Computer vision": "Computer vision systems can analyze medical images",
            "Predictive analytics": "Predictive analytics helps optimize resource allocation",
            "Data science": "Data science drives evidence-based decision making",
            "Neural networks": "Neural networks can model complex relationships",
            "Automation": "Automation improves efficiency in healthcare workflows",
            "Personalized medicine": "Personalized medicine relies on patient-specific data analysis"
        }
        
        episodes = []
        
        for i in range(count):
            topic = topics[i % len(topics)]
            mod = modifications[i % len(modifications)]
            pattern = patterns[i % len(patterns)]
            
            base_text = templates[topic]
            full_text = f"{base_text} {pattern.format(mod=mod)}."
            
            episodes.append(full_text)
            
        return episodes
    
    def detect_insight_every_episode(self, episode_id: int, new_embedding: np.ndarray, 
                                   episode_text: str) -> Dict[str, Any]:
        """æ¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§æ´å¯Ÿæ¤œå‡º"""
        try:
            start_time = time.time()
            
            # TopKæœ€é©åŒ–GEDè¨ˆç®—
            ged_value = self.ged_calculator.calculate_optimized_ged(
                new_embedding, None  # KnowledgeGraphã¯ä½¿ç”¨ã—ãªã„
            )
            
            # ç°¡æ˜“IGè¨ˆç®—ï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ãƒ™ãƒ¼ã‚¹ï¼‰
            ig_value = max(0.1, 10.0 / (1 + episode_id / 50)) + np.random.random() * 0.1
            
            # è¨ˆç®—æ™‚é–“è¨˜éŒ²
            calculation_time = time.time() - start_time
            
            # é–¾å€¤ãƒã‚§ãƒƒã‚¯
            ged_exceeds = ged_value > self.ged_threshold
            ig_exceeds = ig_value > self.ig_threshold
            spike_detected = ged_exceeds or ig_exceeds
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨˜éŒ²
            self.performance_metrics.append({
                'episode_id': episode_id,
                'calculation_time': calculation_time,
                'ged_value': ged_value,
                'ig_value': ig_value,
                'spike_detected': spike_detected
            })
            
            # æ´å¯Ÿæ¤œå‡ºæ™‚ã®è©³ç´°è¨˜éŒ²
            if spike_detected:
                insight_data = self._generate_insight_episode(
                    episode_id, ged_value, ig_value, new_embedding, episode_text
                )
                self.insight_events.append(insight_data)
                print(f"ğŸ’¡ æ´å¯Ÿæ¤œå‡º #{len(self.insight_events)}: Episode {episode_id} "
                      f"(Î”GED={ged_value:.3f}, Î”IG={ig_value:.3f})")
                return insight_data
            
            return None
            
        except Exception as e:
            print(f"âŒ æ´å¯Ÿæ¤œå‡ºã‚¨ãƒ©ãƒ¼ (Episode {episode_id}): {e}")
            return None
    
    def _generate_insight_episode(self, episode_id: int, ged_value: float, 
                                ig_value: float, insight_vector: np.ndarray,
                                trigger_text: str) -> Dict[str, Any]:
        """æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
        try:
            # æ´å¯Ÿã‚¿ã‚¤ãƒ—æ±ºå®š
            if ig_value > 5.0:
                insight_type = "å¤§è¦æ¨¡å­¦ç¿’"
            elif ig_value > 2.0:
                insight_type = "ä¸­è¦æ¨¡çµ±åˆ"
            elif ig_value > 1.0:
                insight_type = "å°è¦æ¨¡æ”¹å–„"
            else:
                insight_type = "å¾®èª¿æ•´"
            
            # é‡è¦åº¦è¨ˆç®—
            importance_score = (ged_value * 2 + ig_value) / 3
            
            # æ´å¯Ÿèª¬æ˜ç”Ÿæˆ
            description = f"Episode {episode_id}ã§{insight_type}ã‚’æ¤œå‡ºã€‚Î”GED={ged_value:.4f}, Î”IG={ig_value:.4f}ã®å¤‰åŒ–ã«ã‚ˆã‚Šæ–°ã—ã„ç†è§£ãŒç²å¾—ã•ã‚ŒãŸã€‚"
            
            # ãƒ™ã‚¯ãƒˆãƒ«â†’è¨€èªå¤‰æ›
            vector_to_language = self.vector_to_text_approximation(insight_vector, top_k=3)
            
            # å ±é…¬è¨ˆç®—
            base_reward = min(50.0, ig_value * 5.0)
            quality_bonus = min(10.0, ged_value * 5.0)
            total_reward = base_reward + quality_bonus
            
            # é–¢é€£ãƒãƒ¼ãƒ‰ï¼ˆè¿‘å‚ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰
            related_nodes = list(range(max(1, episode_id - 5), min(episode_id + 5, len(self.episodes) + 1)))
            
            insight_episode = {
                'insight_id': f"RT_INS_{episode_id:04d}_{len(self.insight_events)+1:03d}",
                'episode_id': episode_id,
                'trigger_text': trigger_text,
                'insight_type': insight_type,
                'description': description,
                'importance_score': importance_score,
                'generated_timestamp': datetime.now().isoformat(),
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                'metrics': {
                    'delta_ged': ged_value,
                    'delta_ig': ig_value,
                    'ged_exceeds_threshold': ged_value > self.ged_threshold,
                    'ig_exceeds_threshold': ig_value > self.ig_threshold
                },
                
                # å ±é…¬
                'reward': {
                    'base_reward': base_reward,
                    'quality_bonus': quality_bonus,
                    'total_reward': total_reward
                },
                
                # ãƒ™ã‚¯ãƒˆãƒ«æƒ…å ±
                'vector_info': {
                    'vector_norm': float(np.linalg.norm(insight_vector)),
                    'vector_sample': insight_vector[:5].tolist()
                },
                
                # è¨€èªå¤‰æ›
                'vector_to_language': [
                    {
                        'rank': i+1,
                        'text': text,
                        'similarity': float(sim)
                    }
                    for i, (text, sim) in enumerate(vector_to_language)
                ],
                
                # é–¢é€£ãƒãƒ¼ãƒ‰
                'related_nodes': related_nodes
            }
            
            return insight_episode
            
        except Exception as e:
            print(f"âŒ æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def update_graph_visualization(self, episode_id: int, new_embedding: np.ndarray):
        """ã‚°ãƒ©ãƒ•ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³æ›´æ–°"""
        try:
            # ãƒãƒ¼ãƒ‰è¿½åŠ 
            self.similarity_network.add_node(episode_id)
            
            # é¡ä¼¼åº¦ã®é«˜ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã‚¨ãƒƒã‚¸è¿½åŠ 
            similarity_threshold = 0.8
            for i, prev_embedding in enumerate(self.embeddings[:-1]):  # æœ€å¾Œã®è¦ç´ ï¼ˆä»Šå›è¿½åŠ åˆ†ï¼‰ã‚’é™¤ã
                similarity = np.dot(new_embedding, prev_embedding) / (
                    np.linalg.norm(new_embedding) * np.linalg.norm(prev_embedding) + 1e-8
                )
                
                if similarity > similarity_threshold:
                    self.similarity_network.add_edge(i + 1, episode_id, weight=similarity)
            
            # ã‚°ãƒ©ãƒ•ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä¿å­˜ï¼ˆ10ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ¯ï¼‰
            if episode_id % 10 == 0:
                snapshot = {
                    'episode_id': episode_id,
                    'num_nodes': self.similarity_network.number_of_nodes(),
                    'num_edges': self.similarity_network.number_of_edges(),
                    'avg_degree': np.mean([d for n, d in self.similarity_network.degree()]) if self.similarity_network.nodes() else 0,
                    'clustering_coefficient': nx.average_clustering(self.similarity_network) if self.similarity_network.nodes() else 0,
                    'timestamp': datetime.now().isoformat()
                }
                self.graph_evolution.append(snapshot)
                
        except Exception as e:
            print(f"âŒ ã‚°ãƒ©ãƒ•æ›´æ–°ã‚¨ãƒ©ãƒ¼ (Episode {episode_id}): {e}")
    
    def run_realtime_experiment(self, num_episodes: int = 1000):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡ºå®Ÿé¨“ã‚’å®Ÿè¡Œ"""
        print(f"ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡ºå®Ÿé¨“é–‹å§‹ ({num_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)")
        print("=" * 70)
        
        start_time = time.time()
        
        # 1. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆ
        episodes = self.generate_episodes(num_episodes)
        
        # 2. å‚ç…§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰
        self.setup_reference_database(episodes)
        
        # 3. æ¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å‡¦ç†
        print(f"\nğŸ“Š æ¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ´å¯Ÿæ¤œå‡ºé–‹å§‹...")
        
        for i, episode_text in enumerate(episodes, 1):
            try:
                # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
                episode_embedding = self.model.encode([episode_text])[0]
                
                # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¿å­˜
                self.episodes.append(episode_text)
                self.embeddings.append(episode_embedding)
                
                # æ´å¯Ÿæ¤œå‡º
                insight = self.detect_insight_every_episode(i, episode_embedding, episode_text)
                
                # ã‚°ãƒ©ãƒ•æ›´æ–°
                self.update_graph_visualization(i, episode_embedding)
                
                # GEDè¨ˆç®—å™¨ã«åŸ‹ã‚è¾¼ã¿è¿½åŠ 
                self.ged_calculator.add_embedding(episode_embedding)
                
                # é€²æ—è¡¨ç¤º
                if i % 50 == 0:
                    elapsed = time.time() - start_time
                    eps_per_sec = i / elapsed
                    insights_count = len(self.insight_events)
                    print(f"ğŸ“ˆ é€²æ—: {i}/{num_episodes} ({eps_per_sec:.1f} eps/sec, æ´å¯Ÿ: {insights_count})")
                
            except Exception as e:
                print(f"âŒ Episode {i} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # 4. å®Ÿé¨“å®Œäº†
        total_time = time.time() - start_time
        final_eps_per_sec = num_episodes / total_time
        
        print(f"\nâœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®Ÿé¨“å®Œäº†!")
        print(f"   ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {num_episodes}")
        print(f"   å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")
        print(f"   å‡¦ç†é€Ÿåº¦: {final_eps_per_sec:.2f} eps/sec")
        print(f"   æ¤œå‡ºã•ã‚ŒãŸæ´å¯Ÿ: {len(self.insight_events)}")
        print(f"   å¹³å‡è¨ˆç®—æ™‚é–“: {np.mean([m['calculation_time'] for m in self.performance_metrics]):.4f}ç§’/ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
        
        return {
            'num_episodes': num_episodes,
            'execution_time': total_time,
            'episodes_per_second': final_eps_per_sec,
            'insights_detected': len(self.insight_events),
            'avg_calculation_time': np.mean([m['calculation_time'] for m in self.performance_metrics]),
            'timestamp': datetime.now().isoformat()
        }
    
    def create_graph_visualization(self):
        """ã‚°ãƒ©ãƒ•æˆé•·ã®ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ"""
        print("\nğŸ¨ ã‚°ãƒ©ãƒ•æˆé•·ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
        
        try:
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            output_dir = Path("experiments/outputs/realtime_experiment")
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # 1. ã‚°ãƒ©ãƒ•æˆé•·çµ±è¨ˆã®ãƒ—ãƒ­ãƒƒãƒˆ
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('InsightSpike-AI: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚°ãƒ©ãƒ•æˆé•·åˆ†æ', fontsize=16)
            
            if self.graph_evolution:
                episodes = [snap['episode_id'] for snap in self.graph_evolution]
                nodes = [snap['num_nodes'] for snap in self.graph_evolution]
                edges = [snap['num_edges'] for snap in self.graph_evolution]
                avg_degrees = [snap['avg_degree'] for snap in self.graph_evolution]
                clustering = [snap['clustering_coefficient'] for snap in self.graph_evolution]
                
                # ãƒãƒ¼ãƒ‰æ•°æˆé•·
                axes[0, 0].plot(episodes, nodes, 'b-o', markersize=4)
                axes[0, 0].set_title('ãƒãƒ¼ãƒ‰æ•°ã®æˆé•·')
                axes[0, 0].set_xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°')
                axes[0, 0].set_ylabel('ãƒãƒ¼ãƒ‰æ•°')
                axes[0, 0].grid(True, alpha=0.3)
                
                # ã‚¨ãƒƒã‚¸æ•°æˆé•·
                axes[0, 1].plot(episodes, edges, 'r-s', markersize=4)
                axes[0, 1].set_title('ã‚¨ãƒƒã‚¸æ•°ã®æˆé•·')
                axes[0, 1].set_xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°')
                axes[0, 1].set_ylabel('ã‚¨ãƒƒã‚¸æ•°')
                axes[0, 1].grid(True, alpha=0.3)
                
                # å¹³å‡æ¬¡æ•°
                axes[1, 0].plot(episodes, avg_degrees, 'g-^', markersize=4)
                axes[1, 0].set_title('å¹³å‡ãƒãƒ¼ãƒ‰æ¬¡æ•°')
                axes[1, 0].set_xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°')
                axes[1, 0].set_ylabel('å¹³å‡æ¬¡æ•°')
                axes[1, 0].grid(True, alpha=0.3)
                
                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä¿‚æ•°
                axes[1, 1].plot(episodes, clustering, 'm-d', markersize=4)
                axes[1, 1].set_title('ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä¿‚æ•°')
                axes[1, 1].set_xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°')
                axes[1, 1].set_ylabel('ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä¿‚æ•°')
                axes[1, 1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            graph_stats_file = output_dir / "graph_growth_statistics.png"
            plt.savefig(graph_stats_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            # 2. æ´å¯Ÿæ¤œå‡ºé »åº¦ã®ãƒ—ãƒ­ãƒƒãƒˆ
            fig, axes = plt.subplots(2, 1, figsize=(15, 10))
            fig.suptitle('æ´å¯Ÿæ¤œå‡ºåˆ†æ', fontsize=16)
            
            if self.insight_events:
                insight_episodes = [event['episode_id'] for event in self.insight_events]
                insight_rewards = [event['reward']['total_reward'] for event in self.insight_events]
                
                # æ´å¯Ÿæ¤œå‡ºã‚¿ã‚¤ãƒŸãƒ³ã‚°
                axes[0].scatter(insight_episodes, range(len(insight_episodes)), 
                              c='red', s=50, alpha=0.7)
                axes[0].set_title('æ´å¯Ÿæ¤œå‡ºã‚¿ã‚¤ãƒŸãƒ³ã‚°')
                axes[0].set_xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°')
                axes[0].set_ylabel('æ´å¯ŸID')
                axes[0].grid(True, alpha=0.3)
                
                # å ±é…¬åˆ†å¸ƒ
                axes[1].bar(range(len(insight_rewards)), insight_rewards, 
                           color='orange', alpha=0.7)
                axes[1].set_title('æ´å¯Ÿå ±é…¬åˆ†å¸ƒ')
                axes[1].set_xlabel('æ´å¯ŸID')
                axes[1].set_ylabel('ç·å ±é…¬')
                axes[1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            insight_analysis_file = output_dir / "insight_detection_analysis.png"
            plt.savefig(insight_analysis_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            # 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
            fig, axes = plt.subplots(2, 1, figsize=(15, 10))
            fig.suptitle('è¨ˆç®—ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ', fontsize=16)
            
            if self.performance_metrics:
                episodes = [m['episode_id'] for m in self.performance_metrics]
                calc_times = [m['calculation_time'] for m in self.performance_metrics]
                ged_values = [m['ged_value'] for m in self.performance_metrics]
                
                # è¨ˆç®—æ™‚é–“æ¨ç§»
                axes[0].plot(episodes, calc_times, 'b-', alpha=0.7, linewidth=1)
                axes[0].set_title('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ¯è¨ˆç®—æ™‚é–“')
                axes[0].set_xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°')
                axes[0].set_ylabel('è¨ˆç®—æ™‚é–“ (ç§’)')
                axes[0].grid(True, alpha=0.3)
                
                # GEDå€¤æ¨ç§»
                axes[1].plot(episodes, ged_values, 'g-', alpha=0.7, linewidth=1)
                axes[1].axhline(y=self.ged_threshold, color='red', linestyle='--', 
                              label=f'é–¾å€¤ ({self.ged_threshold})')
                axes[1].set_title('GEDå€¤æ¨ç§»')
                axes[1].set_xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°')
                axes[1].set_ylabel('GEDå€¤')
                axes[1].legend()
                axes[1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            performance_file = output_dir / "performance_analysis.png"
            plt.savefig(performance_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"âœ… ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆå®Œäº†:")
            print(f"   ğŸ“Š ã‚°ãƒ©ãƒ•æˆé•·çµ±è¨ˆ: {graph_stats_file}")
            print(f"   ğŸ’¡ æ´å¯Ÿæ¤œå‡ºåˆ†æ: {insight_analysis_file}")
            print(f"   âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ: {performance_file}")
            
            return {
                'graph_stats': str(graph_stats_file),
                'insight_analysis': str(insight_analysis_file),
                'performance_analysis': str(performance_file)
            }
            
        except Exception as e:
            print(f"âŒ ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def save_comprehensive_summary(self, experiment_results: Dict[str, Any]):
        """åŒ…æ‹¬çš„ã‚µãƒãƒªã‚’ä¿å­˜"""
        print(f"\nğŸ’¾ åŒ…æ‹¬çš„ã‚µãƒãƒªã‚’ä¿å­˜ä¸­...")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir = Path("experiments/outputs/realtime_experiment")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è©³ç´°CSV
        input_csv_file = output_dir / "01_input_episodes_realtime.csv"
        with open(input_csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['episode_id', 'episode_text', 'processed_timestamp'])
            
            for i, episode_text in enumerate(self.episodes, 1):
                writer.writerow([i, episode_text, datetime.now().isoformat()])
        
        # 2. æ´å¯Ÿã‚¤ãƒ™ãƒ³ãƒˆåŒ…æ‹¬CSV
        insight_csv_file = output_dir / "02_realtime_insights_comprehensive.csv"
        with open(insight_csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'insight_id', 'episode_id', 'insight_type', 'delta_ged', 'delta_ig',
                'generated_timestamp', 'total_reward', 'importance_score',
                'top_vector_conversion', 'vector_similarity', 'related_nodes_count'
            ])
            
            for event in self.insight_events:
                top_conversion = event['vector_to_language'][0] if event['vector_to_language'] else {}
                
                writer.writerow([
                    event['insight_id'],
                    event['episode_id'],
                    event['insight_type'],
                    event['metrics']['delta_ged'],
                    event['metrics']['delta_ig'],
                    event['generated_timestamp'],
                    event['reward']['total_reward'],
                    event['importance_score'],
                    top_conversion.get('text', 'N/A'),
                    top_conversion.get('similarity', 0.0),
                    len(event['related_nodes'])
                ])
        
        # 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©³ç´°JSON
        performance_file = output_dir / "03_performance_metrics.json"
        with open(performance_file, 'w', encoding='utf-8') as f:
            json.dump({
                'experiment_results': experiment_results,
                'performance_metrics': self.performance_metrics,
                'graph_evolution': self.graph_evolution
            }, f, indent=2, ensure_ascii=False)
        
        # 4. ã‚°ãƒ©ãƒ•æˆé•·CSV
        graph_csv_file = output_dir / "04_graph_evolution.csv"
        with open(graph_csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'episode_id', 'num_nodes', 'num_edges', 'avg_degree', 
                'clustering_coefficient', 'timestamp'
            ])
            
            for snapshot in self.graph_evolution:
                writer.writerow([
                    snapshot['episode_id'],
                    snapshot['num_nodes'],
                    snapshot['num_edges'],
                    snapshot['avg_degree'],
                    snapshot['clustering_coefficient'],
                    snapshot['timestamp']
                ])
        
        # 5. å®Ÿé¨“ãƒ¡ã‚¿ã‚µãƒãƒª
        meta_file = output_dir / "05_experiment_meta_summary.json"
        meta_summary = {
            'experiment_type': 'ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ´å¯Ÿæ¤œå‡º',
            'optimization': 'TopKæœ€é©åŒ–GEDè¨ˆç®—',
            'total_episodes': len(self.episodes),
            'total_insights': len(self.insight_events),
            'insight_frequency': len(self.insight_events) / len(self.episodes) if self.episodes else 0,
            'avg_calculation_time': np.mean([m['calculation_time'] for m in self.performance_metrics]) if self.performance_metrics else 0,
            'total_execution_time': experiment_results.get('execution_time', 0),
            'processing_speed': experiment_results.get('episodes_per_second', 0),
            'final_graph_stats': self.graph_evolution[-1] if self.graph_evolution else {},
            'analysis_timestamp': datetime.now().isoformat()
        }
        
        with open(meta_file, 'w', encoding='utf-8') as f:
            json.dump(meta_summary, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… åŒ…æ‹¬çš„ã‚µãƒãƒªä¿å­˜å®Œäº†:")
        print(f"   ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
        print(f"   ğŸ“„ å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {input_csv_file}")
        print(f"   ğŸ“„ æ´å¯Ÿã‚¤ãƒ™ãƒ³ãƒˆ: {insight_csv_file}")
        print(f"   ğŸ“„ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: {performance_file}")
        print(f"   ğŸ“„ ã‚°ãƒ©ãƒ•æˆé•·: {graph_csv_file}")
        print(f"   ğŸ“„ ãƒ¡ã‚¿ã‚µãƒãƒª: {meta_file}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    experiment = RealTimeInsightExperiment()
    
    try:
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®Ÿé¨“å®Ÿè¡Œ
        results = experiment.run_realtime_experiment(num_episodes=1000)
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ
        viz_results = experiment.create_graph_visualization()
        
        # ã‚µãƒãƒªä¿å­˜
        experiment.save_comprehensive_summary(results)
        
        print(f"\nğŸ‰ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡ºå®Ÿé¨“ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ!")
        print(f"\nğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
        print(f"   æ¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨ˆç®—ã«ã‚ˆã‚Š {results['insights_detected']} å€‹ã®æ´å¯Ÿã‚’æ¤œå‡º")
        print(f"   å¹³å‡è¨ˆç®—æ™‚é–“: {results['avg_calculation_time']:.4f}ç§’/ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
        print(f"   å‡¦ç†é€Ÿåº¦: {results['episodes_per_second']:.2f} ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰/ç§’")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ å®Ÿé¨“ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ å®Ÿé¨“ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
