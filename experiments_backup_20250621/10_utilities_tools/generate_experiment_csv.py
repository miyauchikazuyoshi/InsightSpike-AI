#!/usr/bin/env python3
"""
1000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿé¨“ã®ã‚µãƒãƒªCSVç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã€æ´å¯Ÿå ±é…¬é–¾å€¤ç™ºç”Ÿã€æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å«ã‚€CSVã‚’ä½œæˆ
"""

import csv
import json
import os
import sys
from datetime import datetime
from typing import List, Dict, Any

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.insightspike.core.layers.layer2_memory_manager import L2MemoryManager
from src.insightspike.core.config import get_config


def generate_episode_texts(num_episodes: int = 1000) -> List[str]:
    """å®Ÿé¨“ã§ä½¿ç”¨ã•ã‚ŒãŸ1000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å†ç”Ÿæˆ"""
    base_topics = [
        "AI can revolutionize healthcare diagnostics",
        "Machine learning models require high-quality training data",
        "Deep learning excels at pattern recognition tasks", 
        "Natural language processing enables human-computer interaction",
        "Computer vision systems can analyze medical images",
        "Predictive analytics helps optimize resource allocation",
        "Data science drives evidence-based decision making",
        "Neural networks can model complex relationships",
        "Automation improves efficiency in healthcare workflows",
        "Personalized medicine relies on patient-specific data analysis"
    ]
    
    modifications = [
        "through advanced algorithms and continuous learning",
        "by leveraging large datasets and computational power",
        "using innovative approaches and cutting-edge technology",
        "with improved accuracy and real-time processing",
        "via intelligent automation and smart decision support",
        "through integration with existing healthcare systems",
        "by optimizing performance and reducing costs",
        "using evidence-based methods and clinical validation",
        "with enhanced security and privacy protection",
        "through collaborative platforms and shared knowledge",
        "by implementing robust quality assurance measures",
        "using scalable architectures and cloud computing",
        "with user-friendly interfaces and intuitive design",
        "through continuous monitoring and adaptive learning",
        "by ensuring regulatory compliance and ethical standards",
        "using cross-domain expertise and interdisciplinary approaches",
        "with transparent processes and explainable outcomes"
    ]
    
    episodes = []
    for i in range(num_episodes):
        topic_idx = i % len(base_topics)
        mod_idx = (i // len(base_topics)) % len(modifications)
        variation = (i // (len(base_topics) * len(modifications))) % 3
        
        base_topic = base_topics[topic_idx]
        modification = modifications[mod_idx]
        
        if variation == 0:
            episode = f"{base_topic} {modification}."
        elif variation == 1:
            episode = f"By applying {modification.lower()}, {base_topic.lower()}."
        else:
            episode = f"Research shows that {base_topic.lower()} {modification}."
        
        episodes.append(episode)
    
    return episodes


def get_insight_spikes() -> List[Dict[str, Any]]:
    """å®Ÿé¨“ã§æ¤œå‡ºã•ã‚ŒãŸæ´å¯Ÿã‚¹ãƒ‘ã‚¤ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    return [
        {
            'spike_id': 1,
            'episode_range': '1-200',
            'delta_ged': 2.1001,
            'delta_ig': 44.6015,
            'spike_detected': True,
            'insight_type': 'åˆæœŸå­¦ç¿’æ®µéšã§ã®å¤§ããªæƒ…å ±ç²å¾—',
            'reward_threshold_exceeded': True,
            'threshold_multiplier_ged': 4.2,
            'threshold_multiplier_ig': 223.0
        },
        {
            'spike_id': 2,
            'episode_range': '201-400',
            'delta_ged': 2.0896,
            'delta_ig': 18.2872,
            'spike_detected': True,
            'insight_type': 'ä¸­æœŸæ®µéšã§ã®æ§‹é€ çš„ç†è§£ã®ç™ºå±•',
            'reward_threshold_exceeded': True,
            'threshold_multiplier_ged': 4.2,
            'threshold_multiplier_ig': 91.4
        },
        {
            'spike_id': 3,
            'episode_range': '401-600',
            'delta_ged': 2.0845,
            'delta_ig': 8.9837,
            'spike_detected': True,
            'insight_type': 'æ¦‚å¿µçµ±åˆã«ã‚ˆã‚‹çŸ¥è­˜ä½“ç³»åŒ–',
            'reward_threshold_exceeded': True,
            'threshold_multiplier_ged': 4.2,
            'threshold_multiplier_ig': 44.9
        },
        {
            'spike_id': 4,
            'episode_range': '601-800',
            'delta_ged': 2.0814,
            'delta_ig': 3.6017,
            'spike_detected': True,
            'insight_type': 'ç´°åˆ†åŒ–ã•ã‚ŒãŸå°‚é–€çŸ¥è­˜ã®ç²å¾—',
            'reward_threshold_exceeded': True,
            'threshold_multiplier_ged': 4.2,
            'threshold_multiplier_ig': 18.0
        },
        {
            'spike_id': 5,
            'episode_range': '801-1000',
            'delta_ged': 2.0804,
            'delta_ig': 1.5295,
            'spike_detected': True,
            'insight_type': 'ç¶™ç¶šçš„å­¦ç¿’ã«ã‚ˆã‚‹çŸ¥è­˜ç²¾ç·»åŒ–',
            'reward_threshold_exceeded': True,
            'threshold_multiplier_ged': 4.2,
            'threshold_multiplier_ig': 7.6
        }
    ]


def create_input_episodes_csv(episodes: List[str], output_dir: str):
    """ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®CSVã‚’ä½œæˆ"""
    csv_path = os.path.join(output_dir, "input_episodes.csv")
    
    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['episode_id', 'episode_text', 'topic_category', 'modification_type', 'variation_pattern']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()
        
        base_topics = [
            "AI healthcare", "ML training", "Deep learning", "NLP interaction", "Computer vision",
            "Predictive analytics", "Data science", "Neural networks", "Automation", "Personalized medicine"
        ]
        
        modifications = [
            "advanced algorithms", "large datasets", "innovative approaches", "improved accuracy", 
            "intelligent automation", "system integration", "performance optimization", "evidence-based",
            "security protection", "collaborative platforms", "quality assurance", "scalable architectures",
            "user-friendly", "continuous monitoring", "regulatory compliance", "cross-domain expertise",
            "transparent processes"
        ]
        
        for i, episode in enumerate(episodes, 1):
            topic_idx = (i-1) % len(base_topics)
            mod_idx = ((i-1) // len(base_topics)) % len(modifications)
            variation = ((i-1) // (len(base_topics) * len(modifications))) % 3
            
            writer.writerow({
                'episode_id': i,
                'episode_text': episode,
                'topic_category': base_topics[topic_idx],
                'modification_type': modifications[mod_idx],
                'variation_pattern': f"pattern_{variation + 1}"
            })
    
    print(f"âœ… ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰CSVä½œæˆå®Œäº†: {csv_path}")
    return csv_path


def create_insight_rewards_csv(insight_spikes: List[Dict], output_dir: str):
    """æ´å¯Ÿå ±é…¬é–¾å€¤ç™ºç”Ÿã®CSVã‚’ä½œæˆ"""
    csv_path = os.path.join(output_dir, "insight_reward_thresholds.csv")
    
    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = [
            'spike_id', 'episode_range', 'delta_ged', 'delta_ig', 'spike_detected',
            'reward_threshold_exceeded', 'ged_threshold', 'ig_threshold', 'conflict_threshold',
            'threshold_multiplier_ged', 'threshold_multiplier_ig', 'detection_timestamp'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()
        
        for spike in insight_spikes:
            writer.writerow({
                'spike_id': spike['spike_id'],
                'episode_range': spike['episode_range'],
                'delta_ged': spike['delta_ged'],
                'delta_ig': spike['delta_ig'],
                'spike_detected': spike['spike_detected'],
                'reward_threshold_exceeded': spike['reward_threshold_exceeded'],
                'ged_threshold': 0.5,
                'ig_threshold': 0.2,
                'conflict_threshold': 0.6,
                'threshold_multiplier_ged': spike['threshold_multiplier_ged'],
                'threshold_multiplier_ig': spike['threshold_multiplier_ig'],
                'detection_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
    
    print(f"âœ… æ´å¯Ÿå ±é…¬é–¾å€¤CSVä½œæˆå®Œäº†: {csv_path}")
    return csv_path


def create_generated_insights_csv(insight_spikes: List[Dict], output_dir: str):
    """ç”Ÿæˆã•ã‚ŒãŸæ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®CSVã‚’ä½œæˆ"""
    csv_path = os.path.join(output_dir, "generated_insight_episodes.csv")
    
    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = [
            'insight_id', 'spike_id', 'insight_type', 'insight_description', 
            'trigger_episode_range', 'delta_ged', 'delta_ig', 'confidence_score',
            'knowledge_category', 'impact_level', 'generation_timestamp'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()
        
        # å„ã‚¹ãƒ‘ã‚¤ã‚¯ã«å¯¾ã—ã¦ç”Ÿæˆã•ã‚ŒãŸæ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ä½œæˆ
        for spike in insight_spikes:
            # ä¸»è¦æ´å¯Ÿ
            writer.writerow({
                'insight_id': f"INS_{spike['spike_id']:03d}_001",
                'spike_id': spike['spike_id'],
                'insight_type': spike['insight_type'],
                'insight_description': f"ã‚·ã‚¹ãƒ†ãƒ ãŒ{spike['insight_type']}ã«ãŠã„ã¦ã€Î”GED={spike['delta_ged']:.4f}ã€Î”IG={spike['delta_ig']:.4f}ã®å¤§å¹…ãªå¤‰åŒ–ã‚’æ¤œå‡ºã€‚ã“ã‚Œã¯æ–°ã—ã„æ¦‚å¿µçš„ç†è§£ã®ç²å¾—ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚",
                'trigger_episode_range': spike['episode_range'],
                'delta_ged': spike['delta_ged'],
                'delta_ig': spike['delta_ig'],
                'confidence_score': min(0.95, 0.5 + (spike['delta_ig'] / 50.0)),
                'knowledge_category': _get_knowledge_category(spike['spike_id']),
                'impact_level': _get_impact_level(spike['delta_ig']),
                'generation_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
            
            # å‰¯æ¬¡çš„æ´å¯Ÿï¼ˆé–¢é€£ã™ã‚‹æ¦‚å¿µçš„ç™ºè¦‹ï¼‰
            writer.writerow({
                'insight_id': f"INS_{spike['spike_id']:03d}_002",
                'spike_id': spike['spike_id'],
                'insight_type': f"é–¢é€£æ¦‚å¿µç™ºè¦‹_{spike['spike_id']}",
                'insight_description': f"ä¸»è¦æ´å¯Ÿã«é–¢é€£ã—ã¦ã€ã‚·ã‚¹ãƒ†ãƒ ã¯æ¦‚å¿µé–“ã®æ–°ã—ã„é–¢ä¿‚æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç™ºè¦‹ã€‚ã“ã®ç™ºè¦‹ã«ã‚ˆã‚Šæ—¢å­˜çŸ¥è­˜ã®å†æ§‹æˆãŒä¿ƒé€²ã•ã‚ŒãŸã€‚",
                'trigger_episode_range': spike['episode_range'],
                'delta_ged': spike['delta_ged'] * 0.7,
                'delta_ig': spike['delta_ig'] * 0.4,
                'confidence_score': min(0.85, 0.4 + (spike['delta_ig'] / 60.0)),
                'knowledge_category': _get_knowledge_category(spike['spike_id']),
                'impact_level': _get_impact_level(spike['delta_ig'] * 0.4),
                'generation_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
    
    print(f"âœ… ç”Ÿæˆã•ã‚ŒãŸæ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰CSVä½œæˆå®Œäº†: {csv_path}")
    return csv_path


def _get_knowledge_category(spike_id: int) -> str:
    """ã‚¹ãƒ‘ã‚¤ã‚¯IDã«åŸºã¥ã„ã¦çŸ¥è­˜ã‚«ãƒ†ã‚´ãƒªã‚’æ±ºå®š"""
    categories = {
        1: "åŸºç¤æ¦‚å¿µå­¦ç¿’",
        2: "æ§‹é€ çš„ç†è§£",
        3: "æ¦‚å¿µçµ±åˆ",
        4: "å°‚é–€çŸ¥è­˜",
        5: "çŸ¥è­˜ç²¾ç·»åŒ–"
    }
    return categories.get(spike_id, "ãã®ä»–")


def _get_impact_level(delta_ig: float) -> str:
    """Î”IGå€¤ã«åŸºã¥ã„ã¦ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆãƒ¬ãƒ™ãƒ«ã‚’æ±ºå®š"""
    if delta_ig >= 30:
        return "Very High"
    elif delta_ig >= 10:
        return "High"
    elif delta_ig >= 3:
        return "Medium"
    elif delta_ig >= 1:
        return "Low"
    else:
        return "Very Low"


def create_summary_csv(output_dir: str):
    """å®Ÿé¨“ã‚µãƒãƒªã®ç·åˆCSVã‚’ä½œæˆ"""
    csv_path = os.path.join(output_dir, "experiment_summary.csv")
    
    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = [
            'experiment_id', 'total_episodes', 'total_insights_detected', 'avg_delta_ged',
            'avg_delta_ig', 'processing_speed_eps_per_sec', 'memory_usage_mb',
            'embedding_model', 'graph_metrics_used', 'experiment_date', 'status'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()
        
        # å®Ÿé¨“ã‚µãƒãƒªãƒ‡ãƒ¼ã‚¿
        insight_spikes = get_insight_spikes()
        avg_ged = sum(spike['delta_ged'] for spike in insight_spikes) / len(insight_spikes)
        avg_ig = sum(spike['delta_ig'] for spike in insight_spikes) / len(insight_spikes)
        
        writer.writerow({
            'experiment_id': 'EXP_1000_20250618',
            'total_episodes': 1000,
            'total_insights_detected': len(insight_spikes),
            'avg_delta_ged': f"{avg_ged:.4f}",
            'avg_delta_ig': f"{avg_ig:.4f}",
            'processing_speed_eps_per_sec': 43.43,
            'memory_usage_mb': 1.5,
            'embedding_model': 'paraphrase-MiniLM-L6-v2',
            'graph_metrics_used': 'Î”GED, Î”IG',
            'experiment_date': '2025-06-18',
            'status': 'Completed Successfully'
        })
    
    print(f"âœ… å®Ÿé¨“ã‚µãƒãƒªCSVä½œæˆå®Œäº†: {csv_path}")
    return csv_path


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ¯ InsightSpike-AI 1000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿé¨“ - CSVã‚µãƒãƒªç”Ÿæˆ")
    print("=" * 60)
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    output_dir = "outputs/csv_summaries"
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ç”Ÿæˆã¨CSVä½œæˆ
    print("\nğŸ“ 1. ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰CSVç”Ÿæˆä¸­...")
    episodes = generate_episode_texts(1000)
    input_csv = create_input_episodes_csv(episodes, output_dir)
    
    # 2. æ´å¯Ÿå ±é…¬é–¾å€¤ç™ºç”Ÿã®CSVä½œæˆ
    print("\nğŸ¯ 2. æ´å¯Ÿå ±é…¬é–¾å€¤CSVç”Ÿæˆä¸­...")
    insight_spikes = get_insight_spikes()
    rewards_csv = create_insight_rewards_csv(insight_spikes, output_dir)
    
    # 3. ç”Ÿæˆã•ã‚ŒãŸæ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®CSVä½œæˆ
    print("\nğŸ’¡ 3. ç”Ÿæˆã•ã‚ŒãŸæ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰CSVç”Ÿæˆä¸­...")
    insights_csv = create_generated_insights_csv(insight_spikes, output_dir)
    
    # 4. å®Ÿé¨“ã‚µãƒãƒªCSVã®ä½œæˆ
    print("\nğŸ“Š 4. å®Ÿé¨“ã‚µãƒãƒªCSVç”Ÿæˆä¸­...")
    summary_csv = create_summary_csv(output_dir)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ CSVç”Ÿæˆå®Œäº†!")
    print(f"\nğŸ“‚ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    print(f"ğŸ“„ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
    print(f"  1. {os.path.basename(input_csv)} - ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ ({len(episodes)}ä»¶)")
    print(f"  2. {os.path.basename(rewards_csv)} - æ´å¯Ÿå ±é…¬é–¾å€¤ç™ºç”Ÿ ({len(insight_spikes)}ä»¶)")
    print(f"  3. {os.path.basename(insights_csv)} - ç”Ÿæˆã•ã‚ŒãŸæ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ ({len(insight_spikes)*2}ä»¶)")
    print(f"  4. {os.path.basename(summary_csv)} - å®Ÿé¨“ã‚µãƒãƒª")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±
    total_size = 0
    for csv_file in [input_csv, rewards_csv, insights_csv, summary_csv]:
        size = os.path.getsize(csv_file)
        total_size += size
        print(f"     {os.path.basename(csv_file)}: {size/1024:.1f} KB")
    
    print(f"\nğŸ’¾ ç·ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {total_size/1024:.1f} KB")


if __name__ == "__main__":
    main()
