"""
Google Colabç”¨ã®è©•ä¾¡å®Ÿé¨“ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
å­¦è¡“çš„ç ”ç©¶åŸºæº–ã«åŸºã¥ãåŒ…æ‹¬çš„è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®Colabçµ±åˆç‰ˆ

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‹ã‚‰ç°¡å˜ã«å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚
"""

import asyncio
import nest_asyncio
import warnings
from typing import Optional, List, Dict, Any
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from IPython.display import display, HTML, Markdown
import json
from pathlib import Path

# Enable nested asyncio for Colab
nest_asyncio.apply()

# Import the main evaluation framework
from objective_evaluation_framework import (
    ObjectiveEvaluationFramework, 
    ExperimentConfig, 
    ExperimentResult,
    run_quick_evaluation,
    create_evaluation_framework
)

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

class ColabEvaluationInterface:
    """Google Colabç”¨ã®è©•ä¾¡å®Ÿé¨“ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    
    def __init__(self):
        self.framework = create_evaluation_framework()
        self.current_result: Optional[ExperimentResult] = None
        
    def display_welcome_message(self):
        """ã‚¦ã‚§ãƒ«ã‚«ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º"""
        welcome_html = """
        <div style="border: 2px solid #4CAF50; border-radius: 10px; padding: 20px; background-color: #f9f9f9; margin: 10px 0;">
            <h2 style="color: #2E7D32; margin-top: 0;">ğŸ§  InsightSpike-AI è©•ä¾¡å®Ÿé¨“</h2>
            <p style="font-size: 16px; color: #333;">
                <strong>å­¦è¡“ç ”ç©¶åŸºæº–ã«åŸºã¥ãåŒ…æ‹¬çš„è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</strong>
            </p>
            <div style="background-color: #E8F5E8; padding: 15px; border-radius: 5px; margin: 10px 0;">
                <h3 style="color: #2E7D32; margin-top: 0;">ğŸ¯ å®Ÿé¨“ã®ç‰¹å¾´</h3>
                <ul style="color: #333; margin: 0;">
                    <li><strong>æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡</strong>: SQuAD, ARC Challenge, è«–ç†ãƒ‘ã‚ºãƒ«ç­‰</li>
                    <li><strong>å³å¯†ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ</strong>: GPT, Retrieval+LLM, ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æ‰‹æ³•</li>
                    <li><strong>çµ±è¨ˆçš„æ¤œè¨¼</strong>: ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã€æœ‰æ„æ€§æ¤œå®š</li>
                    <li><strong>ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“</strong>: å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å¯„ä¸åº¦åˆ†æ</li>
                    <li><strong>é–¾å€¤æ„Ÿåº¦åˆ†æ</strong>: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ­ãƒã‚¹ãƒˆæ€§æ¤œè¨¼</li>
                </ul>
            </div>
            <div style="background-color: #FFF3E0; padding: 15px; border-radius: 5px; margin: 10px 0;">
                <h3 style="color: #F57C00; margin-top: 0;">âš¡ Colabæœ€é©åŒ–</h3>
                <p style="color: #333; margin: 0;">
                    ã“ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¯Google Colabã§ã®å®Ÿè¡Œã«æœ€é©åŒ–ã•ã‚Œã¦ãŠã‚Šã€
                    è³‡æºåˆ¶ç´„ã‚’è€ƒæ…®ã—ãŸã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã¨é«˜é€Ÿå®Ÿè¡Œã‚’å®Ÿç¾ã—ã¾ã™ã€‚
                </p>
            </div>
        </div>
        """
        display(HTML(welcome_html))
    
    def run_quick_evaluation_demo(self, sample_size: int = 20):
        """ã‚¯ã‚¤ãƒƒã‚¯è©•ä¾¡ãƒ‡ãƒ¢ã®å®Ÿè¡Œ"""
        print("ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯è©•ä¾¡å®Ÿé¨“ã‚’é–‹å§‹ã—ã¾ã™...")
        print(f"ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {sample_size}")
        print("â±ï¸  æ¨å®šå®Ÿè¡Œæ™‚é–“: 2-3åˆ†")
        print("-" * 50)
        
        # Run the evaluation
        loop = asyncio.get_event_loop()
        self.current_result = loop.run_until_complete(run_quick_evaluation(sample_size))
        
        # Display results
        self.display_results_summary()
        
        return self.current_result
    
    def run_comprehensive_evaluation(self, 
                                   datasets: List[str] = ["squad_v2", "logic_puzzles"], 
                                   sample_size: int = 30):
        """åŒ…æ‹¬çš„è©•ä¾¡å®Ÿé¨“ã®å®Ÿè¡Œ"""
        print("ğŸ”¬ åŒ…æ‹¬çš„è©•ä¾¡å®Ÿé¨“ã‚’é–‹å§‹ã—ã¾ã™...")
        print(f"ğŸ“š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {', '.join(datasets)}")
        print(f"ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {sample_size}")
        print("â±ï¸  æ¨å®šå®Ÿè¡Œæ™‚é–“: 5-8åˆ†")
        print("-" * 50)
        
        config = ExperimentConfig(
            name="comprehensive_colab_evaluation",
            description="ColabåŒ…æ‹¬çš„å®¢è¦³è©•ä¾¡å®Ÿé¨“",
            datasets=datasets,
            baselines=["simple_llm", "retrieval_llm", "rule_based", "insightspike"],
            metrics=["accuracy", "confidence", "response_time", "insight_detection"],
            sample_size=sample_size,
            cross_validation_folds=3,
            random_seed=42
        )
        
        loop = asyncio.get_event_loop()
        self.current_result = loop.run_until_complete(
            self.framework.run_comprehensive_evaluation(config)
        )
        
        # Display comprehensive results
        self.display_comprehensive_results()
        
        return self.current_result
    
    def display_results_summary(self):
        """çµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º"""
        if not self.current_result:
            print("âŒ å®Ÿè¡ŒçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«è©•ä¾¡å®Ÿé¨“ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            return
        
        result = self.current_result
        
        # Basic info
        summary_html = f"""
        <div style="border: 2px solid #2196F3; border-radius: 10px; padding: 20px; background-color: #f8f9fa; margin: 20px 0;">
            <h2 style="color: #1976D2; margin-top: 0;">ğŸ“ˆ å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼</h2>
            <div style="display: flex; flex-wrap: wrap; gap: 15px;">
                <div style="background-color: #E3F2FD; padding: 15px; border-radius: 8px; flex: 1; min-width: 200px;">
                    <h4 style="color: #1976D2; margin: 0 0 10px 0;">åŸºæœ¬æƒ…å ±</h4>
                    <p style="margin: 5px 0;"><strong>å®Ÿé¨“å:</strong> {result.config.name}</p>
                    <p style="margin: 5px 0;"><strong>å®Ÿè¡Œæ™‚é–“:</strong> {result.execution_time:.2f}ç§’</p>
                    <p style="margin: 5px 0;"><strong>å‡¦ç†ã‚µãƒ³ãƒ—ãƒ«æ•°:</strong> {result.metadata.get('total_samples_processed', 'N/A')}</p>
                </div>
                <div style="background-color: #E8F5E8; padding: 15px; border-radius: 8px; flex: 1; min-width: 200px;">
                    <h4 style="color: #2E7D32; margin: 0 0 10px 0;">ãƒ†ã‚¹ãƒˆè¨­å®š</h4>
                    <p style="margin: 5px 0;"><strong>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°:</strong> {len(result.config.datasets)}</p>
                    <p style="margin: 5px 0;"><strong>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ•°:</strong> {len(result.config.baselines)}</p>
                    <p style="margin: 5px 0;"><strong>CVåˆ†å‰²æ•°:</strong> {result.config.cross_validation_folds}</p>
                </div>
            </div>
        </div>
        """
        display(HTML(summary_html))
        
        # Performance comparison
        self._display_performance_comparison()
        
        # Statistical significance
        self._display_statistical_results()
        
    def display_comprehensive_results(self):
        """åŒ…æ‹¬çš„çµæœã®è¡¨ç¤º"""
        if not self.current_result:
            print("âŒ å®Ÿè¡ŒçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        # Show summary first
        self.display_results_summary()
        
        # Show additional comprehensive results
        self._display_ablation_results()
        self._display_threshold_analysis()
        self._display_cross_validation_results()
    
    def _display_performance_comparison(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã®è¡¨ç¤º"""
        if not self.current_result or 'datasets' not in self.current_result.results:
            return
        
        print("\n" + "="*60)
        print("ğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ‰‹æ³•ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ")
        print("="*60)
        
        # Create comparison table
        comparison_data = []
        
        for dataset_name, dataset_data in self.current_result.results['datasets'].items():
            baseline_results = dataset_data['baseline_results']
            
            for baseline_name, baseline_data in baseline_results.items():
                metrics = baseline_data['summary_metrics']
                comparison_data.append({
                    'ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ': dataset_name,
                    'ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³': baseline_name,
                    'å¹³å‡ç²¾åº¦': f"{metrics['mean_accuracy']:.3f}",
                    'å¿œç­”æ™‚é–“(ç§’)': f"{metrics['mean_response_time']:.3f}",
                    'æ´å¯Ÿæ¤œå‡ºç‡': f"{metrics.get('insight_detection_rate', 0.0):.3f}",
                    'ã‚¨ãƒ©ãƒ¼ç‡': f"{metrics['error_rate']:.3f}"
                })
        
        if comparison_data:
            df = pd.DataFrame(comparison_data)
            
            # Style the dataframe for better display
            styled_df = df.style.format({
                'å¹³å‡ç²¾åº¦': '{:.3f}',
                'å¿œç­”æ™‚é–“(ç§’)': '{:.3f}',
                'æ´å¯Ÿæ¤œå‡ºç‡': '{:.3f}',
                'ã‚¨ãƒ©ãƒ¼ç‡': '{:.3f}'
            }).background_gradient(subset=['å¹³å‡ç²¾åº¦'], cmap='RdYlGn')
            
            display(styled_df)
            
            # Highlight best performance
            best_baseline = df.loc[df['å¹³å‡ç²¾åº¦'].astype(float).idxmax(), 'ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³']
            print(f"\nğŸ† æœ€é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: {best_baseline}")
    
    def _display_statistical_results(self):
        """çµ±è¨ˆçš„æœ‰æ„æ€§çµæœã®è¡¨ç¤º"""
        if not self.current_result or 'statistical_analysis' not in self.current_result.results:
            return
        
        print("\n" + "="*60)
        print("ğŸ“ˆ çµ±è¨ˆçš„æœ‰æ„æ€§åˆ†æ")
        print("="*60)
        
        stats_data = self.current_result.results['statistical_analysis']
        
        for dataset_name, comparisons in stats_data.items():
            print(f"\nğŸ“š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_name}")
            print("-" * 40)
            
            for baseline_name, significance in comparisons.items():
                if 'error' in significance:
                    print(f"   {baseline_name}: åˆ†æã‚¨ãƒ©ãƒ¼")
                    continue
                
                p_value = significance.get('p_value', 1.0)
                is_significant = significance.get('significant', False)
                effect_size = significance.get('effect_size', 'unknown')
                
                status = "âœ… æœ‰æ„" if is_significant else "âŒ éæœ‰æ„"
                print(f"   vs {baseline_name}: {status} (p={p_value:.4f}, åŠ¹æœé‡: {effect_size})")
    
    def _display_ablation_results(self):
        """ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“çµæœã®è¡¨ç¤º"""
        if not self.current_result or 'ablation_study' not in self.current_result.results:
            return
        
        print("\n" + "="*60)
        print("ğŸ”§ ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“çµæœ")
        print("="*60)
        
        ablation_data = self.current_result.results['ablation_study']
        
        # Create ablation comparison table
        ablation_rows = []
        for variant_name, variant_data in ablation_data.items():
            ablation_rows.append({
                'ãƒãƒªã‚¢ãƒ³ãƒˆ': variant_name,
                'èª¬æ˜': variant_data['description'],
                'å¹³å‡ç²¾åº¦': f"{variant_data['mean_accuracy']:.3f}",
                'å¿œç­”æ™‚é–“(ç§’)': f"{variant_data['mean_response_time']:.3f}",
                'æ´å¯Ÿæ¤œå‡ºç‡': f"{variant_data['insight_detection_rate']:.3f}"
            })
        
        if ablation_rows:
            ablation_df = pd.DataFrame(ablation_rows)
            styled_ablation = ablation_df.style.background_gradient(
                subset=['å¹³å‡ç²¾åº¦'], cmap='RdYlGn'
            )
            display(styled_ablation)
            
            # Show component contribution
            full_performance = next(
                (float(row['å¹³å‡ç²¾åº¦']) for row in ablation_rows if 'full' in row['ãƒãƒªã‚¢ãƒ³ãƒˆ']), 
                0.0
            )
            
            print(f"\nğŸ“Š ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå¯„ä¸åº¦åˆ†æ:")
            for row in ablation_rows:
                if 'full' not in row['ãƒãƒªã‚¢ãƒ³ãƒˆ']:
                    performance_drop = full_performance - float(row['å¹³å‡ç²¾åº¦'])
                    print(f"   {row['ãƒãƒªã‚¢ãƒ³ãƒˆ']}: -{performance_drop:.3f} ({performance_drop/full_performance*100:.1f}%ä½ä¸‹)")
    
    def _display_threshold_analysis(self):
        """é–¾å€¤æ„Ÿåº¦åˆ†æã®è¡¨ç¤º"""
        if not self.current_result or 'threshold_analysis' not in self.current_result.results:
            return
        
        print("\n" + "="*60)
        print("âš™ï¸ é–¾å€¤æ„Ÿåº¦åˆ†æ")
        print("="*60)
        
        threshold_data = self.current_result.results['threshold_analysis']
        
        # Find optimal threshold
        best_threshold = None
        best_f1 = 0.0
        
        for threshold_str, metrics in threshold_data.items():
            f1_score = metrics['f1_score']
            if f1_score > best_f1:
                best_f1 = f1_score
                best_threshold = float(threshold_str)
        
        print(f"ğŸ¯ æœ€é©é–¾å€¤: {best_threshold:.2f} (F1ã‚¹ã‚³ã‚¢: {best_f1:.3f})")
        
        # Show threshold range performance
        threshold_rows = []
        for threshold_str, metrics in list(threshold_data.items())[::2]:  # Show every 2nd for brevity
            threshold_rows.append({
                'é–¾å€¤': float(threshold_str),
                'Precision': f"{metrics['precision']:.3f}",
                'Recall': f"{metrics['recall']:.3f}",
                'F1ã‚¹ã‚³ã‚¢': f"{metrics['f1_score']:.3f}"
            })
        
        if threshold_rows:
            threshold_df = pd.DataFrame(threshold_rows)
            styled_threshold = threshold_df.style.background_gradient(
                subset=['F1ã‚¹ã‚³ã‚¢'], cmap='RdYlGn'
            )
            display(styled_threshold)
    
    def _display_cross_validation_results(self):
        """ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœã®è¡¨ç¤º"""
        if not self.current_result or 'cross_validation' not in self.current_result.results:
            return
        
        print("\n" + "="*60)
        print("ğŸ”„ ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ")
        print("="*60)
        
        cv_data = self.current_result.results['cross_validation']
        
        for baseline_name, cv_results in cv_data.items():
            print(f"\nğŸ“ˆ {baseline_name}:")
            print(f"   å¹³å‡ç²¾åº¦: {cv_results['cv_accuracy_mean']:.3f} Â± {cv_results['cv_accuracy_std']:.3f}")
            print(f"   å¹³å‡å¿œç­”æ™‚é–“: {cv_results['cv_time_mean']:.3f} Â± {cv_results['cv_time_std']:.3f}ç§’")
    
    def create_visual_report(self, save_path: Optional[str] = None):
        """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ"""
        if not self.current_result:
            print("âŒ å®Ÿè¡ŒçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        print("ğŸ“Š ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
        
        # Create comprehensive visualization
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('InsightSpike-AI å®¢è¦³çš„è©•ä¾¡çµæœ', fontsize=16, fontweight='bold')
        
        # Plot 1: Baseline Accuracy Comparison
        self._plot_baseline_comparison(axes[0, 0])
        
        # Plot 2: Response Time Comparison  
        self._plot_response_time_comparison(axes[0, 1])
        
        # Plot 3: Insight Detection Rate
        self._plot_insight_detection_rate(axes[0, 2])
        
        # Plot 4: Ablation Study
        self._plot_ablation_study(axes[1, 0])
        
        # Plot 5: Threshold Analysis
        self._plot_threshold_analysis(axes[1, 1])
        
        # Plot 6: Cross-Validation Stability
        self._plot_cv_stability(axes[1, 2])
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")
        
        plt.show()
    
    def _plot_baseline_comparison(self, ax):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ"""
        if 'datasets' not in self.current_result.results:
            ax.text(0.5, 0.5, 'ãƒ‡ãƒ¼ã‚¿ãªã—', ha='center', va='center', transform=ax.transAxes)
            ax.set_title('ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç²¾åº¦æ¯”è¼ƒ')
            return
        
        # Extract accuracy data
        baselines = []
        accuracies = []
        
        first_dataset = list(self.current_result.results['datasets'].keys())[0]
        baseline_results = self.current_result.results['datasets'][first_dataset]['baseline_results']
        
        for baseline_name, baseline_data in baseline_results.items():
            baselines.append(baseline_name)
            accuracies.append(baseline_data['summary_metrics']['mean_accuracy'])
        
        # Create bar plot
        colors = ['skyblue', 'lightgreen', 'orange', 'gold'] * (len(baselines) // 4 + 1)
        bars = ax.bar(baselines, accuracies, color=colors[:len(baselines)])
        
        # Highlight InsightSpike
        for i, baseline in enumerate(baselines):
            if 'insightspike' in baseline.lower():
                bars[i].set_color('red')
                bars[i].set_edgecolor('darkred')
                bars[i].set_linewidth(2)
        
        ax.set_title('ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç²¾åº¦æ¯”è¼ƒ')
        ax.set_ylabel('å¹³å‡ç²¾åº¦')
        ax.tick_params(axis='x', rotation=45)
        ax.grid(True, alpha=0.3)
    
    def _plot_response_time_comparison(self, ax):
        """å¿œç­”æ™‚é–“æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ"""
        if 'datasets' not in self.current_result.results:
            ax.text(0.5, 0.5, 'ãƒ‡ãƒ¼ã‚¿ãªã—', ha='center', va='center', transform=ax.transAxes)
            ax.set_title('å¿œç­”æ™‚é–“æ¯”è¼ƒ')
            return
        
        # Extract response time data
        baselines = []
        response_times = []
        
        first_dataset = list(self.current_result.results['datasets'].keys())[0]
        baseline_results = self.current_result.results['datasets'][first_dataset]['baseline_results']
        
        for baseline_name, baseline_data in baseline_results.items():
            baselines.append(baseline_name)
            response_times.append(baseline_data['summary_metrics']['mean_response_time'])
        
        ax.bar(baselines, response_times, color='lightcoral')
        ax.set_title('å¿œç­”æ™‚é–“æ¯”è¼ƒ')
        ax.set_ylabel('å¹³å‡å¿œç­”æ™‚é–“ (ç§’)')
        ax.tick_params(axis='x', rotation=45)
        ax.grid(True, alpha=0.3)
    
    def _plot_insight_detection_rate(self, ax):
        """æ´å¯Ÿæ¤œå‡ºç‡ãƒ—ãƒ­ãƒƒãƒˆ"""
        if 'datasets' not in self.current_result.results:
            ax.text(0.5, 0.5, 'ãƒ‡ãƒ¼ã‚¿ãªã—', ha='center', va='center', transform=ax.transAxes)
            ax.set_title('æ´å¯Ÿæ¤œå‡ºç‡')
            return
        
        # Extract insight detection data
        baselines = []
        detection_rates = []
        
        first_dataset = list(self.current_result.results['datasets'].keys())[0]
        baseline_results = self.current_result.results['datasets'][first_dataset]['baseline_results']
        
        for baseline_name, baseline_data in baseline_results.items():
            baselines.append(baseline_name)
            detection_rates.append(baseline_data['summary_metrics'].get('insight_detection_rate', 0.0))
        
        ax.bar(baselines, detection_rates, color='lightblue')
        ax.set_title('æ´å¯Ÿæ¤œå‡ºç‡')
        ax.set_ylabel('æ¤œå‡ºç‡')
        ax.tick_params(axis='x', rotation=45)
        ax.grid(True, alpha=0.3)
        ax.set_ylim(0, 1)
    
    def _plot_ablation_study(self, ax):
        """ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“ãƒ—ãƒ­ãƒƒãƒˆ"""
        if 'ablation_study' not in self.current_result.results:
            ax.text(0.5, 0.5, 'ãƒ‡ãƒ¼ã‚¿ãªã—', ha='center', va='center', transform=ax.transAxes)
            ax.set_title('ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“')
            return
        
        ablation_data = self.current_result.results['ablation_study']
        
        variants = list(ablation_data.keys())
        accuracies = [ablation_data[v]['mean_accuracy'] for v in variants]
        
        ax.barh(variants, accuracies, color='lightyellow')
        ax.set_title('ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“çµæœ')
        ax.set_xlabel('å¹³å‡ç²¾åº¦')
        ax.grid(True, alpha=0.3)
    
    def _plot_threshold_analysis(self, ax):
        """é–¾å€¤åˆ†æãƒ—ãƒ­ãƒƒãƒˆ"""
        if 'threshold_analysis' not in self.current_result.results:
            ax.text(0.5, 0.5, 'ãƒ‡ãƒ¼ã‚¿ãªã—', ha='center', va='center', transform=ax.transAxes)
            ax.set_title('é–¾å€¤æ„Ÿåº¦åˆ†æ')
            return
        
        threshold_data = self.current_result.results['threshold_analysis']
        
        thresholds = [float(k) for k in threshold_data.keys()]
        f1_scores = [threshold_data[k]['f1_score'] for k in threshold_data.keys()]
        precisions = [threshold_data[k]['precision'] for k in threshold_data.keys()]
        recalls = [threshold_data[k]['recall'] for k in threshold_data.keys()]
        
        ax.plot(thresholds, f1_scores, 'o-', label='F1ã‚¹ã‚³ã‚¢', linewidth=2)
        ax.plot(thresholds, precisions, 's-', label='Precision', linewidth=2)
        ax.plot(thresholds, recalls, '^-', label='Recall', linewidth=2)
        
        ax.set_title('é–¾å€¤æ„Ÿåº¦åˆ†æ')
        ax.set_xlabel('æ´å¯Ÿæ¤œå‡ºé–¾å€¤')
        ax.set_ylabel('ã‚¹ã‚³ã‚¢')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_cv_stability(self, ax):
        """ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®‰å®šæ€§ãƒ—ãƒ­ãƒƒãƒˆ"""
        if 'cross_validation' not in self.current_result.results:
            ax.text(0.5, 0.5, 'ãƒ‡ãƒ¼ã‚¿ãªã—', ha='center', va='center', transform=ax.transAxes)
            ax.set_title('CVå®‰å®šæ€§')
            return
        
        cv_data = self.current_result.results['cross_validation']
        
        baselines = list(cv_data.keys())
        means = [cv_data[b]['cv_accuracy_mean'] for b in baselines]
        stds = [cv_data[b]['cv_accuracy_std'] for b in baselines]
        
        ax.errorbar(range(len(baselines)), means, yerr=stds, 
                   fmt='o', capsize=5, capthick=2, linewidth=2)
        ax.set_xticks(range(len(baselines)))
        ax.set_xticklabels(baselines, rotation=45)
        ax.set_title('ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®‰å®šæ€§')
        ax.set_ylabel('ç²¾åº¦ (å¹³å‡ Â± æ¨™æº–åå·®)')
        ax.grid(True, alpha=0.3)
    
    def export_results_json(self, filename: str = "evaluation_results.json"):
        """çµæœã‚’JSONã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        if not self.current_result:
            print("âŒ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        # Convert result to serializable format
        export_data = {
            'config': {
                'name': self.current_result.config.name,
                'description': self.current_result.config.description,
                'datasets': self.current_result.config.datasets,
                'baselines': self.current_result.config.baselines,
                'metrics': self.current_result.config.metrics,
                'sample_size': self.current_result.config.sample_size
            },
            'results': self.current_result.results,
            'execution_time': self.current_result.execution_time,
            'timestamp': self.current_result.timestamp,
            'metadata': self.current_result.metadata
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“ çµæœã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ: {filename}")
    
    def get_summary_markdown(self) -> str:
        """çµæœã®Markdownã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        if not self.current_result:
            return "# ã‚¨ãƒ©ãƒ¼: å®Ÿè¡ŒçµæœãŒã‚ã‚Šã¾ã›ã‚“"
        
        result = self.current_result
        
        markdown = f"""# InsightSpike-AI å®¢è¦³çš„è©•ä¾¡çµæœ

## å®Ÿé¨“æ¦‚è¦
- **å®Ÿé¨“å**: {result.config.name}
- **å®Ÿè¡Œæ™‚é–“**: {result.execution_time:.2f}ç§’
- **å‡¦ç†ã‚µãƒ³ãƒ—ãƒ«æ•°**: {result.metadata.get('total_samples_processed', 'N/A')}
- **ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—**: {result.timestamp}

## ãƒ†ã‚¹ãƒˆè¨­å®š
- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: {', '.join(result.config.datasets)}
- **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ‰‹æ³•**: {', '.join(result.config.baselines)}
- **ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º**: {result.config.sample_size}
- **ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³åˆ†å‰²æ•°**: {result.config.cross_validation_folds}

## ä¸»è¦çµæœ

### ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ
"""
        
        # Add baseline comparison if available
        if 'datasets' in result.results and result.results['datasets']:
            first_dataset = list(result.results['datasets'].keys())[0]
            baseline_results = result.results['datasets'][first_dataset]['baseline_results']
            
            markdown += "| ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ | å¹³å‡ç²¾åº¦ | å¿œç­”æ™‚é–“(ç§’) | æ´å¯Ÿæ¤œå‡ºç‡ |\n"
            markdown += "|-------------|----------|-------------|----------|\n"
            
            for baseline_name, baseline_data in baseline_results.items():
                metrics = baseline_data['summary_metrics']
                markdown += f"| {baseline_name} | {metrics['mean_accuracy']:.3f} | {metrics['mean_response_time']:.3f} | {metrics.get('insight_detection_rate', 0.0):.3f} |\n"
        
        markdown += "\n### çµ±è¨ˆçš„æœ‰æ„æ€§\n"
        
        # Add statistical results if available
        if 'statistical_analysis' in result.results:
            for dataset_name, comparisons in result.results['statistical_analysis'].items():
                markdown += f"\n**{dataset_name}ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**:\n"
                for baseline_name, significance in comparisons.items():
                    if 'error' not in significance:
                        p_value = significance.get('p_value', 1.0)
                        is_significant = "âœ… æœ‰æ„" if significance.get('significant', False) else "âŒ éæœ‰æ„"
                        markdown += f"- vs {baseline_name}: {is_significant} (p={p_value:.4f})\n"
        
        markdown += "\n---\n*ã“ã®çµæœã¯å­¦è¡“çš„ç ”ç©¶åŸºæº–ã«åŸºã¥ãå®¢è¦³çš„è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚*"
        
        return markdown

# Factory function for Colab interface
def create_colab_interface() -> ColabEvaluationInterface:
    """Colab ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ä½œæˆ"""
    return ColabEvaluationInterface()

# Convenience functions for direct use in Colab cells
def quick_demo(sample_size: int = 20):
    """ã‚¯ã‚¤ãƒƒã‚¯ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    interface = create_colab_interface()
    interface.display_welcome_message()
    return interface.run_quick_evaluation_demo(sample_size)

def comprehensive_demo(datasets: List[str] = ["logic_puzzles"], sample_size: int = 30):
    """åŒ…æ‹¬çš„ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    interface = create_colab_interface()
    interface.display_welcome_message()
    return interface.run_comprehensive_evaluation(datasets, sample_size)

def create_visual_report(result: ExperimentResult, save_path: str = "evaluation_report.png"):
    """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ"""
    interface = create_colab_interface()
    interface.current_result = result
    interface.create_visual_report(save_path)

if __name__ == "__main__":
    # Test the interface
    print("Testing Colab Interface...")
    result = quick_demo(10)
    print(f"Demo completed: {result.config.name}")
