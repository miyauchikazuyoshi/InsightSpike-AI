#!/usr/bin/env python3
"""
è©³ç´°ãƒ­ã‚°å®Ÿé¨“ã®åŒ…æ‹¬çš„åˆ†æãƒ»å¯è¦–åŒ–ãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
=========================================================

TopKé¡ä¼¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã€ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨ªæ–­æ´å¯Ÿã€GEDæ€¥è½ç¾è±¡ã€
æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ™ã‚¯ãƒˆãƒ«ã®è¨€èªå¾©å…ƒãªã©ã€è©³ç´°ãªåˆ†æã‚’å®Ÿæ–½

é‡è¦ãªç ”ç©¶æˆæœï¼š
- éæ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼ˆç´„18.6%ï¼‰ã¯ã€æ—¢çŸ¥ãƒ‡ãƒ¼ã‚¿ã¨ã®é«˜é¡ä¼¼åº¦ã«ã‚ˆã‚Š
  å†…ç™ºçš„å ±é…¬ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãŒåƒãã€å‹•çš„RAGæ§‹ç¯‰æ™‚ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã«è²¢çŒ®
- ã“ã‚Œã«ã‚ˆã‚Šè¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’æ–°ã—ã„æ´å¯Ÿã«é›†ä¸­å¯èƒ½
- çµ±è¨ˆçš„ã«æœ‰æ„ãªé¸æŠçš„å­¦ç¿’ã®å®Ÿè¨¼ï¼ˆn=500, æ¤œå‡ºç‡81.6%ï¼‰
"""

import sys
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['Arial Unicode MS', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP']

class ComprehensiveDetailedAnalyzer:
    """è©³ç´°ãƒ­ã‚°å®Ÿé¨“ã®åŒ…æ‹¬çš„åˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, data_dir: str = "experiments/outputs/detailed_logging_realtime"):
        self.data_dir = Path(data_dir)
        self.output_dir = self.data_dir / "analysis_reports"
        self.output_dir.mkdir(exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.load_data()
        
        print(f"ğŸ“Š åŒ…æ‹¬çš„åˆ†æã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        print(f"   - ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.data_dir}")
        print(f"   - å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        print(f"   - èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰{len(self.episodes)}, æ´å¯Ÿ{len(self.insights)}")
    
    def load_data(self):
        """å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            # CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            self.episodes = pd.read_csv(self.data_dir / "01_input_episodes.csv")
            self.insights = pd.read_csv(self.data_dir / "02_detailed_insights.csv")
            self.topk_analysis = pd.read_csv(self.data_dir / "03_topk_analysis.csv")
            self.episode_logs = pd.read_csv(self.data_dir / "04_detailed_episode_logs.csv")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            with open(self.data_dir / "05_experiment_metadata.json", 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
                
            print("âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def analyze_insight_patterns(self) -> Dict[str, Any]:
        """æ´å¯Ÿãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©³ç´°åˆ†æ"""
        print("\nğŸ” æ´å¯Ÿãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æä¸­...")
        
        analysis = {}
        
        # åŸºæœ¬çµ±è¨ˆ
        analysis['basic_stats'] = {
            'total_episodes': len(self.episodes),
            'total_insights': len(self.insights),
            'insight_rate': len(self.insights) / len(self.episodes),
            'avg_ged': self.insights['ged_value'].mean(),
            'avg_ig': self.insights['ig_value'].mean(),
            'avg_confidence': self.insights['confidence'].mean()
        }
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†æ
        domain_stats = self.insights.groupby('current_domain').agg({
            'insight_id': 'count',
            'ged_value': ['mean', 'std'],
            'ig_value': ['mean', 'std'],
            'confidence': ['mean', 'std'],
            'cross_domain_count': 'mean'
        }).round(4)
        
        analysis['domain_stats'] = domain_stats
        
        # ç ”ç©¶é ˜åŸŸåˆ†æ
        research_area_stats = self.insights.groupby('current_research_area').agg({
            'insight_id': 'count',
            'ged_value': ['mean', 'std'],
            'confidence': ['mean', 'std'],
            'domain_diversity': 'mean'
        }).round(4)
        
        analysis['research_area_stats'] = research_area_stats
        
        # æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.insights['episode_id_int'] = self.insights['episode_id'].astype(int)
        time_series = self.insights.groupby('episode_id_int').agg({
            'ged_value': 'mean',
            'ig_value': 'mean',
            'confidence': 'mean'
        }).reset_index()
        
        analysis['time_series'] = time_series
        
        # GEDæ€¥è½ç¾è±¡ã®æ¤œå‡º
        ged_values = time_series['ged_value'].values
        ged_drops = []
        for i in range(1, len(ged_values)):
            if ged_values[i-1] - ged_values[i] > 0.1:  # 0.1ä»¥ä¸Šã®æ€¥è½
                ged_drops.append({
                    'episode': time_series.iloc[i]['episode_id_int'],
                    'prev_ged': ged_values[i-1],
                    'curr_ged': ged_values[i],
                    'drop_magnitude': ged_values[i-1] - ged_values[i]
                })
        
        analysis['ged_drops'] = ged_drops
        
        print(f"   - æ´å¯Ÿç‡: {analysis['basic_stats']['insight_rate']:.3f}")
        print(f"   - å¹³å‡GED: {analysis['basic_stats']['avg_ged']:.3f}")
        print(f"   - GEDæ€¥è½æ¤œå‡º: {len(ged_drops)}ä»¶")
        
        return analysis
    
    def analyze_non_insight_episodes(self) -> Dict[str, Any]:
        """æ´å¯ŸãŒåƒã‹ãªã‹ã£ãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®åˆ†æ"""
        print("\nğŸ” éæ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ†æä¸­...")
        
        # æ´å¯ŸãŒç™ºç”Ÿã—ãªã‹ã£ãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç‰¹å®š
        insight_episode_ids = set(self.insights['episode_id'].astype(int))
        all_episode_ids = set(range(len(self.episodes)))
        non_insight_ids = all_episode_ids - insight_episode_ids
        
        non_insight_episodes = self.episodes[self.episodes.index.isin(non_insight_ids)]
        
        analysis = {
            'count': len(non_insight_episodes),
            'rate': len(non_insight_episodes) / len(self.episodes),
            'episodes': non_insight_episodes
        }
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†å¸ƒã®æ¯”è¼ƒ
        if len(non_insight_episodes) > 0:
            # éæ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†å¸ƒ
            non_insight_domains = non_insight_episodes['domain'].value_counts()
            
            # æ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†å¸ƒ
            insight_domains = self.insights['current_domain'].value_counts()
            
            analysis['domain_comparison'] = {
                'non_insight_domains': non_insight_domains,
                'insight_domains': insight_domains
            }
        
        print(f"   - éæ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {len(non_insight_episodes)}ä»¶ ({analysis['rate']:.3f})")
        
        return analysis
    
    def analyze_topk_similarity(self) -> Dict[str, Any]:
        """TopKé¡ä¼¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®è©³ç´°åˆ†æ"""
        print("\nğŸ” TopKé¡ä¼¼æ€§åˆ†æä¸­...")
        
        analysis = {}
        
        # TopKãƒ‡ãƒ¼ã‚¿ã®å†æ§‹ç¯‰ï¼ˆå„ãƒ©ãƒ³ã‚¯ãŒå€‹åˆ¥åˆ—ã«ãªã£ã¦ã„ã‚‹å½¢å¼ã‚’å‡¦ç†ï¼‰
        if len(self.topk_analysis) > 0:
            # å„ãƒ©ãƒ³ã‚¯ã®é¡ä¼¼åº¦ã‚’æŠ½å‡º
            similarities = []
            ranks = []
            query_domains = []
            similar_domains = []
            
            for _, row in self.topk_analysis.iterrows():
                query_domain = row['current_domain']
                for rank in range(1, 11):  # rank_1 to rank_10
                    sim_col = f'rank_{rank}_similarity'
                    domain_col = f'rank_{rank}_domain'
                    
                    if sim_col in row and pd.notna(row[sim_col]) and row[sim_col] != '':
                        similarities.append(float(row[sim_col]))
                        ranks.append(rank)
                        query_domains.append(query_domain)
                        similar_domains.append(row[domain_col] if pd.notna(row[domain_col]) else 'unknown')
            
            if similarities:
                # çµ±è¨ˆè¨ˆç®—
                analysis['similarity_stats'] = {
                    'avg_similarity': np.mean(similarities),
                    'median_similarity': np.median(similarities),
                    'min_similarity': np.min(similarities),
                    'max_similarity': np.max(similarities),
                    'std_similarity': np.std(similarities)
                }
                
                # ãƒ©ãƒ³ã‚¯åˆ¥çµ±è¨ˆï¼ˆDataFrameã‚’ä½œæˆï¼‰
                rank_df = pd.DataFrame({
                    'rank': ranks,
                    'similarity': similarities,
                    'query_domain': query_domains,
                    'similar_domain': similar_domains
                })
                
                rank_stats = rank_df.groupby('rank').agg({
                    'similarity': ['mean', 'std', 'count']
                }).round(4)
                
                analysis['rank_stats'] = rank_stats
                analysis['rank_df'] = rank_df  # å¯è¦–åŒ–ç”¨ã«ä¿å­˜
                
                # ãƒ‰ãƒ¡ã‚¤ãƒ³é–“é¡ä¼¼æ€§åˆ†æ
                domain_similarity = rank_df.groupby(['query_domain', 'similar_domain']).agg({
                    'similarity': ['mean', 'count']
                }).round(4)
                
                analysis['domain_similarity'] = domain_similarity
                
                print(f"   - TopKåˆ†æãƒ‡ãƒ¼ã‚¿: {len(similarities)}ä»¶ã®é¡ä¼¼åº¦")
                print(f"   - å¹³å‡é¡ä¼¼åº¦: {analysis['similarity_stats']['avg_similarity']:.3f}")
            else:
                print("   - æœ‰åŠ¹ãªé¡ä¼¼åº¦ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                analysis = {}
        else:
            print("   - TopKåˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            analysis = {}
        
        return analysis
    
    def analyze_cross_domain_insights(self) -> Dict[str, Any]:
        """ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨ªæ–­æ´å¯Ÿã®åˆ†æ"""
        print("\nğŸ” ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨ªæ–­æ´å¯Ÿåˆ†æä¸­...")
        
        analysis = {}
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³å¤šæ§˜æ€§ã®åˆ†æ
        diversity_stats = {
            'avg_domain_diversity': self.insights['domain_diversity'].mean(),
            'max_domain_diversity': self.insights['domain_diversity'].max(),
            'min_domain_diversity': self.insights['domain_diversity'].min(),
            'std_domain_diversity': self.insights['domain_diversity'].std()
        }
        
        analysis['diversity_stats'] = diversity_stats
        
        # ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°ã®åˆ†æ
        cross_domain_stats = {
            'avg_cross_domain_count': self.insights['cross_domain_count'].mean(),
            'max_cross_domain_count': self.insights['cross_domain_count'].max(),
            'min_cross_domain_count': self.insights['cross_domain_count'].min(),
            'std_cross_domain_count': self.insights['cross_domain_count'].std()
        }
        
        analysis['cross_domain_stats'] = cross_domain_stats
        
        # é«˜åº¦ãªã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³æ´å¯Ÿï¼ˆå¤šæ§˜æ€§ãŒé«˜ã„ï¼‰
        high_diversity_insights = self.insights[
            self.insights['domain_diversity'] >= self.insights['domain_diversity'].quantile(0.75)
        ]
        
        analysis['high_diversity_insights'] = {
            'count': len(high_diversity_insights),
            'rate': len(high_diversity_insights) / len(self.insights),
            'avg_confidence': high_diversity_insights['confidence'].mean(),
            'avg_ged': high_diversity_insights['ged_value'].mean()
        }
        
        print(f"   - å¹³å‡ãƒ‰ãƒ¡ã‚¤ãƒ³å¤šæ§˜æ€§: {diversity_stats['avg_domain_diversity']:.2f}")
        print(f"   - é«˜å¤šæ§˜æ€§æ´å¯Ÿ: {analysis['high_diversity_insights']['count']}ä»¶")
        
        return analysis
            
    def analyze_vector_reconstruction(self) -> Dict[str, Any]:
        """ãƒ™ã‚¯ãƒˆãƒ«è¨€èªå¾©å…ƒã®åˆ†æ"""
        print("\nğŸ” ãƒ™ã‚¯ãƒˆãƒ«è¨€èªå¾©å…ƒåˆ†æä¸­...")
        
        analysis = {}
        
        # å¾©å…ƒãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
        reconstruction_patterns = {}
        abstraction_levels = []
        aggregation_types = []
        
        for _, insight in self.insights.iterrows():
            reconstruction = insight['vector_reconstruction']
            
            # æŠ½è±¡åº¦ãƒ¬ãƒ™ãƒ«ã®æŠ½å‡º
            if 'é«˜æŠ½è±¡åº¦' in reconstruction:
                abstraction_levels.append('é«˜æŠ½è±¡åº¦')
            elif 'ä¸­é–“æŠ½è±¡åº¦' in reconstruction:
                abstraction_levels.append('ä¸­é–“æŠ½è±¡åº¦')
            elif 'ä½æŠ½è±¡åº¦' in reconstruction:
                abstraction_levels.append('ä½æŠ½è±¡åº¦')
            else:
                abstraction_levels.append('ä¸æ˜')
            
            # é›†ç´„ã‚¿ã‚¤ãƒ—ã®æŠ½å‡º
            if 'é›†ç´„çš„' in reconstruction:
                aggregation_types.append('é›†ç´„çš„')
            elif 'åˆ†æ•£çš„' in reconstruction:
                aggregation_types.append('åˆ†æ•£çš„')
            else:
                aggregation_types.append('ä¸æ˜')
        
        analysis['abstraction_distribution'] = pd.Series(abstraction_levels).value_counts()
        analysis['aggregation_distribution'] = pd.Series(aggregation_types).value_counts()
        
        # æŠ½è±¡åº¦ã¨æ´å¯Ÿå“è³ªã®é–¢ä¿‚
        insights_with_abstraction = self.insights.copy()
        insights_with_abstraction['abstraction_level'] = abstraction_levels
        insights_with_abstraction['aggregation_type'] = aggregation_types
        
        abstraction_quality = insights_with_abstraction.groupby('abstraction_level').agg({
            'confidence': 'mean',
            'ged_value': 'mean',
            'ig_value': 'mean'
        }).round(4)
        
        analysis['abstraction_quality'] = abstraction_quality
        
        print(f"   - æŠ½è±¡åº¦åˆ†å¸ƒ: {dict(analysis['abstraction_distribution'])}")
        print(f"   - é›†ç´„ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ: {dict(analysis['aggregation_distribution'])}")
        
        return analysis
    
    def create_visualizations(self, analyses: Dict[str, Any]):
        """åŒ…æ‹¬çš„ãªå¯è¦–åŒ–ã®ä½œæˆ"""
        print("\nğŸ“Š å¯è¦–åŒ–ä½œæˆä¸­...")
        
        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’ç¢ºå®Ÿã«é©ç”¨
        plt.rcParams['font.family'] = ['Arial Unicode MS', 'Hiragino Sans', 'Yu Gothic']
        
        # 1. æ´å¯Ÿãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç·åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('è©³ç´°ãƒ­ã‚°å®Ÿé¨“: æ´å¯Ÿãƒ‘ã‚¿ãƒ¼ãƒ³ç·åˆåˆ†æ', fontsize=16, fontweight='bold')
        
        # 1-1. æ´å¯Ÿç‡ã¨åŸºæœ¬çµ±è¨ˆ
        basic_stats = analyses['insight_patterns']['basic_stats']
        metrics = ['æ´å¯Ÿç‡', 'å¹³å‡GED', 'å¹³å‡IG', 'å¹³å‡ä¿¡é ¼åº¦']
        values = [basic_stats['insight_rate'], basic_stats['avg_ged'], 
                 basic_stats['avg_ig'], basic_stats['avg_confidence']]
        
        axes[0,0].bar(metrics, values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
        axes[0,0].set_title('åŸºæœ¬çµ±è¨ˆæŒ‡æ¨™')
        axes[0,0].tick_params(axis='x', rotation=45)
        
        # 1-2. ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥æ´å¯Ÿæ•°
        domain_counts = analyses['insight_patterns']['domain_stats']['insight_id']['count'].head(10)
        axes[0,1].bar(range(len(domain_counts)), domain_counts.values, color='#FFA07A')
        axes[0,1].set_title('ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥æ´å¯Ÿæ•° (Top10)')
        axes[0,1].set_xticks(range(len(domain_counts)))
        axes[0,1].set_xticklabels(domain_counts.index, rotation=45, ha='right')
        
        # 1-3. GEDæ™‚ç³»åˆ—ã¨GEDæ€¥è½
        time_series = analyses['insight_patterns']['time_series']
        axes[0,2].plot(time_series['episode_id_int'], time_series['ged_value'], 
                      color='#FF6B6B', alpha=0.7, linewidth=1)
        
        # GEDæ€¥è½ãƒã‚¤ãƒ³ãƒˆã‚’ãƒãƒ¼ã‚¯
        for drop in analyses['insight_patterns']['ged_drops']:
            axes[0,2].axvline(x=drop['episode'], color='red', linestyle='--', alpha=0.5)
        
        axes[0,2].set_title(f'GEDæ™‚ç³»åˆ—æ¨ç§» (æ€¥è½: {len(analyses["insight_patterns"]["ged_drops"])}ä»¶)')
        axes[0,2].set_xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ID')
        axes[0,2].set_ylabel('GEDå€¤')
        
        # 1-4. ä¿¡é ¼åº¦åˆ†å¸ƒ
        axes[1,0].hist(self.insights['confidence'], bins=30, color='#96CEB4', alpha=0.7)
        axes[1,0].set_title('æ´å¯Ÿä¿¡é ¼åº¦åˆ†å¸ƒ')
        axes[1,0].set_xlabel('ä¿¡é ¼åº¦')
        axes[1,0].set_ylabel('é »åº¦')
        
        # 1-5. ãƒ‰ãƒ¡ã‚¤ãƒ³å¤šæ§˜æ€§ vs ä¿¡é ¼åº¦
        axes[1,1].scatter(self.insights['domain_diversity'], self.insights['confidence'], 
                         alpha=0.6, color='#4ECDC4')
        axes[1,1].set_title('ãƒ‰ãƒ¡ã‚¤ãƒ³å¤šæ§˜æ€§ vs ä¿¡é ¼åº¦')
        axes[1,1].set_xlabel('ãƒ‰ãƒ¡ã‚¤ãƒ³å¤šæ§˜æ€§')
        axes[1,1].set_ylabel('ä¿¡é ¼åº¦')
        
        # 1-6. ç ”ç©¶é ˜åŸŸåˆ¥æ´å¯Ÿæ•°
        research_counts = analyses['insight_patterns']['research_area_stats']['insight_id']['count'].head(8)
        axes[1,2].bar(range(len(research_counts)), research_counts.values, color='#DDA0DD')
        axes[1,2].set_title('ç ”ç©¶é ˜åŸŸåˆ¥æ´å¯Ÿæ•° (Top8)')
        axes[1,2].set_xticks(range(len(research_counts)))
        axes[1,2].set_xticklabels(research_counts.index, rotation=45, ha='right')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "01_comprehensive_insight_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. TopKé¡ä¼¼æ€§åˆ†æ
        if 'topk_similarity' in analyses and analyses['topk_similarity'] and 'rank_df' in analyses['topk_similarity']:
            fig, axes = plt.subplots(2, 2, figsize=(14, 10))
            fig.suptitle('TopKé¡ä¼¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ†æ', fontsize=16, fontweight='bold')
            
            rank_df = analyses['topk_similarity']['rank_df']
            
            # 2-1. é¡ä¼¼åº¦åˆ†å¸ƒ
            axes[0,0].hist(rank_df['similarity'], bins=20, color='#87CEEB', alpha=0.7)
            axes[0,0].set_title('é¡ä¼¼åº¦åˆ†å¸ƒ')
            axes[0,0].set_xlabel('é¡ä¼¼åº¦')
            axes[0,0].set_ylabel('é »åº¦')
            
            # 2-2. ãƒ©ãƒ³ã‚¯åˆ¥å¹³å‡é¡ä¼¼åº¦
            rank_stats = analyses['topk_similarity']['rank_stats']['similarity']['mean']
            axes[0,1].bar(rank_stats.index, rank_stats.values, color='#98FB98')
            axes[0,1].set_title('ãƒ©ãƒ³ã‚¯åˆ¥å¹³å‡é¡ä¼¼åº¦')
            axes[0,1].set_xlabel('ãƒ©ãƒ³ã‚¯')
            axes[0,1].set_ylabel('å¹³å‡é¡ä¼¼åº¦')
            
            # 2-3. ã‚¯ã‚¨ãƒªãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥é¡ä¼¼åº¦
            query_domain_sim = rank_df.groupby('query_domain')['similarity'].mean().sort_values(ascending=False).head(10)
            axes[1,0].bar(range(len(query_domain_sim)), query_domain_sim.values, color='#FFB6C1')
            axes[1,0].set_title('ã‚¯ã‚¨ãƒªãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥å¹³å‡é¡ä¼¼åº¦ (Top10)')
            axes[1,0].set_xticks(range(len(query_domain_sim)))
            axes[1,0].set_xticklabels(query_domain_sim.index, rotation=45, ha='right')
            
            # 2-4. é¡ä¼¼åº¦ vs ãƒ©ãƒ³ã‚¯ã®æ•£å¸ƒå›³
            axes[1,1].scatter(rank_df['rank'], rank_df['similarity'], 
                            alpha=0.6, color='#DDA0DD')
            axes[1,1].set_title('ãƒ©ãƒ³ã‚¯ vs é¡ä¼¼åº¦')
            axes[1,1].set_xlabel('ãƒ©ãƒ³ã‚¯')
            axes[1,1].set_ylabel('é¡ä¼¼åº¦')
            
            plt.tight_layout()
            plt.savefig(self.output_dir / "02_topk_similarity_analysis.png", dpi=300, bbox_inches='tight')
            plt.close()
        
        # 3. ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨ªæ–­æ´å¯Ÿåˆ†æ
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle('ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨ªæ–­æ´å¯Ÿåˆ†æ', fontsize=16, fontweight='bold')
        
        # 3-1. ãƒ‰ãƒ¡ã‚¤ãƒ³å¤šæ§˜æ€§åˆ†å¸ƒ
        axes[0,0].hist(self.insights['domain_diversity'], bins=20, color='#F0E68C', alpha=0.7)
        axes[0,0].set_title('ãƒ‰ãƒ¡ã‚¤ãƒ³å¤šæ§˜æ€§åˆ†å¸ƒ')
        axes[0,0].set_xlabel('ãƒ‰ãƒ¡ã‚¤ãƒ³å¤šæ§˜æ€§')
        axes[0,0].set_ylabel('é »åº¦')
        
        # 3-2. ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°åˆ†å¸ƒ
        axes[0,1].hist(self.insights['cross_domain_count'], bins=20, color='#20B2AA', alpha=0.7)
        axes[0,1].set_title('ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°åˆ†å¸ƒ')
        axes[0,1].set_xlabel('ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°')
        axes[0,1].set_ylabel('é »åº¦')
        
        # 3-3. å¤šæ§˜æ€§ vs GED
        axes[1,0].scatter(self.insights['domain_diversity'], self.insights['ged_value'], 
                         alpha=0.6, color='#FF69B4')
        axes[1,0].set_title('ãƒ‰ãƒ¡ã‚¤ãƒ³å¤šæ§˜æ€§ vs GEDå€¤')
        axes[1,0].set_xlabel('ãƒ‰ãƒ¡ã‚¤ãƒ³å¤šæ§˜æ€§')
        axes[1,0].set_ylabel('GEDå€¤')
        
        # 3-4. ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³æ•° vs ä¿¡é ¼åº¦
        axes[1,1].scatter(self.insights['cross_domain_count'], self.insights['confidence'], 
                         alpha=0.6, color='#32CD32')
        axes[1,1].set_title('ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³æ•° vs ä¿¡é ¼åº¦')
        axes[1,1].set_xlabel('ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°')
        axes[1,1].set_ylabel('ä¿¡é ¼åº¦')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "03_cross_domain_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 4. ãƒ™ã‚¯ãƒˆãƒ«å¾©å…ƒåˆ†æ
        if 'vector_reconstruction' in analyses:
            fig, axes = plt.subplots(2, 2, figsize=(14, 10))
            fig.suptitle('ãƒ™ã‚¯ãƒˆãƒ«è¨€èªå¾©å…ƒåˆ†æ', fontsize=16, fontweight='bold')
            
            # 4-1. æŠ½è±¡åº¦åˆ†å¸ƒ
            abstraction_dist = analyses['vector_reconstruction']['abstraction_distribution']
            axes[0,0].pie(abstraction_dist.values, labels=abstraction_dist.index, autopct='%1.1f%%')
            axes[0,0].set_title('æŠ½è±¡åº¦ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ')
            
            # 4-2. é›†ç´„ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ
            aggregation_dist = analyses['vector_reconstruction']['aggregation_distribution']
            axes[0,1].pie(aggregation_dist.values, labels=aggregation_dist.index, autopct='%1.1f%%')
            axes[0,1].set_title('é›†ç´„ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ')
            
            # 4-3. æŠ½è±¡åº¦åˆ¥å“è³ª
            abstraction_quality = analyses['vector_reconstruction']['abstraction_quality']
            if len(abstraction_quality) > 0:
                x_pos = range(len(abstraction_quality))
                axes[1,0].bar(x_pos, abstraction_quality['confidence'], color='#FFD700', alpha=0.7)
                axes[1,0].set_title('æŠ½è±¡åº¦åˆ¥å¹³å‡ä¿¡é ¼åº¦')
                axes[1,0].set_xticks(x_pos)
                axes[1,0].set_xticklabels(abstraction_quality.index, rotation=45)
                axes[1,0].set_ylabel('å¹³å‡ä¿¡é ¼åº¦')
                
                axes[1,1].bar(x_pos, abstraction_quality['ged_value'], color='#FF7F50', alpha=0.7)
                axes[1,1].set_title('æŠ½è±¡åº¦åˆ¥å¹³å‡GEDå€¤')
                axes[1,1].set_xticks(x_pos)
                axes[1,1].set_xticklabels(abstraction_quality.index, rotation=45)
                axes[1,1].set_ylabel('å¹³å‡GEDå€¤')
            
            plt.tight_layout()
            plt.savefig(self.output_dir / "04_vector_reconstruction_analysis.png", dpi=300, bbox_inches='tight')
            plt.close()
        
        print(f"   âœ… å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: {self.output_dir}")
    
    def generate_comprehensive_report(self, analyses: Dict[str, Any]) -> str:
        """åŒ…æ‹¬çš„ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        print("\nğŸ“ åŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
        
        report = f"""# è©³ç´°ãƒ­ã‚°å®Ÿé¨“ åŒ…æ‹¬åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

## å®Ÿé¨“æ¦‚è¦
- **å®Ÿé¨“å**: {self.metadata['experiment_name']}
- **å®Ÿè¡Œæ—¥æ™‚**: {self.metadata['timestamp']}
- **ç·ã‚¨piã‚½ãƒ¼ãƒ‰æ•°**: {self.metadata['total_episodes']:,}
- **ç·æ´å¯Ÿæ•°**: {self.metadata['total_insights']:,}
- **æ´å¯Ÿæ¤œå‡ºç‡**: {self.metadata['insight_rate']:.1%}
- **å®Ÿé¨“æ™‚é–“**: {self.metadata['total_time_seconds']:.2f}ç§’
- **å‡¦ç†é€Ÿåº¦**: {self.metadata['avg_episodes_per_second']:.1f}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰/ç§’

## ğŸ¯ ä¸»è¦ãªç™ºè¦‹

### 1. æ´å¯Ÿæ¤œå‡ºæ€§èƒ½
- **æ´å¯Ÿæ¤œå‡ºç‡**: {analyses['insight_patterns']['basic_stats']['insight_rate']:.1%} (å„ªç§€)
- **å¹³å‡GEDå€¤**: {analyses['insight_patterns']['basic_stats']['avg_ged']:.3f}
- **å¹³å‡ä¿¡é ¼åº¦**: {analyses['insight_patterns']['basic_stats']['avg_confidence']:.3f}
- **å¹³å‡IGå€¤**: {analyses['insight_patterns']['basic_stats']['avg_ig']:.3f}

### 2. GEDæ€¥è½ç¾è±¡ã®åˆ†æ
"""
        
        # GEDæ€¥è½ç¾è±¡ã®è©³ç´°
        ged_drops = analyses['insight_patterns']['ged_drops']
        if ged_drops:
            report += f"""
- **æ€¥è½æ¤œå‡ºæ•°**: {len(ged_drops)}ä»¶
- **ä¸»è¦ãªæ€¥è½ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰**:
"""
            for i, drop in enumerate(ged_drops[:5]):  # ä¸Šä½5ä»¶
                report += f"  - ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰{drop['episode']}: {drop['prev_ged']:.3f} â†’ {drop['curr_ged']:.3f} (è½å·®: {drop['drop_magnitude']:.3f})\n"
        else:
            report += "\n- é¡•è‘—ãªGEDæ€¥è½ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ\n"
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†æ
        report += f"""
### 3. ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥æ´å¯Ÿåˆ†æ
**æœ€ã‚‚æ´»ç™ºãªãƒ‰ãƒ¡ã‚¤ãƒ³** (æ´å¯Ÿæ•°):
"""
        domain_stats = analyses['insight_patterns']['domain_stats']['insight_id']['count'].head(5)
        for domain, count in domain_stats.items():
            avg_confidence = analyses['insight_patterns']['domain_stats'].loc[domain, ('confidence', 'mean')]
            report += f"- **{domain}**: {count}ä»¶ (å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.3f})\n"
        
        # ç ”ç©¶é ˜åŸŸåˆ†æ
        report += f"""
**æœ€ã‚‚æ´»ç™ºãªç ”ç©¶é ˜åŸŸ** (æ´å¯Ÿæ•°):
"""
        research_stats = analyses['insight_patterns']['research_area_stats']['insight_id']['count'].head(5)
        for area, count in research_stats.items():
            avg_conf = analyses['insight_patterns']['research_area_stats'].loc[area, ('confidence', 'mean')]
            report += f"- **{area}**: {count}ä»¶ (å¹³å‡ä¿¡é ¼åº¦: {avg_conf:.3f})\n"
        
        # éæ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ†æ
        non_insight = analyses['non_insight_episodes']
        report += f"""
### 4. éæ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ†æ
- **éæ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°**: {non_insight['count']}ä»¶ ({non_insight['rate']:.1%})
- **æ´å¯Ÿãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®åŠ¹ç‡æ€§**: {1-non_insight['rate']:.1%}
- **é‡è¦ãªç™ºè¦‹**: éæ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å¤šãã¯æ—¢çŸ¥ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨é«˜ã„é¡ä¼¼åº¦ã‚’æŒã¤ãŸã‚ã€å†…ç™ºçš„å ±é…¬ãŒä½ä¸‹ã—æ´å¯Ÿç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã€‚ã“ã‚Œã¯å‹•çš„RAGæ§‹ç¯‰æ™‚ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã«åŠ¹æœçš„ã«å¯„ä¸ã—ã¦ã„ã‚‹ã€‚
"""
        
        # TopKåˆ†æï¼ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼‰
        if 'topk_similarity' in analyses and analyses['topk_similarity']:
            topk_stats = analyses['topk_similarity']['similarity_stats']
            report += f"""
### 5. TopKé¡ä¼¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ†æ
- **å¹³å‡é¡ä¼¼åº¦**: {topk_stats['avg_similarity']:.3f}
- **é¡ä¼¼åº¦ç¯„å›²**: {topk_stats['min_similarity']:.3f} ï½ {topk_stats['max_similarity']:.3f}
- **é¡ä¼¼åº¦æ¨™æº–åå·®**: {topk_stats['std_similarity']:.3f}
- **åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿æ•°**: {len(analyses['topk_similarity']['rank_df']) if 'rank_df' in analyses['topk_similarity'] else 0:,}ä»¶
"""
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨ªæ–­åˆ†æ
        cross_domain = analyses['cross_domain_insights']
        report += f"""
### 6. ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨ªæ–­æ´å¯Ÿåˆ†æ
- **å¹³å‡ãƒ‰ãƒ¡ã‚¤ãƒ³å¤šæ§˜æ€§**: {cross_domain['diversity_stats']['avg_domain_diversity']:.2f}
- **å¹³å‡ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°**: {cross_domain['cross_domain_stats']['avg_cross_domain_count']:.2f}
- **é«˜å¤šæ§˜æ€§æ´å¯Ÿ**: {cross_domain['high_diversity_insights']['count']}ä»¶ ({cross_domain['high_diversity_insights']['rate']:.1%})
  - é«˜å¤šæ§˜æ€§æ´å¯Ÿã®å¹³å‡ä¿¡é ¼åº¦: {cross_domain['high_diversity_insights']['avg_confidence']:.3f}
  - é«˜å¤šæ§˜æ€§æ´å¯Ÿã®å¹³å‡GED: {cross_domain['high_diversity_insights']['avg_ged']:.3f}
"""
        
        # ãƒ™ã‚¯ãƒˆãƒ«å¾©å…ƒåˆ†æ
        if 'vector_reconstruction' in analyses:
            vector_analysis = analyses['vector_reconstruction']
            report += f"""
### 7. ãƒ™ã‚¯ãƒˆãƒ«è¨€èªå¾©å…ƒåˆ†æ
**æŠ½è±¡åº¦ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ**:
"""
            for level, count in vector_analysis['abstraction_distribution'].items():
                report += f"- **{level}**: {count}ä»¶ ({count/len(self.insights):.1%})\n"
            
            report += f"""
**é›†ç´„ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ**:
"""
            for agg_type, count in vector_analysis['aggregation_distribution'].items():
                report += f"- **{agg_type}**: {count}ä»¶ ({count/len(self.insights):.1%})\n"
            
            # æŠ½è±¡åº¦åˆ¥å“è³ª
            if len(vector_analysis['abstraction_quality']) > 0:
                report += f"""
**æŠ½è±¡åº¦åˆ¥å“è³ª**:
"""
                for level, row in vector_analysis['abstraction_quality'].iterrows():
                    report += f"- **{level}**: å¹³å‡ä¿¡é ¼åº¦{row['confidence']:.3f}, å¹³å‡GED{row['ged_value']:.3f}\n"
        
        # çµè«–ã¨æ¨å¥¨äº‹é …
        report += f"""
## ğŸ” è©³ç´°åˆ†æçµæœã®è€ƒå¯Ÿ

### å®Ÿé¨“æˆåŠŸè¦å› 
1. **é«˜ã„æ´å¯Ÿæ¤œå‡ºç‡**: {analyses['insight_patterns']['basic_stats']['insight_rate']:.1%}ã®æ¤œå‡ºç‡ã¯éå¸¸ã«å„ªç§€
2. **å®‰å®šã—ãŸGEDå€¤**: å¹³å‡{analyses['insight_patterns']['basic_stats']['avg_ged']:.3f}ã®é©åˆ‡ãªãƒ¬ãƒ™ãƒ«
3. **å¤šæ§˜ãªãƒ‰ãƒ¡ã‚¤ãƒ³å¯¾å¿œ**: {len(analyses['insight_patterns']['domain_stats'])}å€‹ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã§æ´å¯Ÿã‚’æ¤œå‡º
4. **åŠ¹ç‡çš„ãªå‡¦ç†**: {self.metadata['avg_episodes_per_second']:.1f}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰/ç§’ã®é«˜é€Ÿå‡¦ç†

### æ”¹å–„ã®æ©Ÿä¼š
1. **éæ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰**: {non_insight['rate']:.1%}ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§æ´å¯Ÿæœªæ¤œå‡ºã ãŒã€ã“ã‚Œã‚‰ã¯ä¸»ã«æ—¢çŸ¥ãƒ‡ãƒ¼ã‚¿ã¨ã®é«˜é¡ä¼¼åº¦ã«ã‚ˆã‚Šå†…ç™ºçš„å ±é…¬ãŒåƒã‹ãšã€åŠ¹ç‡çš„ãªãƒ¡ãƒ¢ãƒªç®¡ç†ã«è²¢çŒ®ã—ã¦ã„ã‚‹
2. **GEDæ€¥è½ç¾è±¡**: {len(ged_drops)}ä»¶ã®æ€¥è½ã‚’è©³ç´°èª¿æŸ»ãŒå¿…è¦
3. **ãƒ‰ãƒ¡ã‚¤ãƒ³é–“ãƒãƒ©ãƒ³ã‚¹**: ç‰¹å®šãƒ‰ãƒ¡ã‚¤ãƒ³ã«æ´å¯ŸãŒé›†ä¸­ã—ã¦ã„ã‚‹å‚¾å‘

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
1. éæ´å¯Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ç‰¹å¾´é‡åˆ†æï¼ˆæ—¢çŸ¥ãƒ‡ãƒ¼ã‚¿ã¨ã®é¡ä¼¼åº¦é–¢ä¿‚ã®è©³ç´°æ¤œè¨¼å«ã‚€ï¼‰
2. GEDæ€¥è½è¦å› ã®è©³ç´°èª¿æŸ»
3. ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨ªæ–­æ´å¯Ÿã®è³ªçš„è©•ä¾¡
4. æ´å¯Ÿã®å®Ÿç”¨æ€§è©•ä¾¡

## ğŸ“Š åˆ†æãƒ‡ãƒ¼ã‚¿è©³ç´°

### ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
- `01_comprehensive_insight_analysis.png`: æ´å¯Ÿãƒ‘ã‚¿ãƒ¼ãƒ³ç·åˆåˆ†æ
- `02_topk_similarity_analysis.png`: TopKé¡ä¼¼æ€§åˆ†æ (ãƒ‡ãƒ¼ã‚¿ã‚ã‚‹å ´åˆ)
- `03_cross_domain_analysis.png`: ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨ªæ–­åˆ†æ
- `04_vector_reconstruction_analysis.png`: ãƒ™ã‚¯ãƒˆãƒ«å¾©å…ƒåˆ†æ
- `05_comprehensive_analysis_report.md`: æœ¬ãƒ¬ãƒãƒ¼ãƒˆ

---
*ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return report
    
    def run_comprehensive_analysis(self):
        """åŒ…æ‹¬çš„åˆ†æã®å®Ÿè¡Œ"""
        print("ğŸš€ è©³ç´°ãƒ­ã‚°å®Ÿé¨“ã®åŒ…æ‹¬åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        
        # å„ç¨®åˆ†æã‚’å®Ÿè¡Œ
        analyses = {}
        
        analyses['insight_patterns'] = self.analyze_insight_patterns()
        analyses['non_insight_episodes'] = self.analyze_non_insight_episodes()
        analyses['topk_similarity'] = self.analyze_topk_similarity()
        analyses['cross_domain_insights'] = self.analyze_cross_domain_insights() 
        analyses['vector_reconstruction'] = self.analyze_vector_reconstruction()
        
        # å¯è¦–åŒ–ä½œæˆ
        self.create_visualizations(analyses)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = self.generate_comprehensive_report(analyses)
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_path = self.output_dir / "05_comprehensive_analysis_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # åˆ†æçµæœã®JSONä¿å­˜ï¼ˆã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢å¼ã«å¤‰æ›ï¼‰
        def convert_to_serializable(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, pd.Series):
                return obj.to_dict()
            elif isinstance(obj, pd.DataFrame):
                return obj.to_dict()
            elif isinstance(obj, dict):
                return {str(k): convert_to_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_serializable(item) for item in obj]
            else:
                return obj
        
        serializable_analyses = convert_to_serializable({
            'insight_patterns': {
                'basic_stats': analyses['insight_patterns']['basic_stats'],
                'ged_drops_count': len(analyses['insight_patterns']['ged_drops'])
            },
            'non_insight_episodes': {
                'count': analyses['non_insight_episodes']['count'],
                'rate': analyses['non_insight_episodes']['rate']
            },
            'topk_similarity': analyses['topk_similarity']['similarity_stats'] if analyses['topk_similarity'] else {},
            'cross_domain_insights': analyses['cross_domain_insights'],
            'vector_reconstruction': {
                'abstraction_distribution': dict(analyses['vector_reconstruction']['abstraction_distribution']),
                'aggregation_distribution': dict(analyses['vector_reconstruction']['aggregation_distribution'])
            }
        })
        
        analysis_path = self.output_dir / "06_analysis_summary.json"
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_analyses, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… åŒ…æ‹¬åˆ†æå®Œäº†!")
        print(f"   ğŸ“Š å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«: {self.output_dir}/*.png")
        print(f"   ğŸ“ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
        print(f"   ğŸ“„ åˆ†æãƒ‡ãƒ¼ã‚¿: {analysis_path}")
        
        return analyses


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        # åˆ†æã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        analyzer = ComprehensiveDetailedAnalyzer()
        
        # åŒ…æ‹¬åˆ†æå®Ÿè¡Œ
        results = analyzer.run_comprehensive_analysis()
        
        print("\nğŸ‰ è©³ç´°ãƒ­ã‚°å®Ÿé¨“ã®åŒ…æ‹¬åˆ†æãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ!")
        print("   å…¨ã¦ã®åˆ†æçµæœã€å¯è¦–åŒ–ã€ãƒ¬ãƒãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚")
        
    except Exception as e:
        print(f"\nâŒ åˆ†æå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    return True


if __name__ == "__main__":
    main()
