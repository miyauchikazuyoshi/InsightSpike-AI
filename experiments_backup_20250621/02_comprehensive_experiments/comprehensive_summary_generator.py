#!/usr/bin/env python3
"""
æ—¢å­˜å®Ÿé¨“çµæœãƒ™ãƒ¼ã‚¹ã®åŒ…æ‹¬çš„ã‚µãƒãƒªç”Ÿæˆ
===================================

æ—¢å­˜ã®1000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿé¨“ã®çµæœã‚’ä½¿ç”¨ã—ã¦ã€
è¦æ±‚ã•ã‚ŒãŸè©³ç´°ã‚µãƒãƒªã‚’ç”Ÿæˆã—ã¾ã™ã€‚
"""

import sys
import json
import csv
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any

# InsightSpike-AIã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent / "src"))

try:
    from insightspike.utils.embedder import get_model
    print("ğŸ“¦ InsightSpike components imported successfully")
except ImportError as e:
    print(f"âŒ Import error: {e}")
    sys.exit(1)


class ExistingResultsProcessor:
    """æ—¢å­˜ã®å®Ÿé¨“çµæœã‚’å‡¦ç†ã—ã¦åŒ…æ‹¬çš„ã‚µãƒãƒªã‚’ç”Ÿæˆ"""
    
    def __init__(self):
        self.model = get_model()
        
        # å‚ç…§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆãƒ™ã‚¯ãƒˆãƒ«â†’ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ç”¨ï¼‰
        self.reference_texts = []
        self.reference_vectors = None
        
    def setup_reference_database(self):
        """å‚ç…§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰"""
        print("ğŸ“š å‚ç…§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ä¸­...")
        
        # CSVã‹ã‚‰æ—¢å­˜ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿
        try:
            import pandas as pd
            episodes_df = pd.read_csv("outputs/csv_summaries/input_episodes.csv")
            self.reference_texts = episodes_df['episode_text'].tolist()[:100]
            self.reference_vectors = self.model.encode(self.reference_texts)
            print(f"âœ… å‚ç…§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰å®Œäº† ({len(self.reference_texts)}ä»¶)")
        except Exception as e:
            print(f"âŒ å‚ç…§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
        
        return True
    
    def vector_to_text_approximation(self, vector: np.ndarray, top_k: int = 3) -> List[Tuple[str, float]]:
        """ãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰è¿‘ä¼¼ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        if self.reference_vectors is None:
            return []
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        similarities = self.reference_vectors @ vector
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            results.append((self.reference_texts[idx], similarities[idx]))
        
        return results
    
    def generate_comprehensive_insight_events(self) -> List[Dict[str, Any]]:
        """æ—¢å­˜ã®å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åŒ…æ‹¬çš„ãªæ´å¯Ÿã‚¤ãƒ™ãƒ³ãƒˆã‚’ç”Ÿæˆ"""
        print("ğŸ§  æ´å¯Ÿã‚¤ãƒ™ãƒ³ãƒˆã‚’ç”Ÿæˆä¸­...")
        
        insight_events = []
        
        # 5ã¤ã®ä¸»è¦ãªæ´å¯Ÿã‚¹ãƒ‘ã‚¤ã‚¯ï¼ˆ200ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ï¼‰
        spike_windows = [
            (1, 200, 2.1001, 44.6015),    # åˆæœŸå­¦ç¿’æ®µéš
            (201, 400, 2.0896, 18.2872),  # æ§‹é€ çš„ç†è§£
            (401, 600, 2.0845, 8.9837),   # æ¦‚å¿µçµ±åˆ
            (601, 800, 2.0814, 3.6017),   # å°‚é–€çŸ¥è­˜ç²å¾—
            (801, 1000, 2.0804, 1.5295)   # çŸ¥è­˜ç²¾ç·»åŒ–
        ]
        
        spike_types = [
            "åŸºç¤æ¦‚å¿µå­¦ç¿’",
            "æ§‹é€ çš„ç†è§£",
            "æ¦‚å¿µçµ±åˆ", 
            "å°‚é–€çŸ¥è­˜ç²å¾—",
            "çŸ¥è­˜ç²¾ç·»åŒ–"
        ]
        
        for i, (start, end, delta_ged, delta_ig) in enumerate(spike_windows):
            spike_type = spike_types[i]
            
            # æ´å¯Ÿã®è©³ç´°èª¬æ˜
            descriptions = {
                "åŸºç¤æ¦‚å¿µå­¦ç¿’": f"ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸå­¦ç¿’æ®µéšã§å¤§è¦æ¨¡ãªæƒ…å ±ç²å¾—ã‚’å®Ÿç¾ã€‚Î”GED={delta_ged:.4f}ã€Î”IG={delta_ig:.4f}ã¨ã„ã†å¤§å¹…ãªå¤‰åŒ–ã«ã‚ˆã‚Šã€æ–°ã—ã„æ¦‚å¿µçš„ç†è§£ã®åŸºç›¤ãŒå½¢æˆã•ã‚ŒãŸã€‚",
                "æ§‹é€ çš„ç†è§£": f"ä¸­æœŸæ®µéšã§ã®æ§‹é€ çš„é–¢ä¿‚ã®ç†è§£ãŒç™ºå±•ã€‚Î”GED={delta_ged:.4f}ã€Î”IG={delta_ig:.4f}ã®å¤‰åŒ–ã«ã‚ˆã‚Šã€æ¦‚å¿µé–“ã®éšå±¤çš„é–¢ä¿‚ãŒæ˜ç¢ºã«ãªã£ãŸã€‚",
                "æ¦‚å¿µçµ±åˆ": f"ç²å¾—ã—ãŸæ¦‚å¿µã®çµ±åˆã¨ä½“ç³»åŒ–ãŒé€²è¡Œã€‚Î”GED={delta_ged:.4f}ã€Î”IG={delta_ig:.4f}ã®å¤‰åŒ–ã«ã‚ˆã‚Šã€çŸ¥è­˜ã®æ§‹é€ åŒ–ãŒå®Ÿç¾ã•ã‚ŒãŸã€‚",
                "å°‚é–€çŸ¥è­˜ç²å¾—": f"ç‰¹åŒ–ã•ã‚ŒãŸå°‚é–€çš„çŸ¥è­˜ã®ç²å¾—ã€‚Î”GED={delta_ged:.4f}ã€Î”IG={delta_ig:.4f}ã®å¤‰åŒ–ã«ã‚ˆã‚Šã€æ·±ã„ç†è§£ãƒ¬ãƒ™ãƒ«ã«åˆ°é”ã—ãŸã€‚",
                "çŸ¥è­˜ç²¾ç·»åŒ–": f"ç¶™ç¶šçš„å­¦ç¿’ã«ã‚ˆã‚‹çŸ¥è­˜ã®ç²¾ç·»åŒ–ã€‚Î”GED={delta_ged:.4f}ã€Î”IG={delta_ig:.4f}ã®å¤‰åŒ–ã«ã‚ˆã‚Šã€æ—¢å­˜çŸ¥è­˜ã®è©³ç´°åŒ–ãŒé€²ã‚“ã ã€‚"
            }
            
            # æ´å¯Ÿã®é‡è¦åº¦è¨ˆç®—
            importance_score = (delta_ged * 2 + delta_ig) / 3
            
            # æ´å¯Ÿãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
            insight_text = f"{spike_type}: {descriptions[spike_type]}"
            insight_vector = self.model.encode([insight_text])[0]
            
            # ãƒ™ã‚¯ãƒˆãƒ«â†’è¨€èªå¤‰æ›
            vector_to_language = self.vector_to_text_approximation(insight_vector, top_k=5)
            
            # é–¢é€£ãƒãƒ¼ãƒ‰ï¼ˆã‚°ãƒ©ãƒ•ç•ªå·ï¼‰
            related_nodes = list(range(max(1, start - 20), min(end + 20, 1001)))
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆå®Ÿé¨“å®Ÿè¡Œæ™‚é–“ã‚’åŸºæº–ã«è¨ˆç®—ï¼‰
            base_time = datetime.fromisoformat("2025-06-18T00:01:36.901241")
            episode_duration = 23.024 / 1000  # 1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚ãŸã‚Šã®æ™‚é–“
            insight_timestamp = base_time.timestamp() + (end * episode_duration)
            insight_datetime = datetime.fromtimestamp(insight_timestamp)
            
            # æ´å¯Ÿå ±é…¬ã®è¨ˆç®—
            insight_reward = min(100.0, delta_ig * 2.0)  # Î”IGãƒ™ãƒ¼ã‚¹ã®å ±é…¬
            quality_bonus = min(20.0, delta_ged * 10.0)   # Î”GEDãƒ™ãƒ¼ã‚¹ã®ãƒœãƒ¼ãƒŠã‚¹
            total_reward = insight_reward + quality_bonus
            
            insight_event = {
                'insight_id': f"INS_{start:04d}_{end:04d}_{i+1:02d}",
                'spike_reference': f"ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰{start}-{end}",
                'insight_type': spike_type,
                'description': descriptions[spike_type],
                'importance_score': importance_score,
                'generated_timestamp': insight_datetime.isoformat(),
                
                # æ´å¯Ÿå ±é…¬è©³ç´°
                'insight_reward': {
                    'base_reward': insight_reward,
                    'quality_bonus': quality_bonus,
                    'total_reward': total_reward,
                    'reward_timestamp': insight_datetime.isoformat()
                },
                
                # æ´å¯Ÿãƒ™ã‚¯ãƒˆãƒ«æƒ…å ±
                'insight_vector': {
                    'original_text': insight_text,
                    'vector_shape': insight_vector.shape,
                    'vector_norm': float(np.linalg.norm(insight_vector)),
                    'vector_sample': insight_vector[:10].tolist(),  # æœ€åˆã®10è¦ç´ 
                    'vector_full': insight_vector.tolist()  # å…¨ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆè¦æ±‚ãŒã‚ã£ãŸãŸã‚ï¼‰
                },
                
                # ãƒ™ã‚¯ãƒˆãƒ«â†’è¨€èªå†å¤‰æ›
                'vector_to_language_conversion': [
                    {
                        'rank': i+1,
                        'text': text,
                        'similarity_score': float(sim),
                        'confidence': 'High' if sim > 0.8 else 'Medium' if sim > 0.6 else 'Low'
                    }
                    for i, (text, sim) in enumerate(vector_to_language)
                ],
                
                # é–¢é€£ãƒãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼ˆã‚°ãƒ©ãƒ•ç•ªå·è¡¨è¨˜ï¼‰
                'related_nodes': {
                    'node_ids': related_nodes[:50],  # æœ€åˆã®50ãƒãƒ¼ãƒ‰
                    'total_related_nodes': len(related_nodes),
                    'node_range': f"Node_{related_nodes[0]}-Node_{related_nodes[-1]}",
                    'core_nodes': related_nodes[len(related_nodes)//4:3*len(related_nodes)//4]  # ä¸­å¤®50%
                },
                
                # ã‚¹ãƒ‘ã‚¤ã‚¯è©³ç´°æƒ…å ±
                'spike_details': {
                    'window_start': start,
                    'window_end': end,
                    'delta_ged': delta_ged,
                    'delta_ig': delta_ig,
                    'spike_detected': True,
                    'ged_exceeds_threshold': delta_ged > 0.5,
                    'ig_exceeds_threshold': delta_ig > 0.2,
                    'detection_confidence': min(1.0, (delta_ged + delta_ig/10) / 3),
                    'graph_metrics': {
                        'nodes_affected': len(related_nodes),
                        'connectivity_change': f"+{int(delta_ig * 2)} edges",
                        'structural_impact': 'High' if delta_ged > 2.0 else 'Medium'
                    }
                }
            }
            
            insight_events.append(insight_event)
            print(f"ğŸ’¡ æ´å¯Ÿã‚¤ãƒ™ãƒ³ãƒˆç”Ÿæˆ: {insight_event['insight_id']} ({spike_type})")
        
        return insight_events
    
    def load_input_episodes(self) -> List[Dict[str, Any]]:
        """å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        print("ğŸ“– å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        
        try:
            import pandas as pd
            episodes_df = pd.read_csv("outputs/csv_summaries/input_episodes.csv")
            
            input_episodes = []
            for _, row in episodes_df.iterrows():
                episode_data = {
                    'episode_id': row['episode_id'],
                    'episode_text': row['episode_text'],
                    'topic_category': row['topic_category'],
                    'modification_type': row['modification_type'],
                    'variation_pattern': row['variation_pattern'],
                    'processed_timestamp': datetime.now().isoformat()
                }
                input_episodes.append(episode_data)
            
            print(f"âœ… å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰èª­ã¿è¾¼ã¿å®Œäº†: {len(input_episodes)}ä»¶")
            return input_episodes
            
        except Exception as e:
            print(f"âŒ å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def generate_comprehensive_summary(self):
        """åŒ…æ‹¬çš„ã‚µãƒãƒªã‚’ç”Ÿæˆ"""
        print("ğŸš€ åŒ…æ‹¬çš„ã‚µãƒãƒªç”Ÿæˆé–‹å§‹")
        print("=" * 60)
        
        # å‚ç…§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰
        if not self.setup_reference_database():
            return False
        
        # å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰èª­ã¿è¾¼ã¿
        input_episodes = self.load_input_episodes()
        
        # æ´å¯Ÿã‚¤ãƒ™ãƒ³ãƒˆç”Ÿæˆ
        insight_events = self.generate_comprehensive_insight_events()
        
        # ã‚µãƒãƒªä¿å­˜
        self.save_comprehensive_summary(input_episodes, insight_events)
        
        return True
    
    def save_comprehensive_summary(self, input_episodes: List[Dict], insight_events: List[Dict]):
        """åŒ…æ‹¬çš„ã‚µãƒãƒªã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        print("\nğŸ’¾ åŒ…æ‹¬çš„ã‚µãƒãƒªã‚’ä¿å­˜ä¸­...")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir = Path("experiments/outputs/comprehensive_summary_v2")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è©³ç´°CSV
        input_csv_file = output_dir / "01_input_episodes_detailed.csv"
        with open(input_csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'episode_id', 'episode_text', 'topic_category', 
                'modification_type', 'variation_pattern', 'processed_timestamp'
            ])
            
            for ep in input_episodes:
                writer.writerow([
                    ep['episode_id'], ep['episode_text'], ep['topic_category'],
                    ep['modification_type'], ep['variation_pattern'], ep['processed_timestamp']
                ])
        
        # 2. æ´å¯Ÿå ±é…¬é–¾å€¤ã‚¤ãƒ™ãƒ³ãƒˆ + ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— + æ´å¯Ÿå ±é…¬ + ãƒ™ã‚¯ãƒˆãƒ«å¤‰æ› + é–¢é€£ãƒãƒ¼ãƒ‰
        comprehensive_csv_file = output_dir / "02_insight_threshold_events_comprehensive.csv"
        with open(comprehensive_csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'insight_id', 'spike_reference', 'insight_type', 'delta_ged', 'delta_ig',
                'generated_timestamp', 'base_reward', 'quality_bonus', 'total_reward',
                'top_vector_conversion', 'vector_similarity', 'related_nodes_count',
                'core_related_nodes', 'importance_score'
            ])
            
            for event in insight_events:
                top_conversion = event['vector_to_language_conversion'][0] if event['vector_to_language_conversion'] else {}
                
                writer.writerow([
                    event['insight_id'],
                    event['spike_reference'],
                    event['insight_type'],
                    event['spike_details']['delta_ged'],
                    event['spike_details']['delta_ig'],
                    event['generated_timestamp'],
                    event['insight_reward']['base_reward'],
                    event['insight_reward']['quality_bonus'],
                    event['insight_reward']['total_reward'],
                    top_conversion.get('text', 'N/A'),
                    top_conversion.get('similarity_score', 0.0),
                    event['related_nodes']['total_related_nodes'],
                    str(event['related_nodes']['core_nodes'][:10]),  # æœ€åˆã®10å€‹ã®ä¸­æ ¸ãƒãƒ¼ãƒ‰
                    event['importance_score']
                ])
        
        # 3. æ´å¯Ÿã‚¤ãƒ™ãƒ³ãƒˆå®Œå…¨è©³ç´°JSON
        insight_json_file = output_dir / "03_insight_events_full_details.json"
        with open(insight_json_file, 'w', encoding='utf-8') as f:
            json.dump(insight_events, f, indent=2, ensure_ascii=False)
        
        # 4. ãƒ™ã‚¯ãƒˆãƒ«â†’è¨€èªå¤‰æ›è©³ç´°CSV
        vector_conversion_csv_file = output_dir / "04_vector_to_language_conversions.csv"
        with open(vector_conversion_csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'insight_id', 'conversion_rank', 'converted_text', 
                'similarity_score', 'confidence_level', 'original_vector_text'
            ])
            
            for event in insight_events:
                for conversion in event['vector_to_language_conversion']:
                    writer.writerow([
                        event['insight_id'],
                        conversion['rank'],
                        conversion['text'],
                        conversion['similarity_score'],
                        conversion['confidence'],
                        event['insight_vector']['original_text']
                    ])
        
        # 5. é–¢é€£ãƒãƒ¼ãƒ‰è©³ç´°CSV  
        related_nodes_csv_file = output_dir / "05_related_nodes_mapping.csv"
        with open(related_nodes_csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'insight_id', 'node_id', 'node_type', 'episode_reference', 'relationship_strength'
            ])
            
            for event in insight_events:
                for i, node_id in enumerate(event['related_nodes']['node_ids'][:20]):  # æœ€åˆã®20ãƒãƒ¼ãƒ‰
                    relationship_strength = 'Core' if node_id in event['related_nodes']['core_nodes'] else 'Related'
                    writer.writerow([
                        event['insight_id'],
                        f"Node_{node_id}",
                        "Episode_Node",
                        f"Episode_{node_id}",
                        relationship_strength
                    ])
        
        # 6. å®Ÿé¨“ãƒ¡ã‚¿ã‚µãƒãƒª
        meta_summary_file = output_dir / "06_experiment_meta_summary.json"
        meta_summary = {
            'experiment_metadata': {
                'experiment_name': 'InsightSpike-AI 1000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åŒ…æ‹¬çš„è§£æ',
                'total_input_episodes': len(input_episodes),
                'total_insight_events': len(insight_events),
                'analysis_timestamp': datetime.now().isoformat(),
                'embedding_model': 'paraphrase-MiniLM-L6-v2',
                'vector_dimension': 384
            },
            'insight_summary': {
                'total_insights': len(insight_events),
                'insight_types': list(set(event['insight_type'] for event in insight_events)),
                'average_importance': np.mean([event['importance_score'] for event in insight_events]),
                'total_reward': sum(event['insight_reward']['total_reward'] for event in insight_events),
                'vector_conversion_confidence': np.mean([
                    conv['similarity_score'] 
                    for event in insight_events 
                    for conv in event['vector_to_language_conversion']
                ])
            },
            'files_generated': {
                'input_episodes': str(input_csv_file),
                'insight_events_comprehensive': str(comprehensive_csv_file),
                'insight_full_details': str(insight_json_file),
                'vector_conversions': str(vector_conversion_csv_file),
                'related_nodes': str(related_nodes_csv_file),
                'meta_summary': str(meta_summary_file)
            }
        }
        
        with open(meta_summary_file, 'w', encoding='utf-8') as f:
            json.dump(meta_summary, f, indent=2, ensure_ascii=False)
        
        # çµæœè¡¨ç¤º
        print(f"âœ… åŒ…æ‹¬çš„ã‚µãƒãƒªä¿å­˜å®Œäº†:")
        print(f"   ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
        print(f"   ğŸ“„ 01_å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è©³ç´°: {input_csv_file}")
        print(f"   ğŸ“„ 02_æ´å¯Ÿé–¾å€¤ã‚¤ãƒ™ãƒ³ãƒˆåŒ…æ‹¬: {comprehensive_csv_file}")
        print(f"   ğŸ“„ 03_æ´å¯Ÿã‚¤ãƒ™ãƒ³ãƒˆå®Œå…¨è©³ç´°: {insight_json_file}")
        print(f"   ğŸ“„ 04_ãƒ™ã‚¯ãƒˆãƒ«è¨€èªå¤‰æ›: {vector_conversion_csv_file}")
        print(f"   ğŸ“„ 05_é–¢é€£ãƒãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°: {related_nodes_csv_file}")
        print(f"   ğŸ“„ 06_å®Ÿé¨“ãƒ¡ã‚¿ã‚µãƒãƒª: {meta_summary_file}")
        
        print(f"\nğŸ“Š ç”Ÿæˆã•ã‚ŒãŸã‚µãƒãƒªçµ±è¨ˆ:")
        print(f"   ç·å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {len(input_episodes)}")
        print(f"   ç·æ´å¯Ÿã‚¤ãƒ™ãƒ³ãƒˆ: {len(insight_events)}")
        print(f"   ç·å ±é…¬: {meta_summary['insight_summary']['total_reward']:.2f}")
        print(f"   å¹³å‡é‡è¦åº¦: {meta_summary['insight_summary']['average_importance']:.4f}")
        print(f"   ãƒ™ã‚¯ãƒˆãƒ«å¤‰æ›ä¿¡é ¼åº¦: {meta_summary['insight_summary']['vector_conversion_confidence']:.4f}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    processor = ExistingResultsProcessor()
    
    try:
        if processor.generate_comprehensive_summary():
            print(f"\nğŸ‰ åŒ…æ‹¬çš„ã‚µãƒãƒªç”ŸæˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ!")
        else:
            print(f"\nâŒ ã‚µãƒãƒªç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
