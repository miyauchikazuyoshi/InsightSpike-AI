#!/usr/bin/env python3
"""
ãƒ­ãƒ¼ã‚«ãƒ«ã§Colabå®Ÿé¨“ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å†…å®¹ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import sys
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import torch
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®srcãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

def test_basic_imports():
    """åŸºæœ¬çš„ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ¯ åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒ†ã‚¹ãƒˆ:")
    print(f"   NumPy: {np.__version__}")
    print(f"   PyTorch: {torch.__version__}")
    print(f"   Pandas: {pd.__version__}")
    print(f"   Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    return True

def test_insightspike_imports():
    """InsightSpikeã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ãƒ†ã‚¹ãƒˆ"""
    try:
        from insightspike.core.rag_system import SimpleRAGSystem
        print("âœ… InsightSpike-AI: SimpleRAGSystem ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
        
        # ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
        system = SimpleRAGSystem()
        print("âœ… InsightSpike-AI: ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ InsightSpike-AI ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

def create_fallback_classes():
    """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®åŸºæœ¬ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ"""
    print("ğŸ”§ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªå®Ÿé¨“ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ")
    
    class SimpleGridWorld:
        def __init__(self, size=8, num_obstacles=5):
            self.size = size
            self.grid = np.zeros((size, size))
            
            # ãƒ©ãƒ³ãƒ€ãƒ ã«éšœå®³ç‰©ã‚’é…ç½®
            obstacles = np.random.choice(size*size, num_obstacles, replace=False)
            for obs in obstacles:
                row, col = divmod(obs, size)
                self.grid[row, col] = -1
            
            # ã‚¹ã‚¿ãƒ¼ãƒˆã¨ã‚´ãƒ¼ãƒ«ã‚’è¨­å®š
            self.start_pos = (0, 0)
            self.goal_pos = (size-1, size-1)
            self.grid[self.goal_pos] = 1
            self.current_pos = self.start_pos
            
            self.state_space_size = size * size
            self.action_space_size = 4  # ä¸Šä¸‹å·¦å³
            
        def reset(self):
            self.current_pos = self.start_pos
            return self.current_pos
            
        def step(self, action):
            # ç°¡å˜ãªç§»å‹•ãƒ­ã‚¸ãƒƒã‚¯
            row, col = self.current_pos
            
            if action == 0:  # ä¸Š
                row = max(0, row - 1)
            elif action == 1:  # ä¸‹
                row = min(self.size - 1, row + 1)
            elif action == 2:  # å·¦
                col = max(0, col - 1)
            elif action == 3:  # å³
                col = min(self.size - 1, col + 1)
            
            new_pos = (row, col)
            
            # éšœå®³ç‰©ãƒã‚§ãƒƒã‚¯
            if self.grid[new_pos] == -1:
                new_pos = self.current_pos  # ç§»å‹•ã›ãš
            
            self.current_pos = new_pos
            
            # å ±é…¬ã®è¨ˆç®—
            if new_pos == self.goal_pos:
                reward = 100.0
                done = True
            else:
                reward = -1.0
                done = False
                
            return new_pos, reward, done, {}
    
    class IntrinsicMotivationAgent:
        def __init__(self, state_size, action_size, use_ged=True, use_ig=True):
            self.state_size = state_size
            self.action_size = action_size
            self.use_ged = use_ged
            self.use_ig = use_ig
            self.q_table = np.random.random((state_size, action_size)) * 0.1
            self.learning_rate = 0.1
            self.discount_factor = 0.95
            self.epsilon = 1.0
            self.epsilon_decay = 0.995
            self.epsilon_min = 0.01
            
        def act(self, state):
            if isinstance(state, tuple):
                state_idx = state[0] * int(np.sqrt(self.state_size)) + state[1]  # ã‚°ãƒªãƒƒãƒ‰ä½ç½®ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›
            else:
                state_idx = state
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
            state_idx = min(state_idx, self.state_size - 1)
                
            if np.random.random() < self.epsilon:
                return np.random.randint(self.action_size)
            else:
                return np.argmax(self.q_table[state_idx])
                
        def update_q_table(self, state, action, reward, next_state):
            if isinstance(state, tuple):
                state_idx = state[0] * int(np.sqrt(self.state_size)) + state[1]
            else:
                state_idx = state
                
            if isinstance(next_state, tuple):
                next_state_idx = next_state[0] * int(np.sqrt(self.state_size)) + next_state[1]
            else:
                next_state_idx = next_state
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
            state_idx = min(state_idx, self.state_size - 1)
            next_state_idx = min(next_state_idx, self.state_size - 1)
            
            # å†…ç™ºçš„å ±é…¬ã®è¨ˆç®—
            intrinsic_reward = 0.0
            if self.use_ged:
                intrinsic_reward += np.random.random() * 0.1  # ç°¡å˜ãªGEDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            if self.use_ig:
                intrinsic_reward += np.random.random() * 0.1   # ç°¡å˜ãªIGã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            
            total_reward = reward + intrinsic_reward
            
            # Q-learningæ›´æ–°
            current_q = self.q_table[state_idx, action]
            max_future_q = np.max(self.q_table[next_state_idx])
            new_q = current_q + self.learning_rate * (total_reward + self.discount_factor * max_future_q - current_q)
            self.q_table[state_idx, action] = new_q
            
            # epsilonæ¸›è¡°
            if self.epsilon > self.epsilon_min:
                self.epsilon *= self.epsilon_decay
    
    return SimpleGridWorld, IntrinsicMotivationAgent

def run_simple_experiment():
    """ç°¡å˜ãªå®Ÿé¨“ã‚’å®Ÿè¡Œã—ã¦ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ§ª ç°¡å˜ãªå®Ÿé¨“ã®å®Ÿè¡Œ:")
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ
    SimpleGridWorld, IntrinsicMotivationAgent = create_fallback_classes()
    
    # ç’°å¢ƒã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆ
    env = SimpleGridWorld(size=6, num_obstacles=3)
    
    configs = [
        {"name": "Full (Î”GED Ã— Î”IG)", "use_ged": True, "use_ig": True},
        {"name": "No GED (Î”IG only)", "use_ged": False, "use_ig": True},
        {"name": "No IG (Î”GED only)", "use_ged": True, "use_ig": False},
        {"name": "Baseline (No intrinsic)", "use_ged": False, "use_ig": False}
    ]
    
    results = {}
    
    for config in configs:
        agent = IntrinsicMotivationAgent(
            state_size=env.state_space_size,
            action_size=env.action_space_size,
            use_ged=config["use_ged"],
            use_ig=config["use_ig"]
        )
        
        # çŸ­ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ
        episodes = 50
        success_count = 0
        episode_lengths = []
        
        for episode in range(episodes):
            state = env.reset()
            episode_length = 0
            
            for step in range(100):  # æœ€å¤§100ã‚¹ãƒ†ãƒƒãƒ—
                action = agent.act(state)
                next_state, reward, done, _ = env.step(action)
                agent.update_q_table(state, action, reward, next_state)
                
                state = next_state
                episode_length += 1
                
                if done:
                    success_count += 1
                    break
            
            episode_lengths.append(episode_length)
        
        success_rate = success_count / episodes
        avg_episode_length = np.mean(episode_lengths)
        
        results[config["name"]] = {
            "success_rate": success_rate,
            "avg_episode_length": avg_episode_length
        }
        
        print(f"   {config['name']}: æˆåŠŸç‡ {success_rate:.3f}, å¹³å‡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•· {avg_episode_length:.1f}")
    
    return results

def create_visualization(results):
    """çµæœã®å¯è¦–åŒ–"""
    print("\nğŸ“ˆ çµæœã®å¯è¦–åŒ–:")
    
    configs = list(results.keys())
    success_rates = [results[config]["success_rate"] for config in configs]
    episode_lengths = [results[config]["avg_episode_length"] for config in configs]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # æˆåŠŸç‡ã®ãƒ—ãƒ­ãƒƒãƒˆ
    bars1 = ax1.bar(range(len(configs)), success_rates, 
                    color=sns.color_palette("husl", len(configs)), alpha=0.7)
    ax1.set_title('Success Rates by Configuration')
    ax1.set_ylabel('Success Rate')
    ax1.set_xticks(range(len(configs)))
    ax1.set_xticklabels([c.replace(" (", "\n(") for c in configs], fontsize=9)
    
    # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for i, rate in enumerate(success_rates):
        ax1.text(i, rate + 0.01, f'{rate:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·ã®ãƒ—ãƒ­ãƒƒãƒˆ
    bars2 = ax2.bar(range(len(configs)), episode_lengths,
                    color=sns.color_palette("husl", len(configs)), alpha=0.7)
    ax2.set_title('Average Episode Length by Configuration')
    ax2.set_ylabel('Episode Length')
    ax2.set_xticks(range(len(configs)))
    ax2.set_xticklabels([c.replace(" (", "\n(") for c in configs], fontsize=9)
    
    # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for i, length in enumerate(episode_lengths):
        ax2.text(i, length + 1, f'{length:.1f}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('colab_experiment_test_results.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("âœ… çµæœã‚’colab_experiment_test_results.pngã«ä¿å­˜")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ Colabå®Ÿé¨“ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 50)
    
    # åŸºæœ¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
    test_basic_imports()
    
    # InsightSpikeã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
    insightspike_available = test_insightspike_imports()
    
    if not insightspike_available:
        print("âš ï¸  InsightSpike-AIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨")
    
    # ç°¡å˜ãªå®Ÿé¨“ã‚’å®Ÿè¡Œ
    results = run_simple_experiment()
    
    # çµæœã®å¯è¦–åŒ–
    create_visualization(results)
    
    print("\nğŸ‰ ãƒ†ã‚¹ãƒˆå®Œäº†!")
    print("ğŸ“Š çµæœã‚µãƒãƒªãƒ¼:")
    for config, result in results.items():
        print(f"   {config}: æˆåŠŸç‡ {result['success_rate']:.3f}")
    
    print("\nğŸ’¡ å®Ÿéš›ã®Colabå®Ÿé¨“ã§ã¯ã€ã‚ˆã‚Šè©³ç´°ãªå†…ç™ºçš„å ±é…¬è¨ˆç®—ã¨ã‚ˆã‚Šå¤šãã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒå®Ÿè¡Œã•ã‚Œã¾ã™")

if __name__ == "__main__":
    # ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®š
    np.random.seed(42)
    torch.manual_seed(42)
    
    main()