#!/usr/bin/env python3
"""
é‡è¦åº¦åˆ†æãƒ»å®Ÿé¨“ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å®Ÿé¨“ã®é‡è¦åº¦ã‚’åˆ†æã—ã€å‰Šé™¤å€™è£œã‚’ç‰¹å®š
"""

import os
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple

def analyze_experiment_importance(experiments_dir: str = "experiments") -> Dict:
    """å®Ÿé¨“ã®é‡è¦åº¦ã‚’åˆ†æ"""
    
    results = {
        "critical_experiments": [],
        "important_experiments": [],
        "standard_experiments": [],
        "deprecated_candidates": [],
        "analysis_timestamp": datetime.now().isoformat()
    }
    
    # æœ€é‡è¦å®Ÿé¨“ï¼ˆé©å‘½çš„ç™ºè¦‹ã‚’ã—ãŸå®Ÿé¨“ï¼‰
    critical_files = [
        "01_realtime_insight_experiments/detailed_logging_realtime_experiment.py",
        "02_comprehensive_experiments/comprehensive_analysis_detailed_logging.py"
    ]
    
    # é‡è¦å®Ÿé¨“ï¼ˆç¾åœ¨ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨äº’æ›æ€§ãŒã‚ã‚Šã€ä¾¡å€¤ã®é«˜ã„å®Ÿé¨“ï¼‰
    important_patterns = [
        "comprehensive_experiment_framework.py",
        "objective_evaluation_framework.py",
        "rag_enhanced_experiment_framework.py",
        "large_scale",
        "integration",
        "performance"
    ]
    
    # å‰Šé™¤å€™è£œãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå¤ã„è¨­è¨ˆãƒ»ãƒ†ã‚¹ãƒˆãƒ»å®Ÿé¨“çš„ãªã‚‚ã®ï¼‰
    deprecated_patterns = [
        "test_",  # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
        "debug_",  # ãƒ‡ãƒãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ«
        "_old",   # æ˜ç¤ºçš„ã«å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«
        "_backup",  # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«
        "experimental_",  # å®Ÿé¨“çš„ãªãƒ•ã‚¡ã‚¤ãƒ«
        "prototype_",  # ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—
        "draft_"  # ä¸‹æ›¸ã
    ]
    
    # å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’èµ°æŸ»
    exp_path = Path(experiments_dir)
    
    for category_dir in exp_path.iterdir():
        if not category_dir.is_dir() or category_dir.name.startswith('.'):
            continue
            
        category_name = category_dir.name
        category_files = []
        
        for file_path in category_dir.rglob("*.py"):
            if file_path.is_file():
                rel_path = str(file_path.relative_to(exp_path))
                file_size = file_path.stat().st_size
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
                analysis = {
                    "path": rel_path,
                    "category": category_name,
                    "size": file_size,
                    "lines": count_lines(file_path),
                    "last_modified": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
                }
                
                # é‡è¦åº¦åˆ¤å®š
                if rel_path in critical_files:
                    analysis["importance"] = "CRITICAL"
                    analysis["reason"] = "Revolutionary discovery - 81.6% insight detection achieved"
                    results["critical_experiments"].append(analysis)
                    
                elif any(pattern in rel_path.lower() for pattern in important_patterns):
                    analysis["importance"] = "IMPORTANT"
                    analysis["reason"] = "Current architecture compatible, high value"
                    results["important_experiments"].append(analysis)
                    
                elif any(pattern in rel_path.lower() for pattern in deprecated_patterns):
                    analysis["importance"] = "DEPRECATED"
                    analysis["reason"] = "Test/debug/old design - candidate for removal"
                    results["deprecated_candidates"].append(analysis)
                    
                else:
                    analysis["importance"] = "STANDARD"
                    analysis["reason"] = "Standard experiment file"
                    results["standard_experiments"].append(analysis)
    
    return results

def count_lines(file_path: Path) -> int:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return sum(1 for _ in f)
    except:
        return 0

def generate_cleanup_report(analysis: Dict) -> str:
    """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    
    report = f"""# å®Ÿé¨“é‡è¦åº¦åˆ†æãƒ»ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ¬ãƒãƒ¼ãƒˆ

## ğŸ“Š åˆ†æã‚µãƒãƒªãƒ¼

| ã‚«ãƒ†ã‚´ãƒª | ãƒ•ã‚¡ã‚¤ãƒ«æ•° | åˆè¨ˆã‚µã‚¤ã‚º |
|----------|------------|------------|
| ğŸš¨ **CRITICAL** | {len(analysis['critical_experiments'])} | {sum(exp['size'] for exp in analysis['critical_experiments'])} bytes |
| â­ **IMPORTANT** | {len(analysis['important_experiments'])} | {sum(exp['size'] for exp in analysis['important_experiments'])} bytes |
| ğŸ“„ **STANDARD** | {len(analysis['standard_experiments'])} | {sum(exp['size'] for exp in analysis['standard_experiments'])} bytes |
| ğŸ—‘ï¸ **DEPRECATED** | {len(analysis['deprecated_candidates'])} | {sum(exp['size'] for exp in analysis['deprecated_candidates'])} bytes |

## ğŸš¨ **æœ€é‡è¦å®Ÿé¨“ï¼ˆCRITICALï¼‰** - çµ¶å¯¾ã«ä¿æŒ

"""
    
    for exp in analysis['critical_experiments']:
        report += f"- **{exp['path']}** ({exp['lines']} lines)\n"
        report += f"  - {exp['reason']}\n"
        report += f"  - ã‚µã‚¤ã‚º: {exp['size']} bytes\n\n"
    
    report += f"""## â­ **é‡è¦å®Ÿé¨“ï¼ˆIMPORTANTï¼‰** - ç¾åœ¨ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§æœ‰ç”¨

"""
    
    for exp in sorted(analysis['important_experiments'], key=lambda x: x['size'], reverse=True)[:10]:
        report += f"- **{exp['path']}** ({exp['lines']} lines)\n"
        report += f"  - {exp['reason']}\n\n"
    
    if len(analysis['important_experiments']) > 10:
        report += f"... ãŠã‚ˆã³ä»– {len(analysis['important_experiments']) - 10} ãƒ•ã‚¡ã‚¤ãƒ«\n\n"
    
    report += f"""## ğŸ—‘ï¸ **å‰Šé™¤å€™è£œï¼ˆDEPRECATEDï¼‰** - å¤ã„è¨­è¨ˆãƒ»ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«

"""
    
    total_deprecated_size = sum(exp['size'] for exp in analysis['deprecated_candidates'])
    
    for exp in analysis['deprecated_candidates']:
        report += f"- **{exp['path']}** ({exp['lines']} lines)\n"
        report += f"  - {exp['reason']}\n\n"
    
    report += f"""## ğŸ’¾ **ãƒ‡ã‚£ã‚¹ã‚¯ç¯€ç´„åŠ¹æœ**

å‰Šé™¤å€™è£œã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ã§ **{total_deprecated_size} bytes** ({total_deprecated_size/1024/1024:.1f} MB) ã®ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã‚’ç¯€ç´„ã§ãã¾ã™ã€‚

## ğŸ¯ **æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**

1. **CRITICALå®Ÿé¨“ã®å®Œå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**: è©³ç´°ãƒ­ã‚°å®Ÿé¨“ã¨ãã®åˆ†æçµæœ
2. **DEPRECATEDå‰Šé™¤**: å¤ã„è¨­è¨ˆã®ãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
3. **IMPORTANTå®Ÿé¨“ã®æ•´ç†**: ç¾åœ¨ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åˆã‚ã›ã¦æ›´æ–°
4. **STANDARDå®Ÿé¨“ã®è©•ä¾¡**: å€‹åˆ¥ã«ä¾¡å€¤ã‚’åˆ¤å®š

---
*åˆ†æå®Ÿè¡Œæ—¥æ™‚: {analysis['analysis_timestamp']}*
"""
    
    return report

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸ” å®Ÿé¨“é‡è¦åº¦åˆ†æã‚’é–‹å§‹...")
    
    # åˆ†æå®Ÿè¡Œ
    analysis = analyze_experiment_importance()
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = generate_cleanup_report(analysis)
    
    # çµæœä¿å­˜
    output_dir = Path("experiments/00_data_backups")
    output_dir.mkdir(exist_ok=True)
    
    # JSONä¿å­˜
    json_file = output_dir / f"experiment_importance_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)
    
    # Markdownãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    md_file = output_dir / f"cleanup_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ“Š åˆ†æå®Œäº†!")
    print(f"ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {md_file}")
    print(f"ğŸ“„ JSONä¿å­˜: {json_file}")
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print(f"\nğŸ“Š **ã‚µãƒãƒªãƒ¼**")
    print(f"ğŸš¨ CRITICAL: {len(analysis['critical_experiments'])} files")
    print(f"â­ IMPORTANT: {len(analysis['important_experiments'])} files")
    print(f"ğŸ“„ STANDARD: {len(analysis['standard_experiments'])} files")
    print(f"ğŸ—‘ï¸ DEPRECATED: {len(analysis['deprecated_candidates'])} files")
    
    deprecated_size = sum(exp['size'] for exp in analysis['deprecated_candidates'])
    print(f"ğŸ’¾ å‰Šé™¤å¯èƒ½ã‚µã‚¤ã‚º: {deprecated_size/1024/1024:.1f} MB")

if __name__ == "__main__":
    main()
