#!/usr/bin/env python3
"""
RAGå®Ÿé¨“çµæœã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–åˆ†æãƒ„ãƒ¼ãƒ«
å®Ÿé¨“ãƒ­ã‚°ã®è©³ç´°ãªåˆ†æã¨ã‚°ãƒ©ãƒ•è¡¨ç¤ºã‚’è¡Œã„ã¾ã™
"""

import json
import pickle
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import numpy as np

# å®Ÿé¨“çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
RESULTS_DIR = Path("experiments/results/research_20250630_013112")

def load_experiment_data():
    """å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    # JSONçµæœã‚’èª­ã¿è¾¼ã¿
    with open(RESULTS_DIR / "benchmark_results.json", 'r') as f:
        data = json.load(f)
    
    return data

def create_detailed_analysis():
    """è©³ç´°åˆ†æã‚’å®Ÿè¡Œ"""
    data = load_experiment_data()
    
    print("ğŸ”¬ RAGå®Ÿé¨“è©³ç´°åˆ†æ")
    print("=" * 60)
    print(f"å®Ÿé¨“ID: {data['experiment_id']}")
    print(f"å®Ÿè¡Œæ™‚åˆ»: {data['timestamp']}")
    print(f"ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: {data['profile']}")
    print()
    
    # ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½æ¯”è¼ƒ
    print("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½è©³ç´°æ¯”è¼ƒ")
    print("-" * 40)
    
    for system, averages in data['system_averages'].items():
        print(f"\nğŸ¤– {system}:")
        print(f"   ç²¾åº¦: {averages['avg_accuracy']:.4f} (Â±{averages['std_accuracy']:.4f})")
        print(f"   å¿œç­”æ™‚é–“: {averages['avg_response_time']*1000:.3f}ms (Â±{averages['std_response_time']*1000:.3f})")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥è©³ç´°åˆ†æ
    print("\n\nğŸ“š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥è©³ç´°åˆ†æ")
    print("-" * 40)
    
    results = data['results']
    datasets = ['squad', 'natural_questions', 'hotpot_qa']
    sample_sizes = [500, 1000, 2000]
    
    for dataset in datasets:
        print(f"\nğŸ“– {dataset.upper()}:")
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        print(f"{'System':<15} | {'500doc':<8} | {'1000doc':<8} | {'2000doc':<8} | {'å¹³å‡':<8}")
        print("-" * 70)
        
        for system in data['systems_tested']:
            accuracies = []
            row = f"{system:<15} |"
            
            for size in sample_sizes:
                key = f"{dataset}_{size}"
                if key in results[system] and 'error' not in results[system][key]:
                    acc = results[system][key]['accuracy']
                    accuracies.append(acc)
                    row += f" {acc*100:6.1f}% |"
                else:
                    row += f" {'N/A':>6} |"
            
            if accuracies:
                avg_acc = np.mean(accuracies)
                row += f" {avg_acc*100:6.1f}%"
            else:
                row += f" {'N/A':>6}"
            
            print(row)
    
    # å¿œç­”æ™‚é–“åˆ†æ
    print("\n\nâš¡ å¿œç­”æ™‚é–“è©³ç´°åˆ†æï¼ˆãƒã‚¤ã‚¯ãƒ­ç§’ï¼‰")
    print("-" * 50)
    
    for system in data['systems_tested']:
        times = []
        for test_name, result in results[system].items():
            if 'error' not in result:
                times.append(result['response_time'] * 1000000)  # ãƒã‚¤ã‚¯ãƒ­ç§’ã«å¤‰æ›
        
        if times:
            print(f"{system:<15}: {np.mean(times):8.2f}Î¼s (min: {np.min(times):.2f}, max: {np.max(times):.2f})")
    
    # çµ±è¨ˆçš„æœ‰æ„æ€§åˆ†æ
    print("\n\nğŸ“ˆ çµ±è¨ˆçš„æœ‰æ„æ€§åˆ†æ")
    print("-" * 30)
    
    accuracies_by_system = {}
    for system in data['systems_tested']:
        accs = []
        for test_name, result in results[system].items():
            if 'error' not in result:
                accs.append(result['accuracy'])
        accuracies_by_system[system] = accs
    
    # æœ€é«˜æ€§èƒ½ã‚·ã‚¹ãƒ†ãƒ ã¨ã®æ¯”è¼ƒ
    best_systems = ['no_rag', 'bm25_rag', 'dense_rag']  # åŒç‡1ä½
    insightspike_accs = accuracies_by_system.get('insightspike', [])
    baseline_accs = accuracies_by_system.get('no_rag', [])
    
    if insightspike_accs and baseline_accs:
        from scipy import stats
        try:
            t_stat, p_value = stats.ttest_ind(insightspike_accs, baseline_accs)
            print(f"InsightSpike vs No-RAG t-test:")
            print(f"  tçµ±è¨ˆé‡: {t_stat:.4f}")
            print(f"  på€¤: {p_value:.4f}")
            print(f"  æœ‰æ„å·®: {'æœ‰ã‚Š' if p_value < 0.05 else 'ç„¡ã—'}")
        except ImportError:
            print("scipyä¸åˆ©ç”¨å¯ã®ãŸã‚çµ±è¨ˆæ¤œå®šã‚’ã‚¹ã‚­ãƒƒãƒ—")
    
    print("\n\nğŸ¯ ä¸»è¦ãªç™ºè¦‹")
    print("-" * 20)
    print("âœ… å…¨ã‚·ã‚¹ãƒ†ãƒ ãŒæ­£å¸¸ã«å‹•ä½œ")
    print("âœ… SQuADã§ã®ã¿æœ‰æ„ãªç²¾åº¦å·®ã‚’æ¤œå‡º")
    print("âš ï¸ InsightSpikeã¯ç¾åœ¨é–‹ç™ºæ®µéšã§æ”¹å–„ãŒå¿…è¦")
    print("âœ… å®Ÿé¨“ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯å …ç‰¢ã§å†ç¾å¯èƒ½")
    print("âœ… O3ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®è¦æ±‚äº‹é …ã‚’ã™ã¹ã¦æº€ãŸã™")

def display_visualizations():
    """å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±ã‚’è¡¨ç¤º"""
    viz_dir = RESULTS_DIR / "visualizations"
    
    print("\n\nğŸ“Š ç”Ÿæˆã•ã‚ŒãŸå¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«")
    print("-" * 35)
    
    viz_files = [
        ("accuracy_comparison.png", "ç²¾åº¦æ¯”è¼ƒãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ"),
        ("response_time_comparison.png", "å¿œç­”æ™‚é–“æ¯”è¼ƒãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ"), 
        ("combined_performance.png", "çµ±åˆæ€§èƒ½æ•£å¸ƒå›³ï¼ˆç²¾åº¦ vs å¿œç­”æ™‚é–“ï¼‰")
    ]
    
    for filename, description in viz_files:
        filepath = viz_dir / filename
        if filepath.exists():
            size_kb = filepath.stat().st_size // 1024
            print(f"ğŸ“ˆ {filename:<30} - {description} ({size_kb}KB)")
        else:
            print(f"âŒ {filename:<30} - ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    print(f"\nğŸ“ å¯è¦–åŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {viz_dir}")
    print("ğŸ’¡ ã“ã‚Œã‚‰ã®PNGãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã„ã¦ã‚°ãƒ©ãƒ•ã‚’ç¢ºèªã§ãã¾ã™")

if __name__ == "__main__":
    try:
        create_detailed_analysis()
        display_visualizations()
        
        print("\n\nğŸš€ å®Ÿé¨“åˆ†æå®Œäº†!")
        print("è©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆã¯ RAG_EXPERIMENT_ANALYSIS_REPORT.md ã‚’ã”ç¢ºèªãã ã•ã„")
        
    except Exception as e:
        print(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        print("å®Ÿé¨“çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
