#!/usr/bin/env python3
"""
InsightSpike-AI å®Ÿé¨“ç®¡ç†CLI
==============================

åŒä¸€æ¡ä»¶ã§ã®å¯¾ç…§å®Ÿé¨“ã‚’å¯èƒ½ã«ã™ã‚‹åŒ…æ‹¬çš„ãªå®Ÿé¨“ç®¡ç†ãƒ„ãƒ¼ãƒ«
"""

import sys
import argparse
import json
import shutil
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
import sqlite3

# InsightSpike-AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))

try:
    from insightspike.core.config import get_config
    from insightspike.utils.embedder import get_model
    from insightspike.core.layers.layer2_memory_manager import L2MemoryManager
    from insightspike.core.learning.knowledge_graph_memory import KnowledgeGraphMemory
except ImportError as e:
    print(f"âŒ InsightSpike-AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    sys.exit(1)


class ExperimentCLI:
    """å®Ÿé¨“ç®¡ç†CLIã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.data_dir = Path("data")
        self.experiments_dir = Path("experiments")
        self.outputs_dir = Path("experiments/outputs")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 
        self.raw_dir = self.data_dir / "raw"
        self.processed_dir = self.data_dir / "processed"
        self.embedding_dir = self.data_dir / "embedding"
        self.logs_dir = self.data_dir / "logs"
        self.cache_dir = self.data_dir / "cache"
        
        # å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        for dir_path in [self.raw_dir, self.processed_dir, self.embedding_dir, 
                        self.logs_dir, self.cache_dir, self.outputs_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
    
    def create_experiment_session(self, session_name: str, description: str = "") -> str:
        """å®Ÿé¨“ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_id = f"{session_name}_{timestamp}"
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        session_dir = self.outputs_dir / session_id
        session_dir.mkdir(exist_ok=True)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ä¿å­˜
        session_info = {
            "session_id": session_id,
            "session_name": session_name,
            "description": description,
            "created_at": datetime.now().isoformat(),
            "status": "created",
            "experiments": []
        }
        
        with open(session_dir / "session_info.json", 'w', encoding='utf-8') as f:
            json.dump(session_info, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… å®Ÿé¨“ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ: {session_id}")
        print(f"ğŸ“ ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {session_dir}")
        
        return session_id
    
    def backup_data_state(self, backup_name: str) -> Path:
        """ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_id = f"{backup_name}_{timestamp}"
        backup_path = self.data_dir / "cache" / "backups" / backup_id
        backup_path.mkdir(parents=True, exist_ok=True)
        
        # é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        important_files = [
            "index.faiss",
            "graph_pyg.pt", 
            "insight_facts.db",
            "unknown_learning.db"
        ]
        
        backed_up_files = []
        for file_name in important_files:
            src = self.data_dir / file_name
            if src.exists():
                dst = backup_path / file_name
                shutil.copy2(src, dst)
                backed_up_files.append(file_name)
                print(f"ğŸ“¦ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {file_name}")
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±ã‚’ä¿å­˜
        backup_info = {
            "backup_id": backup_id,
            "backup_name": backup_name,
            "timestamp": datetime.now().isoformat(),
            "backed_up_files": backed_up_files,
            "backup_path": str(backup_path)
        }
        
        with open(backup_path / "backup_info.json", 'w', encoding='utf-8') as f:
            json.dump(backup_info, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {backup_id}")
        return backup_path
    
    def restore_data_state(self, backup_id: str) -> bool:
        """ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹ã‚’å¾©å…ƒ"""
        backup_path = self.data_dir / "cache" / "backups" / backup_id
        
        if not backup_path.exists():
            print(f"âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {backup_id}")
            return False
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±ã‚’èª­ã¿è¾¼ã¿
        backup_info_path = backup_path / "backup_info.json"
        if not backup_info_path.exists():
            print(f"âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {backup_info_path}")
            return False
        
        with open(backup_info_path, 'r', encoding='utf-8') as f:
            backup_info = json.load(f)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¾©å…ƒ
        restored_files = []
        for file_name in backup_info["backed_up_files"]:
            src = backup_path / file_name
            dst = self.data_dir / file_name
            if src.exists():
                shutil.copy2(src, dst)
                restored_files.append(file_name)
                print(f"ğŸ”„ å¾©å…ƒ: {file_name}")
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿å¾©å…ƒå®Œäº†: {backup_id}")
        print(f"   å¾©å…ƒãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(restored_files)}")
        return True
    
    def clean_data_folder(self, keep_structure: bool = True, preserve_graph: bool = True, preserve_index: bool = True) -> None:
        """dataãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        print("ğŸ§¹ dataãƒ•ã‚©ãƒ«ãƒ€ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹...")
        
        # å‰Šé™¤å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæ¡ä»¶ä»˜ãå‰Šé™¤ï¼‰
        cleanup_files = [
            "insight_facts.db", 
            "unknown_learning.db"
        ]
        
        # index.faissã®å‡¦ç†
        if not preserve_index:
            cleanup_files.append("index.faiss")
        else:
            print("ğŸ”’ index.faiss ã‚’ä¿æŒã—ã¾ã™")
        
        # graph_pyg.ptã®å‡¦ç†ã‚’åˆ†é›¢
        if not preserve_graph:
            cleanup_files.append("graph_pyg.pt")
        else:
            print("ğŸ”’ graph_pyg.pt ã‚’ä¿æŒã—ã¾ã™")
        
        # å‰Šé™¤å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä¸­èº«
        cleanup_dirs = ["cache", "processed"]
        
        removed_files = []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        for file_name in cleanup_files:
            file_path = self.data_dir / file_name
            if file_path.exists():
                file_path.unlink()
                removed_files.append(file_name)
                print(f"ğŸ—‘ï¸ å‰Šé™¤: {file_name}")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä¸­èº«å‰Šé™¤
        for dir_name in cleanup_dirs:
            dir_path = self.data_dir / dir_name
            if dir_path.exists():
                for item in dir_path.iterdir():
                    if item.is_file():
                        item.unlink()
                        removed_files.append(f"{dir_name}/{item.name}")
                    elif item.is_dir() and item.name != "backups":  # backupsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ä¿æŒ
                        shutil.rmtree(item)
                        removed_files.append(f"{dir_name}/{item.name}/")
                print(f"ğŸ—‘ï¸ ã‚¯ãƒªã‚¢: {dir_name}/")
        
        print(f"âœ… ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {len(removed_files)}é …ç›®å‰Šé™¤")
        
        if keep_structure:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’å†ä½œæˆ
            for dir_path in [self.raw_dir, self.processed_dir, self.embedding_dir, 
                            self.logs_dir, self.cache_dir]:
                dir_path.mkdir(exist_ok=True)
            print("ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’å†ä½œæˆ")
    
    def clean_temp_files(self) -> None:
        """å®Ÿé¨“ç”¨ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆé‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¿æŒï¼‰"""
        print("ğŸ§¹ å®Ÿé¨“ç”¨ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹...")
        
        # å®Ÿé¨“ä¸­ã«ä½œæˆã•ã‚Œã‚‹ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‰Šé™¤
        temp_files = [
            "insight_facts.db", 
            "unknown_learning.db"
        ]
        
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã¿ï¼‰
        temp_dirs = ["cache"]
        
        removed_files = []
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        for file_name in temp_files:
            file_path = self.data_dir / file_name
            if file_path.exists():
                file_path.unlink()
                removed_files.append(file_name)
                print(f"ğŸ—‘ï¸ å‰Šé™¤: {file_name}")
        
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä¸­èº«å‰Šé™¤
        for dir_name in temp_dirs:
            dir_path = self.data_dir / dir_name
            if dir_path.exists():
                for item in dir_path.iterdir():
                    if item.is_file():
                        item.unlink()
                        removed_files.append(f"{dir_name}/{item.name}")
                    elif item.is_dir() and item.name != "backups":
                        shutil.rmtree(item)
                        removed_files.append(f"{dir_name}/{item.name}/")
                print(f"ğŸ—‘ï¸ ã‚¯ãƒªã‚¢: {dir_name}/")
        
        print(f"âœ… ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {len(removed_files)}é …ç›®å‰Šé™¤")
        print("ğŸ”’ é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ä¿æŒ: graph_pyg.pt, index.faiss, index.json, episodes.json, processed/, raw/, samples/, embedding/")
    
    def build_initial_memory(self, episodes_count: int = 50, seed: int = 42) -> Dict[str, Any]:
        """åˆæœŸãƒ¡ãƒ¢ãƒª/ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰"""
        print(f"ğŸ§  åˆæœŸãƒ¡ãƒ¢ãƒªæ§‹ç¯‰é–‹å§‹ ({episodes_count}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰, seed={seed})")
        
        import random
        import numpy as np
        
        # ã‚·ãƒ¼ãƒ‰è¨­å®š
        random.seed(seed)
        np.random.seed(seed)
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        config = get_config()
        model = get_model()
        memory_manager = L2MemoryManager(dim=384)
        knowledge_graph = KnowledgeGraphMemory(embedding_dim=384, similarity_threshold=0.3)
        
        # åŸºç¤ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆï¼ˆå†ç¾å¯èƒ½ï¼‰
        research_areas = [
            "Large Language Models", "Computer Vision", "Reinforcement Learning",
            "Graph Neural Networks", "Federated Learning", "Explainable AI",
            "Multimodal Learning", "Few-shot Learning", "Transfer Learning",
            "Adversarial Machine Learning"
        ]
        
        activity_types = [
            "achieves breakthrough performance on",
            "introduces novel architecture for", 
            "demonstrates significant improvements in",
            "reveals new insights about",
            "establishes new benchmarks for"
        ]
        
        domains = [
            "medical diagnosis", "autonomous systems", "natural language processing",
            "computer vision", "robotics", "cybersecurity", "climate modeling",
            "drug discovery", "financial prediction", "educational technology"
        ]
        
        episodes = []
        start_time = time.time()
        
        for i in range(1, episodes_count + 1):
            research_area = research_areas[(i - 1) % len(research_areas)]
            activity_type = activity_types[(i - 1) % len(activity_types)]
            domain = domains[(i - 1) % len(domains)]
            
            text = f"Initial research in {research_area} {activity_type} {domain}, establishing foundational knowledge for future insights."
            
            # ãƒ¡ãƒ¢ãƒªã«ä¿å­˜
            memory_manager.store_episode(
                text=text,
                c_value=0.5,  # åˆæœŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¯ä¸­ç¨‹åº¦ã®é‡è¦åº¦
                metadata={
                    'id': i,
                    'type': 'initial',
                    'domain': domain,
                    'research_area': research_area,
                    'seed': seed
                }
            )
            
            episodes.append({
                'id': i,
                'text': text,
                'research_area': research_area,
                'activity_type': activity_type,
                'domain': domain,
                'type': 'initial',
                'timestamp': datetime.now().isoformat()
            })
            
            if i % 10 == 0:
                print(f"ğŸ“ {i}/{episodes_count}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ§‹ç¯‰å®Œäº†")
        
        build_time = time.time() - start_time
        
        # åˆæœŸçŠ¶æ…‹æƒ…å ±ã‚’ä¿å­˜
        initial_state_info = {
            "episodes_count": episodes_count,
            "seed": seed,
            "build_time_seconds": build_time,
            "timestamp": datetime.now().isoformat(),
            "episodes": episodes
        }
        
        # rawãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
        with open(self.raw_dir / f"initial_episodes_seed{seed}_count{episodes_count}.json", 'w', encoding='utf-8') as f:
            json.dump(initial_state_info, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… åˆæœŸãƒ¡ãƒ¢ãƒªæ§‹ç¯‰å®Œäº†!")
        print(f"   ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {episodes_count}")
        print(f"   æ§‹ç¯‰æ™‚é–“: {build_time:.2f}ç§’")
        print(f"   ã‚·ãƒ¼ãƒ‰å€¤: {seed}")
        print(f"   ãƒ‡ãƒ¼ã‚¿ä¿å­˜: data/raw/initial_episodes_seed{seed}_count{episodes_count}.json")
        
        return initial_state_info
    
    def list_backups(self) -> List[Dict[str, Any]]:
        """åˆ©ç”¨å¯èƒ½ãªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§"""
        backups_dir = self.data_dir / "cache" / "backups"
        if not backups_dir.exists():
            return []
        
        backups = []
        for backup_dir in backups_dir.iterdir():
            if backup_dir.is_dir():
                info_file = backup_dir / "backup_info.json"
                if info_file.exists():
                    with open(info_file, 'r', encoding='utf-8') as f:
                        backup_info = json.load(f)
                    backups.append(backup_info)
        
        # ä½œæˆæ—¥æ™‚ã§é™é †ã‚½ãƒ¼ãƒˆ
        backups.sort(key=lambda x: x["timestamp"], reverse=True)
        return backups
    
    def show_data_status(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹ã‚’è¡¨ç¤º"""
        status = {
            "timestamp": datetime.now().isoformat(),
            "data_files": {},
            "directories": {},
            "memory_info": {},
            "experiments_info": {}
        }
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®çŠ¶æ…‹
        data_files = ["index.faiss", "graph_pyg.pt", "insight_facts.db", "unknown_learning.db"]
        for file_name in data_files:
            file_path = self.data_dir / file_name
            if file_path.exists():
                stat = file_path.stat()
                status["data_files"][file_name] = {
                    "exists": True,
                    "size_bytes": stat.st_size,
                    "size_mb": stat.st_size / (1024 * 1024),
                    "modified": datetime.fromtimestamp(stat.st_mtime).isoformat()
                }
            else:
                status["data_files"][file_name] = {"exists": False}
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®çŠ¶æ…‹
        for dir_name in ["raw", "processed", "embedding", "logs", "cache"]:
            dir_path = self.data_dir / dir_name
            if dir_path.exists():
                files = list(dir_path.glob("*"))
                file_count = len([f for f in files if f.is_file()])
                dir_count = len([f for f in files if f.is_dir()])
                total_size = sum(f.stat().st_size for f in files if f.is_file())
                
                status["directories"][dir_name] = {
                    "exists": True,
                    "file_count": file_count,
                    "dir_count": dir_count,
                    "total_size_mb": total_size / (1024 * 1024)
                }
            else:
                status["directories"][dir_name] = {"exists": False}
        
        # ãƒ¡ãƒ¢ãƒªæƒ…å ±ï¼ˆDBã‹ã‚‰å–å¾—ï¼‰
        try:
            if (self.data_dir / "insight_facts.db").exists():
                conn = sqlite3.connect(self.data_dir / "insight_facts.db")
                cursor = conn.cursor()
                
                # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°
                cursor.execute("SELECT COUNT(*) FROM episodes")
                episode_count = cursor.fetchone()[0]
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = [row[0] for row in cursor.fetchall()]
                
                status["memory_info"] = {
                    "episodes_count": episode_count,
                    "database_tables": tables
                }
                conn.close()
        except Exception as e:
            status["memory_info"]["error"] = str(e)
        
        # æœ€è¿‘ã®å®Ÿé¨“æƒ…å ±
        try:
            if self.outputs_dir.exists():
                recent_experiments = []
                for exp_dir in self.outputs_dir.iterdir():
                    if exp_dir.is_dir() and not exp_dir.name.startswith('.'):
                        session_info_file = exp_dir / "session_info.json"
                        if session_info_file.exists():
                            with open(session_info_file, 'r', encoding='utf-8') as f:
                                session_info = json.load(f)
                            recent_experiments.append({
                                "session_id": exp_dir.name,
                                "created_at": session_info.get("created_at", "unknown"),
                                "experiment_count": len(list(exp_dir.glob("*/06_experiment_results.json")))
                            })
                
                # ä½œæˆæ—¥æ™‚ã§é™é †ã‚½ãƒ¼ãƒˆ
                recent_experiments.sort(key=lambda x: x.get("created_at", ""), reverse=True)
                status["experiments_info"]["recent_sessions"] = recent_experiments[:5]  # æœ€æ–°5ä»¶
                
        except Exception as e:
            status["experiments_info"]["error"] = str(e)
        
        return status
    
    def validate_data_integrity(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"""
        print("ğŸ” ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯é–‹å§‹...")
        
        integrity_report = {
            "timestamp": datetime.now().isoformat(),
            "checks": {},
            "warnings": [],
            "errors": [],
            "overall_status": "unknown"
        }
        
        # 1. å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        required_files = {
            "index.faiss": "FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹",
            "insight_facts.db": "æ´å¯Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹"
        }
        
        for file_name, description in required_files.items():
            file_path = self.data_dir / file_name
            if file_path.exists():
                integrity_report["checks"][file_name] = {"status": "OK", "description": description}
            else:
                integrity_report["errors"].append(f"å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_name} ({description})")
                integrity_report["checks"][file_name] = {"status": "MISSING", "description": description}
        
        # 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        try:
            if (self.data_dir / "insight_facts.db").exists():
                conn = sqlite3.connect(self.data_dir / "insight_facts.db")
                cursor = conn.cursor()
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = [row[0] for row in cursor.fetchall()]
                
                if "episodes" in tables:
                    cursor.execute("SELECT COUNT(*) FROM episodes")
                    episode_count = cursor.fetchone()[0]
                    integrity_report["checks"]["database_episodes"] = {
                        "status": "OK", 
                        "count": episode_count,
                        "description": f"ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ†ãƒ¼ãƒ–ãƒ« ({episode_count}ä»¶)"
                    }
                else:
                    integrity_report["errors"].append("ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                
                conn.close()
        except Exception as e:
            integrity_report["errors"].append(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        
        # 3. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ãƒã‚§ãƒƒã‚¯
        required_dirs = ["raw", "processed", "embedding", "logs", "cache"]
        for dir_name in required_dirs:
            dir_path = self.data_dir / dir_name
            if dir_path.exists():
                integrity_report["checks"][f"dir_{dir_name}"] = {"status": "OK", "description": f"{dir_name}ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"}
            else:
                integrity_report["warnings"].append(f"æ¨å¥¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dir_name}")
                integrity_report["checks"][f"dir_{dir_name}"] = {"status": "MISSING", "description": f"{dir_name}ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"}
        
        # 4. ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç•°å¸¸ãƒã‚§ãƒƒã‚¯
        for file_name in ["index.faiss", "graph_pyg.pt", "insight_facts.db"]:
            file_path = self.data_dir / file_name
            if file_path.exists():
                size_mb = file_path.stat().st_size / (1024 * 1024)
                if size_mb < 0.001:  # 1KBæœªæº€
                    integrity_report["warnings"].append(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã¾ã™: {file_name} ({size_mb:.3f}MB)")
                elif size_mb > 1000:  # 1GBè¶…
                    integrity_report["warnings"].append(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™: {file_name} ({size_mb:.1f}MB)")
        
        # ç·åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ±ºå®š
        if integrity_report["errors"]:
            integrity_report["overall_status"] = "ERROR"
        elif integrity_report["warnings"]:
            integrity_report["overall_status"] = "WARNING"  
        else:
            integrity_report["overall_status"] = "OK"
        
        # çµæœè¡¨ç¤º
        if integrity_report["overall_status"] == "OK":
            print("âœ… ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Œäº†: å•é¡Œãªã—")
        elif integrity_report["overall_status"] == "WARNING":
            print("âš ï¸ ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Œäº†: è­¦å‘Šã‚ã‚Š")
            for warning in integrity_report["warnings"]:
                print(f"   âš ï¸ {warning}")
        else:
            print("âŒ ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Œäº†: ã‚¨ãƒ©ãƒ¼ã‚ã‚Š")
            for error in integrity_report["errors"]:
                print(f"   âŒ {error}")
        
        return integrity_report


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="InsightSpike-AI å®Ÿé¨“ç®¡ç†CLI")
    subparsers = parser.add_subparsers(dest="command", help="åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
    session_parser = subparsers.add_parser("create-session", help="å®Ÿé¨“ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ")
    session_parser.add_argument("name", help="ã‚»ãƒƒã‚·ãƒ§ãƒ³å")
    session_parser.add_argument("--description", default="", help="ã‚»ãƒƒã‚·ãƒ§ãƒ³èª¬æ˜")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    backup_parser = subparsers.add_parser("backup", help="ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—")
    backup_parser.add_argument("name", help="ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å")
    
    # ãƒ‡ãƒ¼ã‚¿å¾©å…ƒ
    restore_parser = subparsers.add_parser("restore", help="ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹å¾©å…ƒ")
    restore_parser.add_argument("backup_id", help="ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ID")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    clean_parser = subparsers.add_parser("clean", help="dataãƒ•ã‚©ãƒ«ãƒ€ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")
    clean_parser.add_argument("--no-structure", action="store_true", help="ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚‚å‰Šé™¤")
    clean_parser.add_argument("--delete-graph", action="store_true", help="graph_pyg.ptã‚‚å‰Šé™¤ã™ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ä¿æŒï¼‰")
    clean_parser.add_argument("--delete-index", action="store_true", help="index.faissã‚‚å‰Šé™¤ã™ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ä¿æŒï¼‰")
    
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    temp_clean_parser = subparsers.add_parser("clean-temp", help="å®Ÿé¨“ç”¨ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆgraph_pyg.ptã¯ä¿æŒï¼‰")
    
    # åˆæœŸãƒ¡ãƒ¢ãƒªæ§‹ç¯‰
    memory_parser = subparsers.add_parser("build-memory", help="åˆæœŸãƒ¡ãƒ¢ãƒªæ§‹ç¯‰")
    memory_parser.add_argument("--episodes", type=int, default=50, help="ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•° (default: 50)")
    memory_parser.add_argument("--seed", type=int, default=42, help="ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ (default: 42)")
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§
    subparsers.add_parser("list-backups", help="ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§è¡¨ç¤º")
    
    # ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹è¡¨ç¤º
    subparsers.add_parser("status", help="ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹è¡¨ç¤º")
    
    # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
    subparsers.add_parser("check", help="ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    cli = ExperimentCLI()
    
    try:
        if args.command == "create-session":
            cli.create_experiment_session(args.name, args.description)
        
        elif args.command == "backup":
            cli.backup_data_state(args.name)
        
        elif args.command == "restore":
            cli.restore_data_state(args.backup_id)
        
        elif args.command == "clean":
            cli.clean_data_folder(
                keep_structure=not args.no_structure,
                preserve_graph=not args.delete_graph,
                preserve_index=not args.delete_index
            )
        
        elif args.command == "clean-temp":
            cli.clean_temp_files()
        
        elif args.command == "build-memory":
            cli.build_initial_memory(args.episodes, args.seed)
        
        elif args.command == "list-backups":
            backups = cli.list_backups()
            if not backups:
                print("ğŸ“¦ åˆ©ç”¨å¯èƒ½ãªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¯ã‚ã‚Šã¾ã›ã‚“")
            else:
                print(f"ğŸ“¦ åˆ©ç”¨å¯èƒ½ãªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— ({len(backups)}å€‹):")
                for backup in backups:
                    print(f"   ğŸ”¸ {backup['backup_id']}")
                    print(f"      åå‰: {backup['backup_name']}")
                    print(f"      ä½œæˆ: {backup['timestamp']}")
                    print(f"      ãƒ•ã‚¡ã‚¤ãƒ«: {len(backup['backed_up_files'])}å€‹")
                    print()
        
        elif args.command == "status":
            status = cli.show_data_status()
            print("ğŸ“Š ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹:")
            print(f"   ç¢ºèªæ™‚åˆ»: {status['timestamp']}")
            print()
            
            print("ğŸ“„ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«:")
            for file_name, info in status["data_files"].items():
                if info["exists"]:
                    print(f"   âœ… {file_name}: {info['size_mb']:.2f}MB (æ›´æ–°: {info['modified']})")
                else:
                    print(f"   âŒ {file_name}: å­˜åœ¨ã›ãš")
            print()
            
            print("ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª:")
            for dir_name, info in status["directories"].items():
                if info["exists"]:
                    print(f"   âœ… {dir_name}/: {info['file_count']}ãƒ•ã‚¡ã‚¤ãƒ«, {info['dir_count']}ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª ({info['total_size_mb']:.2f}MB)")
                else:
                    print(f"   âŒ {dir_name}/: å­˜åœ¨ã›ãš")
            print()
            
            if "episodes_count" in status["memory_info"]:
                print(f"ğŸ§  ãƒ¡ãƒ¢ãƒªæƒ…å ±:")
                print(f"   ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {status['memory_info']['episodes_count']}")
                print(f"   DBãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(status['memory_info']['database_tables'])}")
            
            if "recent_sessions" in status.get("experiments_info", {}):
                print(f"\nğŸ”¬ æœ€è¿‘ã®å®Ÿé¨“ã‚»ãƒƒã‚·ãƒ§ãƒ³:")
                for session in status["experiments_info"]["recent_sessions"]:
                    print(f"   ğŸ“‹ {session['session_id']}: {session['experiment_count']}å®Ÿé¨“ ({session['created_at']})")
        
        elif args.command == "check":
            cli.validate_data_integrity()
    
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
