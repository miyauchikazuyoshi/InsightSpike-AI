#!/usr/bin/env python3
"""
æ¨™æº–åŒ–å®Ÿé¨“å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
====================

åŒä¸€æ¡ä»¶ã§ã®å¯¾ç…§å®Ÿé¨“ã‚’å®Ÿç¾ã™ã‚‹çµ±ä¸€çš„ãªå®Ÿé¨“å®Ÿè¡Œãƒ„ãƒ¼ãƒ«
"""

import sys
import argparse
import json
import time
import numpy as np
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# InsightSpike-AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))

try:
    from insightspike.core.config import get_config
    from insightspike.utils.embedder import get_model
    from insightspike.core.layers.layer2_memory_manager import L2MemoryManager
    from insightspike.core.learning.knowledge_graph_memory import KnowledgeGraphMemory
except ImportError as e:
    print(f"âŒ InsightSpike-AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    sys.exit(1)


class StandardizedExperiment:
    """æ¨™æº–åŒ–å®Ÿé¨“ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, session_id: str, experiment_config: Dict[str, Any]):
        self.session_id = session_id
        self.config = experiment_config
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.outputs_dir = Path("experiments/outputs")
        self.session_dir = self.outputs_dir / session_id
        self.session_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.data_dir = Path("data")
        
        # å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.memory_dim = self.config.get("memory_dim", 384)
        self.topk = self.config.get("topk", 10)
        self.ged_threshold = self.config.get("ged_threshold", 0.15)
        self.ig_threshold = self.config.get("ig_threshold", 0.10)
        self.similarity_threshold = self.config.get("similarity_threshold", 0.3)
        
        # å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿
        self.experiment_logs = []
        self.insight_logs = []
        self.memory_snapshots = []
        self.topk_logs = []
        
        print(f"ğŸ“‹ æ¨™æº–åŒ–å®Ÿé¨“åˆæœŸåŒ–å®Œäº†")
        print(f"   ã‚»ãƒƒã‚·ãƒ§ãƒ³: {session_id}")
        print(f"   ãƒ¡ãƒ¢ãƒªæ¬¡å…ƒ: {self.memory_dim}")
        print(f"   TopK: {self.topk}")
        print(f"   GEDé–¾å€¤: {self.ged_threshold}")
        print(f"   IGé–¾å€¤: {self.ig_threshold}")
    
    def initialize_components(self):
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–"""
        self.model = get_model()
        self.memory_manager = L2MemoryManager(dim=self.memory_dim)
        self.knowledge_graph = KnowledgeGraphMemory(
            embedding_dim=self.memory_dim,
            similarity_threshold=self.similarity_threshold
        )
        print("âœ… ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
    
    def generate_episodes(self, num_episodes: int, seed: int = 42, episode_type: str = "experiment") -> List[Dict]:
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆï¼ˆå†ç¾å¯èƒ½ï¼‰"""
        print(f"ğŸ“ {num_episodes}å€‹ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆä¸­ (seed={seed}, type={episode_type})...")
        
        import random
        random.seed(seed)
        np.random.seed(seed)
        
        research_areas = [
            "Large Language Models", "Computer Vision", "Reinforcement Learning",
            "Graph Neural Networks", "Federated Learning", "Explainable AI",
            "Multimodal Learning", "Few-shot Learning", "Transfer Learning",
            "Adversarial Machine Learning"
        ]
        
        activity_types = [
            "achieves breakthrough performance on",
            "introduces novel architecture for", 
            "demonstrates significant improvements in",
            "reveals new insights about",
            "establishes new benchmarks for"
        ]
        
        domains = [
            "medical diagnosis", "autonomous systems", "natural language processing",
            "computer vision", "robotics", "cybersecurity", "climate modeling",
            "drug discovery", "financial prediction", "educational technology"
        ]
        
        episodes = []
        for i in range(1, num_episodes + 1):
            research_area = research_areas[(i - 1) % len(research_areas)]
            activity_type = activity_types[(i - 1) % len(activity_types)]
            domain = domains[(i - 1) % len(domains)]
            
            if episode_type == "initial":
                text = f"Initial research in {research_area} {activity_type} {domain}, establishing foundational knowledge for future insights."
            else:
                text = f"Recent research in {research_area} {activity_type} {domain}, showing promising results with practical implications for real-world deployment."
            
            episodes.append({
                'id': i,
                'text': text,
                'research_area': research_area,
                'activity_type': activity_type,
                'domain': domain,
                'type': episode_type,
                'seed': seed,
                'timestamp': datetime.now().isoformat()
            })
        
        print(f"âœ… {len(episodes)}å€‹ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†")
        return episodes
    
    def take_memory_snapshot(self, phase: str, episode_num: int) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå–å¾—"""
        try:
            snapshot = {
                'timestamp': datetime.now().isoformat(),
                'phase': phase,
                'episode_number': episode_num,
                'total_episodes': len(self.memory_manager.episodes),
                'memory_size_mb': sys.getsizeof(self.memory_manager) / (1024 * 1024),
                'config': {
                    'memory_dim': self.memory_dim,
                    'topk': self.topk,
                    'ged_threshold': self.ged_threshold,
                    'ig_threshold': self.ig_threshold
                }
            }
            
            # ãƒ¡ãƒ¢ãƒªçµ±è¨ˆ
            if len(self.memory_manager.episodes) > 0:
                c_values = [ep.c_value for ep in self.memory_manager.episodes if hasattr(ep, 'c_value')]
                if c_values:
                    snapshot['c_value_stats'] = {
                        'mean': float(np.mean(c_values)),
                        'std': float(np.std(c_values)),
                        'min': float(np.min(c_values)),
                        'max': float(np.max(c_values))
                    }
            
            self.memory_snapshots.append(snapshot)
            return snapshot
            
        except Exception as e:
            print(f"âš ï¸ ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def calculate_ged_ig_metrics(self, current_embedding: np.ndarray, episode_num: int) -> tuple[float, float]:
        """GEDã¨IGå€¤ã‚’è¨ˆç®—"""
        try:
            if len(self.memory_manager.episodes) < 2:
                return 0.5, 0.0
            
            # ç›´è¿‘ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
            prev_episode = self.memory_manager.episodes[-1]
            similarity = np.dot(current_embedding, prev_episode.vec)
            
            # GED: ã‚°ãƒ­ãƒ¼ãƒãƒ«ç·¨é›†è·é›¢ï¼ˆé¡ä¼¼åº¦ã®é€†æ•°ã¨ã—ã¦è¿‘ä¼¼ï¼‰
            ged = max(0.0, 1.0 - similarity)
            
            # IG: æƒ…å ±ã‚²ã‚¤ãƒ³ï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã«åŸºã¥ãç°¡æ˜“è¨ˆç®—ï¼‰
            ig = min(0.3, episode_num * 0.001)
            
            return float(ged), float(ig)
            
        except Exception as e:
            print(f"âš ï¸ GED/IGè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.5, 0.0
    
    def check_insight_condition(self, ged: float, ig: float, use_random: bool = True) -> bool:
        """æ´å¯Ÿæ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯"""
        ged_condition = ged > self.ged_threshold
        ig_condition = ig > self.ig_threshold
        
        if use_random:
            # ç¢ºç‡çš„è¦ç´ ã‚’åŠ ãˆã‚‹ï¼ˆå†ç¾æ€§ã®ãŸã‚å›ºå®šã‚·ãƒ¼ãƒ‰ä½¿ç”¨ï¼‰
            random_factor = np.random.random() < 0.05  # 5%ã®ç¢ºç‡
            return ged_condition and ig_condition or random_factor
        else:
            return ged_condition and ig_condition
    
    def get_topk_similar_episodes(self, current_episode: Dict, embedding: np.ndarray) -> List[Dict]:
        """TopKé¡ä¼¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å–å¾—"""
        try:
            similarities, indices = self.memory_manager.search(embedding, top_k=self.topk)
            
            topk_episodes = []
            for idx, (similarity, episode_idx) in enumerate(zip(similarities, indices)):
                if episode_idx >= len(self.memory_manager.episodes):
                    continue
                    
                stored_episode = self.memory_manager.episodes[episode_idx]
                
                # ãƒ‰ãƒ¡ã‚¤ãƒ³é–“åˆ†æ
                current_domain = current_episode.get('domain', 'unknown')
                similar_domain = getattr(stored_episode, 'metadata', {}).get('domain', 'unknown')
                is_cross_domain = current_domain != similar_domain
                
                episode_info = {
                    'rank': idx + 1,
                    'similarity': float(similarity),
                    'episode_id': getattr(stored_episode, 'id', episode_idx),
                    'domain': similar_domain,
                    'research_area': getattr(stored_episode, 'metadata', {}).get('research_area', 'unknown'),
                    'is_cross_domain': is_cross_domain
                }
                
                topk_episodes.append(episode_info)
            
            return topk_episodes
            
        except Exception as e:
            print(f"âš ï¸ TopKå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def run_experiment(self, experiment_name: str, num_episodes: int, seed: int = 42) -> Dict[str, Any]:
        """å®Ÿé¨“å®Ÿè¡Œ"""
        print(f"ğŸš€ å®Ÿé¨“é–‹å§‹: {experiment_name}")
        print(f"   ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {num_episodes}")
        print(f"   ã‚·ãƒ¼ãƒ‰å€¤: {seed}")
        print("=" * 60)
        
        start_time = time.time()
        
        # ä¹±æ•°ã‚·ãƒ¼ãƒ‰è¨­å®š
        np.random.seed(seed)
        
        # åˆæœŸã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
        self.take_memory_snapshot("experiment_start", 0)
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆ
        episodes = self.generate_episodes(num_episodes, seed, "experiment")
        
        # å®Ÿé¨“å®Ÿè¡Œ
        processing_times = []
        
        for i, episode in enumerate(episodes):
            episode_start = time.time()
            
            try:
                # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
                embedding = self.model.encode(episode['text'])
                
                # TopKé¡ä¼¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å–å¾—
                topk_episodes = self.get_topk_similar_episodes(episode, embedding)
                
                # æ´å¯Ÿãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
                delta_ged, delta_ig = self.calculate_ged_ig_metrics(embedding, i + 1)
                
                # æ´å¯Ÿæ¡ä»¶ãƒã‚§ãƒƒã‚¯
                is_insight = self.check_insight_condition(delta_ged, delta_ig)
                
                # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†æ
                cross_domain_count = sum(1 for ep in topk_episodes if ep['is_cross_domain'])
                domain_diversity = len(set(ep['domain'] for ep in topk_episodes))
                
                if is_insight:
                    insight_id = f"{experiment_name}_INS_{episode['id']:04d}_{int(time.time() * 1000) % 10000}"
                    
                    # æ´å¯Ÿã‚¿ã‚¤ãƒ—åˆ†é¡
                    if delta_ged > 0.3:
                        insight_type = "Significant_Insight"
                    elif delta_ged > 0.2:
                        insight_type = "Notable_Pattern"  
                    else:
                        insight_type = "Micro_Insight"
                    
                    print(f"ğŸ”¥ æ´å¯Ÿæ¤œå‡º: {insight_id} (Episode {episode['id']})")
                    print(f"   Î”GED: {delta_ged:.4f}, Î”IG: {delta_ig:.4f}, Type: {insight_type}")
                    
                    insight_data = {
                        'insight_id': insight_id,
                        'episode_id': episode['id'],
                        'episode_text': episode['text'],
                        'ged_value': delta_ged,
                        'ig_value': delta_ig,
                        'confidence': (delta_ged + delta_ig) / 2,
                        'insight_type': insight_type,
                        'cross_domain_count': cross_domain_count,
                        'domain_diversity': domain_diversity,
                        'current_domain': episode.get('domain', 'unknown'),
                        'current_research_area': episode.get('research_area', 'unknown'),
                        'detection_timestamp': datetime.now().isoformat(),
                        'experiment_name': experiment_name,
                        'seed': seed
                    }
                    
                    self.insight_logs.append(insight_data)
                
                # è©³ç´°ãƒ­ã‚°ã®è¨˜éŒ²
                experiment_log = {
                    'episode_id': episode['id'],
                    'episode_text': episode['text'],
                    'domain': episode.get('domain', 'unknown'),
                    'research_area': episode.get('research_area', 'unknown'),
                    'delta_ged': delta_ged,
                    'delta_ig': delta_ig,
                    'is_insight': is_insight,
                    'cross_domain_count': cross_domain_count,
                    'domain_diversity': domain_diversity,
                    'topk_count': len(topk_episodes),
                    'memory_size_before': len(self.memory_manager.episodes),
                    'timestamp': datetime.now().isoformat(),
                    'experiment_name': experiment_name,
                    'seed': seed
                }
                
                self.experiment_logs.append(experiment_log)
                
                # TopKãƒ­ã‚°ã®è¨˜éŒ²
                if topk_episodes:
                    topk_log = {
                        'current_episode_id': episode['id'],
                        'current_domain': episode.get('domain', 'unknown'),
                        'current_research_area': episode.get('research_area', 'unknown'),
                        'topk_episodes': topk_episodes,
                        'cross_domain_count': cross_domain_count,
                        'experiment_name': experiment_name,
                        'timestamp': datetime.now().isoformat()
                    }
                    self.topk_logs.append(topk_log)
                
                # ãƒ¡ãƒ¢ãƒªã«ä¿å­˜
                self.memory_manager.store_episode(
                    text=episode['text'], 
                    c_value=0.2,
                    metadata={
                        'id': episode['id'], 
                        'domain': episode.get('domain', 'unknown'),
                        'research_area': episode.get('research_area', 'unknown'),
                        'experiment_name': experiment_name
                    }
                )
                
                # å‡¦ç†æ™‚é–“è¨˜éŒ²
                episode_time = time.time() - episode_start
                processing_times.append(episode_time)
                
                # å®šæœŸã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
                if (i + 1) % 50 == 0:
                    self.take_memory_snapshot(f"episode_{i+1}", i + 1)
                
                # é€²æ—è¡¨ç¤º
                if (i + 1) % 100 == 0:
                    avg_time = np.mean(processing_times[-100:])
                    eps_per_sec = 1.0 / avg_time if avg_time > 0 else 0
                    print(f"ğŸ“ˆ é€²æ—: {i+1}/{num_episodes} ({eps_per_sec:.1f} eps/sec, {len(self.insight_logs)} insights)")
                
            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode['id']} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # å®Ÿé¨“å®Œäº†
        total_time = time.time() - start_time
        avg_eps_per_sec = num_episodes / total_time if total_time > 0 else 0
        avg_processing_time = np.mean(processing_times) if processing_times else 0
        
        # æœ€çµ‚ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
        self.take_memory_snapshot("experiment_end", num_episodes)
        
        # çµæœã‚µãƒãƒªãƒ¼
        results = {
            'experiment_name': experiment_name,
            'session_id': self.session_id,
            'total_episodes': num_episodes,
            'total_insights': len(self.insight_logs),
            'insight_rate': len(self.insight_logs) / num_episodes if num_episodes > 0 else 0,
            'total_time_seconds': total_time,
            'avg_episodes_per_second': avg_eps_per_sec,
            'avg_processing_time': avg_processing_time,
            'seed': seed,
            'config': {
                'memory_dim': self.memory_dim,
                'topk': self.topk,
                'ged_threshold': self.ged_threshold,
                'ig_threshold': self.ig_threshold,
                'similarity_threshold': self.similarity_threshold
            },
            'timestamp': datetime.now().isoformat()
        }
        
        print(f"\nâœ… å®Ÿé¨“å®Œäº†!")
        print(f"   ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {num_episodes}")
        print(f"   æ¤œå‡ºã•ã‚ŒãŸæ´å¯Ÿ: {len(self.insight_logs)}")
        print(f"   å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")
        print(f"   å‡¦ç†é€Ÿåº¦: {avg_eps_per_sec:.2f} eps/sec")
        print(f"   æ´å¯Ÿæ¤œå‡ºç‡: {len(self.insight_logs)/num_episodes*100:.2f}%")
        
        return results
    
    def save_results(self, experiment_name: str, results: Dict[str, Any]):
        """çµæœä¿å­˜"""
        print("ğŸ’¾ å®Ÿé¨“çµæœã‚’ä¿å­˜ä¸­...")
        
        # å®Ÿé¨“å›ºæœ‰ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        exp_output_dir = self.session_dir / experiment_name
        exp_output_dir.mkdir(exist_ok=True)
        
        # 1. å…¥åŠ›ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰CSV
        if self.experiment_logs:
            episodes_data = []
            for log in self.experiment_logs:
                episodes_data.append({
                    'episode_id': log['episode_id'],
                    'episode_text': log['episode_text'],
                    'domain': log['domain'],
                    'research_area': log['research_area'],
                    'experiment_name': log['experiment_name'],
                    'seed': log['seed'],
                    'timestamp': log['timestamp']
                })
            
            episodes_df = pd.DataFrame(episodes_data)
            episodes_df.to_csv(exp_output_dir / "01_input_episodes.csv", index=False)
        
        # 2. æ´å¯Ÿæ¤œå‡ºçµæœCSV
        if self.insight_logs:
            insights_df = pd.DataFrame(self.insight_logs)
            insights_df.to_csv(exp_output_dir / "02_insights.csv", index=False)
        
        # 3. è©³ç´°å®Ÿé¨“ãƒ­ã‚°CSV
        if self.experiment_logs:
            experiment_df = pd.DataFrame(self.experiment_logs)
            experiment_df.to_csv(exp_output_dir / "03_experiment_logs.csv", index=False)
        
        # 4. TopKåˆ†æCSV
        if self.topk_logs:
            topk_df = pd.DataFrame(self.topk_logs)
            topk_df.to_csv(exp_output_dir / "04_topk_analysis.csv", index=False)
        
        # 5. ãƒ¡ãƒ¢ãƒªã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆCSV
        if self.memory_snapshots:
            snapshots_df = pd.DataFrame(self.memory_snapshots)
            snapshots_df.to_csv(exp_output_dir / "05_memory_snapshots.csv", index=False)
        
        # 6. å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼JSON
        with open(exp_output_dir / "06_experiment_results.json", 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… å®Ÿé¨“çµæœä¿å­˜å®Œäº†:")
        print(f"   ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {exp_output_dir}")
        print(f"   ğŸ“„ çµæœãƒ•ã‚¡ã‚¤ãƒ«æ•°: 6å€‹")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="æ¨™æº–åŒ–å®Ÿé¨“å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument("session_id", help="å®Ÿé¨“ã‚»ãƒƒã‚·ãƒ§ãƒ³ID")
    parser.add_argument("experiment_name", help="å®Ÿé¨“å")
    parser.add_argument("--episodes", type=int, default=500, help="ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•° (default: 500)")
    parser.add_argument("--seed", type=int, default=42, help="ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ (default: 42)")
    parser.add_argument("--memory-dim", type=int, default=384, help="ãƒ¡ãƒ¢ãƒªæ¬¡å…ƒ (default: 384)")
    parser.add_argument("--topk", type=int, default=10, help="TopKè¿‘å‚æ•° (default: 10)")
    parser.add_argument("--ged-threshold", type=float, default=0.15, help="GEDé–¾å€¤ (default: 0.15)")
    parser.add_argument("--ig-threshold", type=float, default=0.10, help="IGé–¾å€¤ (default: 0.10)")
    parser.add_argument("--similarity-threshold", type=float, default=0.3, help="é¡ä¼¼åº¦é–¾å€¤ (default: 0.3)")
    
    args = parser.parse_args()
    
    # å®Ÿé¨“è¨­å®š
    experiment_config = {
        "memory_dim": args.memory_dim,
        "topk": args.topk,
        "ged_threshold": args.ged_threshold,
        "ig_threshold": args.ig_threshold,
        "similarity_threshold": args.similarity_threshold
    }
    
    try:
        # å®Ÿé¨“å®Ÿè¡Œ
        experiment = StandardizedExperiment(args.session_id, experiment_config)
        experiment.initialize_components()
        
        results = experiment.run_experiment(args.experiment_name, args.episodes, args.seed)
        experiment.save_results(args.experiment_name, results)
        
        print("\nğŸ‰ æ¨™æº–åŒ–å®Ÿé¨“ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ!")
        
    except Exception as e:
        print(f"âŒ å®Ÿé¨“å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
