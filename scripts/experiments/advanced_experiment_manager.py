#!/usr/bin/env python3
"""
é«˜åº¦ãªå®Ÿé¨“ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
====================

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚¤ãƒ¼ãƒ—ã€æ¯”è¼ƒå®Ÿé¨“ã€ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆã‚’çµ±åˆã—ãŸ
åŒ…æ‹¬çš„ãªå®Ÿé¨“ç®¡ç†ãƒ„ãƒ¼ãƒ«
"""

import sys
import argparse
import json
import time
import yaml
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from concurrent.futures import ProcessPoolExecutor
import subprocess
import warnings
warnings.filterwarnings('ignore')

# ã‚°ãƒ©ãƒ•è¨­å®š
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")


class AdvancedExperimentManager:
    """é«˜åº¦ãªå®Ÿé¨“ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.data_dir = Path("data")
        self.experiments_dir = Path("experiments")
        self.outputs_dir = Path("experiments/outputs")
        self.scripts_dir = Path("scripts/experiments")
        
        # è¨­å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.config_dir = self.scripts_dir / "configs"
        self.config_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.reports_dir = self.outputs_dir / "reports"
        self.reports_dir.mkdir(parents=True, exist_ok=True)
        
        # å®Ÿé¨“ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.experiment_templates = {
            "quick_test": {
                "episodes": 100,
                "memory_dim": 384,
                "topk": 5,
                "ged_threshold": 0.15,
                "ig_threshold": 0.10,
                "similarity_threshold": 0.3
            },
            "standard": {
                "episodes": 500,
                "memory_dim": 384,
                "topk": 10,
                "ged_threshold": 0.15,
                "ig_threshold": 0.10,
                "similarity_threshold": 0.3
            },
            "comprehensive": {
                "episodes": 1000,
                "memory_dim": 384,
                "topk": 15,
                "ged_threshold": 0.15,
                "ig_threshold": 0.10,
                "similarity_threshold": 0.3
            },
            "high_sensitivity": {
                "episodes": 500,
                "memory_dim": 384,
                "topk": 10,
                "ged_threshold": 0.10,
                "ig_threshold": 0.05,
                "similarity_threshold": 0.25
            },
            "low_sensitivity": {
                "episodes": 500,
                "memory_dim": 384,
                "topk": 10,
                "ged_threshold": 0.25,
                "ig_threshold": 0.15,
                "similarity_threshold": 0.35
            }
        }
    
    def create_experiment_config(self, template_name: str, custom_params: Dict = None) -> Dict:
        """å®Ÿé¨“è¨­å®šä½œæˆ"""
        if template_name not in self.experiment_templates:
            raise ValueError(f"æœªçŸ¥ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {template_name}")
        
        config = self.experiment_templates[template_name].copy()
        if custom_params:
            config.update(custom_params)
        
        return config
    
    def save_experiment_config(self, config_name: str, config: Dict) -> Path:
        """å®Ÿé¨“è¨­å®šä¿å­˜"""
        config_path = self.config_dir / f"{config_name}.yaml"
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        
        print(f"âœ… å®Ÿé¨“è¨­å®šä¿å­˜: {config_path}")
        return config_path
    
    def load_experiment_config(self, config_name: str) -> Dict:
        """å®Ÿé¨“è¨­å®šèª­ã¿è¾¼ã¿"""
        config_path = self.config_dir / f"{config_name}.yaml"
        if not config_path.exists():
            raise FileNotFoundError(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        return config
    
    def generate_parameter_sweep_configs(self, base_config: Dict, sweep_params: Dict[str, List]) -> List[Dict]:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚¤ãƒ¼ãƒ—è¨­å®šç”Ÿæˆ"""
        import itertools
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµ„ã¿åˆã‚ã›ã‚’ç”Ÿæˆ
        param_names = list(sweep_params.keys())
        param_values = list(sweep_params.values())
        
        configs = []
        for combination in itertools.product(*param_values):
            config = base_config.copy()
            for param_name, param_value in zip(param_names, combination):
                config[param_name] = param_value
            
            # è¨­å®šåã‚’ç”Ÿæˆ
            config_name = "_".join([f"{name}{value}" for name, value in zip(param_names, combination)])
            config['config_name'] = config_name
            configs.append(config)
        
        return configs
    
    def run_single_experiment(self, session_id: str, experiment_name: str, config: Dict, seed: int = 42) -> Dict:
        """å˜ä¸€å®Ÿé¨“å®Ÿè¡Œ"""
        print(f"ğŸš€ å®Ÿé¨“å®Ÿè¡Œé–‹å§‹: {experiment_name}")
        
        # æ¨™æº–å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å‘¼ã³å‡ºã—
        cmd = [
            "python", str(self.scripts_dir / "run_standardized_experiment.py"),
            session_id, experiment_name,
            "--episodes", str(config["episodes"]),
            "--seed", str(seed),
            "--memory-dim", str(config["memory_dim"]),
            "--topk", str(config["topk"]),
            "--ged-threshold", str(config["ged_threshold"]),
            "--ig-threshold", str(config["ig_threshold"]),
            "--similarity-threshold", str(config["similarity_threshold"])
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)
            
            if result.returncode == 0:
                print(f"âœ… å®Ÿé¨“å®Œäº†: {experiment_name}")
                return {"status": "success", "stdout": result.stdout, "stderr": result.stderr}
            else:
                print(f"âŒ å®Ÿé¨“å¤±æ•—: {experiment_name}")
                print(f"ã‚¨ãƒ©ãƒ¼: {result.stderr}")
                return {"status": "failed", "stdout": result.stdout, "stderr": result.stderr}
                
        except subprocess.TimeoutExpired:
            return {"status": "timeout", "stdout": "", "stderr": "å®Ÿé¨“ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ"}
        except Exception as e:
            return {"status": "error", "stdout": "", "stderr": str(e)}
    
    def run_parameter_sweep(self, session_id: str, base_name: str, configs: List[Dict], 
                           parallel: bool = False, max_workers: int = 2) -> List[Dict]:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚¤ãƒ¼ãƒ—å®Ÿè¡Œ"""
        print(f"ğŸ”„ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚¤ãƒ¼ãƒ—é–‹å§‹: {len(configs)}å€‹ã®è¨­å®š")
        
        results = []
        
        if parallel and len(configs) > 1:
            print(f"âš¡ ä¸¦åˆ—å®Ÿè¡Œ (æœ€å¤§{max_workers}ä¸¦åˆ—)")
            # Note: ä¸¦åˆ—å®Ÿè¡Œã¯æ…é‡ã«å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼ˆãƒªã‚½ãƒ¼ã‚¹ç«¶åˆã®ãŸã‚ï¼‰
            # ä»Šå›ã¯é †æ¬¡å®Ÿè¡Œã‚’åŸºæœ¬ã¨ã™ã‚‹
            parallel = False
        
        for i, config in enumerate(configs):
            experiment_name = f"{base_name}_{config.get('config_name', f'exp{i:03d}')}"
            
            print(f"\n--- {i+1}/{len(configs)}: {experiment_name} ---")
            print(f"è¨­å®š: {config}")
            
            result = self.run_single_experiment(session_id, experiment_name, config)
            result['experiment_name'] = experiment_name
            result['config'] = config
            result['index'] = i
            
            results.append(result)
            
            # å¤±æ•—ã—ãŸå ´åˆã®å‡¦ç†
            if result['status'] != 'success':
                print(f"âš ï¸ {experiment_name} ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚ç¶šè¡Œã—ã¾ã™...")
        
        print(f"\nâœ… ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚¤ãƒ¼ãƒ—å®Œäº†: {len(results)}å®Ÿé¨“")
        return results
    
    def collect_experiment_results(self, session_id: str, experiment_names: List[str]) -> pd.DataFrame:
        """å®Ÿé¨“çµæœåé›†"""
        session_dir = self.outputs_dir / session_id
        
        all_results = []
        
        for exp_name in experiment_names:
            exp_dir = session_dir / exp_name
            results_file = exp_dir / "06_experiment_results.json"
            
            if results_file.exists():
                with open(results_file, 'r', encoding='utf-8') as f:
                    result = json.load(f)
                result['experiment_name'] = exp_name
                all_results.append(result)
            else:
                print(f"âš ï¸ çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {results_file}")
        
        if all_results:
            return pd.DataFrame(all_results)
        else:
            return pd.DataFrame()
    
    def generate_comparison_report(self, session_id: str, experiment_names: List[str], 
                                 report_name: str = "comparison_report") -> Path:
        """æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print(f"ğŸ“Š æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­: {report_name}")
        
        # å®Ÿé¨“çµæœåé›†
        results_df = self.collect_experiment_results(session_id, experiment_names)
        
        if results_df.empty:
            print("âŒ æ¯”è¼ƒå¯èƒ½ãªå®Ÿé¨“çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return None
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        report_dir = self.reports_dir / f"{report_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        report_dir.mkdir(exist_ok=True)
        
        # 1. åŸºæœ¬çµ±è¨ˆ
        summary_stats = {
            'total_experiments': len(results_df),
            'total_episodes': results_df['total_episodes'].sum(),
            'total_insights': results_df['total_insights'].sum(),
            'avg_insight_rate': results_df['insight_rate'].mean(),
            'avg_processing_speed': results_df['avg_episodes_per_second'].mean(),
            'session_id': session_id,
            'report_generated': datetime.now().isoformat()
        }
        
        # 2. è©³ç´°çµæœCSV
        results_df.to_csv(report_dir / "experiment_results.csv", index=False)
        
        # 3. çµ±è¨ˆã‚µãƒãƒªãƒ¼JSON
        with open(report_dir / "summary_stats.json", 'w', encoding='utf-8') as f:
            json.dump(summary_stats, f, indent=2, ensure_ascii=False)
        
        # 4. å¯è¦–åŒ–ã‚°ãƒ©ãƒ•
        self._generate_comparison_plots(results_df, report_dir)
        
        # 5. HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        html_report = self._generate_html_report(results_df, summary_stats, report_dir)
        
        print(f"âœ… æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†:")
        print(f"   ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {report_dir}")
        print(f"   ğŸ“„ HTMLãƒ¬ãƒãƒ¼ãƒˆ: {html_report}")
        
        return report_dir
    
    def _generate_comparison_plots(self, results_df: pd.DataFrame, report_dir: Path):
        """æ¯”è¼ƒã‚°ãƒ©ãƒ•ç”Ÿæˆ"""
        # å›³ã®ã‚µã‚¤ã‚ºã¨ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
        plt.rcParams['figure.figsize'] = (12, 8)
        plt.rcParams['font.size'] = 10
        
        # 1. æ´å¯Ÿæ¤œå‡ºç‡æ¯”è¼ƒ
        plt.figure(figsize=(12, 6))
        plt.bar(range(len(results_df)), results_df['insight_rate'], 
                color=plt.cm.viridis(np.linspace(0, 1, len(results_df))))
        plt.xlabel('å®Ÿé¨“')
        plt.ylabel('æ´å¯Ÿæ¤œå‡ºç‡')
        plt.title('å®Ÿé¨“åˆ¥æ´å¯Ÿæ¤œå‡ºç‡æ¯”è¼ƒ')
        plt.xticks(range(len(results_df)), results_df['experiment_name'], rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(report_dir / "insight_rate_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. å‡¦ç†é€Ÿåº¦æ¯”è¼ƒ
        plt.figure(figsize=(12, 6))
        plt.bar(range(len(results_df)), results_df['avg_episodes_per_second'],
                color=plt.cm.plasma(np.linspace(0, 1, len(results_df))))
        plt.xlabel('å®Ÿé¨“')
        plt.ylabel('å‡¦ç†é€Ÿåº¦ (episodes/sec)')
        plt.title('å®Ÿé¨“åˆ¥å‡¦ç†é€Ÿåº¦æ¯”è¼ƒ')
        plt.xticks(range(len(results_df)), results_df['experiment_name'], rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(report_dir / "processing_speed_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. æ•£å¸ƒå›³: æ´å¯Ÿç‡ vs å‡¦ç†é€Ÿåº¦
        plt.figure(figsize=(10, 8))
        plt.scatter(results_df['insight_rate'], results_df['avg_episodes_per_second'], 
                   s=100, alpha=0.7, c=range(len(results_df)), cmap='viridis')
        
        for i, row in results_df.iterrows():
            plt.annotate(row['experiment_name'], 
                        (row['insight_rate'], row['avg_episodes_per_second']),
                        xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        plt.xlabel('æ´å¯Ÿæ¤œå‡ºç‡')
        plt.ylabel('å‡¦ç†é€Ÿåº¦ (episodes/sec)')
        plt.title('æ´å¯Ÿæ¤œå‡ºç‡ vs å‡¦ç†é€Ÿåº¦')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(report_dir / "insight_vs_speed_scatter.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 4. è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
        config_columns = []
        for col in results_df.columns:
            if col.startswith('config.'):
                config_columns.append(col)
        
        if config_columns:
            n_params = len(config_columns)
            fig, axes = plt.subplots(2, (n_params + 1) // 2, figsize=(15, 10))
            axes = axes.flatten() if n_params > 1 else [axes]
            
            for i, param in enumerate(config_columns):
                if i < len(axes):
                    axes[i].scatter(results_df[param], results_df['insight_rate'], alpha=0.7)
                    axes[i].set_xlabel(param.replace('config.', ''))
                    axes[i].set_ylabel('æ´å¯Ÿæ¤œå‡ºç‡')
                    axes[i].set_title(f'{param.replace("config.", "")} ã®å½±éŸ¿')
                    axes[i].grid(True, alpha=0.3)
            
            # ä½™ã£ãŸã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤º
            for i in range(len(config_columns), len(axes)):
                axes[i].set_visible(False)
            
            plt.tight_layout()
            plt.savefig(report_dir / "parameter_effects.png", dpi=300, bbox_inches='tight')
            plt.close()
    
    def _generate_html_report(self, results_df: pd.DataFrame, summary_stats: Dict, 
                            report_dir: Path) -> Path:
        """HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>InsightSpike-AI å®Ÿé¨“æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
        .summary {{ background-color: #e8f4f8; padding: 15px; margin: 20px 0; border-radius: 5px; }}
        .experiment {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
        .graph {{ text-align: center; margin: 20px 0; }}
        .graph img {{ max-width: 100%; height: auto; }}
        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        .metric {{ display: inline-block; margin: 10px 20px 10px 0; }}
        .metric-value {{ font-size: 1.5em; font-weight: bold; color: #2c3e50; }}
        .metric-label {{ font-size: 0.9em; color: #7f8c8d; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ”¬ InsightSpike-AI å®Ÿé¨“æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ</h1>
        <p>ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</p>
        <p>ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {summary_stats['session_id']}</p>
    </div>
    
    <div class="summary">
        <h2>ğŸ“Š å®Ÿé¨“ã‚µãƒãƒªãƒ¼</h2>
        <div class="metric">
            <div class="metric-value">{summary_stats['total_experiments']}</div>
            <div class="metric-label">å®Ÿé¨“æ•°</div>
        </div>
        <div class="metric">
            <div class="metric-value">{summary_stats['total_episodes']:,}</div>
            <div class="metric-label">ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°</div>
        </div>
        <div class="metric">
            <div class="metric-value">{summary_stats['total_insights']:,}</div>
            <div class="metric-label">ç·æ´å¯Ÿæ•°</div>
        </div>
        <div class="metric">
            <div class="metric-value">{summary_stats['avg_insight_rate']:.3f}</div>
            <div class="metric-label">å¹³å‡æ´å¯Ÿç‡</div>
        </div>
        <div class="metric">
            <div class="metric-value">{summary_stats['avg_processing_speed']:.1f}</div>
            <div class="metric-label">å¹³å‡å‡¦ç†é€Ÿåº¦ (eps/sec)</div>
        </div>
    </div>
    
    <h2>ğŸ“ˆ æ¯”è¼ƒã‚°ãƒ©ãƒ•</h2>
    <div class="graph">
        <h3>æ´å¯Ÿæ¤œå‡ºç‡æ¯”è¼ƒ</h3>
        <img src="insight_rate_comparison.png" alt="æ´å¯Ÿæ¤œå‡ºç‡æ¯”è¼ƒ">
    </div>
    
    <div class="graph">
        <h3>å‡¦ç†é€Ÿåº¦æ¯”è¼ƒ</h3>
        <img src="processing_speed_comparison.png" alt="å‡¦ç†é€Ÿåº¦æ¯”è¼ƒ">
    </div>
    
    <div class="graph">
        <h3>æ´å¯Ÿæ¤œå‡ºç‡ vs å‡¦ç†é€Ÿåº¦</h3>
        <img src="insight_vs_speed_scatter.png" alt="æ´å¯Ÿæ¤œå‡ºç‡ vs å‡¦ç†é€Ÿåº¦">
    </div>
    
    <h2>ğŸ“‹ è©³ç´°çµæœ</h2>
    <table>
        <tr>
            <th>å®Ÿé¨“å</th>
            <th>ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°</th>
            <th>æ´å¯Ÿæ•°</th>
            <th>æ´å¯Ÿç‡</th>
            <th>å‡¦ç†é€Ÿåº¦</th>
            <th>å®Ÿè¡Œæ™‚é–“</th>
        </tr>"""
        
        for _, row in results_df.iterrows():
            html_content += f"""
        <tr>
            <td>{row['experiment_name']}</td>
            <td>{row['total_episodes']:,}</td>
            <td>{row['total_insights']:,}</td>
            <td>{row['insight_rate']:.4f}</td>
            <td>{row['avg_episodes_per_second']:.2f}</td>
            <td>{row['total_time_seconds']:.1f}s</td>
        </tr>"""
        
        html_content += """
    </table>
    
    <div class="summary">
        <h2>ğŸ¯ çµè«–ã¨æ¨å¥¨äº‹é …</h2>
        <ul>"""
        
        # è‡ªå‹•æ¨å¥¨äº‹é …ç”Ÿæˆ
        best_insight_exp = results_df.loc[results_df['insight_rate'].idxmax()]
        best_speed_exp = results_df.loc[results_df['avg_episodes_per_second'].idxmax()]
        
        html_content += f"""
            <li><strong>æœ€é«˜æ´å¯Ÿç‡:</strong> {best_insight_exp['experiment_name']} ({best_insight_exp['insight_rate']:.4f})</li>
            <li><strong>æœ€é«˜å‡¦ç†é€Ÿåº¦:</strong> {best_speed_exp['experiment_name']} ({best_speed_exp['avg_episodes_per_second']:.2f} eps/sec)</li>"""
        
        if results_df['insight_rate'].std() > 0.01:
            html_content += "<li>æ´å¯Ÿç‡ã«å¤§ããªå¤‰å‹•ãŒã‚ã‚Šã¾ã™ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚</li>"
        
        if results_df['avg_episodes_per_second'].std() > 1.0:
            html_content += "<li>å‡¦ç†é€Ÿåº¦ã«å¤§ããªå¤‰å‹•ãŒã‚ã‚Šã¾ã™ã€‚å®Ÿè¡Œç’°å¢ƒã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚</li>"
        
        html_content += """
        </ul>
    </div>
</body>
</html>"""
        
        html_path = report_dir / "report.html"
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return html_path


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="é«˜åº¦ãªå®Ÿé¨“ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ")
    subparsers = parser.add_subparsers(dest="command", help="åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰")
    
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä¸€è¦§
    subparsers.add_parser("list-templates", help="å®Ÿé¨“ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä¸€è¦§")
    
    # å®Ÿé¨“è¨­å®šä½œæˆ
    config_parser = subparsers.add_parser("create-config", help="å®Ÿé¨“è¨­å®šä½œæˆ")
    config_parser.add_argument("config_name", help="è¨­å®šå")
    config_parser.add_argument("template", help="ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå")
    config_parser.add_argument("--custom", help="ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (JSONå½¢å¼)")
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚¤ãƒ¼ãƒ—å®Ÿè¡Œ
    sweep_parser = subparsers.add_parser("run-sweep", help="ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚¤ãƒ¼ãƒ—å®Ÿè¡Œ")
    sweep_parser.add_argument("session_id", help="ã‚»ãƒƒã‚·ãƒ§ãƒ³ID")
    sweep_parser.add_argument("base_name", help="å®Ÿé¨“ãƒ™ãƒ¼ã‚¹å")
    sweep_parser.add_argument("config_name", help="ãƒ™ãƒ¼ã‚¹è¨­å®šå")
    sweep_parser.add_argument("--sweep-params", help="ã‚¹ã‚¤ãƒ¼ãƒ—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (JSONå½¢å¼)")
    
    # æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report_parser = subparsers.add_parser("generate-report", help="æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
    report_parser.add_argument("session_id", help="ã‚»ãƒƒã‚·ãƒ§ãƒ³ID")
    report_parser.add_argument("experiments", nargs="+", help="å®Ÿé¨“åãƒªã‚¹ãƒˆ")
    report_parser.add_argument("--report-name", default="comparison_report", help="ãƒ¬ãƒãƒ¼ãƒˆå")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    manager = AdvancedExperimentManager()
    
    try:
        if args.command == "list-templates":
            print("ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªå®Ÿé¨“ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:")
            for name, config in manager.experiment_templates.items():
                print(f"   ğŸ”¸ {name}:")
                for key, value in config.items():
                    print(f"      {key}: {value}")
                print()
        
        elif args.command == "create-config":
            custom_params = {}
            if args.custom:
                custom_params = json.loads(args.custom)
            
            config = manager.create_experiment_config(args.template, custom_params)
            manager.save_experiment_config(args.config_name, config)
        
        elif args.command == "run-sweep":
            base_config = manager.load_experiment_config(args.config_name)
            
            if args.sweep_params:
                sweep_params = json.loads(args.sweep_params)
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚¤ãƒ¼ãƒ—
                sweep_params = {
                    "ged_threshold": [0.10, 0.15, 0.20],
                    "ig_threshold": [0.05, 0.10, 0.15]
                }
            
            configs = manager.generate_parameter_sweep_configs(base_config, sweep_params)
            results = manager.run_parameter_sweep(args.session_id, args.base_name, configs)
            
            print(f"\nğŸ“Š ã‚¹ã‚¤ãƒ¼ãƒ—çµæœ:")
            success_count = sum(1 for r in results if r['status'] == 'success')
            print(f"   æˆåŠŸ: {success_count}/{len(results)}")
        
        elif args.command == "generate-report":
            report_dir = manager.generate_comparison_report(
                args.session_id, args.experiments, args.report_name
            )
            
            if report_dir:
                html_report = report_dir / "report.html"
                print(f"\nğŸŒ HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’é–‹ãã«ã¯:")
                print(f"   open {html_report}")
    
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
