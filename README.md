# InsightSpike-AI
**Brain-Inspired Multi-Agent Architecture for â€œSpike of Insightâ€ (Î”GED Ã— Î”IG)**  

> Quantized RAG ï¼‹ GNN ï¼‹ Internal Reward (Î”GED/Î”IG)  
> Implementing a cerebellumâ€“LCâ€“hippocampusâ€“VTA loop to study *insight*.

[![License: InsightSpike Community License](https://img.shields.io/badge/License-InsightSpike--Community--1.0-blue)](https://github.com/miyauchikazuyoshi/InsightSpike-AI/blob/main/LICENSE)  
<a href="https://arxiv.org/abs/YYMM.NNNNN"><img src="https://img.shields.io/badge/arXiv-YYMM.NNNNN-b31b1b.svg" alt="arXiv"></a>  
<a href="https://github.com/miyauchikazuyoshi/InsightSpike-AI/releases"><img src="https://img.shields.io/github/v/release/miyauchikazuyoshi/InsightSpike-AI"></a>

## Patent Notice
The core Î”GED/Î”IG intrinsic-reward mechanism and the hierarchical VQ memory module
are **patent-pending** in Japan.

- JP Application No. **ç‰¹é¡˜2025-082988** â€” â€œÎ”GED/Î”IG å†…ç™ºå ±é…¬ç”Ÿæˆæ–¹æ³•â€
- JP Application No. **ç‰¹é¡˜2025-082989** â€” â€œéšå±¤ãƒ™ã‚¯ãƒˆãƒ«é‡å­åŒ–ã«ã‚ˆã‚‹å‹•çš„ãƒ¡ãƒ¢ãƒªæ–¹æ³•â€

<br> Further filings (US/PCT) will follow within the priority year.

---

### âœ¨ Features
* **Î”GED** â€“ Graph-edit distance between successive RAG search graphs  
* **Î”IG** â€“ Entropy gain from*

## âœ¨ Why
Human â€œaha!â€ moments often arise from abrupt structural re-arrangements of episodic memory.  
InsightSpike-AI models this process and exposes the *spike* as an internal reward signal.

## ğŸ§  Architecture (Enhanced v0.7-Eureka)

**Layer1: Enhanced Known/Unknown Information Separation**  
- ğŸ“‹ Intelligent query analysis and concept extraction
- ğŸ¯ Automatic synthesis requirement detection  
- ğŸ”„ Adaptive topK optimization for chain reaction insights
- ğŸ§  Human-like learning system with weak relationship formation

**Layer2: Quantum-RAG + C-value Memory (Faiss)**  
- ğŸ“š Vector quantized episodic memory with IVF-PQ
- ğŸ” Adaptive retrieval with Layer1-optimized topK values

**Layer3: GNN + Î”GED/Î”IG + Conflict Score**  
- ğŸ•¸ï¸ Graph neural network reasoning with PyTorch Geometric
- ğŸ“Š Î”GED/Î”IG metrics for insight spike detection
- âš¡ Enhanced graph density for chain reaction insights

**Layer4: LLM Interface**  
- ğŸ—£ï¸ Natural language generation with TinyLlama
- ğŸ¨ Context-aware response synthesis

<!-- <p align="center"><img src="docs/diagram/overview_v0.png" width="70%"></p> -->

# InsightSpike AI (v0.7-Eureka)

Proofâ€‘ofâ€‘concept brainâ€‘inspired architecture with a 4â€‘layer subcortical loop.

| Layer | Brain analog  | Main file(s)                  |
|-------|---------------|-------------------------------|
| L1    | Cerebellum    | layer1_error_monitor.py       |
| L2    | LC + Hippocampus | layer2_memory_manager.py  |
| L3    | PFC           | layer3_graph_pyg.py,<br>layer3_reasoner_gnn.py |
| L4    | Language area | layer4_llm.py                 |

EurekaSpike fires when **Î”GED drops â‰¥ 0.5** *and* **Î”IG rises â‰¥ 0.2**.

---

## Quickâ€‘start (local)
```bash
git clone ...
cd InsightSpike-AI
chmod +x [setup.sh]
[setup.sh]
```

## ğŸš€ Quick Demo

Try the insight detection capabilities immediately:

```bash
# Clone and setup
git clone https://github.com/miyauchikazuyoshi/InsightSpike-AI.git
cd InsightSpike-AI

# Run interactive demo (no setup required)
poetry run insightspike demo
```

This demo showcases InsightSpike's ability to synthesize insights across domains like probability theory, mathematics, and philosophy - even when the knowledge base contains no direct answers to the questions!

## âš¡ Quick Start on Google Colab (GPU)

### ğŸ¯ One-Click Setup (Recommended)
The easiest way to get started is with our simplified Colab notebook:

1. **Open the notebook**: [`InsightSpike_Colab_Demo.ipynb`](InsightSpike_Colab_Demo.ipynb) in Google Colab
2. **Choose GPU runtime**: Runtime > Change runtime type > GPU  
3. **Run cells in order**: The notebook guides you through setup and demo

### ğŸš€ Validated Setup Scripts

#### âš¡ Standard Setup (8-12 minutes, recommended)
```bash
!git clone https://github.com/miyauchikazuyoshi/InsightSpike-AI.git
%cd InsightSpike-AI
!chmod +x scripts/colab/setup_colab.sh
!./scripts/colab/setup_colab.sh
```

#### ğŸ”¥ Minimal Setup (<60 seconds, for testing)
```bash
!git clone https://github.com/miyauchikazuyoshi/InsightSpike-AI.git
%cd InsightSpike-AI
!chmod +x scripts/colab/setup_colab.sh
!./scripts/colab/setup_colab.sh minimal
```

#### ğŸ“‹ Production Setup (10-15 minutes, complete)
```bash
!git clone https://github.com/miyauchikazuyoshi/InsightSpike-AI.git
%cd InsightSpike-AI
!chmod +x scripts/colab/setup_colab.sh
!./scripts/colab/setup_colab.sh
```

#### ğŸ” Debug Setup (15-20 minutes, troubleshooting)
```bash
!git clone https://github.com/miyauchikazuyoshi/InsightSpike-AI.git
%cd InsightSpike-AI
!chmod +x scripts/colab/setup_colab_debug.sh
!./scripts/colab/setup_colab_debug.sh
```

> **âœ… All setup scripts are fully tested and validated**  
> See [`scripts/colab/VALIDATION_SUMMARY.md`](scripts/colab/VALIDATION_SUMMARY.md) for detailed validation results.

#### ğŸš€ Ultra-Fast Setup (<60 seconds)
```bash
# Essential dependencies only, good for testing
scripts/colab/setup_colab.sh minimal
```

#### ğŸ“‹ Standard Setup (8-12 minutes)
```bash
# Complete installation with all features
scripts/colab/setup_colab.sh standard
```

#### ğŸ” Debug Setup (15-20 minutes)
```bash
# Detailed logging for troubleshooting
scripts/colab/setup_colab.sh debug
```

### ğŸ“” Interactive Notebook
Our simplified [`InsightSpike_Colab_Demo.ipynb`](InsightSpike_Colab_Demo.ipynb) provides:
- **One-click setup** with 4 options (fast/minimal/standard/debug)
- **Guided demo** with preset questions
- **Built-in troubleshooting** guide
- **Clean interface** - no complex menus or redundant options

ğŸ’¡ **Simplified**: We've streamlined the Colab experience! The new notebook uses our comprehensive setup script, eliminating redundant setup options and complex menus for a much cleaner user experience.

### ğŸ”§ Troubleshooting
If you encounter issues, see our [Colab Troubleshooting Guide](documentation/guides/COLAB_TROUBLESHOOTING_GUIDE.md).

### Development & PoC/Experiment Environment Setup
---
## ğŸš€ Quick Start

### ğŸ”§ Three-Environment Installation Strategy

InsightSpike-AI supports three distinct environments, each optimized for specific use cases:

#### ğŸ  Local Development Environment (faiss-cpu)
**Best for**: Development, testing, CPU-only machines

**âœ… DEPENDENCY CONFLICTS RESOLVED**: NumPy 1.x compatibility across all packages
```bash
# Clone repository
git clone https://github.com/miyauchikazuyoshi/InsightSpike-AI.git
cd InsightSpike-AI

# Automated setup with dependency resolution
./scripts/setup/setup.sh

# Alternative: Manual Poetry setup
poetry install --with dev

# Verify installation
poetry run insightspike --help
poetry run config-info
```

**Key Benefits**:
- âœ… NumPy 1.26.4 + FAISS 1.11.0 + spaCy 3.7.5 compatibility 
- âœ… Poetry-managed dependencies with resolved lock file
- âœ… Full development environment with testing tools

#### â˜ï¸ Google Colab Environment (faiss-gpu-cu12)
**Best for**: GPU acceleration, large-scale experiments, research

**âœ… DEPENDENCY CONFLICTS RESOLVED**: NumPy 1.x + FAISS GPU coordination
**Strategic Dependency Coordination**: Our Colab setup implements sophisticated coordination between pip and Poetry to avoid GPU package conflicts:

```bash
# Method 1: Enhanced automated setup (recommended)
!git clone https://github.com/miyauchikazuyoshi/InsightSpike-AI.git
%cd InsightSpike-AI
!chmod +x scripts/colab/setup_colab.sh
!bash scripts/colab/setup_colab.sh
```

**Setup Options** (all use NumPy 1.x compatibility):
- `setup_colab.sh` - Standard coordinated setup with dependency resolution (8-12 min)
- `setup_colab.sh minimal` - Ultra-fast essential only (<60 sec)
- `setup_colab.sh debug` - Detailed logging for troubleshooting (15-20 min)
- `setup_colab_debug.sh` - Alternative debug script with comprehensive diagnostics

**Coordination Strategy**:
1. **NumPy 1.26.4** installed first for FAISS + thinc compatibility
2. **faiss-gpu-cu12==1.11.0** installed via pip with CUDA support
3. **spaCy 3.7.5 + thinc 8.2.5** for NumPy 1.x ecosystem
4. **Poetry coordination** for remaining dependencies using `requirements-colab.txt`
5. **Conflict prevention**: Strategic package ordering and version constraints

```bash
# Method 2: Use pre-configured notebook (simplified)
# Open: InsightSpike_Colab_Demo.ipynb
```

**Verification Commands**:
```bash
# Test resolved dependencies
!python -c "import numpy, faiss, spacy; print(f'NumPy: {numpy.__version__}, FAISS: {faiss.__version__}, spaCy: {spacy.__version__}')"
!poetry run insightspike --help
```

#### ğŸ”§ CI/Testing Environment (minimal dependencies)
**Best for**: Continuous integration, automated testing

**âœ… DEPENDENCY CONFLICTS RESOLVED**: LITE_MODE with NumPy 1.x compatibility
```bash
# Poetry-based CI setup (automated via .github/workflows/ci.yml)
./scripts/setup/setup.sh
export INSIGHTSPIKE_LITE_MODE=1

# Alternative: Minimal installation for CI
pip install pytest numpy==1.26.4 pyyaml networkx scikit-learn psutil faiss-cpu typer rich click
pip install -e .

# Run tests with environment detection
python -m pytest development/tests/unit/ -v
```

**Key Benefits**:
- âœ… NumPy 1.x compatibility maintained in CI
- âœ… Mock models for fast testing (no model downloads)
- âœ… Unified setup script across all environments

### ğŸ“¦ Strategic Dependency Management

**âœ… DEPENDENCY RESOLUTION WORK COMPLETE**: All cross-environment compatibility achieved

**Final Validation Results**: 
- âœ… System Validation: **6/6 tests PASSED**
- âœ… Cross-Environment Testing: **5/5 tests PASSED** 
- âœ… Safe Mode Testing: **5/5 tests PASSED**
- âœ… FAISS Clustering Issue: **RESOLVED**
- âœ… Production Deployment: **READY**

Our multi-environment approach ensures optimal performance across different deployment contexts:

- **`dev`**: Local development with Poetry + NumPy 1.26.4 + FAISS 1.11.0
- **`colab`**: Google Colab with pip coordination + NumPy 1.26.4 + FAISS GPU 1.11.0  
- **`ci`**: CI testing with Poetry + NumPy 1.26.4 + LITE_MODE

**Key Innovation**: 
- **Unified NumPy 1.x**: `numpy>=1.24.0,<2.0.0` across all environments
- **FAISS Compatibility**: `faiss-cpu/gpu==1.11.0` (latest NumPy 1.x compatible)
- **spaCy Ecosystem**: `spacy==3.7.5` + `thinc==8.2.5` (NumPy 1.x compatible)
- **Poetry Lock Resolution**: All conflicts resolved in `poetry.lock`
- **Environment Scripts**: Automated setup with version validation
- **Safe Mode System**: Production-ready fallback for stability (`INSIGHTSPIKE_SAFE_MODE=1`)

**Resolution Status**: âœ… COMPLETE
```bash
# Validation Results (5/5 Passed)
âœ… Local Environment - Configuration & Core functionality
âœ… Google Colab Simulation - Environment detection & GPU compatibility  
âœ… Dependency Compatibility - NumPy 1.26.4 + PyTorch 2.2.2 + FAISS 1.11.0
âœ… Safe Mode Robustness - Fallback systems & production stability
âœ… End-to-End Workflow - Document processing & similarity search
```

**Resolved Conflicts**:
```bash
# BEFORE: Dependency hell
âŒ FAISS requires numpy<2.0
âŒ thinc 8.3.6 requires numpy>=2.0
âŒ Poetry lock conflicts

# AFTER: Unified ecosystem  
âœ… numpy==1.26.4 (all environments)
âœ… faiss-gpu-cu12==1.11.0 (Colab) / faiss-cpu==1.11.0 (local/CI)
âœ… spacy==3.7.5 + thinc==8.2.5
âœ… poetry.lock successfully resolved
```

**Requirements File Structure**:
```
deployment/configs/
â”œâ”€â”€ requirements-colab.txt              # Poetry-managed (NumPy 1.x coordinated)
â”œâ”€â”€ requirements-colab-comprehensive.txt # Complete reference documentation
â”œâ”€â”€ requirements-torch.txt              # PyTorch with CUDA support
â””â”€â”€ requirements-PyG.txt                # PyTorch Geometric components
```

**Development Setup**:
```bash
# Enhanced development environment with resolved dependencies
poetry install --with dev

# Verify installation 
poetry run insightspike --help
poetry run config-info

# Check resolved dependencies
poetry run python -c "import numpy, faiss, spacy; print(f'NumPy: {numpy.__version__}, FAISS: {faiss.__version__}, spaCy: {spacy.__version__}')"
```

For development, PoC, or experiments, the full development environment includes:
- âœ… matplotlib (for visualization) 
- âœ… pytest (for testing)
- âœ… All resolved dependencies with NumPy 1.x ecosystem

When running `run_poc.py` offline, set the environment variable `EMBED_MODEL_PATH` to a locally downloaded SentenceTransformer model directory.

### Docker
---
The included `Dockerfile` is based on `pytorch/pytorch:2.2.0-cuda12.1-cudnn8-runtime`. It installs dependencies from `pyproject.toml` using Poetry, and then installs additional packages such as `torch-geometric` via `pip`. Note that local scripts use `torch==2.2.2`, so be aware of version differences. The base image uses **Python 3.10**, which differs from the Python 3.11 series required in `pyproject.toml`. Also, after installing `faiss-cpu` with Poetry, `faiss-gpu-cu11` is added; if you do not need the CPU version, please uninstall it.

**Note**: Docker configuration needs updating for NumPy 1.x compatibility. Use local or Colab environments for immediate usage.

## âœ… Minimal Working Example (Dependency Conflicts Resolved)

```bash
# 1. Clone and set up environment with resolved dependencies
git clone https://github.com/miyauchikazuyoshi/InsightSpike-AI.git
cd InsightSpike-AI

# 2. Automated setup with dependency resolution
./scripts/setup/setup.sh

# 3. Verify resolved installation
poetry run insightspike --help
poetry run config-info

# 4. Check dependency compatibility
poetry run python -c "import numpy, faiss, spacy; print(f'âœ… NumPy: {numpy.__version__}, FAISS: {faiss.__version__}, spaCy: {spacy.__version__}')"

# 5. Prepare data (download & vectorize Wikipedia sentences)
poetry run python scripts/databake.py

# 6. Embed your own corpus (Specify any text file)
# Example: Convert data/raw/your_corpus.txt into episodic memory
# *Note: Each line in the text file is treated as a separate document.*
poetry run insightspike embed --path data/raw/your_corpus.txt

# 4. Build similarity graph
poetry run insightspike graph

# 5. Run a reasoning loop with a sample question
poetry run insightspike loop "What is quantum entanglement?"

# 6. Run the PoC pipeline (with visualization)
poetry run python scripts/run_poc.py
```

---

## CLI Commands

| Command                                      | Description                                                                                 |
|-----------------------------------------------|--------------------------------------------------------------------------------------------|
| `poetry run insightspike ask "question"`      | Ask a question using the new MainAgent architecture                                        |
| `poetry run insightspike demo`                | **Run interactive demo of insight detection capabilities**                             |
| `poetry run insightspike load_documents <path>` | Load documents into the agent's memory from file or directory                            |
| `poetry run insightspike stats`               | Show agent and memory statistics                                                           |
| `poetry run insightspike config_info`         | Display current configuration settings                                                      |
| `poetry run insightspike true_insight`        | **Run rigorous insight detection experiment (no direct answers)**                     |
| `poetry run insightspike compare_experiments` | **Compare different experimental designs (direct vs insight)**                        |
| `poetry run insightspike experiment_suite`    | **Run complete experimental validation suite with multiple experiment types**              |
| `poetry run insightspike experiment`          | **Run complete experimental validation suite** *(Legacy - use experiment_suite)*           |
| `poetry run insightspike benchmark [--dataset]` | **Run performance benchmarks (simple/enhanced/custom)**                                  |
| `poetry run insightspike embed --path <file>` | *(Legacy)* Embed a text corpus and save episodic memory                                   |
| `poetry run insightspike query "question"`    | *(Legacy)* Run one L1-L4 reasoning cycle                                                   |
| `poetry run databake`                         | Download 10,000 Wikipedia sentences, vectorize, and index with faiss                      |
| `poetry run run-poc`                          | Run the full PoC pipeline with visualization and logging                                   |

---

## Common Errors & Troubleshooting

| Error Message                                      | Cause / Solution                                                                                 |
|----------------------------------------------------|--------------------------------------------------------------------------------------------------|
| `ModuleNotFoundError: No module named 'matplotlib'`| Run `poetry install` to include dev dependencies, or add `matplotlib` to your environment.       |
| `FileNotFoundError: ... episodic memory ...`       | Run `poetry run insightspike embed` or `poetry run databake` to generate the required data files.|
| `torch version mismatch`                           | Ensure Docker and local environments use the same torch version (see Dockerfile notes).          |
| `CUDA not available`                               | If running on CPU, make sure to use CPU versions of torch/faiss; for GPU, check CUDA drivers.    |
| `RuntimeError: Failed to load embedding model`     | Pre-download the SentenceTransformer model and set `EMBED_MODEL_PATH` to its directory.           |

---

## Data Preparation & Preprocessing

To obtain 10,000 sentences from Wikipedia, save them in `data/raw/`, vectorize them using sentence-transformers, and index them with faiss:

```bash
poetry run databake
```

## PoC (Proof of Concept) Usage

To run the PoC pipeline after data preparation, follow these steps:

1. **Start the main process**  
    Run the main script to launch the multi-agent architecture:
    ```bash
    poetry run run-poc
    ```

2. **Monitor outputs**  
    Results, logs, and EurekaSpike events will be saved in the `outputs/` directory.

3. **Experiment with parameters**  
    You can adjust parameters such as thresholds for Î”GED and Î”IG in the configuration files (e.g., `config.yaml`) to observe different behaviors.

For detailed experiments or custom runs, refer to the scripts in the `experiments/` directory.

---

## ğŸ§ª Experimental Validation

InsightSpike-AI has been rigorously tested through controlled experiments demonstrating its effectiveness in detecting cognitive insights and improving response quality through two distinct experimental frameworks.

### Insight Detection Experiment Results (Rigorous Validation)

**ğŸ¯ Breakthrough: 108.3% improvement in synthesis tasks requiring genuine cross-domain reasoning**

- **âœ… 83.3% response quality** vs 40.0% baseline (108.3% improvement)
- **âœ… 66.7% synthesis rate** vs 0% baseline (successful cross-domain connections)  
- **âœ… 4/6 successful insight syntheses** on questions with NO direct answers in knowledge base
- **âœ… Insight detection** validates genuine reasoning rather than mere information retrieval

### Traditional Experiment Results (Legacy Framework)

- **âœ… 133.3% improvement** in response quality over baseline systems
- **âœ… 100% insight detection rate** on cognitive paradoxes (Monty Hall, Zeno's, Ship of Theseus)
- **âœ… 0% false positive rate** on control questions
- **âœ… 287x faster processing** than baseline approaches

### Running New Experimental Framework

```bash
# Run rigorous insight experiment (no direct answers in knowledge base)
poetry run insightspike true_insight

# Compare experimental designs (traditional vs insight)
poetry run insightspike compare_experiments

# Run complete validation suite with multiple experiment types
poetry run insightspike experiment_suite

# Legacy experiments
poetry run python scripts/databake_simple.py
poetry run python scripts/run_poc_simple.py
```

### Experimental Framework Design

InsightSpike-AI uses two complementary experimental approaches to validate its insight detection capabilities:

#### ğŸ¯ Insight Experiments (Rigorous Validation)
**Revolutionary experimental design with NO direct answers in knowledge base**

- **Indirect Knowledge Base**: 57 facts containing only related concepts, NOT direct answers
- **Synthesis-Required Questions**: 6 questions requiring genuine cross-domain reasoning
- **Examples**: Monty Hall (probability + information theory), Zeno's paradox (calculus + motion), Ship of Theseus (philosophy + practical criteria)
- **Validation**: Tests true synthesis capability rather than information retrieval

#### ğŸ“Š Traditional Experiments (Legacy Framework)
**Standard evaluation on cognitive paradoxes with complete knowledge base**

- **Direct Knowledge Base**: Contains answers alongside related information
- **Cognitive Paradoxes**: Monty Hall problem variations, mathematical paradoxes, philosophical questions
- **Validation**: Tests insight detection on known challenging problems

#### ğŸ”¬ Comparative Analysis
The experimental suite includes comparative analysis showing:
- **Insight experiments** eliminate confounding factors and validate genuine synthesis
- **Traditional experiments** demonstrate performance on standard cognitive challenges
- **Cross-validation** ensures robust insight detection across multiple domains

### Experimental Framework

The validation framework tests InsightSpike-AI across multiple cognitive domains:

- **Probability Paradoxes**: Monty Hall problem variations
- **Mathematical Paradoxes**: Zeno's paradox resolution  
- **Philosophical Paradoxes**: Ship of Theseus identity questions
- **Concept Hierarchies**: Mathematical abstraction levels
- **Conceptual Revolutions**: Physics paradigm shifts
- **Control Conditions**: Standard academic content

Results demonstrate that the Î”GED/Î”IG mechanism effectively identifies breakthrough moments in cognitive processing, validating the core hypothesis of spike-based insight detection.

ğŸ“„ **Full Reports**: 
- [`EXPERIMENTAL_VALIDATION_REPORT.md`](EXPERIMENTAL_VALIDATION_REPORT.md) - Traditional framework results
- [`COMPARATIVE_EXPERIMENTAL_ANALYSIS.md`](COMPARATIVE_EXPERIMENTAL_ANALYSIS.md) - Insight vs traditional comparison

---

## ğŸ¯ Layer1 Enhanced Features (v0.7-Eureka)

### Known/Unknown Information Separation
- **Concept Extraction**: Automatic identification of key concepts using regex and NLP
- **Certainty Analysis**: Context-based confidence scoring for concept familiarity  
- **Synthesis Detection**: Intelligent detection of queries requiring multi-concept synthesis

### Adaptive TopK Optimization  
- **Dynamic Scaling**: topK values scale 1.5x-6x based on query complexity
- **Chain Reaction Enablement**: Higher graph density for "é€£é–åå¿œçš„æ´å¯Ÿå‘ä¸Š"
- **Layer-Specific Tuning**: L1(20â†’50), L2(15â†’30), L3(12â†’25) adaptive scaling

### Human-Like Learning System
- **Weak Relationships**: Automatic registration of concept co-occurrences (confidence: 0.1)
- **Sleep-Mode Cleanup**: Background pruning of relationships below 0.15 confidence
- **Gradual Reinforcement**: +0.05 confidence boost per concept reappearance
- **Graph Explosion Prevention**: Maximum 1000 weak edges with natural pruning

### Integration Results
- âœ… **75% synthesis prediction accuracy** for Layer1 analysis
- âœ… **2.5x average topK scaling** for complex queries  
- âœ… **329 relationships learned** from 5 test questions
- âœ… **61.6% â†’ 84.1% chain reaction potential** scaling

---

## Makefile ã‚³ãƒãƒ³ãƒ‰ä¾‹
- `make test` ... ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- `make embed` ... ãƒ†ã‚¹ãƒˆç”¨Memoryç”Ÿæˆ
- `make clean` ... ãƒ†ã‚¹ãƒˆæˆæœç‰©ã®å‰Šé™¤

## .envãƒ•ã‚¡ã‚¤ãƒ«ã«ã¤ã„ã¦
- `PYTHONPATH=src` ãªã©ã€ç’°å¢ƒå¤‰æ•°ã‚’ä¸€å…ƒç®¡ç†ã§ãã¾ã™ã€‚
- å¿…è¦ã«å¿œã˜ã¦ `DATA_DIR` ã‚„ `API_KEY` ãªã©ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚
