# InsightSpike-AI
**Brain-Inspired Multi-Agent Architecture for â€œSpike of Insightâ€ (Î”GED Ã— Î”IG)**  

> Quantized RAG ï¼‹ GNN ï¼‹ Internal Reward (Î”GED/Î”IG)  
> Implementing a cerebellumâ€“LCâ€“hippocampusâ€“VTA loop to study *insight*.

[![License: InsightSpike Community License](https://img.shields.io/badge/License-InsightSpike--Community--1.0-blue)](https://github.com/miyauchikazuyoshi/InsightSpike-AI/blob/main/LICENSE)  
<a href="https://arxiv.org/abs/YYMM.NNNNN"><img src="https://img.shields.io/badge/arXiv-YYMM.NNNNN-b31b1b.svg" alt="arXiv"></a>  
<a href="https://github.com/miyauchikazuyoshi/InsightSpike-AI/releases"><img src="https://img.shields.io/github/v/release/miyauchikazuyoshi/InsightSpike-AI"></a>

## Patent Notice
The core Î”GED/Î”IG intrinsic-reward mechanism and the hierarchical VQ memory module
are **patent-pending** in Japan.

- JP Application No. **ç‰¹é¡˜2025-082988** â€” â€œÎ”GED/Î”IG å†…ç™ºå ±é…¬ç”Ÿæˆæ–¹æ³•â€
- JP Application No. **ç‰¹é¡˜2025-082989** â€” â€œéšå±¤ãƒ™ã‚¯ãƒˆãƒ«é‡å­åŒ–ã«ã‚ˆã‚‹å‹•çš„ãƒ¡ãƒ¢ãƒªæ–¹æ³•â€

<br> Further filings (US/PCT) will follow within the priority year.

---

### âœ¨ Features
* **Î”GED** â€“ Graph-edit distance between successive RAG search graphs  
* **Î”IG** â€“ Entropy gain from*

## âœ¨ Why
Human â€œaha!â€ moments often arise from abrupt structural re-arrangements of episodic memory.  
InsightSpike-AI models this process and exposes the *spike* as an internal reward signal.

## ğŸ§  Architecture (MVP)
L1 Error Monitor (Ï„_err)<br>L2 Quantum-RAG + C-value (Faiss)<br>L3 GNN + Î”GED/Î”IG + Conflict Score<br>L4 LLM interface<br>

<!-- <p align="center"><img src="docs/diagram/overview_v0.png" width="70%"></p> -->

# InsightSpike AI (v0.7-Eureka)

Proofâ€‘ofâ€‘concept brainâ€‘inspired architecture with a 4â€‘layer subcortical loop.

| Layer | Brain analog  | Main file(s)                  |
|-------|---------------|-------------------------------|
| L1    | Cerebellum    | layer1_error_monitor.py       |
| L2    | LC + Hippocampus | layer2_memory_manager.py  |
| L3    | PFC           | layer3_graph_pyg.py,<br>layer3_reasoner_gnn.py |
| L4    | Language area | layer4_llm.py                 |

EurekaSpike fires when **Î”GED drops â‰¥ 0.5** *and* **Î”IG rises â‰¥ 0.2**.

---

## Quickâ€‘start (local)
```bash
git clone ...
cd InsightSpike-AI
chmod +x [setup.sh]
[setup.sh]
```

## Quickâ€‘start on GoogleColab(GPU)
```bash
git clone https://github.com/miyauchikazuyoshi/InsightSpike-AI.git
cd InsightSpike-AI
chmod +x scripts/setup.sh
./scripts/setup.sh
```

### Development & PoC/Experiment Environment Setup
---
For development, PoC, or experiments, please make sure to install all dependencies including dev packages:
```poetry install```<br>
Or, if you want to explicitly include only dev dependencies:```poetry install --with dev```

This will ensure that packages like matplotlib (for visualization) and pytest (for testing) are available in your environment.

### Docker
---
The included `Dockerfile` is based on `pytorch/pytorch:2.2.0-cuda12.1-cudnn8-runtime`. It installs dependencies from `pyproject.toml` using Poetry, and then installs additional packages such as `torch-geometric` via `pip`. Note that local scripts use `torch==2.2.2`, so be aware of version differences. The base image uses **Python 3.10**, which differs from the Python 3.11 series required in `pyproject.toml`. Also, after installing `faiss-cpu` with Poetry, `faiss-gpu-cu11` is added; if you do not need the CPU version, please uninstall it.

## Minimal Working Example

```bash
# 1. Clone and set up environment
git clone https://github.com/miyauchikazuyoshi/InsightSpike-AI.git
cd InsightSpike-AI
chmod +x scripts/setup.sh
./scripts/setup.sh

# 2. Prepare data (download & vectorize Wikipedia sentences)
poetry run databake

# 3. Embed your own corpus (ä»»æ„ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®š)
# ä¾‹: data/raw/your_corpus.txt ã‚’MemoryåŒ–
poetry run insightspike embed --path data/raw/your_corpus.txt

# 4. Build similarity graph
poetry run insightspike graph

# 5. Run a reasoning loop with a sample question
poetry run insightspike loop "What is quantum entanglement?"

# 6. Run the PoC pipeline (with visualization)
poetry run run-poc
```

---

## CLI Commands

| Command                                      | Description                                                                                 |
|-----------------------------------------------|--------------------------------------------------------------------------------------------|
| `poetry run insightspike embed --path <file>` | Embed a text corpus and save episodic memory (vectorization)                               |
| `poetry run insightspike graph`               | Build a similarity graph from episodic memory                                              |
| `poetry run insightspike loop "question"`     | Run one L1-L4 reasoning cycle with the given question                                      |
| `poetry run databake`                         | Download 10,000 Wikipedia sentences, vectorize, and index with faiss                      |
| `poetry run run-poc`                          | Run the full PoC pipeline with visualization and logging                                   |

---

## Common Errors & Troubleshooting

| Error Message                                      | Cause / Solution                                                                                 |
|----------------------------------------------------|--------------------------------------------------------------------------------------------------|
| `ModuleNotFoundError: No module named 'matplotlib'`| Run `poetry install` to include dev dependencies, or add `matplotlib` to your environment.       |
| `FileNotFoundError: ... episodic memory ...`       | Run `poetry run insightspike embed` or `poetry run databake` to generate the required data files.|
| `torch version mismatch`                           | Ensure Docker and local environments use the same torch version (see Dockerfile notes).          |
| `CUDA not available`                               | If running on CPU, make sure to use CPU versions of torch/faiss; for GPU, check CUDA drivers.    |

---

## Data Preparation & Preprocessing

To obtain 10,000 sentences from Wikipedia, save them in `data/raw/`, vectorize them using sentence-transformers, and index them with faiss:

```bash
poetry run databake
```

## PoC (Proof of Concept) Usage

To run the PoC pipeline after data preparation, follow these steps:

1. **Start the main process**  
    Run the main script to launch the multi-agent architecture:
    ```bash
    poetry run run-poc
    ```

2. **Monitor outputs**  
    Results, logs, and EurekaSpike events will be saved in the `outputs/` directory.

3. **Experiment with parameters**  
    You can adjust parameters such as thresholds for Î”GED and Î”IG in the configuration files (e.g., `config.yaml`) to observe different behaviors.

For detailed experiments or custom runs, refer to the scripts in the `experiments/` directory.

## Makefile ã‚³ãƒãƒ³ãƒ‰ä¾‹
- `make test` ... ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- `make embed` ... ãƒ†ã‚¹ãƒˆç”¨Memoryç”Ÿæˆ
- `make clean` ... ãƒ†ã‚¹ãƒˆæˆæœç‰©ã®å‰Šé™¤

## .envãƒ•ã‚¡ã‚¤ãƒ«ã«ã¤ã„ã¦
- `PYTHONPATH=src` ãªã©ã€ç’°å¢ƒå¤‰æ•°ã‚’ä¸€å…ƒç®¡ç†ã§ãã¾ã™ã€‚
- å¿…è¦ã«å¿œã˜ã¦ `DATA_DIR` ã‚„ `API_KEY` ãªã©ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚
